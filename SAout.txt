#loading drugbank train data
scare : 58361
x0_train[0].shape : <class 'torch.Tensor'>
len of x0_train : 187
y_train[0].shape : tensor([0, 0, 0,  ..., 1, 0, 0])
# loading drugbank test data
x0_test[0].shape : <class 'torch.Tensor'>
len of x0_test : 207
y_test[0].shape : tensor([0, 1, 0,  ..., 1, 1, 1])
# initializing model with:
# attention_size: 64
# input_dim: 128
# rnn_dim: 128
# num_layers: 2
# dropout: 0.1
# allow_insert: False
# training with Adam: lr=0.001, weight_decay=0
# training model
epoch	split	loss	mse	accuracy	r	rho
# [1/100] training 0.2% loss=1.53172, acc=0.51562
# [1/100] training 0.4% loss=0.75443, acc=0.51562
# [1/100] training 0.5% loss=0.70109, acc=0.53125
# [1/100] training 0.8% loss=0.73523, acc=0.54688
# [1/100] training 0.9% loss=0.72484, acc=0.46875
# [1/100] training 1.1% loss=0.69918, acc=0.51562
# [1/100] training 1.2% loss=0.68802, acc=0.50000
# [1/100] training 1.4% loss=0.68470, acc=0.56250
# [1/100] training 1.6% loss=0.62668, acc=0.67188
# [1/100] training 1.8% loss=0.61406, acc=0.57812
# [1/100] training 2.0% loss=0.67140, acc=0.67188
# [1/100] training 2.1% loss=0.66662, acc=0.57812
# [1/100] training 2.3% loss=0.62090, acc=0.67188
# [1/100] training 2.4% loss=0.64971, acc=0.56250
# [1/100] training 2.6% loss=0.77003, acc=0.42188
# [1/100] training 2.7% loss=0.73344, acc=0.46875
# [1/100] training 3.0% loss=0.74313, acc=0.42188
# [1/100] training 3.2% loss=0.71602, acc=0.42188
# [1/100] training 3.3% loss=0.63573, acc=0.67188
# [1/100] training 3.5% loss=0.65105, acc=0.59375
# [1/100] training 3.6% loss=0.65778, acc=0.57812
# [1/100] training 3.8% loss=0.59500, acc=0.71875
# [1/100] training 3.9% loss=0.67031, acc=0.56250
# [1/100] training 4.2% loss=0.58314, acc=0.68750
# [1/100] training 4.4% loss=0.76823, acc=0.57812
# [1/100] training 4.5% loss=0.65847, acc=0.65625
# [1/100] training 4.7% loss=0.66027, acc=0.54688
# [1/100] training 4.8% loss=0.78414, acc=0.48438
# [1/100] training 5.0% loss=0.63928, acc=0.68750
# [1/100] training 5.2% loss=0.65867, acc=0.65625
# [1/100] training 5.4% loss=0.63542, acc=0.59375
# [1/100] training 5.5% loss=0.65125, acc=0.68750
# [1/100] training 5.7% loss=0.59424, acc=0.65625
# [1/100] training 5.9% loss=0.73419, acc=0.54688
# [1/100] training 6.0% loss=0.62649, acc=0.68750
# [1/100] training 6.3% loss=0.62560, acc=0.60938
# [1/100] training 6.4% loss=0.62236, acc=0.70312
# [1/100] training 6.6% loss=0.64701, acc=0.64062
# [1/100] training 6.7% loss=0.63051, acc=0.56250
# [1/100] training 6.9% loss=0.58000, acc=0.67188
# [1/100] training 7.1% loss=0.62495, acc=0.65625
# [1/100] training 7.2% loss=0.58355, acc=0.73438
# [1/100] training 7.5% loss=0.59918, acc=0.59375
# [1/100] training 7.6% loss=0.54678, acc=0.73438
# [1/100] training 7.8% loss=0.58708, acc=0.75000
# [1/100] training 7.9% loss=0.55422, acc=0.73438
# [1/100] training 8.1% loss=0.66169, acc=0.53125
# [1/100] training 8.2% loss=0.55227, acc=0.71875
# [1/100] training 8.4% loss=0.62845, acc=0.65625
# [1/100] training 8.7% loss=0.62720, acc=0.65625
# [1/100] training 8.8% loss=0.60911, acc=0.64062
# [1/100] training 9.0% loss=0.58477, acc=0.59375
# [1/100] training 9.1% loss=0.51090, acc=0.71875
# [1/100] training 9.3% loss=0.62966, acc=0.65625
# [1/100] training 9.4% loss=0.62560, acc=0.68750
# [1/100] training 9.7% loss=0.61393, acc=0.68750
# [1/100] training 9.9% loss=0.57906, acc=0.73438
# [1/100] training 10.0% loss=0.48067, acc=0.81250
# [1/100] training 10.2% loss=0.55538, acc=0.73438
# [1/100] training 10.3% loss=0.52050, acc=0.78125
# [1/100] training 10.5% loss=0.66999, acc=0.62500
# [1/100] training 10.6% loss=0.52124, acc=0.73438
# [1/100] training 10.9% loss=0.66308, acc=0.65625
# [1/100] training 11.0% loss=0.56127, acc=0.68750
# [1/100] training 11.2% loss=0.58368, acc=0.65625
# [1/100] training 11.4% loss=0.57711, acc=0.75000
# [1/100] training 11.5% loss=0.70041, acc=0.60938
# [1/100] training 11.7% loss=0.59037, acc=0.67188
# [1/100] training 11.8% loss=0.55324, acc=0.70312
# [1/100] training 12.1% loss=0.60233, acc=0.70312
# [1/100] training 12.2% loss=0.53924, acc=0.73438
# [1/100] training 12.4% loss=0.53051, acc=0.75000
# [1/100] training 12.6% loss=0.54718, acc=0.68750
# [1/100] training 12.7% loss=0.73094, acc=0.60938
# [1/100] training 12.9% loss=0.58127, acc=0.71875
# [1/100] training 13.0% loss=0.65650, acc=0.59375
# [1/100] training 13.3% loss=0.63831, acc=0.64062
# [1/100] training 13.4% loss=0.68205, acc=0.57812
# [1/100] training 13.6% loss=0.63619, acc=0.71875
# [1/100] training 13.7% loss=0.68776, acc=0.56250
# [1/100] training 13.9% loss=0.62439, acc=0.65625
# [1/100] training 14.1% loss=0.57967, acc=0.67188
# [1/100] training 14.3% loss=0.59519, acc=0.67188
# [1/100] training 14.5% loss=0.48767, acc=0.81250
# [1/100] training 14.6% loss=0.65026, acc=0.59375
# [1/100] training 14.8% loss=0.48904, acc=0.75000
# [1/100] training 14.9% loss=0.81661, acc=0.59375
# [1/100] training 15.1% loss=0.69474, acc=0.64062
# [1/100] training 15.4% loss=0.57698, acc=0.70312
# [1/100] training 15.5% loss=0.61877, acc=0.62500
# [1/100] training 15.7% loss=0.78245, acc=0.42188
# [1/100] training 15.8% loss=0.63527, acc=0.59375
# [1/100] training 16.0% loss=0.65573, acc=0.62500
# [1/100] training 16.1% loss=0.64924, acc=0.65625
# [1/100] training 16.3% loss=0.64652, acc=0.64062
# [1/100] training 16.4% loss=0.65367, acc=0.59375
# [1/100] training 16.7% loss=0.63258, acc=0.62500
# [1/100] training 16.9% loss=0.64037, acc=0.57812
# [1/100] training 17.0% loss=0.64896, acc=0.62500
# [1/100] training 17.2% loss=0.52367, acc=0.73438
# [1/100] training 17.3% loss=0.60284, acc=0.75000
# [1/100] training 17.5% loss=0.58173, acc=0.70312
# [1/100] training 17.7% loss=0.66246, acc=0.59375
# [1/100] training 17.9% loss=0.55970, acc=0.70312
# [1/100] training 18.1% loss=0.61005, acc=0.65625
# [1/100] training 18.2% loss=0.60058, acc=0.64062
# [1/100] training 18.4% loss=0.58263, acc=0.73438
# [1/100] training 18.5% loss=0.60271, acc=0.76562
# [1/100] training 18.8% loss=0.59644, acc=0.68750
# [1/100] training 18.9% loss=0.56880, acc=0.68750
# [1/100] training 19.1% loss=0.66734, acc=0.62500
# [1/100] training 19.2% loss=0.57018, acc=0.71875
# [1/100] training 19.4% loss=0.55752, acc=0.71875
# [1/100] training 19.6% loss=0.61320, acc=0.65625
# [1/100] training 19.7% loss=0.51595, acc=0.81250
# [1/100] training 20.0% loss=0.65984, acc=0.60938
# [1/100] training 20.1% loss=0.56295, acc=0.73438
# [1/100] training 20.3% loss=0.70031, acc=0.59375
# [1/100] training 20.4% loss=0.55872, acc=0.68750
# [1/100] training 20.6% loss=0.57345, acc=0.76562
# [1/100] training 20.8% loss=0.55268, acc=0.73438
# [1/100] training 20.9% loss=0.63242, acc=0.65625
# [1/100] training 21.2% loss=0.59095, acc=0.70312
# [1/100] training 21.3% loss=0.56631, acc=0.70312
# [1/100] training 21.5% loss=0.58082, acc=0.65625
# [1/100] training 21.6% loss=0.50676, acc=0.71875
# [1/100] training 21.8% loss=0.52929, acc=0.78125
# [1/100] training 21.9% loss=0.67259, acc=0.60938
# [1/100] training 22.2% loss=0.52377, acc=0.75000
# [1/100] training 22.4% loss=0.60076, acc=0.73438
# [1/100] training 22.5% loss=0.63062, acc=0.65625
# [1/100] training 22.7% loss=0.60078, acc=0.68750
# [1/100] training 22.8% loss=0.58785, acc=0.68750
# [1/100] training 23.0% loss=0.58143, acc=0.67188
# [1/100] training 23.1% loss=0.63267, acc=0.62500
# [1/100] training 23.4% loss=0.60724, acc=0.73438
# [1/100] training 23.6% loss=0.54815, acc=0.75000
# [1/100] training 23.7% loss=0.68810, acc=0.60938
# [1/100] training 23.9% loss=0.50789, acc=0.75000
# [1/100] training 24.0% loss=0.60624, acc=0.75000
# [1/100] training 24.2% loss=0.56824, acc=0.68750
# [1/100] training 24.3% loss=0.52877, acc=0.76562
# [1/100] training 24.6% loss=0.54899, acc=0.67188
# [1/100] training 24.7% loss=0.56010, acc=0.60938
# [1/100] training 24.9% loss=0.65841, acc=0.65625
# [1/100] training 25.1% loss=0.59872, acc=0.68750
# [1/100] training 25.2% loss=0.56984, acc=0.73438
# [1/100] training 25.4% loss=0.57844, acc=0.70312
# [1/100] training 25.6% loss=0.49825, acc=0.71875
# [1/100] training 25.8% loss=0.66570, acc=0.71875
# [1/100] training 25.9% loss=0.63854, acc=0.65625
# [1/100] training 26.1% loss=0.63901, acc=0.59375
# [1/100] training 26.3% loss=0.55637, acc=0.75000
# [1/100] training 26.4% loss=0.51352, acc=0.73438
# [1/100] training 26.6% loss=0.58591, acc=0.75000
# [1/100] training 26.8% loss=0.51950, acc=0.73438
# [1/100] training 27.0% loss=0.65650, acc=0.62500
# [1/100] training 27.1% loss=0.65156, acc=0.59375
# [1/100] training 27.3% loss=0.68931, acc=0.54688
# [1/100] training 27.4% loss=0.54558, acc=0.70312
# [1/100] training 27.6% loss=0.57383, acc=0.68750
# [1/100] training 27.9% loss=0.57208, acc=0.65625
# [1/100] training 28.0% loss=0.59270, acc=0.70312
# [1/100] training 28.2% loss=0.58822, acc=0.67188
# [1/100] training 28.3% loss=0.45910, acc=0.79688
# [1/100] training 28.5% loss=0.59047, acc=0.68750
# [1/100] training 28.6% loss=0.61661, acc=0.65625
# [1/100] training 28.8% loss=0.50088, acc=0.76562
# [1/100] training 29.1% loss=0.71810, acc=0.53125
# [1/100] training 29.2% loss=0.63087, acc=0.60938
# [1/100] training 29.4% loss=0.62934, acc=0.64062
# [1/100] training 29.5% loss=0.57484, acc=0.64062
# [1/100] training 29.7% loss=0.63502, acc=0.57812
# [1/100] training 29.8% loss=0.55359, acc=0.70312
# [1/100] training 30.0% loss=0.56719, acc=0.65625
# [1/100] training 30.2% loss=0.60618, acc=0.73438
# [1/100] training 30.4% loss=0.48364, acc=0.76562
# [1/100] training 30.6% loss=0.54955, acc=0.75000
# [1/100] training 30.7% loss=0.48188, acc=0.79688
# [1/100] training 30.9% loss=0.59299, acc=0.71875
# [1/100] training 31.0% loss=0.59403, acc=0.65625
# [1/100] training 31.3% loss=0.60457, acc=0.68750
# [1/100] training 31.4% loss=0.56798, acc=0.73438
# [1/100] training 31.6% loss=0.61727, acc=0.62500
# [1/100] training 31.8% loss=0.44095, acc=0.79688
# [1/100] training 31.9% loss=0.60391, acc=0.71875
# [1/100] training 32.1% loss=0.56546, acc=0.71875
# [1/100] training 32.2% loss=0.51409, acc=0.71875
# [1/100] training 32.5% loss=0.52595, acc=0.75000
# [1/100] training 32.6% loss=0.52562, acc=0.70312
# [1/100] training 32.8% loss=0.49473, acc=0.79688
# [1/100] training 32.9% loss=0.69190, acc=0.60938
# [1/100] training 33.1% loss=0.54749, acc=0.71875
# [1/100] training 33.3% loss=0.53294, acc=0.70312
# [1/100] training 33.4% loss=0.64839, acc=0.60938
# [1/100] training 33.7% loss=0.67341, acc=0.68750
# [1/100] training 33.8% loss=0.56774, acc=0.62500
# [1/100] training 34.0% loss=0.60084, acc=0.67188
# [1/100] training 34.1% loss=0.59876, acc=0.67188
# [1/100] training 34.3% loss=0.59180, acc=0.67188
# [1/100] training 34.5% loss=0.67711, acc=0.60938
# [1/100] training 34.7% loss=0.50266, acc=0.73438
# [1/100] training 34.9% loss=0.67940, acc=0.57812
# [1/100] training 35.0% loss=0.60689, acc=0.65625
# [1/100] training 35.2% loss=0.78174, acc=0.59375
# [1/100] training 35.3% loss=0.80215, acc=0.56250
# [1/100] training 35.5% loss=0.74918, acc=0.40625
# [1/100] training 35.6% loss=0.65342, acc=0.39062
# [1/100] training 35.9% loss=0.62598, acc=0.62500
# [1/100] training 36.1% loss=0.47146, acc=0.71875
# [1/100] training 36.2% loss=0.65452, acc=0.60938
# [1/100] training 36.4% loss=0.58883, acc=0.70312
# [1/100] training 36.5% loss=0.51229, acc=0.78125
# [1/100] training 36.7% loss=0.51394, acc=0.71875
# [1/100] training 36.8% loss=0.56546, acc=0.71875
# [1/100] training 37.1% loss=0.47448, acc=0.76562
# [1/100] training 37.3% loss=0.67374, acc=0.60938
# [1/100] training 37.4% loss=0.61750, acc=0.62500
# [1/100] training 37.6% loss=0.65746, acc=0.67188
# [1/100] training 37.7% loss=0.56994, acc=0.68750
# [1/100] training 37.9% loss=0.48601, acc=0.81250
# [1/100] training 38.1% loss=0.57924, acc=0.64062
# [1/100] training 38.3% loss=0.64311, acc=0.70312
# [1/100] training 38.4% loss=0.46457, acc=0.71875
# [1/100] training 38.6% loss=0.64681, acc=0.70312
# [1/100] training 38.8% loss=0.58063, acc=0.73438
# [1/100] training 38.9% loss=0.57404, acc=0.67188
# [1/100] training 39.1% loss=0.56983, acc=0.73438
# [1/100] training 39.3% loss=0.55501, acc=0.73438
# [1/100] training 39.5% loss=0.60512, acc=0.65625
# [1/100] training 39.6% loss=0.63413, acc=0.60938
# [1/100] training 39.8% loss=0.53672, acc=0.67188
# [1/100] training 40.0% loss=0.57162, acc=0.67188
# [1/100] training 40.1% loss=0.53706, acc=0.75000
# [1/100] training 40.4% loss=0.62159, acc=0.67188
# [1/100] training 40.5% loss=0.58153, acc=0.71875
# [1/100] training 40.7% loss=0.47423, acc=0.75000
# [1/100] training 40.8% loss=0.50231, acc=0.76562
# [1/100] training 41.0% loss=0.57999, acc=0.60938
# [1/100] training 41.1% loss=0.56965, acc=0.68750
# [1/100] training 41.3% loss=0.50788, acc=0.81250
# [1/100] training 41.6% loss=0.54824, acc=0.68750
# [1/100] training 41.7% loss=0.73559, acc=0.67188
# [1/100] training 41.9% loss=0.56158, acc=0.68750
# [1/100] training 42.0% loss=0.45169, acc=0.73438
# [1/100] training 42.2% loss=0.52221, acc=0.73438
# [1/100] training 42.3% loss=0.62715, acc=0.73438
# [1/100] training 42.5% loss=0.50015, acc=0.73438
# [1/100] training 42.8% loss=0.57320, acc=0.62500
# [1/100] training 42.9% loss=0.50116, acc=0.73438
# [1/100] training 43.1% loss=0.48906, acc=0.78125
# [1/100] training 43.2% loss=0.49019, acc=0.73438
# [1/100] training 43.4% loss=0.58271, acc=0.71875
# [1/100] training 43.5% loss=0.54849, acc=0.70312
# [1/100] training 43.8% loss=0.56815, acc=0.73438
# [1/100] training 43.9% loss=0.50105, acc=0.73438
# [1/100] training 44.1% loss=0.44104, acc=0.78125
# [1/100] training 44.3% loss=0.61960, acc=0.64062
# [1/100] training 44.4% loss=0.53518, acc=0.68750
# [1/100] training 44.6% loss=0.44716, acc=0.79688
# [1/100] training 44.7% loss=0.55287, acc=0.70312
# [1/100] training 45.0% loss=0.45788, acc=0.81250
# [1/100] training 45.1% loss=0.58158, acc=0.68750
# [1/100] training 45.3% loss=0.51798, acc=0.71875
# [1/100] training 45.5% loss=0.45600, acc=0.76562
# [1/100] training 45.6% loss=0.57633, acc=0.70312
# [1/100] training 45.8% loss=0.45952, acc=0.75000
# [1/100] training 45.9% loss=0.44508, acc=0.78125
# [1/100] training 46.2% loss=0.44238, acc=0.75000
# [1/100] training 46.3% loss=0.58717, acc=0.70312
# [1/100] training 46.5% loss=0.57950, acc=0.65625
# [1/100] training 46.6% loss=0.59213, acc=0.70312
# [1/100] training 46.8% loss=0.52936, acc=0.76562
# [1/100] training 47.0% loss=0.55258, acc=0.70312
# [1/100] training 47.2% loss=0.73240, acc=0.50000
# [1/100] training 47.4% loss=0.55715, acc=0.65625
# [1/100] training 47.5% loss=0.52037, acc=0.75000
# [1/100] training 47.7% loss=0.52301, acc=0.78125
# [1/100] training 47.8% loss=0.50605, acc=0.75000
# [1/100] training 48.0% loss=0.57633, acc=0.75000
# [1/100] training 48.3% loss=0.44146, acc=0.75000
# [1/100] training 48.4% loss=0.56520, acc=0.70312
# [1/100] training 48.6% loss=0.47442, acc=0.76562
# [1/100] training 48.7% loss=0.53938, acc=0.73438
# [1/100] training 48.9% loss=0.59985, acc=0.70312
# [1/100] training 49.0% loss=0.50341, acc=0.82812
# [1/100] training 49.2% loss=0.46091, acc=0.78125
# [1/100] training 49.3% loss=0.54578, acc=0.67188
# [1/100] training 49.6% loss=0.55585, acc=0.70312
# [1/100] training 49.8% loss=0.53925, acc=0.70312
# [1/100] training 49.9% loss=0.61525, acc=0.70312
# [1/100] training 50.1% loss=0.62749, acc=0.71875
# [1/100] training 50.2% loss=0.51562, acc=0.73438
# [1/100] training 50.4% loss=0.51253, acc=0.76562
# [1/100] training 50.6% loss=0.60333, acc=0.68750
# [1/100] training 50.8% loss=0.63427, acc=0.64062
# [1/100] training 51.0% loss=0.59363, acc=0.67188
# [1/100] training 51.1% loss=0.57466, acc=0.67188
# [1/100] training 51.3% loss=0.56552, acc=0.70312
# [1/100] training 51.4% loss=0.55545, acc=0.70312
# [1/100] training 51.7% loss=0.60042, acc=0.65625
# [1/100] training 51.8% loss=0.50360, acc=0.73438
# [1/100] training 52.0% loss=0.57666, acc=0.73438
# [1/100] training 52.1% loss=0.63632, acc=0.71875
# [1/100] training 52.3% loss=0.59581, acc=0.67188
# [1/100] training 52.5% loss=0.49334, acc=0.75000
# [1/100] training 52.6% loss=0.57833, acc=0.67188
# [1/100] training 52.9% loss=0.67047, acc=0.56250
# [1/100] training 53.0% loss=0.60663, acc=0.65625
# [1/100] training 53.2% loss=0.57925, acc=0.67188
# [1/100] training 53.3% loss=0.42449, acc=0.81250
# [1/100] training 53.5% loss=0.62624, acc=0.70312
# [1/100] training 53.7% loss=0.58454, acc=0.71875
# [1/100] training 53.8% loss=0.47648, acc=0.78125
# [1/100] training 54.1% loss=0.54995, acc=0.76562
# [1/100] training 54.2% loss=0.60933, acc=0.65625
# [1/100] training 54.4% loss=0.49443, acc=0.73438
# [1/100] training 54.5% loss=0.52020, acc=0.70312
# [1/100] training 54.7% loss=0.55574, acc=0.78125
# [1/100] training 54.8% loss=0.46133, acc=0.76562
# [1/100] training 55.1% loss=0.44224, acc=0.82812
# [1/100] training 55.3% loss=0.54819, acc=0.70312
# [1/100] training 55.4% loss=0.50390, acc=0.81250
# [1/100] training 55.6% loss=0.52885, acc=0.75000
# [1/100] training 55.7% loss=0.60227, acc=0.70312
# [1/100] training 55.9% loss=0.51257, acc=0.68750
# [1/100] training 56.0% loss=0.56655, acc=0.73438
# [1/100] training 56.3% loss=0.60384, acc=0.62500
# [1/100] training 56.5% loss=0.65688, acc=0.67188
# [1/100] training 56.6% loss=0.52794, acc=0.73438
# [1/100] training 56.8% loss=0.60196, acc=0.64062
# [1/100] training 56.9% loss=0.58854, acc=0.68750
# [1/100] training 57.1% loss=0.54939, acc=0.70312
# [1/100] training 57.2% loss=0.49648, acc=0.81250
# [1/100] training 57.5% loss=0.64621, acc=0.57812
# [1/100] training 57.6% loss=0.58966, acc=0.59375
# [1/100] training 57.8% loss=0.53722, acc=0.70312
# [1/100] training 58.0% loss=0.50822, acc=0.81250
# [1/100] training 58.1% loss=0.50293, acc=0.78125
# [1/100] training 58.3% loss=0.54100, acc=0.64062
# [1/100] training 58.4% loss=0.50029, acc=0.79688
# [1/100] training 58.7% loss=0.57602, acc=0.64062
# [1/100] training 58.8% loss=0.62567, acc=0.67188
# [1/100] training 59.0% loss=0.55992, acc=0.75000
# [1/100] training 59.2% loss=0.55460, acc=0.64062
# [1/100] training 59.3% loss=0.59577, acc=0.70312
# [1/100] training 59.5% loss=0.61012, acc=0.68750
# [1/100] training 59.7% loss=0.65292, acc=0.60938
# [1/100] training 59.9% loss=0.55785, acc=0.73438
# [1/100] training 60.0% loss=0.55367, acc=0.68750
# [1/100] training 60.2% loss=0.59553, acc=0.64062
# [1/100] training 60.3% loss=0.55647, acc=0.65625
# [1/100] training 60.5% loss=0.53764, acc=0.68750
# [1/100] training 60.8% loss=0.49756, acc=0.71875
# [1/100] training 60.9% loss=0.54277, acc=0.68750
# [1/100] training 61.1% loss=0.66142, acc=0.62500
# [1/100] training 61.2% loss=0.50430, acc=0.76562
# [1/100] training 61.4% loss=0.54117, acc=0.67188
# [1/100] training 61.5% loss=0.64412, acc=0.67188
# [1/100] training 61.7% loss=0.59892, acc=0.65625
# [1/100] training 62.0% loss=0.55311, acc=0.71875
# [1/100] training 62.1% loss=0.61460, acc=0.65625
# [1/100] training 62.3% loss=0.55422, acc=0.68750
# [1/100] training 62.4% loss=0.54148, acc=0.70312
# [1/100] training 62.6% loss=0.53084, acc=0.71875
# [1/100] training 62.7% loss=0.45638, acc=0.78125
# [1/100] training 62.9% loss=0.66212, acc=0.60938
# [1/100] training 63.1% loss=0.53772, acc=0.78125
# [1/100] training 63.3% loss=0.60372, acc=0.62500
# [1/100] training 63.5% loss=0.57142, acc=0.70312
# [1/100] training 63.6% loss=0.55641, acc=0.67188
# [1/100] training 63.8% loss=0.54061, acc=0.78125
# [1/100] training 63.9% loss=0.51420, acc=0.73438
# [1/100] training 64.2% loss=0.46720, acc=0.76562
# [1/100] training 64.3% loss=0.52881, acc=0.68750
# [1/100] training 64.5% loss=0.53529, acc=0.68750
# [1/100] training 64.7% loss=0.51754, acc=0.71875
# [1/100] training 64.8% loss=0.59102, acc=0.71875
# [1/100] training 65.0% loss=0.60709, acc=0.65625
# [1/100] training 65.1% loss=0.59223, acc=0.70312
# [1/100] training 65.4% loss=0.61816, acc=0.67188
# [1/100] training 65.5% loss=0.54508, acc=0.73438
# [1/100] training 65.7% loss=0.53120, acc=0.78125
# [1/100] training 65.8% loss=0.56222, acc=0.73438
# [1/100] training 66.0% loss=0.54357, acc=0.71875
# [1/100] training 66.2% loss=0.46926, acc=0.75000
# [1/100] training 66.3% loss=0.56192, acc=0.68750
# [1/100] training 66.6% loss=0.46759, acc=0.76562
# [1/100] training 66.7% loss=0.47506, acc=0.71875
# [1/100] training 66.9% loss=0.43071, acc=0.82812
# [1/100] training 67.0% loss=0.59132, acc=0.70312
# [1/100] training 67.2% loss=0.50106, acc=0.76562
# [1/100] training 67.4% loss=0.56102, acc=0.73438
# [1/100] training 67.6% loss=0.57090, acc=0.70312
# [1/100] training 67.8% loss=0.50161, acc=0.71875
# [1/100] training 67.9% loss=0.50070, acc=0.75000
# [1/100] training 68.1% loss=0.40583, acc=0.78125
# [1/100] training 68.2% loss=0.47272, acc=0.75000
# [1/100] training 68.4% loss=0.53633, acc=0.71875
# [1/100] training 68.5% loss=0.56142, acc=0.70312
# [1/100] training 68.8% loss=0.56312, acc=0.65625
# [1/100] training 69.0% loss=0.57787, acc=0.64062
# [1/100] training 69.1% loss=0.52961, acc=0.70312
# [1/100] training 69.3% loss=0.52073, acc=0.75000
# [1/100] training 69.4% loss=0.41965, acc=0.82812
# [1/100] training 69.6% loss=0.67956, acc=0.67188
# [1/100] training 69.7% loss=0.52011, acc=0.76562
# [1/100] training 70.0% loss=0.52268, acc=0.71875
# [1/100] training 70.2% loss=0.58477, acc=0.64062
# [1/100] training 70.3% loss=0.47470, acc=0.78125
# [1/100] training 70.5% loss=0.61260, acc=0.57812
# [1/100] training 70.6% loss=0.52009, acc=0.78125
# [1/100] training 70.8% loss=0.52180, acc=0.65625
# [1/100] training 71.0% loss=0.42941, acc=0.78125
# [1/100] training 71.2% loss=0.44093, acc=0.79688
# [1/100] training 71.3% loss=0.39822, acc=0.81250
# [1/100] training 71.5% loss=0.63318, acc=0.67188
# [1/100] training 71.7% loss=0.67629, acc=0.70312
# [1/100] training 71.8% loss=0.46397, acc=0.78125
# [1/100] training 72.0% loss=0.55520, acc=0.65625
# [1/100] training 72.2% loss=0.48521, acc=0.76562
# [1/100] training 72.4% loss=0.63288, acc=0.75000
# [1/100] training 72.5% loss=0.58340, acc=0.70312
# [1/100] training 72.7% loss=0.59497, acc=0.64062
# [1/100] training 72.9% loss=0.57865, acc=0.67188
# [1/100] training 73.0% loss=0.63767, acc=0.59375
# [1/100] training 73.3% loss=0.53625, acc=0.65625
# [1/100] training 73.4% loss=0.47041, acc=0.67188
# [1/100] training 73.6% loss=0.50220, acc=0.71875
# [1/100] training 73.7% loss=0.48199, acc=0.75000
# [1/100] training 73.9% loss=0.52606, acc=0.76562
# [1/100] training 74.0% loss=0.60717, acc=0.70312
# [1/100] training 74.2% loss=0.49560, acc=0.73438
# [1/100] training 74.5% loss=0.54247, acc=0.70312
# [1/100] training 74.6% loss=0.51790, acc=0.76562
# [1/100] training 74.8% loss=0.60087, acc=0.64062
# [1/100] training 74.9% loss=0.57615, acc=0.73438
# [1/100] training 75.1% loss=0.53532, acc=0.75000
# [1/100] training 75.2% loss=0.48557, acc=0.76562
# [1/100] training 75.4% loss=0.47444, acc=0.75000
# [1/100] training 75.7% loss=0.45367, acc=0.81250
# [1/100] training 75.8% loss=0.63609, acc=0.64062
# [1/100] training 76.0% loss=0.42932, acc=0.78125
# [1/100] training 76.1% loss=0.62107, acc=0.64062
# [1/100] training 76.3% loss=0.56023, acc=0.68750
# [1/100] training 76.4% loss=0.52399, acc=0.70312
# [1/100] training 76.7% loss=0.50256, acc=0.76562
# [1/100] training 76.8% loss=0.47462, acc=0.78125
# [1/100] training 77.0% loss=0.45595, acc=0.76562
# [1/100] training 77.2% loss=0.52076, acc=0.70312
# [1/100] training 77.3% loss=0.47694, acc=0.76562
# [1/100] training 77.5% loss=0.62557, acc=0.67188
# [1/100] training 77.6% loss=0.46813, acc=0.75000
# [1/100] training 77.9% loss=0.52424, acc=0.76562
# [1/100] training 78.0% loss=0.54427, acc=0.70312
# [1/100] training 78.2% loss=0.54099, acc=0.70312
# [1/100] training 78.4% loss=0.52919, acc=0.70312
# [1/100] training 78.5% loss=0.61461, acc=0.67188
# [1/100] training 78.7% loss=0.50987, acc=0.68750
# [1/100] training 78.8% loss=0.48219, acc=0.76562
# [1/100] training 79.1% loss=0.39778, acc=0.81250
# [1/100] training 79.2% loss=0.56510, acc=0.76562
# [1/100] training 79.4% loss=0.64266, acc=0.65625
# [1/100] training 79.5% loss=0.49307, acc=0.70312
# [1/100] training 79.7% loss=0.56207, acc=0.71875
# [1/100] training 79.9% loss=0.55577, acc=0.70312
# [1/100] training 80.1% loss=0.60549, acc=0.64062
# [1/100] training 80.3% loss=0.57092, acc=0.73438
# [1/100] training 80.4% loss=0.57740, acc=0.71875
# [1/100] training 80.6% loss=0.55216, acc=0.68750
# [1/100] training 80.7% loss=0.56591, acc=0.65625
# [1/100] training 80.9% loss=0.56311, acc=0.71875
# [1/100] training 81.2% loss=0.50331, acc=0.71875
# [1/100] training 81.3% loss=0.50892, acc=0.76562
# [1/100] training 81.5% loss=0.43448, acc=0.82812
# [1/100] training 81.6% loss=0.46416, acc=0.67188
# [1/100] training 81.8% loss=0.43195, acc=0.78125
# [1/100] training 81.9% loss=0.68396, acc=0.65625
# [1/100] training 82.1% loss=0.53326, acc=0.62500
# [1/100] training 82.2% loss=0.77800, acc=0.60938
# [1/100] training 82.5% loss=0.58560, acc=0.67188
# [1/100] training 82.7% loss=0.50513, acc=0.79688
# [1/100] training 82.8% loss=0.56257, acc=0.70312
# [1/100] training 83.0% loss=0.48652, acc=0.81250
# [1/100] training 83.1% loss=0.54688, acc=0.71875
# [1/100] training 83.3% loss=0.62778, acc=0.70312
# [1/100] training 83.5% loss=0.59926, acc=0.68750
# [1/100] training 83.7% loss=0.55494, acc=0.68750
# [1/100] training 83.9% loss=0.57208, acc=0.71875
# [1/100] training 84.0% loss=0.47704, acc=0.78125
# [1/100] training 84.2% loss=0.50448, acc=0.78125
# [1/100] training 84.3% loss=0.45198, acc=0.79688
# [1/100] training 84.5% loss=0.58967, acc=0.71875
# [1/100] training 84.7% loss=0.61948, acc=0.62500
# [1/100] training 84.9% loss=0.58189, acc=0.64062
# [1/100] training 85.0% loss=0.52375, acc=0.68750
# [1/100] training 85.2% loss=0.65308, acc=0.60938
# [1/100] training 85.4% loss=0.52374, acc=0.75000
# [1/100] training 85.5% loss=0.47047, acc=0.76562
# [1/100] training 85.8% loss=0.53543, acc=0.71875
# [1/100] training 85.9% loss=0.49358, acc=0.71875
# [1/100] training 86.1% loss=0.53216, acc=0.78125
# [1/100] training 86.2% loss=0.57203, acc=0.75000
# [1/100] training 86.4% loss=0.60246, acc=0.65625
# [1/100] training 86.6% loss=0.63176, acc=0.67188
# [1/100] training 86.7% loss=0.67525, acc=0.57812
# [1/100] training 87.0% loss=0.51623, acc=0.75000
# [1/100] training 87.1% loss=0.47598, acc=0.71875
# [1/100] training 87.3% loss=0.49724, acc=0.75000
# [1/100] training 87.4% loss=0.51764, acc=0.75000
# [1/100] training 87.6% loss=0.51906, acc=0.67188
# [1/100] training 87.7% loss=0.50773, acc=0.71875
# [1/100] training 87.9% loss=0.46682, acc=0.81250
# [1/100] training 88.2% loss=0.36939, acc=0.85938
# [1/100] training 88.3% loss=0.44717, acc=0.76562
# [1/100] training 88.5% loss=0.52475, acc=0.70312
# [1/100] training 88.6% loss=0.37246, acc=0.84375
# [1/100] training 88.8% loss=0.43786, acc=0.81250
# [1/100] training 88.9% loss=0.42017, acc=0.76562
# [1/100] training 89.2% loss=0.46068, acc=0.78125
# [1/100] training 89.4% loss=0.42949, acc=0.78125
# [1/100] training 89.5% loss=0.56851, acc=0.73438
# [1/100] training 89.7% loss=0.65268, acc=0.71875
# [1/100] training 89.8% loss=0.40246, acc=0.82812
# [1/100] training 90.0% loss=0.41980, acc=0.79688
# [1/100] training 90.1% loss=0.46179, acc=0.75000
# [1/100] training 90.4% loss=0.47163, acc=0.73438
# [1/100] training 90.5% loss=0.41001, acc=0.85938
# [1/100] training 90.7% loss=0.49222, acc=0.70312
# [1/100] training 90.9% loss=0.52680, acc=0.76562
# [1/100] training 91.0% loss=0.56385, acc=0.73438
# [1/100] training 91.2% loss=0.63061, acc=0.64062
# [1/100] training 91.3% loss=0.53721, acc=0.70312
# [1/100] training 91.6% loss=0.55276, acc=0.75000
# [1/100] training 91.7% loss=0.45543, acc=0.78125
# [1/100] training 91.9% loss=0.51702, acc=0.75000
# [1/100] training 92.1% loss=0.47363, acc=0.76562
# [1/100] training 92.2% loss=0.39937, acc=0.87500
# [1/100] training 92.4% loss=0.44879, acc=0.76562
# [1/100] training 92.6% loss=0.50407, acc=0.73438
# [1/100] training 92.8% loss=0.60991, acc=0.67188
# [1/100] training 92.9% loss=0.40409, acc=0.82812
# [1/100] training 93.1% loss=0.57309, acc=0.71875
# [1/100] training 93.2% loss=0.51698, acc=0.79688
# [1/100] training 93.4% loss=0.49008, acc=0.75000
# [1/100] training 93.7% loss=0.58054, acc=0.64062
# [1/100] training 93.8% loss=0.57488, acc=0.62500
# [1/100] training 94.0% loss=0.49382, acc=0.73438
# [1/100] training 94.1% loss=0.54249, acc=0.68750
# [1/100] training 94.3% loss=0.49112, acc=0.73438
# [1/100] training 94.4% loss=0.59591, acc=0.62500
# [1/100] training 94.6% loss=0.56121, acc=0.65625
# [1/100] training 94.9% loss=0.56366, acc=0.67188
# [1/100] training 95.0% loss=0.53987, acc=0.75000
# [1/100] training 95.2% loss=0.52269, acc=0.71875
# [1/100] training 95.3% loss=0.48820, acc=0.73438
# [1/100] training 95.5% loss=0.47266, acc=0.81250
# [1/100] training 95.6% loss=0.64217, acc=0.68750
# [1/100] training 95.8% loss=0.54650, acc=0.71875
# [1/100] training 96.0% loss=0.50829, acc=0.70312
# [1/100] training 96.2% loss=0.38138, acc=0.84375
# [1/100] training 96.4% loss=0.47190, acc=0.76562
# [1/100] training 96.5% loss=0.54414, acc=0.68750
# [1/100] training 96.7% loss=0.48711, acc=0.75000
# [1/100] training 96.8% loss=0.49351, acc=0.75000
# [1/100] training 97.1% loss=0.51778, acc=0.76562
# [1/100] training 97.2% loss=0.50426, acc=0.73438
# [1/100] training 97.4% loss=0.52123, acc=0.71875
# [1/100] training 97.6% loss=0.46403, acc=0.81250
# [1/100] training 97.7% loss=0.48930, acc=0.71875
# [1/100] training 97.9% loss=0.53776, acc=0.68750
# [1/100] training 98.0% loss=0.53713, acc=0.70312
# [1/100] training 98.3% loss=0.52616, acc=0.73438
# [1/100] training 98.4% loss=0.50918, acc=0.71875
# [1/100] training 98.6% loss=0.63957, acc=0.59375
# [1/100] training 98.7% loss=0.59863, acc=0.64062
# [1/100] training 98.9% loss=0.55884, acc=0.73438
# [1/100] training 99.1% loss=0.43440, acc=0.75000
# [1/100] training 99.2% loss=0.50842, acc=0.71875
# [1/100] training 99.5% loss=0.53014, acc=0.75000
# [1/100] training 99.6% loss=0.56040, acc=0.67188
# [1/100] training 99.8% loss=0.44773, acc=0.79688
# [1/100] training 99.9% loss=0.50096, acc=0.71875
# [1/100] testing 0.9% loss=0.45089, acc=0.78125
# [1/100] testing 1.8% loss=0.58217, acc=0.64062
# [1/100] testing 2.2% loss=0.48721, acc=0.75000
# [1/100] testing 3.1% loss=0.49080, acc=0.76562
# [1/100] testing 3.5% loss=0.50299, acc=0.71875
# [1/100] testing 4.4% loss=0.49658, acc=0.75000
# [1/100] testing 4.8% loss=0.54431, acc=0.65625
# [1/100] testing 5.7% loss=0.56601, acc=0.68750
# [1/100] testing 6.6% loss=0.64742, acc=0.60938
# [1/100] testing 7.0% loss=0.49983, acc=0.75000
# [1/100] testing 7.9% loss=0.57124, acc=0.68750
# [1/100] testing 8.3% loss=0.42607, acc=0.81250
# [1/100] testing 9.2% loss=0.57638, acc=0.67188
# [1/100] testing 9.7% loss=0.47556, acc=0.76562
# [1/100] testing 10.5% loss=0.60164, acc=0.59375
# [1/100] testing 11.0% loss=0.47871, acc=0.75000
# [1/100] testing 11.8% loss=0.49702, acc=0.79688
# [1/100] testing 12.7% loss=0.61639, acc=0.62500
# [1/100] testing 13.2% loss=0.48233, acc=0.73438
# [1/100] testing 14.0% loss=0.56084, acc=0.73438
# [1/100] testing 14.5% loss=0.66798, acc=0.59375
# [1/100] testing 15.4% loss=0.47278, acc=0.79688
# [1/100] testing 15.8% loss=0.52457, acc=0.75000
# [1/100] testing 16.7% loss=0.50296, acc=0.71875
# [1/100] testing 17.5% loss=0.48004, acc=0.73438
# [1/100] testing 18.0% loss=0.47491, acc=0.71875
# [1/100] testing 18.9% loss=0.44264, acc=0.81250
# [1/100] testing 19.3% loss=0.49400, acc=0.75000
# [1/100] testing 20.2% loss=0.57420, acc=0.68750
# [1/100] testing 20.6% loss=0.54448, acc=0.67188
# [1/100] testing 21.5% loss=0.50974, acc=0.71875
# [1/100] testing 21.9% loss=0.58048, acc=0.62500
# [1/100] testing 22.8% loss=0.54072, acc=0.68750
# [1/100] testing 23.7% loss=0.55498, acc=0.71875
# [1/100] testing 24.1% loss=0.59642, acc=0.68750
# [1/100] testing 25.0% loss=0.51627, acc=0.71875
# [1/100] testing 25.4% loss=0.46671, acc=0.78125
# [1/100] testing 26.3% loss=0.52893, acc=0.68750
# [1/100] testing 26.8% loss=0.58175, acc=0.70312
# [1/100] testing 27.6% loss=0.60899, acc=0.65625
# [1/100] testing 28.5% loss=0.56198, acc=0.67188
# [1/100] testing 29.0% loss=0.54792, acc=0.68750
# [1/100] testing 29.8% loss=0.56009, acc=0.67188
# [1/100] testing 30.3% loss=0.69374, acc=0.54688
# [1/100] testing 31.1% loss=0.53168, acc=0.71875
# [1/100] testing 31.6% loss=0.47640, acc=0.73438
# [1/100] testing 32.5% loss=0.56281, acc=0.71875
# [1/100] testing 32.9% loss=0.50912, acc=0.73438
# [1/100] testing 33.8% loss=0.58028, acc=0.70312
# [1/100] testing 34.7% loss=0.56153, acc=0.64062
# [1/100] testing 35.1% loss=0.45664, acc=0.76562
# [1/100] testing 36.0% loss=0.63310, acc=0.62500
# [1/100] testing 36.4% loss=0.58765, acc=0.75000
# [1/100] testing 37.3% loss=0.48710, acc=0.68750
# [1/100] testing 37.7% loss=0.55576, acc=0.65625
# [1/100] testing 38.6% loss=0.50377, acc=0.79688
# [1/100] testing 39.5% loss=0.64435, acc=0.71875
# [1/100] testing 39.9% loss=0.49105, acc=0.79688
# [1/100] testing 40.8% loss=0.54953, acc=0.71875
# [1/100] testing 41.2% loss=0.41501, acc=0.84375
# [1/100] testing 42.1% loss=0.57708, acc=0.71875
# [1/100] testing 42.5% loss=0.47565, acc=0.79688
# [1/100] testing 43.4% loss=0.54980, acc=0.75000
# [1/100] testing 43.9% loss=0.57985, acc=0.70312
# [1/100] testing 44.7% loss=0.54661, acc=0.71875
# [1/100] testing 45.6% loss=0.56441, acc=0.71875
# [1/100] testing 46.1% loss=0.52452, acc=0.73438
# [1/100] testing 46.9% loss=0.43902, acc=0.76562
# [1/100] testing 47.4% loss=0.48784, acc=0.73438
# [1/100] testing 48.3% loss=0.59612, acc=0.65625
# [1/100] testing 48.7% loss=0.61303, acc=0.68750
# [1/100] testing 49.6% loss=0.54861, acc=0.71875
# [1/100] testing 50.4% loss=0.39668, acc=0.82812
# [1/100] testing 50.9% loss=0.49843, acc=0.76562
# [1/100] testing 51.8% loss=0.51864, acc=0.70312
# [1/100] testing 52.2% loss=0.47015, acc=0.78125
# [1/100] testing 53.1% loss=0.54976, acc=0.71875
# [1/100] testing 53.5% loss=0.45680, acc=0.84375
# [1/100] testing 54.4% loss=0.58100, acc=0.68750
# [1/100] testing 54.8% loss=0.59875, acc=0.70312
# [1/100] testing 55.7% loss=0.50161, acc=0.81250
# [1/100] testing 56.6% loss=0.54123, acc=0.71875
# [1/100] testing 57.0% loss=0.64860, acc=0.64062
# [1/100] testing 57.9% loss=0.60525, acc=0.64062
# [1/100] testing 58.3% loss=0.55589, acc=0.71875
# [1/100] testing 59.2% loss=0.54005, acc=0.70312
# [1/100] testing 59.7% loss=0.51809, acc=0.71875
# [1/100] testing 60.5% loss=0.48354, acc=0.71875
# [1/100] testing 61.4% loss=0.44129, acc=0.81250
# [1/100] testing 61.9% loss=0.43971, acc=0.79688
# [1/100] testing 62.7% loss=0.48372, acc=0.73438
# [1/100] testing 63.2% loss=0.53530, acc=0.67188
# [1/100] testing 64.0% loss=0.52062, acc=0.75000
# [1/100] testing 64.5% loss=0.54598, acc=0.75000
# [1/100] testing 65.4% loss=0.53017, acc=0.71875
# [1/100] testing 65.8% loss=0.58545, acc=0.68750
# [1/100] testing 66.7% loss=0.43442, acc=0.82812
# [1/100] testing 67.6% loss=0.52237, acc=0.76562
# [1/100] testing 68.0% loss=0.59340, acc=0.67188
# [1/100] testing 68.9% loss=0.52252, acc=0.70312
# [1/100] testing 69.3% loss=0.52652, acc=0.78125
# [1/100] testing 70.2% loss=0.58349, acc=0.71875
# [1/100] testing 70.6% loss=0.53215, acc=0.73438
# [1/100] testing 71.5% loss=0.58647, acc=0.70312
# [1/100] testing 72.4% loss=0.50081, acc=0.76562
# [1/100] testing 72.8% loss=0.57959, acc=0.68750
# [1/100] testing 73.7% loss=0.50648, acc=0.73438
# [1/100] testing 74.1% loss=0.50132, acc=0.73438
# [1/100] testing 75.0% loss=0.45721, acc=0.78125
# [1/100] testing 75.4% loss=0.42068, acc=0.79688
# [1/100] testing 76.3% loss=0.45682, acc=0.82812
# [1/100] testing 76.8% loss=0.52321, acc=0.71875
# [1/100] testing 77.6% loss=0.52468, acc=0.70312
# [1/100] testing 78.5% loss=0.65875, acc=0.64062
# [1/100] testing 79.0% loss=0.53078, acc=0.76562
# [1/100] testing 79.8% loss=0.50166, acc=0.70312
# [1/100] testing 80.3% loss=0.50689, acc=0.75000
# [1/100] testing 81.2% loss=0.55362, acc=0.70312
# [1/100] testing 81.6% loss=0.50869, acc=0.79688
# [1/100] testing 82.5% loss=0.53756, acc=0.68750
# [1/100] testing 83.3% loss=0.43942, acc=0.75000
# [1/100] testing 83.8% loss=0.46252, acc=0.79688
# [1/100] testing 84.7% loss=0.48954, acc=0.78125
# [1/100] testing 85.1% loss=0.58812, acc=0.70312
# [1/100] testing 86.0% loss=0.57516, acc=0.71875
# [1/100] testing 86.4% loss=0.65363, acc=0.65625
# [1/100] testing 87.3% loss=0.59815, acc=0.65625
# [1/100] testing 87.7% loss=0.45042, acc=0.84375
# [1/100] testing 88.6% loss=0.42247, acc=0.82812
# [1/100] testing 89.5% loss=0.59859, acc=0.65625
# [1/100] testing 89.9% loss=0.53812, acc=0.75000
# [1/100] testing 90.8% loss=0.47346, acc=0.78125
# [1/100] testing 91.2% loss=0.48477, acc=0.75000
# [1/100] testing 92.1% loss=0.50719, acc=0.70312
# [1/100] testing 92.6% loss=0.55730, acc=0.73438
# [1/100] testing 93.4% loss=0.55286, acc=0.67188
# [1/100] testing 94.3% loss=0.47816, acc=0.73438
# [1/100] testing 94.7% loss=0.53460, acc=0.68750
# [1/100] testing 95.6% loss=0.55540, acc=0.68750
# [1/100] testing 96.1% loss=0.50547, acc=0.75000
# [1/100] testing 96.9% loss=0.52214, acc=0.71875
# [1/100] testing 97.4% loss=0.42334, acc=0.84375
# [1/100] testing 98.3% loss=0.54343, acc=0.76562
# [1/100] testing 98.7% loss=0.57694, acc=0.67188
# [1/100] testing 99.6% loss=0.51242, acc=0.78125
# [2/100] training 0.2% loss=0.61062, acc=0.67188
# [2/100] training 0.4% loss=0.55463, acc=0.71875
# [2/100] training 0.5% loss=0.53193, acc=0.68750
# [2/100] training 0.8% loss=0.47364, acc=0.76562
# [2/100] training 0.9% loss=0.51151, acc=0.73438
# [2/100] training 1.1% loss=0.38425, acc=0.85938
# [2/100] training 1.2% loss=0.52705, acc=0.71875
# [2/100] training 1.4% loss=0.53792, acc=0.71875
# [2/100] training 1.6% loss=0.44313, acc=0.75000
# [2/100] training 1.8% loss=0.46952, acc=0.78125
# [2/100] training 2.0% loss=0.44909, acc=0.76562
# [2/100] training 2.1% loss=0.53322, acc=0.75000
# [2/100] training 2.3% loss=0.41178, acc=0.81250
# [2/100] training 2.4% loss=0.60517, acc=0.67188
# [2/100] training 2.6% loss=0.62898, acc=0.65625
# [2/100] training 2.7% loss=0.52264, acc=0.68750
# [2/100] training 3.0% loss=0.69162, acc=0.53125
# [2/100] training 3.2% loss=0.51675, acc=0.73438
# [2/100] training 3.3% loss=0.54087, acc=0.68750
# [2/100] training 3.5% loss=0.50199, acc=0.68750
# [2/100] training 3.6% loss=0.61462, acc=0.68750
# [2/100] training 3.8% loss=0.46130, acc=0.70312
# [2/100] training 3.9% loss=0.57579, acc=0.68750
# [2/100] training 4.2% loss=0.44015, acc=0.78125
# [2/100] training 4.4% loss=0.49592, acc=0.71875
# [2/100] training 4.5% loss=0.47416, acc=0.75000
# [2/100] training 4.7% loss=0.53347, acc=0.76562
# [2/100] training 4.8% loss=0.51464, acc=0.81250
# [2/100] training 5.0% loss=0.46873, acc=0.76562
# [2/100] training 5.2% loss=0.51376, acc=0.75000
# [2/100] training 5.4% loss=0.42685, acc=0.82812
# [2/100] training 5.5% loss=0.46553, acc=0.76562
# [2/100] training 5.7% loss=0.42161, acc=0.78125
# [2/100] training 5.9% loss=0.56475, acc=0.71875
# [2/100] training 6.0% loss=0.46985, acc=0.79688
# [2/100] training 6.3% loss=0.57993, acc=0.71875
# [2/100] training 6.4% loss=0.57046, acc=0.59375
# [2/100] training 6.6% loss=0.50560, acc=0.71875
# [2/100] training 6.7% loss=0.47563, acc=0.71875
# [2/100] training 6.9% loss=0.45031, acc=0.81250
# [2/100] training 7.1% loss=0.52436, acc=0.68750
# [2/100] training 7.2% loss=0.36697, acc=0.84375
# [2/100] training 7.5% loss=0.41045, acc=0.82812
# [2/100] training 7.6% loss=0.40589, acc=0.82812
# [2/100] training 7.8% loss=0.51222, acc=0.67188
# [2/100] training 7.9% loss=0.43350, acc=0.76562
# [2/100] training 8.1% loss=0.45612, acc=0.76562
# [2/100] training 8.2% loss=0.37539, acc=0.82812
# [2/100] training 8.4% loss=0.58605, acc=0.68750
# [2/100] training 8.7% loss=0.47410, acc=0.73438
# [2/100] training 8.8% loss=0.45639, acc=0.76562
# [2/100] training 9.0% loss=0.57207, acc=0.68750
# [2/100] training 9.1% loss=0.57557, acc=0.68750
# [2/100] training 9.3% loss=0.54661, acc=0.70312
# [2/100] training 9.4% loss=0.58853, acc=0.67188
# [2/100] training 9.7% loss=0.50407, acc=0.75000
# [2/100] training 9.9% loss=0.53084, acc=0.65625
# [2/100] training 10.0% loss=0.41618, acc=0.82812
# [2/100] training 10.2% loss=0.47882, acc=0.73438
# [2/100] training 10.3% loss=0.43470, acc=0.82812
# [2/100] training 10.5% loss=0.50068, acc=0.76562
# [2/100] training 10.6% loss=0.45336, acc=0.78125
# [2/100] training 10.9% loss=0.62885, acc=0.76562
# [2/100] training 11.0% loss=0.45831, acc=0.76562
# [2/100] training 11.2% loss=0.46181, acc=0.78125
# [2/100] training 11.4% loss=0.48415, acc=0.71875
# [2/100] training 11.5% loss=0.62764, acc=0.70312
# [2/100] training 11.7% loss=0.49930, acc=0.76562
# [2/100] training 11.8% loss=0.45247, acc=0.81250
# [2/100] training 12.1% loss=0.48018, acc=0.78125
# [2/100] training 12.2% loss=0.51682, acc=0.70312
# [2/100] training 12.4% loss=0.42794, acc=0.81250
# [2/100] training 12.6% loss=0.41571, acc=0.81250
# [2/100] training 12.7% loss=0.60384, acc=0.71875
# [2/100] training 12.9% loss=0.48800, acc=0.76562
# [2/100] training 13.0% loss=0.51963, acc=0.76562
# [2/100] training 13.3% loss=0.51097, acc=0.68750
# [2/100] training 13.4% loss=0.57602, acc=0.68750
# [2/100] training 13.6% loss=0.45524, acc=0.84375
# [2/100] training 13.7% loss=0.62817, acc=0.64062
# [2/100] training 13.9% loss=0.57063, acc=0.70312
# [2/100] training 14.1% loss=0.45630, acc=0.81250
# [2/100] training 14.3% loss=0.46335, acc=0.78125
# [2/100] training 14.5% loss=0.38789, acc=0.81250
# [2/100] training 14.6% loss=0.53769, acc=0.71875
# [2/100] training 14.8% loss=0.46494, acc=0.71875
# [2/100] training 14.9% loss=0.52844, acc=0.68750
# [2/100] training 15.1% loss=0.61176, acc=0.67188
# [2/100] training 15.4% loss=0.47276, acc=0.73438
# [2/100] training 15.5% loss=0.54310, acc=0.65625
# [2/100] training 15.7% loss=0.70244, acc=0.53125
# [2/100] training 15.8% loss=0.53033, acc=0.78125
# [2/100] training 16.0% loss=0.62679, acc=0.59375
# [2/100] training 16.1% loss=0.62259, acc=0.62500
# [2/100] training 16.3% loss=0.54336, acc=0.68750
# [2/100] training 16.4% loss=0.52196, acc=0.75000
# [2/100] training 16.7% loss=0.57850, acc=0.67188
# [2/100] training 16.9% loss=0.56948, acc=0.73438
# [2/100] training 17.0% loss=0.51670, acc=0.67188
# [2/100] training 17.2% loss=0.45528, acc=0.76562
# [2/100] training 17.3% loss=0.49316, acc=0.76562
# [2/100] training 17.5% loss=0.52622, acc=0.73438
# [2/100] training 17.7% loss=0.59905, acc=0.68750
# [2/100] training 17.9% loss=0.49452, acc=0.78125
# [2/100] training 18.1% loss=0.53435, acc=0.75000
# [2/100] training 18.2% loss=0.57898, acc=0.62500
# [2/100] training 18.4% loss=0.59397, acc=0.59375
# [2/100] training 18.5% loss=0.54044, acc=0.70312
# [2/100] training 18.8% loss=0.48072, acc=0.71875
# [2/100] training 18.9% loss=0.45539, acc=0.81250
# [2/100] training 19.1% loss=0.60874, acc=0.67188
# [2/100] training 19.2% loss=0.37325, acc=0.87500
# [2/100] training 19.4% loss=0.41442, acc=0.75000
# [2/100] training 19.6% loss=0.58144, acc=0.71875
# [2/100] training 19.7% loss=0.44185, acc=0.76562
# [2/100] training 20.0% loss=0.54281, acc=0.73438
# [2/100] training 20.1% loss=0.46020, acc=0.81250
# [2/100] training 20.3% loss=0.62315, acc=0.73438
# [2/100] training 20.4% loss=0.52004, acc=0.71875
# [2/100] training 20.6% loss=0.45590, acc=0.76562
# [2/100] training 20.8% loss=0.49559, acc=0.71875
# [2/100] training 20.9% loss=0.47660, acc=0.71875
# [2/100] training 21.2% loss=0.48521, acc=0.76562
# [2/100] training 21.3% loss=0.50897, acc=0.71875
# [2/100] training 21.5% loss=0.45616, acc=0.79688
# [2/100] training 21.6% loss=0.38782, acc=0.82812
# [2/100] training 21.8% loss=0.45712, acc=0.78125
# [2/100] training 21.9% loss=0.54123, acc=0.71875
# [2/100] training 22.2% loss=0.44520, acc=0.78125
# [2/100] training 22.4% loss=0.58817, acc=0.70312
# [2/100] training 22.5% loss=0.55091, acc=0.68750
# [2/100] training 22.7% loss=0.40203, acc=0.81250
# [2/100] training 22.8% loss=0.53491, acc=0.73438
# [2/100] training 23.0% loss=0.45294, acc=0.75000
# [2/100] training 23.1% loss=0.61567, acc=0.65625
# [2/100] training 23.4% loss=0.46920, acc=0.78125
# [2/100] training 23.6% loss=0.51158, acc=0.75000
# [2/100] training 23.7% loss=0.60760, acc=0.59375
# [2/100] training 23.9% loss=0.51815, acc=0.67188
# [2/100] training 24.0% loss=0.65633, acc=0.70312
# [2/100] training 24.2% loss=0.51229, acc=0.73438
# [2/100] training 24.3% loss=0.49919, acc=0.78125
# [2/100] training 24.6% loss=0.48704, acc=0.75000
# [2/100] training 24.7% loss=0.49328, acc=0.67188
# [2/100] training 24.9% loss=0.54141, acc=0.75000
# [2/100] training 25.1% loss=0.62960, acc=0.68750
# [2/100] training 25.2% loss=0.44919, acc=0.81250
# [2/100] training 25.4% loss=0.51052, acc=0.76562
# [2/100] training 25.6% loss=0.45083, acc=0.76562
# [2/100] training 25.8% loss=0.59144, acc=0.71875
# [2/100] training 25.9% loss=0.50304, acc=0.78125
# [2/100] training 26.1% loss=0.50355, acc=0.78125
# [2/100] training 26.3% loss=0.48442, acc=0.76562
# [2/100] training 26.4% loss=0.43268, acc=0.78125
# [2/100] training 26.6% loss=0.49430, acc=0.73438
# [2/100] training 26.8% loss=0.54227, acc=0.75000
# [2/100] training 27.0% loss=0.48438, acc=0.73438
# [2/100] training 27.1% loss=0.50945, acc=0.71875
# [2/100] training 27.3% loss=0.47478, acc=0.79688
# [2/100] training 27.4% loss=0.42317, acc=0.81250
# [2/100] training 27.6% loss=0.49941, acc=0.76562
# [2/100] training 27.9% loss=0.57071, acc=0.70312
# [2/100] training 28.0% loss=0.52090, acc=0.73438
# [2/100] training 28.2% loss=0.50925, acc=0.73438
# [2/100] training 28.3% loss=0.40928, acc=0.82812
# [2/100] training 28.5% loss=0.50510, acc=0.71875
# [2/100] training 28.6% loss=0.54847, acc=0.75000
# [2/100] training 28.8% loss=0.38955, acc=0.85938
# [2/100] training 29.1% loss=0.50017, acc=0.75000
# [2/100] training 29.2% loss=0.45203, acc=0.81250
# [2/100] training 29.4% loss=0.56685, acc=0.62500
# [2/100] training 29.5% loss=0.47049, acc=0.79688
# [2/100] training 29.7% loss=0.52060, acc=0.71875
# [2/100] training 29.8% loss=0.42248, acc=0.76562
# [2/100] training 30.0% loss=0.56204, acc=0.65625
# [2/100] training 30.2% loss=0.48432, acc=0.70312
# [2/100] training 30.4% loss=0.46568, acc=0.75000
# [2/100] training 30.6% loss=0.46513, acc=0.75000
# [2/100] training 30.7% loss=0.48473, acc=0.68750
# [2/100] training 30.9% loss=0.49931, acc=0.78125
# [2/100] training 31.0% loss=0.47197, acc=0.71875
# [2/100] training 31.3% loss=0.48580, acc=0.75000
# [2/100] training 31.4% loss=0.50649, acc=0.76562
# [2/100] training 31.6% loss=0.56305, acc=0.64062
# [2/100] training 31.8% loss=0.43120, acc=0.82812
# [2/100] training 31.9% loss=0.49432, acc=0.78125
# [2/100] training 32.1% loss=0.50604, acc=0.73438
# [2/100] training 32.2% loss=0.41997, acc=0.84375
# [2/100] training 32.5% loss=0.46692, acc=0.71875
# [2/100] training 32.6% loss=0.53864, acc=0.73438
# [2/100] training 32.8% loss=0.45970, acc=0.81250
# [2/100] training 32.9% loss=0.73177, acc=0.59375
# [2/100] training 33.1% loss=0.49557, acc=0.75000
# [2/100] training 33.3% loss=0.50913, acc=0.78125
# [2/100] training 33.4% loss=0.54703, acc=0.81250
# [2/100] training 33.7% loss=0.56205, acc=0.70312
# [2/100] training 33.8% loss=0.60070, acc=0.60938
# [2/100] training 34.0% loss=0.59198, acc=0.65625
# [2/100] training 34.1% loss=0.50045, acc=0.71875
# [2/100] training 34.3% loss=0.51397, acc=0.68750
# [2/100] training 34.5% loss=0.59175, acc=0.65625
# [2/100] training 34.7% loss=0.41545, acc=0.82812
# [2/100] training 34.9% loss=0.49127, acc=0.76562
# [2/100] training 35.0% loss=0.43291, acc=0.78125
# [2/100] training 35.2% loss=0.58529, acc=0.68750
# [2/100] training 35.3% loss=0.58405, acc=0.64062
# [2/100] training 35.5% loss=0.74330, acc=0.53125
# [2/100] training 35.6% loss=0.51239, acc=0.70312
# [2/100] training 35.9% loss=0.57777, acc=0.57812
# [2/100] training 36.1% loss=0.45939, acc=0.73438
# [2/100] training 36.2% loss=0.54299, acc=0.71875
# [2/100] training 36.4% loss=0.47904, acc=0.79688
# [2/100] training 36.5% loss=0.43011, acc=0.79688
# [2/100] training 36.7% loss=0.36823, acc=0.84375
# [2/100] training 36.8% loss=0.43209, acc=0.76562
# [2/100] training 37.1% loss=0.49334, acc=0.78125
# [2/100] training 37.3% loss=0.60657, acc=0.64062
# [2/100] training 37.4% loss=0.46395, acc=0.79688
# [2/100] training 37.6% loss=0.46986, acc=0.79688
# [2/100] training 37.7% loss=0.43840, acc=0.84375
# [2/100] training 37.9% loss=0.43956, acc=0.81250
# [2/100] training 38.1% loss=0.56976, acc=0.68750
# [2/100] training 38.3% loss=0.48059, acc=0.73438
# [2/100] training 38.4% loss=0.41721, acc=0.84375
# [2/100] training 38.6% loss=0.47609, acc=0.70312
# [2/100] training 38.8% loss=0.52659, acc=0.71875
# [2/100] training 38.9% loss=0.50591, acc=0.70312
# [2/100] training 39.1% loss=0.47551, acc=0.71875
# [2/100] training 39.3% loss=0.43744, acc=0.82812
# [2/100] training 39.5% loss=0.52883, acc=0.68750
# [2/100] training 39.6% loss=0.50219, acc=0.76562
# [2/100] training 39.8% loss=0.44801, acc=0.78125
# [2/100] training 40.0% loss=0.48671, acc=0.78125
# [2/100] training 40.1% loss=0.56982, acc=0.68750
# [2/100] training 40.4% loss=0.46946, acc=0.75000
# [2/100] training 40.5% loss=0.40105, acc=0.81250
# [2/100] training 40.7% loss=0.40549, acc=0.79688
# [2/100] training 40.8% loss=0.43088, acc=0.76562
# [2/100] training 41.0% loss=0.46002, acc=0.78125
# [2/100] training 41.1% loss=0.50816, acc=0.71875
# [2/100] training 41.3% loss=0.46837, acc=0.75000
# [2/100] training 41.6% loss=0.47991, acc=0.73438
# [2/100] training 41.7% loss=0.62008, acc=0.65625
# [2/100] training 41.9% loss=0.59994, acc=0.59375
# [2/100] training 42.0% loss=0.39133, acc=0.81250
# [2/100] training 42.2% loss=0.48643, acc=0.76562
# [2/100] training 42.3% loss=0.51518, acc=0.76562
# [2/100] training 42.5% loss=0.39493, acc=0.81250
# [2/100] training 42.8% loss=0.44603, acc=0.78125
# [2/100] training 42.9% loss=0.40291, acc=0.81250
# [2/100] training 43.1% loss=0.48250, acc=0.78125
# [2/100] training 43.2% loss=0.46263, acc=0.78125
# [2/100] training 43.4% loss=0.54028, acc=0.71875
# [2/100] training 43.5% loss=0.43726, acc=0.84375
# [2/100] training 43.8% loss=0.50916, acc=0.75000
# [2/100] training 43.9% loss=0.48571, acc=0.73438
# [2/100] training 44.1% loss=0.38464, acc=0.81250
# [2/100] training 44.3% loss=0.55538, acc=0.73438
# [2/100] training 44.4% loss=0.52408, acc=0.73438
# [2/100] training 44.6% loss=0.47293, acc=0.76562
# [2/100] training 44.7% loss=0.46853, acc=0.73438
# [2/100] training 45.0% loss=0.40235, acc=0.85938
# [2/100] training 45.1% loss=0.58624, acc=0.73438
# [2/100] training 45.3% loss=0.51015, acc=0.78125
# [2/100] training 45.5% loss=0.37589, acc=0.84375
# [2/100] training 45.6% loss=0.50621, acc=0.78125
# [2/100] training 45.8% loss=0.38280, acc=0.81250
# [2/100] training 45.9% loss=0.42415, acc=0.79688
# [2/100] training 46.2% loss=0.41607, acc=0.79688
# [2/100] training 46.3% loss=0.46392, acc=0.79688
# [2/100] training 46.5% loss=0.56428, acc=0.68750
# [2/100] training 46.6% loss=0.53650, acc=0.68750
# [2/100] training 46.8% loss=0.45471, acc=0.84375
# [2/100] training 47.0% loss=0.60701, acc=0.67188
# [2/100] training 47.2% loss=0.56532, acc=0.67188
# [2/100] training 47.4% loss=0.42139, acc=0.71875
# [2/100] training 47.5% loss=0.50578, acc=0.73438
# [2/100] training 47.7% loss=0.46451, acc=0.81250
# [2/100] training 47.8% loss=0.44426, acc=0.82812
# [2/100] training 48.0% loss=0.55203, acc=0.78125
# [2/100] training 48.3% loss=0.40369, acc=0.81250
# [2/100] training 48.4% loss=0.44158, acc=0.79688
# [2/100] training 48.6% loss=0.41059, acc=0.78125
# [2/100] training 48.7% loss=0.54131, acc=0.70312
# [2/100] training 48.9% loss=0.57301, acc=0.70312
# [2/100] training 49.0% loss=0.40092, acc=0.84375
# [2/100] training 49.2% loss=0.43130, acc=0.75000
# [2/100] training 49.3% loss=0.45745, acc=0.79688
# [2/100] training 49.6% loss=0.47994, acc=0.71875
# [2/100] training 49.8% loss=0.45332, acc=0.76562
# [2/100] training 49.9% loss=0.52896, acc=0.76562
# [2/100] training 50.1% loss=0.42254, acc=0.82812
# [2/100] training 50.2% loss=0.49277, acc=0.75000
# [2/100] training 50.4% loss=0.51130, acc=0.79688
# [2/100] training 50.6% loss=0.57490, acc=0.67188
# [2/100] training 50.8% loss=0.54767, acc=0.75000
# [2/100] training 51.0% loss=0.68516, acc=0.65625
# [2/100] training 51.1% loss=0.46315, acc=0.78125
# [2/100] training 51.3% loss=0.49853, acc=0.73438
# [2/100] training 51.4% loss=0.50282, acc=0.78125
# [2/100] training 51.7% loss=0.62310, acc=0.60938
# [2/100] training 51.8% loss=0.42462, acc=0.79688
# [2/100] training 52.0% loss=0.49150, acc=0.79688
# [2/100] training 52.1% loss=0.60056, acc=0.71875
# [2/100] training 52.3% loss=0.52114, acc=0.75000
# [2/100] training 52.5% loss=0.43317, acc=0.82812
# [2/100] training 52.6% loss=0.53081, acc=0.71875
# [2/100] training 52.9% loss=0.61158, acc=0.59375
# [2/100] training 53.0% loss=0.51069, acc=0.73438
# [2/100] training 53.2% loss=0.51831, acc=0.78125
# [2/100] training 53.3% loss=0.35106, acc=0.85938
# [2/100] training 53.5% loss=0.57983, acc=0.75000
# [2/100] training 53.7% loss=0.38163, acc=0.84375
# [2/100] training 53.8% loss=0.43061, acc=0.76562
# [2/100] training 54.1% loss=0.56632, acc=0.73438
# [2/100] training 54.2% loss=0.54639, acc=0.75000
# [2/100] training 54.4% loss=0.48997, acc=0.67188
# [2/100] training 54.5% loss=0.49588, acc=0.73438
# [2/100] training 54.7% loss=0.52330, acc=0.71875
# [2/100] training 54.8% loss=0.37774, acc=0.82812
# [2/100] training 55.1% loss=0.38162, acc=0.87500
# [2/100] training 55.3% loss=0.41589, acc=0.81250
# [2/100] training 55.4% loss=0.53534, acc=0.70312
# [2/100] training 55.6% loss=0.48000, acc=0.79688
# [2/100] training 55.7% loss=0.47988, acc=0.81250
# [2/100] training 55.9% loss=0.44669, acc=0.76562
# [2/100] training 56.0% loss=0.50580, acc=0.73438
# [2/100] training 56.3% loss=0.56944, acc=0.67188
# [2/100] training 56.5% loss=0.53618, acc=0.68750
# [2/100] training 56.6% loss=0.44675, acc=0.78125
# [2/100] training 56.8% loss=0.49120, acc=0.75000
# [2/100] training 56.9% loss=0.42354, acc=0.81250
# [2/100] training 57.1% loss=0.50069, acc=0.82812
# [2/100] training 57.2% loss=0.51213, acc=0.70312
# [2/100] training 57.5% loss=0.58836, acc=0.67188
# [2/100] training 57.6% loss=0.52932, acc=0.67188
# [2/100] training 57.8% loss=0.47075, acc=0.73438
# [2/100] training 58.0% loss=0.41656, acc=0.81250
# [2/100] training 58.1% loss=0.43254, acc=0.82812
# [2/100] training 58.3% loss=0.48401, acc=0.73438
# [2/100] training 58.4% loss=0.42876, acc=0.79688
# [2/100] training 58.7% loss=0.53976, acc=0.70312
# [2/100] training 58.8% loss=0.64389, acc=0.68750
# [2/100] training 59.0% loss=0.46624, acc=0.82812
# [2/100] training 59.2% loss=0.56589, acc=0.67188
# [2/100] training 59.3% loss=0.55532, acc=0.62500
# [2/100] training 59.5% loss=0.52641, acc=0.71875
# [2/100] training 59.7% loss=0.54644, acc=0.68750
# [2/100] training 59.9% loss=0.50463, acc=0.71875
# [2/100] training 60.0% loss=0.50636, acc=0.70312
# [2/100] training 60.2% loss=0.51780, acc=0.70312
# [2/100] training 60.3% loss=0.51801, acc=0.67188
# [2/100] training 60.5% loss=0.46009, acc=0.73438
# [2/100] training 60.8% loss=0.45076, acc=0.75000
# [2/100] training 60.9% loss=0.52304, acc=0.70312
# [2/100] training 61.1% loss=0.63883, acc=0.65625
# [2/100] training 61.2% loss=0.45909, acc=0.79688
# [2/100] training 61.4% loss=0.47787, acc=0.78125
# [2/100] training 61.5% loss=0.55646, acc=0.70312
# [2/100] training 61.7% loss=0.58256, acc=0.68750
# [2/100] training 62.0% loss=0.49260, acc=0.76562
# [2/100] training 62.1% loss=0.53305, acc=0.68750
# [2/100] training 62.3% loss=0.46657, acc=0.78125
# [2/100] training 62.4% loss=0.46595, acc=0.71875
# [2/100] training 62.6% loss=0.46203, acc=0.81250
# [2/100] training 62.7% loss=0.39716, acc=0.85938
# [2/100] training 62.9% loss=0.58505, acc=0.68750
# [2/100] training 63.1% loss=0.39447, acc=0.84375
# [2/100] training 63.3% loss=0.55059, acc=0.78125
# [2/100] training 63.5% loss=0.49661, acc=0.76562
# [2/100] training 63.6% loss=0.47955, acc=0.71875
# [2/100] training 63.8% loss=0.42009, acc=0.79688
# [2/100] training 63.9% loss=0.45468, acc=0.79688
# [2/100] training 64.2% loss=0.42084, acc=0.78125
# [2/100] training 64.3% loss=0.48108, acc=0.70312
# [2/100] training 64.5% loss=0.49242, acc=0.73438
# [2/100] training 64.7% loss=0.41513, acc=0.79688
# [2/100] training 64.8% loss=0.67374, acc=0.73438
# [2/100] training 65.0% loss=0.52377, acc=0.71875
# [2/100] training 65.1% loss=0.51926, acc=0.78125
# [2/100] training 65.4% loss=0.49400, acc=0.75000
# [2/100] training 65.5% loss=0.43036, acc=0.84375
# [2/100] training 65.7% loss=0.43993, acc=0.75000
# [2/100] training 65.8% loss=0.55401, acc=0.75000
# [2/100] training 66.0% loss=0.45325, acc=0.79688
# [2/100] training 66.2% loss=0.45851, acc=0.75000
# [2/100] training 66.3% loss=0.45382, acc=0.81250
# [2/100] training 66.6% loss=0.43510, acc=0.76562
# [2/100] training 66.7% loss=0.39049, acc=0.84375
# [2/100] training 66.9% loss=0.40936, acc=0.84375
# [2/100] training 67.0% loss=0.53945, acc=0.73438
# [2/100] training 67.2% loss=0.47156, acc=0.75000
# [2/100] training 67.4% loss=0.47397, acc=0.78125
# [2/100] training 67.6% loss=0.41711, acc=0.82812
# [2/100] training 67.8% loss=0.47392, acc=0.73438
# [2/100] training 67.9% loss=0.46250, acc=0.82812
# [2/100] training 68.1% loss=0.37136, acc=0.79688
# [2/100] training 68.2% loss=0.38569, acc=0.81250
# [2/100] training 68.4% loss=0.52633, acc=0.75000
# [2/100] training 68.5% loss=0.56839, acc=0.70312
# [2/100] training 68.8% loss=0.53724, acc=0.71875
# [2/100] training 69.0% loss=0.54411, acc=0.60938
# [2/100] training 69.1% loss=0.48836, acc=0.79688
# [2/100] training 69.3% loss=0.43347, acc=0.82812
# [2/100] training 69.4% loss=0.37533, acc=0.82812
# [2/100] training 69.6% loss=0.58447, acc=0.70312
# [2/100] training 69.7% loss=0.47189, acc=0.73438
# [2/100] training 70.0% loss=0.52872, acc=0.70312
# [2/100] training 70.2% loss=0.49627, acc=0.79688
# [2/100] training 70.3% loss=0.45384, acc=0.79688
# [2/100] training 70.5% loss=0.55626, acc=0.64062
# [2/100] training 70.6% loss=0.53141, acc=0.79688
# [2/100] training 70.8% loss=0.49390, acc=0.76562
# [2/100] training 71.0% loss=0.46834, acc=0.76562
# [2/100] training 71.2% loss=0.44969, acc=0.78125
# [2/100] training 71.3% loss=0.38482, acc=0.79688
# [2/100] training 71.5% loss=0.59018, acc=0.68750
# [2/100] training 71.7% loss=0.61228, acc=0.60938
# [2/100] training 71.8% loss=0.43756, acc=0.79688
# [2/100] training 72.0% loss=0.51015, acc=0.68750
# [2/100] training 72.2% loss=0.42506, acc=0.78125
# [2/100] training 72.4% loss=0.63919, acc=0.81250
# [2/100] training 72.5% loss=0.67337, acc=0.68750
# [2/100] training 72.7% loss=0.51301, acc=0.71875
# [2/100] training 72.9% loss=0.53263, acc=0.75000
# [2/100] training 73.0% loss=0.55823, acc=0.68750
# [2/100] training 73.3% loss=0.52828, acc=0.65625
# [2/100] training 73.4% loss=0.47384, acc=0.71875
# [2/100] training 73.6% loss=0.43113, acc=0.76562
# [2/100] training 73.7% loss=0.46741, acc=0.73438
# [2/100] training 73.9% loss=0.48563, acc=0.78125
# [2/100] training 74.0% loss=0.54135, acc=0.73438
# [2/100] training 74.2% loss=0.53437, acc=0.64062
# [2/100] training 74.5% loss=0.42911, acc=0.78125
# [2/100] training 74.6% loss=0.58178, acc=0.70312
# [2/100] training 74.8% loss=0.57691, acc=0.71875
# [2/100] training 74.9% loss=0.61207, acc=0.65625
# [2/100] training 75.1% loss=0.49373, acc=0.79688
# [2/100] training 75.2% loss=0.48925, acc=0.78125
# [2/100] training 75.4% loss=0.47850, acc=0.76562
# [2/100] training 75.7% loss=0.46094, acc=0.79688
# [2/100] training 75.8% loss=0.56059, acc=0.70312
# [2/100] training 76.0% loss=0.41827, acc=0.82812
# [2/100] training 76.1% loss=0.53367, acc=0.75000
# [2/100] training 76.3% loss=0.50784, acc=0.70312
# [2/100] training 76.4% loss=0.47242, acc=0.79688
# [2/100] training 76.7% loss=0.52186, acc=0.76562
# [2/100] training 76.8% loss=0.46880, acc=0.75000
# [2/100] training 77.0% loss=0.42206, acc=0.82812
# [2/100] training 77.2% loss=0.51594, acc=0.84375
# [2/100] training 77.3% loss=0.39764, acc=0.84375
# [2/100] training 77.5% loss=0.50450, acc=0.75000
# [2/100] training 77.6% loss=0.46843, acc=0.73438
# [2/100] training 77.9% loss=0.49315, acc=0.68750
# [2/100] training 78.0% loss=0.52198, acc=0.70312
# [2/100] training 78.2% loss=0.56121, acc=0.67188
# [2/100] training 78.4% loss=0.44380, acc=0.82812
# [2/100] training 78.5% loss=0.52388, acc=0.76562
# [2/100] training 78.7% loss=0.51745, acc=0.71875
# [2/100] training 78.8% loss=0.44431, acc=0.78125
# [2/100] training 79.1% loss=0.39270, acc=0.81250
# [2/100] training 79.2% loss=0.57303, acc=0.76562
# [2/100] training 79.4% loss=0.57865, acc=0.68750
# [2/100] training 79.5% loss=0.42967, acc=0.76562
# [2/100] training 79.7% loss=0.51796, acc=0.70312
# [2/100] training 79.9% loss=0.45204, acc=0.76562
# [2/100] training 80.1% loss=0.50368, acc=0.73438
# [2/100] training 80.3% loss=0.49961, acc=0.78125
# [2/100] training 80.4% loss=0.43672, acc=0.79688
# [2/100] training 80.6% loss=0.46817, acc=0.75000
# [2/100] training 80.7% loss=0.52006, acc=0.73438
# [2/100] training 80.9% loss=0.52088, acc=0.75000
# [2/100] training 81.2% loss=0.44139, acc=0.75000
# [2/100] training 81.3% loss=0.48534, acc=0.76562
# [2/100] training 81.5% loss=0.39737, acc=0.78125
# [2/100] training 81.6% loss=0.46069, acc=0.76562
# [2/100] training 81.8% loss=0.40453, acc=0.79688
# [2/100] training 81.9% loss=0.53969, acc=0.71875
# [2/100] training 82.1% loss=0.39431, acc=0.84375
# [2/100] training 82.2% loss=0.73057, acc=0.67188
# [2/100] training 82.5% loss=0.52386, acc=0.78125
# [2/100] training 82.7% loss=0.40137, acc=0.82812
# [2/100] training 82.8% loss=0.49271, acc=0.76562
# [2/100] training 83.0% loss=0.43487, acc=0.81250
# [2/100] training 83.1% loss=0.44554, acc=0.76562
# [2/100] training 83.3% loss=0.52861, acc=0.82812
# [2/100] training 83.5% loss=0.54511, acc=0.71875
# [2/100] training 83.7% loss=0.51970, acc=0.76562
# [2/100] training 83.9% loss=0.43353, acc=0.79688
# [2/100] training 84.0% loss=0.45128, acc=0.68750
# [2/100] training 84.2% loss=0.46727, acc=0.76562
# [2/100] training 84.3% loss=0.38930, acc=0.76562
# [2/100] training 84.5% loss=0.50628, acc=0.75000
# [2/100] training 84.7% loss=0.52450, acc=0.73438
# [2/100] training 84.9% loss=0.48350, acc=0.73438
# [2/100] training 85.0% loss=0.40130, acc=0.79688
# [2/100] training 85.2% loss=0.52042, acc=0.76562
# [2/100] training 85.4% loss=0.47738, acc=0.81250
# [2/100] training 85.5% loss=0.42539, acc=0.79688
# [2/100] training 85.8% loss=0.46416, acc=0.71875
# [2/100] training 85.9% loss=0.47203, acc=0.76562
# [2/100] training 86.1% loss=0.39731, acc=0.82812
# [2/100] training 86.2% loss=0.45954, acc=0.75000
# [2/100] training 86.4% loss=0.54594, acc=0.71875
# [2/100] training 86.6% loss=0.54307, acc=0.70312
# [2/100] training 86.7% loss=0.59530, acc=0.65625
# [2/100] training 87.0% loss=0.40592, acc=0.78125
# [2/100] training 87.1% loss=0.44237, acc=0.75000
# [2/100] training 87.3% loss=0.43367, acc=0.79688
# [2/100] training 87.4% loss=0.48294, acc=0.79688
# [2/100] training 87.6% loss=0.41797, acc=0.81250
# [2/100] training 87.7% loss=0.48566, acc=0.75000
# [2/100] training 87.9% loss=0.40474, acc=0.87500
# [2/100] training 88.2% loss=0.31717, acc=0.89062
# [2/100] training 88.3% loss=0.40046, acc=0.81250
# [2/100] training 88.5% loss=0.46477, acc=0.75000
# [2/100] training 88.6% loss=0.35503, acc=0.84375
# [2/100] training 88.8% loss=0.40542, acc=0.76562
# [2/100] training 88.9% loss=0.39776, acc=0.76562
# [2/100] training 89.2% loss=0.39749, acc=0.81250
# [2/100] training 89.4% loss=0.39766, acc=0.78125
# [2/100] training 89.5% loss=0.55512, acc=0.75000
# [2/100] training 89.7% loss=0.65160, acc=0.75000
# [2/100] training 89.8% loss=0.35400, acc=0.79688
# [2/100] training 90.0% loss=0.39427, acc=0.82812
# [2/100] training 90.1% loss=0.39375, acc=0.82812
# [2/100] training 90.4% loss=0.44221, acc=0.79688
# [2/100] training 90.5% loss=0.40862, acc=0.82812
# [2/100] training 90.7% loss=0.53017, acc=0.75000
# [2/100] training 90.9% loss=0.39379, acc=0.85938
# [2/100] training 91.0% loss=0.55509, acc=0.78125
# [2/100] training 91.2% loss=0.50886, acc=0.71875
# [2/100] training 91.3% loss=0.58639, acc=0.64062
# [2/100] training 91.6% loss=0.56732, acc=0.67188
# [2/100] training 91.7% loss=0.43868, acc=0.79688
# [2/100] training 91.9% loss=0.46616, acc=0.81250
# [2/100] training 92.1% loss=0.42888, acc=0.82812
# [2/100] training 92.2% loss=0.35605, acc=0.82812
# [2/100] training 92.4% loss=0.42866, acc=0.81250
# [2/100] training 92.6% loss=0.48662, acc=0.73438
# [2/100] training 92.8% loss=0.60125, acc=0.71875
# [2/100] training 92.9% loss=0.48760, acc=0.68750
# [2/100] training 93.1% loss=0.53824, acc=0.75000
# [2/100] training 93.2% loss=0.52311, acc=0.71875
# [2/100] training 93.4% loss=0.44604, acc=0.81250
# [2/100] training 93.7% loss=0.48032, acc=0.78125
# [2/100] training 93.8% loss=0.47063, acc=0.71875
# [2/100] training 94.0% loss=0.43444, acc=0.75000
# [2/100] training 94.1% loss=0.47380, acc=0.73438
# [2/100] training 94.3% loss=0.38495, acc=0.79688
# [2/100] training 94.4% loss=0.51459, acc=0.73438
# [2/100] training 94.6% loss=0.46910, acc=0.76562
# [2/100] training 94.9% loss=0.47634, acc=0.76562
# [2/100] training 95.0% loss=0.46605, acc=0.75000
# [2/100] training 95.2% loss=0.50974, acc=0.76562
# [2/100] training 95.3% loss=0.44465, acc=0.78125
# [2/100] training 95.5% loss=0.43025, acc=0.79688
# [2/100] training 95.6% loss=0.55475, acc=0.79688
# [2/100] training 95.8% loss=0.52371, acc=0.73438
# [2/100] training 96.0% loss=0.47023, acc=0.76562
# [2/100] training 96.2% loss=0.33866, acc=0.87500
# [2/100] training 96.4% loss=0.33168, acc=0.89062
# [2/100] training 96.5% loss=0.46670, acc=0.78125
# [2/100] training 96.7% loss=0.41993, acc=0.78125
# [2/100] training 96.8% loss=0.38276, acc=0.82812
# [2/100] training 97.1% loss=0.50511, acc=0.71875
# [2/100] training 97.2% loss=0.55350, acc=0.76562
# [2/100] training 97.4% loss=0.46110, acc=0.73438
# [2/100] training 97.6% loss=0.55863, acc=0.70312
# [2/100] training 97.7% loss=0.45389, acc=0.70312
# [2/100] training 97.9% loss=0.48267, acc=0.78125
# [2/100] training 98.0% loss=0.49926, acc=0.75000
# [2/100] training 98.3% loss=0.42594, acc=0.78125
# [2/100] training 98.4% loss=0.40677, acc=0.78125
# [2/100] training 98.6% loss=0.71573, acc=0.67188
# [2/100] training 98.7% loss=0.49338, acc=0.73438
# [2/100] training 98.9% loss=0.61427, acc=0.68750
# [2/100] training 99.1% loss=0.39988, acc=0.79688
# [2/100] training 99.2% loss=0.41857, acc=0.82812
# [2/100] training 99.5% loss=0.43845, acc=0.71875
# [2/100] training 99.6% loss=0.46165, acc=0.73438
# [2/100] training 99.8% loss=0.37185, acc=0.87500
# [2/100] training 99.9% loss=0.37440, acc=0.82812
# [2/100] testing 0.9% loss=0.40073, acc=0.81250
# [2/100] testing 1.8% loss=0.55537, acc=0.71875
# [2/100] testing 2.2% loss=0.43933, acc=0.76562
# [2/100] testing 3.1% loss=0.48375, acc=0.73438
# [2/100] testing 3.5% loss=0.42448, acc=0.81250
# [2/100] testing 4.4% loss=0.47973, acc=0.70312
# [2/100] testing 4.8% loss=0.54246, acc=0.67188
# [2/100] testing 5.7% loss=0.53191, acc=0.73438
# [2/100] testing 6.6% loss=0.61414, acc=0.64062
# [2/100] testing 7.0% loss=0.44634, acc=0.75000
# [2/100] testing 7.9% loss=0.59573, acc=0.67188
# [2/100] testing 8.3% loss=0.36374, acc=0.84375
# [2/100] testing 9.2% loss=0.57213, acc=0.65625
# [2/100] testing 9.7% loss=0.43688, acc=0.79688
# [2/100] testing 10.5% loss=0.63301, acc=0.67188
# [2/100] testing 11.0% loss=0.47256, acc=0.73438
# [2/100] testing 11.8% loss=0.43569, acc=0.79688
# [2/100] testing 12.7% loss=0.70164, acc=0.60938
# [2/100] testing 13.2% loss=0.49910, acc=0.73438
# [2/100] testing 14.0% loss=0.53551, acc=0.76562
# [2/100] testing 14.5% loss=0.62236, acc=0.62500
# [2/100] testing 15.4% loss=0.45351, acc=0.76562
# [2/100] testing 15.8% loss=0.47414, acc=0.73438
# [2/100] testing 16.7% loss=0.48900, acc=0.76562
# [2/100] testing 17.5% loss=0.48675, acc=0.75000
# [2/100] testing 18.0% loss=0.42559, acc=0.78125
# [2/100] testing 18.9% loss=0.38367, acc=0.82812
# [2/100] testing 19.3% loss=0.41885, acc=0.81250
# [2/100] testing 20.2% loss=0.63303, acc=0.65625
# [2/100] testing 20.6% loss=0.61179, acc=0.65625
# [2/100] testing 21.5% loss=0.43756, acc=0.79688
# [2/100] testing 21.9% loss=0.61932, acc=0.64062
# [2/100] testing 22.8% loss=0.45516, acc=0.78125
# [2/100] testing 23.7% loss=0.54586, acc=0.70312
# [2/100] testing 24.1% loss=0.55752, acc=0.70312
# [2/100] testing 25.0% loss=0.54729, acc=0.71875
# [2/100] testing 25.4% loss=0.40521, acc=0.82812
# [2/100] testing 26.3% loss=0.52202, acc=0.71875
# [2/100] testing 26.8% loss=0.53222, acc=0.71875
# [2/100] testing 27.6% loss=0.62018, acc=0.70312
# [2/100] testing 28.5% loss=0.55131, acc=0.68750
# [2/100] testing 29.0% loss=0.46687, acc=0.76562
# [2/100] testing 29.8% loss=0.53059, acc=0.70312
# [2/100] testing 30.3% loss=0.73600, acc=0.59375
# [2/100] testing 31.1% loss=0.54180, acc=0.75000
# [2/100] testing 31.6% loss=0.44683, acc=0.79688
# [2/100] testing 32.5% loss=0.54358, acc=0.71875
# [2/100] testing 32.9% loss=0.51482, acc=0.73438
# [2/100] testing 33.8% loss=0.54336, acc=0.73438
# [2/100] testing 34.7% loss=0.58182, acc=0.65625
# [2/100] testing 35.1% loss=0.45939, acc=0.76562
# [2/100] testing 36.0% loss=0.70886, acc=0.64062
# [2/100] testing 36.4% loss=0.56501, acc=0.76562
# [2/100] testing 37.3% loss=0.45813, acc=0.76562
# [2/100] testing 37.7% loss=0.55700, acc=0.68750
# [2/100] testing 38.6% loss=0.40842, acc=0.85938
# [2/100] testing 39.5% loss=0.69589, acc=0.71875
# [2/100] testing 39.9% loss=0.49358, acc=0.75000
# [2/100] testing 40.8% loss=0.47139, acc=0.78125
# [2/100] testing 41.2% loss=0.42377, acc=0.79688
# [2/100] testing 42.1% loss=0.58473, acc=0.70312
# [2/100] testing 42.5% loss=0.43692, acc=0.78125
# [2/100] testing 43.4% loss=0.50090, acc=0.76562
# [2/100] testing 43.9% loss=0.52109, acc=0.73438
# [2/100] testing 44.7% loss=0.55510, acc=0.75000
# [2/100] testing 45.6% loss=0.53199, acc=0.71875
# [2/100] testing 46.1% loss=0.43279, acc=0.75000
# [2/100] testing 46.9% loss=0.35781, acc=0.81250
# [2/100] testing 47.4% loss=0.43469, acc=0.76562
# [2/100] testing 48.3% loss=0.57273, acc=0.71875
# [2/100] testing 48.7% loss=0.61836, acc=0.70312
# [2/100] testing 49.6% loss=0.49433, acc=0.75000
# [2/100] testing 50.4% loss=0.35064, acc=0.81250
# [2/100] testing 50.9% loss=0.45666, acc=0.81250
# [2/100] testing 51.8% loss=0.50887, acc=0.75000
# [2/100] testing 52.2% loss=0.46213, acc=0.78125
# [2/100] testing 53.1% loss=0.55478, acc=0.67188
# [2/100] testing 53.5% loss=0.34685, acc=0.82812
# [2/100] testing 54.4% loss=0.58617, acc=0.67188
# [2/100] testing 54.8% loss=0.60355, acc=0.67188
# [2/100] testing 55.7% loss=0.48144, acc=0.78125
# [2/100] testing 56.6% loss=0.50573, acc=0.73438
# [2/100] testing 57.0% loss=0.64279, acc=0.65625
# [2/100] testing 57.9% loss=0.66925, acc=0.70312
# [2/100] testing 58.3% loss=0.59308, acc=0.70312
# [2/100] testing 59.2% loss=0.48714, acc=0.75000
# [2/100] testing 59.7% loss=0.54755, acc=0.75000
# [2/100] testing 60.5% loss=0.47850, acc=0.67188
# [2/100] testing 61.4% loss=0.35243, acc=0.82812
# [2/100] testing 61.9% loss=0.42126, acc=0.78125
# [2/100] testing 62.7% loss=0.45149, acc=0.76562
# [2/100] testing 63.2% loss=0.49874, acc=0.73438
# [2/100] testing 64.0% loss=0.44599, acc=0.78125
# [2/100] testing 64.5% loss=0.51186, acc=0.75000
# [2/100] testing 65.4% loss=0.49492, acc=0.73438
# [2/100] testing 65.8% loss=0.55144, acc=0.70312
# [2/100] testing 66.7% loss=0.39127, acc=0.82812
# [2/100] testing 67.6% loss=0.50082, acc=0.73438
# [2/100] testing 68.0% loss=0.55560, acc=0.73438
# [2/100] testing 68.9% loss=0.53508, acc=0.68750
# [2/100] testing 69.3% loss=0.45828, acc=0.79688
# [2/100] testing 70.2% loss=0.62761, acc=0.65625
# [2/100] testing 70.6% loss=0.49199, acc=0.78125
# [2/100] testing 71.5% loss=0.54213, acc=0.70312
# [2/100] testing 72.4% loss=0.49391, acc=0.75000
# [2/100] testing 72.8% loss=0.60188, acc=0.65625
# [2/100] testing 73.7% loss=0.37166, acc=0.85938
# [2/100] testing 74.1% loss=0.51822, acc=0.71875
# [2/100] testing 75.0% loss=0.44342, acc=0.78125
# [2/100] testing 75.4% loss=0.34679, acc=0.87500
# [2/100] testing 76.3% loss=0.29672, acc=0.85938
# [2/100] testing 76.8% loss=0.49892, acc=0.78125
# [2/100] testing 77.6% loss=0.49748, acc=0.68750
# [2/100] testing 78.5% loss=0.71621, acc=0.64062
# [2/100] testing 79.0% loss=0.46221, acc=0.78125
# [2/100] testing 79.8% loss=0.45340, acc=0.73438
# [2/100] testing 80.3% loss=0.46553, acc=0.76562
# [2/100] testing 81.2% loss=0.46608, acc=0.79688
# [2/100] testing 81.6% loss=0.44572, acc=0.81250
# [2/100] testing 82.5% loss=0.42282, acc=0.81250
# [2/100] testing 83.3% loss=0.37912, acc=0.79688
# [2/100] testing 83.8% loss=0.42353, acc=0.82812
# [2/100] testing 84.7% loss=0.39063, acc=0.81250
# [2/100] testing 85.1% loss=0.55470, acc=0.70312
# [2/100] testing 86.0% loss=0.54995, acc=0.71875
# [2/100] testing 86.4% loss=0.64182, acc=0.65625
# [2/100] testing 87.3% loss=0.62630, acc=0.67188
# [2/100] testing 87.7% loss=0.40520, acc=0.82812
# [2/100] testing 88.6% loss=0.32067, acc=0.89062
# [2/100] testing 89.5% loss=0.57803, acc=0.73438
# [2/100] testing 89.9% loss=0.51533, acc=0.75000
# [2/100] testing 90.8% loss=0.47752, acc=0.70312
# [2/100] testing 91.2% loss=0.36991, acc=0.82812
# [2/100] testing 92.1% loss=0.46236, acc=0.78125
# [2/100] testing 92.6% loss=0.51863, acc=0.73438
# [2/100] testing 93.4% loss=0.56505, acc=0.68750
# [2/100] testing 94.3% loss=0.38255, acc=0.78125
# [2/100] testing 94.7% loss=0.47944, acc=0.76562
# [2/100] testing 95.6% loss=0.48911, acc=0.75000
# [2/100] testing 96.1% loss=0.51620, acc=0.71875
# [2/100] testing 96.9% loss=0.43222, acc=0.81250
# [2/100] testing 97.4% loss=0.32647, acc=0.89062
# [2/100] testing 98.3% loss=0.50098, acc=0.71875
# [2/100] testing 98.7% loss=0.55588, acc=0.71875
# [2/100] testing 99.6% loss=0.48223, acc=0.76562
# [3/100] training 0.2% loss=0.54529, acc=0.70312
# [3/100] training 0.4% loss=0.60541, acc=0.56250
# [3/100] training 0.5% loss=0.50387, acc=0.73438
# [3/100] training 0.8% loss=0.44427, acc=0.82812
# [3/100] training 0.9% loss=0.45561, acc=0.76562
# [3/100] training 1.1% loss=0.41444, acc=0.82812
# [3/100] training 1.2% loss=0.44651, acc=0.76562
# [3/100] training 1.4% loss=0.42125, acc=0.79688
# [3/100] training 1.6% loss=0.34495, acc=0.84375
# [3/100] training 1.8% loss=0.43499, acc=0.79688
# [3/100] training 2.0% loss=0.45321, acc=0.75000
# [3/100] training 2.1% loss=0.46907, acc=0.76562
# [3/100] training 2.3% loss=0.37938, acc=0.84375
# [3/100] training 2.4% loss=0.51563, acc=0.71875
# [3/100] training 2.6% loss=0.52989, acc=0.70312
# [3/100] training 2.7% loss=0.48147, acc=0.71875
# [3/100] training 3.0% loss=0.61971, acc=0.67188
# [3/100] training 3.2% loss=0.48273, acc=0.75000
# [3/100] training 3.3% loss=0.46398, acc=0.73438
# [3/100] training 3.5% loss=0.50049, acc=0.71875
# [3/100] training 3.6% loss=0.68158, acc=0.71875
# [3/100] training 3.8% loss=0.38134, acc=0.84375
# [3/100] training 3.9% loss=0.56755, acc=0.68750
# [3/100] training 4.2% loss=0.37602, acc=0.81250
# [3/100] training 4.4% loss=0.43426, acc=0.79688
# [3/100] training 4.5% loss=0.39874, acc=0.79688
# [3/100] training 4.7% loss=0.52646, acc=0.76562
# [3/100] training 4.8% loss=0.46864, acc=0.75000
# [3/100] training 5.0% loss=0.45816, acc=0.78125
# [3/100] training 5.2% loss=0.57091, acc=0.71875
# [3/100] training 5.4% loss=0.41424, acc=0.84375
# [3/100] training 5.5% loss=0.43663, acc=0.78125
# [3/100] training 5.7% loss=0.42334, acc=0.81250
# [3/100] training 5.9% loss=0.54453, acc=0.73438
# [3/100] training 6.0% loss=0.48265, acc=0.76562
# [3/100] training 6.3% loss=0.51449, acc=0.73438
# [3/100] training 6.4% loss=0.45288, acc=0.70312
# [3/100] training 6.6% loss=0.44819, acc=0.82812
# [3/100] training 6.7% loss=0.46402, acc=0.68750
# [3/100] training 6.9% loss=0.41180, acc=0.82812
# [3/100] training 7.1% loss=0.52236, acc=0.73438
# [3/100] training 7.2% loss=0.44727, acc=0.81250
# [3/100] training 7.5% loss=0.39374, acc=0.85938
# [3/100] training 7.6% loss=0.32981, acc=0.89062
# [3/100] training 7.8% loss=0.45340, acc=0.75000
# [3/100] training 7.9% loss=0.40029, acc=0.78125
# [3/100] training 8.1% loss=0.36986, acc=0.78125
# [3/100] training 8.2% loss=0.29388, acc=0.84375
# [3/100] training 8.4% loss=0.60224, acc=0.73438
# [3/100] training 8.7% loss=0.42519, acc=0.79688
# [3/100] training 8.8% loss=0.47328, acc=0.70312
# [3/100] training 9.0% loss=0.49653, acc=0.79688
# [3/100] training 9.1% loss=0.51374, acc=0.73438
# [3/100] training 9.3% loss=0.61903, acc=0.70312
# [3/100] training 9.4% loss=0.48633, acc=0.78125
# [3/100] training 9.7% loss=0.50477, acc=0.75000
# [3/100] training 9.9% loss=0.46337, acc=0.76562
# [3/100] training 10.0% loss=0.39809, acc=0.81250
# [3/100] training 10.2% loss=0.46238, acc=0.71875
# [3/100] training 10.3% loss=0.39893, acc=0.81250
# [3/100] training 10.5% loss=0.46708, acc=0.81250
# [3/100] training 10.6% loss=0.43629, acc=0.79688
# [3/100] training 10.9% loss=0.48673, acc=0.78125
# [3/100] training 11.0% loss=0.42881, acc=0.78125
# [3/100] training 11.2% loss=0.45084, acc=0.75000
# [3/100] training 11.4% loss=0.52202, acc=0.73438
# [3/100] training 11.5% loss=0.64190, acc=0.64062
# [3/100] training 11.7% loss=0.47377, acc=0.79688
# [3/100] training 11.8% loss=0.38262, acc=0.82812
# [3/100] training 12.1% loss=0.47136, acc=0.76562
# [3/100] training 12.2% loss=0.46033, acc=0.75000
# [3/100] training 12.4% loss=0.43433, acc=0.78125
# [3/100] training 12.6% loss=0.37148, acc=0.78125
# [3/100] training 12.7% loss=0.64047, acc=0.75000
# [3/100] training 12.9% loss=0.49473, acc=0.73438
# [3/100] training 13.0% loss=0.44576, acc=0.78125
# [3/100] training 13.3% loss=0.48787, acc=0.76562
# [3/100] training 13.4% loss=0.52866, acc=0.73438
# [3/100] training 13.6% loss=0.44686, acc=0.79688
# [3/100] training 13.7% loss=0.61795, acc=0.71875
# [3/100] training 13.9% loss=0.55588, acc=0.73438
# [3/100] training 14.1% loss=0.44316, acc=0.81250
# [3/100] training 14.3% loss=0.42237, acc=0.84375
# [3/100] training 14.5% loss=0.29173, acc=0.85938
# [3/100] training 14.6% loss=0.52117, acc=0.75000
# [3/100] training 14.8% loss=0.42230, acc=0.78125
# [3/100] training 14.9% loss=0.48658, acc=0.75000
# [3/100] training 15.1% loss=0.47629, acc=0.76562
# [3/100] training 15.4% loss=0.51204, acc=0.64062
# [3/100] training 15.5% loss=0.57942, acc=0.70312
# [3/100] training 15.7% loss=0.65214, acc=0.62500
# [3/100] training 15.8% loss=0.44390, acc=0.84375
# [3/100] training 16.0% loss=0.58883, acc=0.67188
# [3/100] training 16.1% loss=0.56719, acc=0.68750
# [3/100] training 16.3% loss=0.50732, acc=0.79688
# [3/100] training 16.4% loss=0.49799, acc=0.79688
# [3/100] training 16.7% loss=0.54717, acc=0.70312
# [3/100] training 16.9% loss=0.55603, acc=0.71875
# [3/100] training 17.0% loss=0.50582, acc=0.71875
# [3/100] training 17.2% loss=0.37835, acc=0.85938
# [3/100] training 17.3% loss=0.46590, acc=0.75000
# [3/100] training 17.5% loss=0.47281, acc=0.75000
# [3/100] training 17.7% loss=0.56970, acc=0.70312
# [3/100] training 17.9% loss=0.46873, acc=0.76562
# [3/100] training 18.1% loss=0.47996, acc=0.73438
# [3/100] training 18.2% loss=0.53392, acc=0.78125
# [3/100] training 18.4% loss=0.47613, acc=0.75000
# [3/100] training 18.5% loss=0.46294, acc=0.76562
# [3/100] training 18.8% loss=0.42003, acc=0.76562
# [3/100] training 18.9% loss=0.42368, acc=0.78125
# [3/100] training 19.1% loss=0.54733, acc=0.73438
# [3/100] training 19.2% loss=0.34292, acc=0.84375
# [3/100] training 19.4% loss=0.35520, acc=0.85938
# [3/100] training 19.6% loss=0.54948, acc=0.70312
# [3/100] training 19.7% loss=0.45901, acc=0.76562
# [3/100] training 20.0% loss=0.38631, acc=0.79688
# [3/100] training 20.1% loss=0.41988, acc=0.90625
# [3/100] training 20.3% loss=0.50945, acc=0.75000
# [3/100] training 20.4% loss=0.46831, acc=0.73438
# [3/100] training 20.6% loss=0.46073, acc=0.76562
# [3/100] training 20.8% loss=0.41448, acc=0.82812
# [3/100] training 20.9% loss=0.45646, acc=0.70312
# [3/100] training 21.2% loss=0.49011, acc=0.73438
# [3/100] training 21.3% loss=0.46347, acc=0.78125
# [3/100] training 21.5% loss=0.39556, acc=0.85938
# [3/100] training 21.6% loss=0.25384, acc=0.93750
# [3/100] training 21.8% loss=0.39690, acc=0.85938
# [3/100] training 21.9% loss=0.51810, acc=0.75000
# [3/100] training 22.2% loss=0.39539, acc=0.82812
# [3/100] training 22.4% loss=0.53660, acc=0.76562
# [3/100] training 22.5% loss=0.42187, acc=0.76562
# [3/100] training 22.7% loss=0.37213, acc=0.82812
# [3/100] training 22.8% loss=0.47848, acc=0.76562
# [3/100] training 23.0% loss=0.43863, acc=0.68750
# [3/100] training 23.1% loss=0.53675, acc=0.64062
# [3/100] training 23.4% loss=0.60505, acc=0.70312
# [3/100] training 23.6% loss=0.40661, acc=0.85938
# [3/100] training 23.7% loss=0.61550, acc=0.59375
# [3/100] training 23.9% loss=0.47702, acc=0.75000
# [3/100] training 24.0% loss=0.47192, acc=0.81250
# [3/100] training 24.2% loss=0.51778, acc=0.79688
# [3/100] training 24.3% loss=0.53634, acc=0.79688
# [3/100] training 24.6% loss=0.40632, acc=0.76562
# [3/100] training 24.7% loss=0.53049, acc=0.64062
# [3/100] training 24.9% loss=0.45471, acc=0.79688
# [3/100] training 25.1% loss=0.57444, acc=0.76562
# [3/100] training 25.2% loss=0.37314, acc=0.85938
# [3/100] training 25.4% loss=0.40468, acc=0.76562
# [3/100] training 25.6% loss=0.45827, acc=0.79688
# [3/100] training 25.8% loss=0.60298, acc=0.73438
# [3/100] training 25.9% loss=0.40214, acc=0.79688
# [3/100] training 26.1% loss=0.50059, acc=0.78125
# [3/100] training 26.3% loss=0.50383, acc=0.79688
# [3/100] training 26.4% loss=0.43104, acc=0.81250
# [3/100] training 26.6% loss=0.45381, acc=0.75000
# [3/100] training 26.8% loss=0.54386, acc=0.73438
# [3/100] training 27.0% loss=0.46217, acc=0.73438
# [3/100] training 27.1% loss=0.41976, acc=0.81250
# [3/100] training 27.3% loss=0.49476, acc=0.73438
# [3/100] training 27.4% loss=0.39797, acc=0.79688
# [3/100] training 27.6% loss=0.48697, acc=0.82812
# [3/100] training 27.9% loss=0.47417, acc=0.76562
# [3/100] training 28.0% loss=0.50081, acc=0.78125
# [3/100] training 28.2% loss=0.48161, acc=0.75000
# [3/100] training 28.3% loss=0.39529, acc=0.87500
# [3/100] training 28.5% loss=0.47061, acc=0.79688
# [3/100] training 28.6% loss=0.56929, acc=0.70312
# [3/100] training 28.8% loss=0.30919, acc=0.90625
# [3/100] training 29.1% loss=0.52812, acc=0.65625
# [3/100] training 29.2% loss=0.46168, acc=0.73438
# [3/100] training 29.4% loss=0.53366, acc=0.71875
# [3/100] training 29.5% loss=0.37855, acc=0.81250
# [3/100] training 29.7% loss=0.50273, acc=0.78125
# [3/100] training 29.8% loss=0.35334, acc=0.87500
# [3/100] training 30.0% loss=0.51466, acc=0.73438
# [3/100] training 30.2% loss=0.52382, acc=0.75000
# [3/100] training 30.4% loss=0.38920, acc=0.78125
# [3/100] training 30.6% loss=0.58691, acc=0.67188
# [3/100] training 30.7% loss=0.37140, acc=0.84375
# [3/100] training 30.9% loss=0.54199, acc=0.73438
# [3/100] training 31.0% loss=0.42896, acc=0.82812
# [3/100] training 31.3% loss=0.47999, acc=0.70312
# [3/100] training 31.4% loss=0.52274, acc=0.73438
# [3/100] training 31.6% loss=0.50572, acc=0.65625
# [3/100] training 31.8% loss=0.34085, acc=0.89062
# [3/100] training 31.9% loss=0.42392, acc=0.85938
# [3/100] training 32.1% loss=0.48706, acc=0.76562
# [3/100] training 32.2% loss=0.42948, acc=0.79688
# [3/100] training 32.5% loss=0.39752, acc=0.84375
# [3/100] training 32.6% loss=0.38707, acc=0.84375
# [3/100] training 32.8% loss=0.36113, acc=0.85938
# [3/100] training 32.9% loss=0.59005, acc=0.71875
# [3/100] training 33.1% loss=0.47531, acc=0.73438
# [3/100] training 33.3% loss=0.41146, acc=0.82812
# [3/100] training 33.4% loss=0.52636, acc=0.82812
# [3/100] training 33.7% loss=0.50988, acc=0.71875
# [3/100] training 33.8% loss=0.55447, acc=0.67188
# [3/100] training 34.0% loss=0.50865, acc=0.73438
# [3/100] training 34.1% loss=0.44913, acc=0.71875
# [3/100] training 34.3% loss=0.44627, acc=0.81250
# [3/100] training 34.5% loss=0.60960, acc=0.68750
# [3/100] training 34.7% loss=0.35187, acc=0.85938
# [3/100] training 34.9% loss=0.47775, acc=0.67188
# [3/100] training 35.0% loss=0.44534, acc=0.70312
# [3/100] training 35.2% loss=0.51279, acc=0.79688
# [3/100] training 35.3% loss=0.52188, acc=0.76562
# [3/100] training 35.5% loss=0.59789, acc=0.60938
# [3/100] training 35.6% loss=0.46018, acc=0.68750
# [3/100] training 35.9% loss=0.52833, acc=0.62500
# [3/100] training 36.1% loss=0.49424, acc=0.71875
# [3/100] training 36.2% loss=0.51381, acc=0.76562
# [3/100] training 36.4% loss=0.40058, acc=0.82812
# [3/100] training 36.5% loss=0.51256, acc=0.76562
# [3/100] training 36.7% loss=0.37647, acc=0.81250
# [3/100] training 36.8% loss=0.42125, acc=0.81250
# [3/100] training 37.1% loss=0.45754, acc=0.78125
# [3/100] training 37.3% loss=0.52756, acc=0.73438
# [3/100] training 37.4% loss=0.40866, acc=0.84375
# [3/100] training 37.6% loss=0.41764, acc=0.78125
# [3/100] training 37.7% loss=0.36462, acc=0.89062
# [3/100] training 37.9% loss=0.37792, acc=0.89062
# [3/100] training 38.1% loss=0.59850, acc=0.65625
# [3/100] training 38.3% loss=0.45423, acc=0.82812
# [3/100] training 38.4% loss=0.37044, acc=0.84375
# [3/100] training 38.6% loss=0.43694, acc=0.78125
# [3/100] training 38.8% loss=0.45140, acc=0.82812
# [3/100] training 38.9% loss=0.44137, acc=0.76562
# [3/100] training 39.1% loss=0.42074, acc=0.76562
# [3/100] training 39.3% loss=0.41018, acc=0.81250
# [3/100] training 39.5% loss=0.47338, acc=0.81250
# [3/100] training 39.6% loss=0.52775, acc=0.76562
# [3/100] training 39.8% loss=0.36942, acc=0.78125
# [3/100] training 40.0% loss=0.46290, acc=0.79688
# [3/100] training 40.1% loss=0.55759, acc=0.67188
# [3/100] training 40.4% loss=0.45543, acc=0.73438
# [3/100] training 40.5% loss=0.38039, acc=0.81250
# [3/100] training 40.7% loss=0.38504, acc=0.87500
# [3/100] training 40.8% loss=0.44537, acc=0.75000
# [3/100] training 41.0% loss=0.45244, acc=0.79688
# [3/100] training 41.1% loss=0.52666, acc=0.67188
# [3/100] training 41.3% loss=0.43357, acc=0.76562
# [3/100] training 41.6% loss=0.46846, acc=0.75000
# [3/100] training 41.7% loss=0.68805, acc=0.65625
# [3/100] training 41.9% loss=0.45424, acc=0.76562
# [3/100] training 42.0% loss=0.46786, acc=0.71875
# [3/100] training 42.2% loss=0.52777, acc=0.76562
# [3/100] training 42.3% loss=0.44409, acc=0.78125
# [3/100] training 42.5% loss=0.34607, acc=0.81250
# [3/100] training 42.8% loss=0.41443, acc=0.82812
# [3/100] training 42.9% loss=0.34458, acc=0.85938
# [3/100] training 43.1% loss=0.43474, acc=0.78125
# [3/100] training 43.2% loss=0.48796, acc=0.79688
# [3/100] training 43.4% loss=0.50071, acc=0.76562
# [3/100] training 43.5% loss=0.42399, acc=0.79688
# [3/100] training 43.8% loss=0.53307, acc=0.78125
# [3/100] training 43.9% loss=0.50828, acc=0.75000
# [3/100] training 44.1% loss=0.36773, acc=0.81250
# [3/100] training 44.3% loss=0.53195, acc=0.70312
# [3/100] training 44.4% loss=0.47269, acc=0.75000
# [3/100] training 44.6% loss=0.42946, acc=0.78125
# [3/100] training 44.7% loss=0.43751, acc=0.79688
# [3/100] training 45.0% loss=0.35031, acc=0.87500
# [3/100] training 45.1% loss=0.61446, acc=0.71875
# [3/100] training 45.3% loss=0.51754, acc=0.75000
# [3/100] training 45.5% loss=0.31922, acc=0.85938
# [3/100] training 45.6% loss=0.42844, acc=0.82812
# [3/100] training 45.8% loss=0.41112, acc=0.79688
# [3/100] training 45.9% loss=0.47727, acc=0.78125
# [3/100] training 46.2% loss=0.39264, acc=0.79688
# [3/100] training 46.3% loss=0.37139, acc=0.84375
# [3/100] training 46.5% loss=0.52486, acc=0.76562
# [3/100] training 46.6% loss=0.42442, acc=0.78125
# [3/100] training 46.8% loss=0.45292, acc=0.81250
# [3/100] training 47.0% loss=0.42539, acc=0.75000
# [3/100] training 47.2% loss=0.50475, acc=0.73438
# [3/100] training 47.4% loss=0.39029, acc=0.79688
# [3/100] training 47.5% loss=0.42891, acc=0.79688
# [3/100] training 47.7% loss=0.36838, acc=0.85938
# [3/100] training 47.8% loss=0.36752, acc=0.82812
# [3/100] training 48.0% loss=0.60124, acc=0.73438
# [3/100] training 48.3% loss=0.33175, acc=0.89062
# [3/100] training 48.4% loss=0.46286, acc=0.76562
# [3/100] training 48.6% loss=0.39943, acc=0.81250
# [3/100] training 48.7% loss=0.51276, acc=0.73438
# [3/100] training 48.9% loss=0.54931, acc=0.70312
# [3/100] training 49.0% loss=0.40397, acc=0.84375
# [3/100] training 49.2% loss=0.40224, acc=0.81250
# [3/100] training 49.3% loss=0.45767, acc=0.73438
# [3/100] training 49.6% loss=0.45104, acc=0.78125
# [3/100] training 49.8% loss=0.43276, acc=0.76562
# [3/100] training 49.9% loss=0.45904, acc=0.75000
# [3/100] training 50.1% loss=0.44373, acc=0.81250
# [3/100] training 50.2% loss=0.39523, acc=0.87500
# [3/100] training 50.4% loss=0.46319, acc=0.78125
# [3/100] training 50.6% loss=0.49947, acc=0.79688
# [3/100] training 50.8% loss=0.59274, acc=0.68750
# [3/100] training 51.0% loss=0.50156, acc=0.75000
# [3/100] training 51.1% loss=0.38266, acc=0.84375
# [3/100] training 51.3% loss=0.41066, acc=0.82812
# [3/100] training 51.4% loss=0.47094, acc=0.76562
# [3/100] training 51.7% loss=0.55240, acc=0.67188
# [3/100] training 51.8% loss=0.38893, acc=0.84375
# [3/100] training 52.0% loss=0.43173, acc=0.82812
# [3/100] training 52.1% loss=0.53052, acc=0.81250
# [3/100] training 52.3% loss=0.43771, acc=0.75000
# [3/100] training 52.5% loss=0.41575, acc=0.82812
# [3/100] training 52.6% loss=0.42411, acc=0.78125
# [3/100] training 52.9% loss=0.73474, acc=0.60938
# [3/100] training 53.0% loss=0.46378, acc=0.76562
# [3/100] training 53.2% loss=0.57038, acc=0.82812
# [3/100] training 53.3% loss=0.33706, acc=0.89062
# [3/100] training 53.5% loss=0.48429, acc=0.76562
# [3/100] training 53.7% loss=0.43673, acc=0.81250
# [3/100] training 53.8% loss=0.44540, acc=0.73438
# [3/100] training 54.1% loss=0.45047, acc=0.76562
# [3/100] training 54.2% loss=0.48381, acc=0.76562
# [3/100] training 54.4% loss=0.43023, acc=0.81250
# [3/100] training 54.5% loss=0.43549, acc=0.76562
# [3/100] training 54.7% loss=0.44363, acc=0.81250
# [3/100] training 54.8% loss=0.30719, acc=0.82812
# [3/100] training 55.1% loss=0.33761, acc=0.82812
# [3/100] training 55.3% loss=0.27765, acc=0.90625
# [3/100] training 55.4% loss=0.46768, acc=0.79688
# [3/100] training 55.6% loss=0.45382, acc=0.73438
# [3/100] training 55.7% loss=0.44019, acc=0.84375
# [3/100] training 55.9% loss=0.39587, acc=0.76562
# [3/100] training 56.0% loss=0.50679, acc=0.75000
# [3/100] training 56.3% loss=0.60207, acc=0.70312
# [3/100] training 56.5% loss=0.52339, acc=0.75000
# [3/100] training 56.6% loss=0.41785, acc=0.81250
# [3/100] training 56.8% loss=0.58549, acc=0.67188
# [3/100] training 56.9% loss=0.44800, acc=0.78125
# [3/100] training 57.1% loss=0.47756, acc=0.76562
# [3/100] training 57.2% loss=0.46414, acc=0.76562
# [3/100] training 57.5% loss=0.45978, acc=0.79688
# [3/100] training 57.6% loss=0.48946, acc=0.70312
# [3/100] training 57.8% loss=0.38179, acc=0.78125
# [3/100] training 58.0% loss=0.35197, acc=0.89062
# [3/100] training 58.1% loss=0.42497, acc=0.82812
# [3/100] training 58.3% loss=0.36308, acc=0.79688
# [3/100] training 58.4% loss=0.44010, acc=0.81250
# [3/100] training 58.7% loss=0.46584, acc=0.70312
# [3/100] training 58.8% loss=0.59963, acc=0.75000
# [3/100] training 59.0% loss=0.44424, acc=0.84375
# [3/100] training 59.2% loss=0.47229, acc=0.73438
# [3/100] training 59.3% loss=0.42332, acc=0.71875
# [3/100] training 59.5% loss=0.51117, acc=0.73438
# [3/100] training 59.7% loss=0.57620, acc=0.73438
# [3/100] training 59.9% loss=0.43617, acc=0.81250
# [3/100] training 60.0% loss=0.38637, acc=0.82812
# [3/100] training 60.2% loss=0.55244, acc=0.65625
# [3/100] training 60.3% loss=0.49628, acc=0.68750
# [3/100] training 60.5% loss=0.37168, acc=0.84375
# [3/100] training 60.8% loss=0.47918, acc=0.73438
# [3/100] training 60.9% loss=0.41621, acc=0.75000
# [3/100] training 61.1% loss=0.80977, acc=0.68750
# [3/100] training 61.2% loss=0.37077, acc=0.79688
# [3/100] training 61.4% loss=0.45124, acc=0.76562
# [3/100] training 61.5% loss=0.53628, acc=0.73438
# [3/100] training 61.7% loss=0.58007, acc=0.65625
# [3/100] training 62.0% loss=0.53044, acc=0.75000
# [3/100] training 62.1% loss=0.55002, acc=0.64062
# [3/100] training 62.3% loss=0.46425, acc=0.79688
# [3/100] training 62.4% loss=0.48390, acc=0.78125
# [3/100] training 62.6% loss=0.47350, acc=0.79688
# [3/100] training 62.7% loss=0.36795, acc=0.89062
# [3/100] training 62.9% loss=0.49150, acc=0.73438
# [3/100] training 63.1% loss=0.49162, acc=0.79688
# [3/100] training 63.3% loss=0.51318, acc=0.75000
# [3/100] training 63.5% loss=0.49326, acc=0.75000
# [3/100] training 63.6% loss=0.41296, acc=0.78125
# [3/100] training 63.8% loss=0.44501, acc=0.76562
# [3/100] training 63.9% loss=0.45893, acc=0.71875
# [3/100] training 64.2% loss=0.37578, acc=0.84375
# [3/100] training 64.3% loss=0.42923, acc=0.78125
# [3/100] training 64.5% loss=0.57516, acc=0.70312
# [3/100] training 64.7% loss=0.44172, acc=0.73438
# [3/100] training 64.8% loss=0.68567, acc=0.64062
# [3/100] training 65.0% loss=0.49893, acc=0.73438
# [3/100] training 65.1% loss=0.47283, acc=0.78125
# [3/100] training 65.4% loss=0.50653, acc=0.75000
# [3/100] training 65.5% loss=0.43401, acc=0.81250
# [3/100] training 65.7% loss=0.36413, acc=0.82812
# [3/100] training 65.8% loss=0.57917, acc=0.73438
# [3/100] training 66.0% loss=0.47368, acc=0.78125
# [3/100] training 66.2% loss=0.39866, acc=0.79688
# [3/100] training 66.3% loss=0.48816, acc=0.75000
# [3/100] training 66.6% loss=0.46632, acc=0.75000
# [3/100] training 66.7% loss=0.36918, acc=0.79688
# [3/100] training 66.9% loss=0.44007, acc=0.78125
# [3/100] training 67.0% loss=0.48021, acc=0.78125
# [3/100] training 67.2% loss=0.40458, acc=0.75000
# [3/100] training 67.4% loss=0.42308, acc=0.78125
# [3/100] training 67.6% loss=0.40622, acc=0.82812
# [3/100] training 67.8% loss=0.46536, acc=0.73438
# [3/100] training 67.9% loss=0.46742, acc=0.78125
# [3/100] training 68.1% loss=0.36224, acc=0.82812
# [3/100] training 68.2% loss=0.36804, acc=0.84375
# [3/100] training 68.4% loss=0.34960, acc=0.85938
# [3/100] training 68.5% loss=0.60021, acc=0.65625
# [3/100] training 68.8% loss=0.49265, acc=0.71875
# [3/100] training 69.0% loss=0.45855, acc=0.81250
# [3/100] training 69.1% loss=0.32573, acc=0.90625
# [3/100] training 69.3% loss=0.40787, acc=0.82812
# [3/100] training 69.4% loss=0.44287, acc=0.78125
# [3/100] training 69.6% loss=0.42869, acc=0.76562
# [3/100] training 69.7% loss=0.47457, acc=0.78125
# [3/100] training 70.0% loss=0.46900, acc=0.71875
# [3/100] training 70.2% loss=0.44034, acc=0.81250
# [3/100] training 70.3% loss=0.39327, acc=0.84375
# [3/100] training 70.5% loss=0.50160, acc=0.70312
# [3/100] training 70.6% loss=0.48341, acc=0.76562
# [3/100] training 70.8% loss=0.58770, acc=0.71875
# [3/100] training 71.0% loss=0.49436, acc=0.75000
# [3/100] training 71.2% loss=0.39066, acc=0.85938
# [3/100] training 71.3% loss=0.36717, acc=0.84375
# [3/100] training 71.5% loss=0.53694, acc=0.73438
# [3/100] training 71.7% loss=0.62163, acc=0.67188
# [3/100] training 71.8% loss=0.44144, acc=0.79688
# [3/100] training 72.0% loss=0.43275, acc=0.73438
# [3/100] training 72.2% loss=0.43776, acc=0.76562
# [3/100] training 72.4% loss=0.50566, acc=0.75000
# [3/100] training 72.5% loss=0.55762, acc=0.70312
# [3/100] training 72.7% loss=0.52081, acc=0.71875
# [3/100] training 72.9% loss=0.48063, acc=0.78125
# [3/100] training 73.0% loss=0.58192, acc=0.67188
# [3/100] training 73.3% loss=0.44974, acc=0.81250
# [3/100] training 73.4% loss=0.38957, acc=0.84375
# [3/100] training 73.6% loss=0.33388, acc=0.87500
# [3/100] training 73.7% loss=0.33714, acc=0.79688
# [3/100] training 73.9% loss=0.45514, acc=0.82812
# [3/100] training 74.0% loss=0.53276, acc=0.73438
# [3/100] training 74.2% loss=0.47754, acc=0.70312
# [3/100] training 74.5% loss=0.42251, acc=0.76562
# [3/100] training 74.6% loss=0.54595, acc=0.70312
# [3/100] training 74.8% loss=0.58723, acc=0.67188
# [3/100] training 74.9% loss=0.52916, acc=0.68750
# [3/100] training 75.1% loss=0.49006, acc=0.79688
# [3/100] training 75.2% loss=0.44218, acc=0.79688
# [3/100] training 75.4% loss=0.40687, acc=0.76562
# [3/100] training 75.7% loss=0.43818, acc=0.79688
# [3/100] training 75.8% loss=0.53962, acc=0.70312
# [3/100] training 76.0% loss=0.37983, acc=0.81250
# [3/100] training 76.1% loss=0.45896, acc=0.78125
# [3/100] training 76.3% loss=0.45620, acc=0.76562
# [3/100] training 76.4% loss=0.46237, acc=0.78125
# [3/100] training 76.7% loss=0.47187, acc=0.78125
# [3/100] training 76.8% loss=0.39462, acc=0.81250
# [3/100] training 77.0% loss=0.40281, acc=0.76562
# [3/100] training 77.2% loss=0.55386, acc=0.79688
# [3/100] training 77.3% loss=0.32952, acc=0.87500
# [3/100] training 77.5% loss=0.45722, acc=0.78125
# [3/100] training 77.6% loss=0.40742, acc=0.81250
# [3/100] training 77.9% loss=0.48199, acc=0.71875
# [3/100] training 78.0% loss=0.48782, acc=0.71875
# [3/100] training 78.2% loss=0.47338, acc=0.76562
# [3/100] training 78.4% loss=0.37802, acc=0.82812
# [3/100] training 78.5% loss=0.51704, acc=0.71875
# [3/100] training 78.7% loss=0.39482, acc=0.81250
# [3/100] training 78.8% loss=0.40870, acc=0.76562
# [3/100] training 79.1% loss=0.35101, acc=0.82812
# [3/100] training 79.2% loss=0.47407, acc=0.79688
# [3/100] training 79.4% loss=0.54622, acc=0.75000
# [3/100] training 79.5% loss=0.39343, acc=0.78125
# [3/100] training 79.7% loss=0.39080, acc=0.78125
# [3/100] training 79.9% loss=0.43015, acc=0.78125
# [3/100] training 80.1% loss=0.41066, acc=0.82812
# [3/100] training 80.3% loss=0.43819, acc=0.76562
# [3/100] training 80.4% loss=0.41656, acc=0.78125
# [3/100] training 80.6% loss=0.46299, acc=0.70312
# [3/100] training 80.7% loss=0.48018, acc=0.78125
# [3/100] training 80.9% loss=0.44634, acc=0.79688
# [3/100] training 81.2% loss=0.45489, acc=0.75000
# [3/100] training 81.3% loss=0.45289, acc=0.76562
# [3/100] training 81.5% loss=0.31530, acc=0.92188
# [3/100] training 81.6% loss=0.42748, acc=0.73438
# [3/100] training 81.8% loss=0.42392, acc=0.84375
# [3/100] training 81.9% loss=0.52270, acc=0.79688
# [3/100] training 82.1% loss=0.38265, acc=0.82812
# [3/100] training 82.2% loss=0.64651, acc=0.70312
# [3/100] training 82.5% loss=0.43329, acc=0.78125
# [3/100] training 82.7% loss=0.36722, acc=0.85938
# [3/100] training 82.8% loss=0.44442, acc=0.81250
# [3/100] training 83.0% loss=0.50677, acc=0.65625
# [3/100] training 83.1% loss=0.46789, acc=0.78125
# [3/100] training 83.3% loss=0.45192, acc=0.79688
# [3/100] training 83.5% loss=0.48218, acc=0.75000
# [3/100] training 83.7% loss=0.54305, acc=0.71875
# [3/100] training 83.9% loss=0.42956, acc=0.79688
# [3/100] training 84.0% loss=0.43660, acc=0.81250
# [3/100] training 84.2% loss=0.45090, acc=0.78125
# [3/100] training 84.3% loss=0.42147, acc=0.78125
# [3/100] training 84.5% loss=0.42234, acc=0.78125
# [3/100] training 84.7% loss=0.53786, acc=0.70312
# [3/100] training 84.9% loss=0.47301, acc=0.75000
# [3/100] training 85.0% loss=0.39659, acc=0.85938
# [3/100] training 85.2% loss=0.49484, acc=0.75000
# [3/100] training 85.4% loss=0.53108, acc=0.81250
# [3/100] training 85.5% loss=0.40462, acc=0.78125
# [3/100] training 85.8% loss=0.42354, acc=0.76562
# [3/100] training 85.9% loss=0.50352, acc=0.73438
# [3/100] training 86.1% loss=0.35566, acc=0.84375
# [3/100] training 86.2% loss=0.51218, acc=0.73438
# [3/100] training 86.4% loss=0.50407, acc=0.73438
# [3/100] training 86.6% loss=0.53251, acc=0.75000
# [3/100] training 86.7% loss=0.54162, acc=0.75000
# [3/100] training 87.0% loss=0.40200, acc=0.81250
# [3/100] training 87.1% loss=0.43603, acc=0.75000
# [3/100] training 87.3% loss=0.41960, acc=0.79688
# [3/100] training 87.4% loss=0.43295, acc=0.78125
# [3/100] training 87.6% loss=0.42660, acc=0.82812
# [3/100] training 87.7% loss=0.45111, acc=0.81250
# [3/100] training 87.9% loss=0.36830, acc=0.84375
# [3/100] training 88.2% loss=0.28618, acc=0.87500
# [3/100] training 88.3% loss=0.43170, acc=0.78125
# [3/100] training 88.5% loss=0.44024, acc=0.78125
# [3/100] training 88.6% loss=0.27526, acc=0.92188
# [3/100] training 88.8% loss=0.49953, acc=0.78125
# [3/100] training 88.9% loss=0.42764, acc=0.81250
# [3/100] training 89.2% loss=0.40285, acc=0.82812
# [3/100] training 89.4% loss=0.35860, acc=0.81250
# [3/100] training 89.5% loss=0.51358, acc=0.70312
# [3/100] training 89.7% loss=0.54643, acc=0.75000
# [3/100] training 89.8% loss=0.37542, acc=0.82812
# [3/100] training 90.0% loss=0.39018, acc=0.78125
# [3/100] training 90.1% loss=0.42098, acc=0.78125
# [3/100] training 90.4% loss=0.45252, acc=0.81250
# [3/100] training 90.5% loss=0.35737, acc=0.81250
# [3/100] training 90.7% loss=0.57870, acc=0.68750
# [3/100] training 90.9% loss=0.37714, acc=0.87500
# [3/100] training 91.0% loss=0.53750, acc=0.79688
# [3/100] training 91.2% loss=0.54176, acc=0.68750
# [3/100] training 91.3% loss=0.53917, acc=0.73438
# [3/100] training 91.6% loss=0.61061, acc=0.60938
# [3/100] training 91.7% loss=0.46686, acc=0.71875
# [3/100] training 91.9% loss=0.41941, acc=0.82812
# [3/100] training 92.1% loss=0.36234, acc=0.84375
# [3/100] training 92.2% loss=0.34865, acc=0.78125
# [3/100] training 92.4% loss=0.38172, acc=0.82812
# [3/100] training 92.6% loss=0.53117, acc=0.67188
# [3/100] training 92.8% loss=0.53597, acc=0.70312
# [3/100] training 92.9% loss=0.49545, acc=0.68750
# [3/100] training 93.1% loss=0.46154, acc=0.79688
# [3/100] training 93.2% loss=0.45985, acc=0.76562
# [3/100] training 93.4% loss=0.37649, acc=0.84375
# [3/100] training 93.7% loss=0.52173, acc=0.70312
# [3/100] training 93.8% loss=0.48485, acc=0.71875
# [3/100] training 94.0% loss=0.34329, acc=0.85938
# [3/100] training 94.1% loss=0.53305, acc=0.75000
# [3/100] training 94.3% loss=0.36318, acc=0.81250
# [3/100] training 94.4% loss=0.49691, acc=0.75000
# [3/100] training 94.6% loss=0.39152, acc=0.84375
# [3/100] training 94.9% loss=0.45076, acc=0.81250
# [3/100] training 95.0% loss=0.42154, acc=0.81250
# [3/100] training 95.2% loss=0.47427, acc=0.75000
# [3/100] training 95.3% loss=0.45559, acc=0.78125
# [3/100] training 95.5% loss=0.36460, acc=0.84375
# [3/100] training 95.6% loss=0.53849, acc=0.81250
# [3/100] training 95.8% loss=0.50685, acc=0.76562
# [3/100] training 96.0% loss=0.38809, acc=0.81250
# [3/100] training 96.2% loss=0.29489, acc=0.89062
# [3/100] training 96.4% loss=0.35402, acc=0.84375
# [3/100] training 96.5% loss=0.43346, acc=0.79688
# [3/100] training 96.7% loss=0.34260, acc=0.84375
# [3/100] training 96.8% loss=0.40598, acc=0.78125
# [3/100] training 97.1% loss=0.39010, acc=0.79688
# [3/100] training 97.2% loss=0.46712, acc=0.78125
# [3/100] training 97.4% loss=0.44624, acc=0.78125
# [3/100] training 97.6% loss=0.46176, acc=0.79688
# [3/100] training 97.7% loss=0.41061, acc=0.75000
# [3/100] training 97.9% loss=0.39795, acc=0.81250
# [3/100] training 98.0% loss=0.52556, acc=0.71875
# [3/100] training 98.3% loss=0.44378, acc=0.79688
# [3/100] training 98.4% loss=0.38943, acc=0.81250
# [3/100] training 98.6% loss=0.58827, acc=0.65625
# [3/100] training 98.7% loss=0.52902, acc=0.70312
# [3/100] training 98.9% loss=0.52898, acc=0.70312
# [3/100] training 99.1% loss=0.42889, acc=0.78125
# [3/100] training 99.2% loss=0.40554, acc=0.81250
# [3/100] training 99.5% loss=0.43701, acc=0.76562
# [3/100] training 99.6% loss=0.41982, acc=0.75000
# [3/100] training 99.8% loss=0.35496, acc=0.82812
# [3/100] training 99.9% loss=0.33985, acc=0.85938
# [3/100] testing 0.9% loss=0.36075, acc=0.84375
# [3/100] testing 1.8% loss=0.48916, acc=0.71875
# [3/100] testing 2.2% loss=0.45399, acc=0.73438
# [3/100] testing 3.1% loss=0.42657, acc=0.75000
# [3/100] testing 3.5% loss=0.44590, acc=0.78125
# [3/100] testing 4.4% loss=0.42749, acc=0.76562
# [3/100] testing 4.8% loss=0.57465, acc=0.70312
# [3/100] testing 5.7% loss=0.47277, acc=0.82812
# [3/100] testing 6.6% loss=0.58356, acc=0.68750
# [3/100] testing 7.0% loss=0.42048, acc=0.79688
# [3/100] testing 7.9% loss=0.53665, acc=0.70312
# [3/100] testing 8.3% loss=0.33288, acc=0.89062
# [3/100] testing 9.2% loss=0.50098, acc=0.73438
# [3/100] testing 9.7% loss=0.43283, acc=0.79688
# [3/100] testing 10.5% loss=0.61948, acc=0.67188
# [3/100] testing 11.0% loss=0.42813, acc=0.76562
# [3/100] testing 11.8% loss=0.48314, acc=0.75000
# [3/100] testing 12.7% loss=0.57150, acc=0.70312
# [3/100] testing 13.2% loss=0.39814, acc=0.82812
# [3/100] testing 14.0% loss=0.47666, acc=0.75000
# [3/100] testing 14.5% loss=0.58592, acc=0.70312
# [3/100] testing 15.4% loss=0.48754, acc=0.78125
# [3/100] testing 15.8% loss=0.42702, acc=0.73438
# [3/100] testing 16.7% loss=0.47523, acc=0.76562
# [3/100] testing 17.5% loss=0.39566, acc=0.82812
# [3/100] testing 18.0% loss=0.37960, acc=0.79688
# [3/100] testing 18.9% loss=0.36167, acc=0.81250
# [3/100] testing 19.3% loss=0.40626, acc=0.82812
# [3/100] testing 20.2% loss=0.59810, acc=0.67188
# [3/100] testing 20.6% loss=0.58587, acc=0.71875
# [3/100] testing 21.5% loss=0.42655, acc=0.78125
# [3/100] testing 21.9% loss=0.57048, acc=0.71875
# [3/100] testing 22.8% loss=0.41398, acc=0.84375
# [3/100] testing 23.7% loss=0.55978, acc=0.75000
# [3/100] testing 24.1% loss=0.40569, acc=0.81250
# [3/100] testing 25.0% loss=0.52708, acc=0.73438
# [3/100] testing 25.4% loss=0.42014, acc=0.76562
# [3/100] testing 26.3% loss=0.50729, acc=0.75000
# [3/100] testing 26.8% loss=0.53004, acc=0.73438
# [3/100] testing 27.6% loss=0.58854, acc=0.70312
# [3/100] testing 28.5% loss=0.55614, acc=0.73438
# [3/100] testing 29.0% loss=0.46904, acc=0.75000
# [3/100] testing 29.8% loss=0.57567, acc=0.68750
# [3/100] testing 30.3% loss=0.62440, acc=0.70312
# [3/100] testing 31.1% loss=0.51187, acc=0.78125
# [3/100] testing 31.6% loss=0.39365, acc=0.82812
# [3/100] testing 32.5% loss=0.47308, acc=0.75000
# [3/100] testing 32.9% loss=0.45374, acc=0.75000
# [3/100] testing 33.8% loss=0.55690, acc=0.73438
# [3/100] testing 34.7% loss=0.49204, acc=0.73438
# [3/100] testing 35.1% loss=0.45513, acc=0.73438
# [3/100] testing 36.0% loss=0.63648, acc=0.67188
# [3/100] testing 36.4% loss=0.55694, acc=0.75000
# [3/100] testing 37.3% loss=0.44740, acc=0.73438
# [3/100] testing 37.7% loss=0.53017, acc=0.71875
# [3/100] testing 38.6% loss=0.38499, acc=0.87500
# [3/100] testing 39.5% loss=0.67055, acc=0.70312
# [3/100] testing 39.9% loss=0.45275, acc=0.76562
# [3/100] testing 40.8% loss=0.45737, acc=0.82812
# [3/100] testing 41.2% loss=0.39604, acc=0.84375
# [3/100] testing 42.1% loss=0.56891, acc=0.71875
# [3/100] testing 42.5% loss=0.46509, acc=0.71875
# [3/100] testing 43.4% loss=0.46354, acc=0.76562
# [3/100] testing 43.9% loss=0.48385, acc=0.76562
# [3/100] testing 44.7% loss=0.49021, acc=0.78125
# [3/100] testing 45.6% loss=0.45964, acc=0.81250
# [3/100] testing 46.1% loss=0.43728, acc=0.85938
# [3/100] testing 46.9% loss=0.38790, acc=0.79688
# [3/100] testing 47.4% loss=0.39566, acc=0.81250
# [3/100] testing 48.3% loss=0.53998, acc=0.78125
# [3/100] testing 48.7% loss=0.55385, acc=0.76562
# [3/100] testing 49.6% loss=0.45099, acc=0.79688
# [3/100] testing 50.4% loss=0.32741, acc=0.89062
# [3/100] testing 50.9% loss=0.45196, acc=0.84375
# [3/100] testing 51.8% loss=0.47498, acc=0.76562
# [3/100] testing 52.2% loss=0.46635, acc=0.79688
# [3/100] testing 53.1% loss=0.50776, acc=0.70312
# [3/100] testing 53.5% loss=0.38990, acc=0.81250
# [3/100] testing 54.4% loss=0.55756, acc=0.70312
# [3/100] testing 54.8% loss=0.57441, acc=0.73438
# [3/100] testing 55.7% loss=0.41080, acc=0.75000
# [3/100] testing 56.6% loss=0.49336, acc=0.81250
# [3/100] testing 57.0% loss=0.64064, acc=0.68750
# [3/100] testing 57.9% loss=0.57648, acc=0.70312
# [3/100] testing 58.3% loss=0.52913, acc=0.71875
# [3/100] testing 59.2% loss=0.50283, acc=0.78125
# [3/100] testing 59.7% loss=0.50152, acc=0.78125
# [3/100] testing 60.5% loss=0.47305, acc=0.75000
# [3/100] testing 61.4% loss=0.33219, acc=0.79688
# [3/100] testing 61.9% loss=0.41508, acc=0.79688
# [3/100] testing 62.7% loss=0.44494, acc=0.81250
# [3/100] testing 63.2% loss=0.48854, acc=0.79688
# [3/100] testing 64.0% loss=0.42404, acc=0.82812
# [3/100] testing 64.5% loss=0.46185, acc=0.81250
# [3/100] testing 65.4% loss=0.38736, acc=0.84375
# [3/100] testing 65.8% loss=0.53724, acc=0.67188
# [3/100] testing 66.7% loss=0.40210, acc=0.81250
# [3/100] testing 67.6% loss=0.48443, acc=0.81250
# [3/100] testing 68.0% loss=0.49696, acc=0.78125
# [3/100] testing 68.9% loss=0.43534, acc=0.81250
# [3/100] testing 69.3% loss=0.46661, acc=0.78125
# [3/100] testing 70.2% loss=0.56506, acc=0.78125
# [3/100] testing 70.6% loss=0.46349, acc=0.75000
# [3/100] testing 71.5% loss=0.50363, acc=0.81250
# [3/100] testing 72.4% loss=0.41080, acc=0.73438
# [3/100] testing 72.8% loss=0.49254, acc=0.79688
# [3/100] testing 73.7% loss=0.43047, acc=0.78125
# [3/100] testing 74.1% loss=0.45392, acc=0.79688
# [3/100] testing 75.0% loss=0.36315, acc=0.84375
# [3/100] testing 75.4% loss=0.33443, acc=0.82812
# [3/100] testing 76.3% loss=0.27194, acc=0.92188
# [3/100] testing 76.8% loss=0.47753, acc=0.73438
# [3/100] testing 77.6% loss=0.46396, acc=0.79688
# [3/100] testing 78.5% loss=0.63383, acc=0.68750
# [3/100] testing 79.0% loss=0.48577, acc=0.78125
# [3/100] testing 79.8% loss=0.36422, acc=0.85938
# [3/100] testing 80.3% loss=0.48667, acc=0.70312
# [3/100] testing 81.2% loss=0.52743, acc=0.76562
# [3/100] testing 81.6% loss=0.48226, acc=0.82812
# [3/100] testing 82.5% loss=0.40357, acc=0.79688
# [3/100] testing 83.3% loss=0.37287, acc=0.78125
# [3/100] testing 83.8% loss=0.40754, acc=0.75000
# [3/100] testing 84.7% loss=0.36028, acc=0.82812
# [3/100] testing 85.1% loss=0.53952, acc=0.76562
# [3/100] testing 86.0% loss=0.51944, acc=0.73438
# [3/100] testing 86.4% loss=0.53629, acc=0.70312
# [3/100] testing 87.3% loss=0.62891, acc=0.65625
# [3/100] testing 87.7% loss=0.41268, acc=0.79688
# [3/100] testing 88.6% loss=0.33332, acc=0.85938
# [3/100] testing 89.5% loss=0.60035, acc=0.71875
# [3/100] testing 89.9% loss=0.51046, acc=0.78125
# [3/100] testing 90.8% loss=0.39763, acc=0.81250
# [3/100] testing 91.2% loss=0.39898, acc=0.76562
# [3/100] testing 92.1% loss=0.42878, acc=0.76562
# [3/100] testing 92.6% loss=0.47341, acc=0.78125
# [3/100] testing 93.4% loss=0.54657, acc=0.73438
# [3/100] testing 94.3% loss=0.36773, acc=0.84375
# [3/100] testing 94.7% loss=0.42076, acc=0.78125
# [3/100] testing 95.6% loss=0.48259, acc=0.81250
# [3/100] testing 96.1% loss=0.49476, acc=0.73438
# [3/100] testing 96.9% loss=0.40990, acc=0.78125
# [3/100] testing 97.4% loss=0.34102, acc=0.87500
# [3/100] testing 98.3% loss=0.47268, acc=0.73438
# [3/100] testing 98.7% loss=0.49113, acc=0.75000
# [3/100] testing 99.6% loss=0.48415, acc=0.79688
# [4/100] training 0.2% loss=0.48451, acc=0.78125
# [4/100] training 0.4% loss=0.55027, acc=0.64062
# [4/100] training 0.5% loss=0.44376, acc=0.75000
# [4/100] training 0.8% loss=0.43373, acc=0.85938
# [4/100] training 0.9% loss=0.43886, acc=0.76562
# [4/100] training 1.1% loss=0.42467, acc=0.81250
# [4/100] training 1.2% loss=0.45712, acc=0.76562
# [4/100] training 1.4% loss=0.37823, acc=0.82812
# [4/100] training 1.6% loss=0.33074, acc=0.87500
# [4/100] training 1.8% loss=0.47178, acc=0.76562
# [4/100] training 2.0% loss=0.46514, acc=0.81250
# [4/100] training 2.1% loss=0.43930, acc=0.79688
# [4/100] training 2.3% loss=0.34991, acc=0.85938
# [4/100] training 2.4% loss=0.52513, acc=0.73438
# [4/100] training 2.6% loss=0.49010, acc=0.67188
# [4/100] training 2.7% loss=0.50897, acc=0.71875
# [4/100] training 3.0% loss=0.55906, acc=0.70312
# [4/100] training 3.2% loss=0.40405, acc=0.87500
# [4/100] training 3.3% loss=0.45992, acc=0.73438
# [4/100] training 3.5% loss=0.38327, acc=0.79688
# [4/100] training 3.6% loss=0.63849, acc=0.73438
# [4/100] training 3.8% loss=0.37107, acc=0.78125
# [4/100] training 3.9% loss=0.51477, acc=0.70312
# [4/100] training 4.2% loss=0.41276, acc=0.76562
# [4/100] training 4.4% loss=0.42431, acc=0.78125
# [4/100] training 4.5% loss=0.46417, acc=0.75000
# [4/100] training 4.7% loss=0.53118, acc=0.79688
# [4/100] training 4.8% loss=0.44918, acc=0.78125
# [4/100] training 5.0% loss=0.39239, acc=0.82812
# [4/100] training 5.2% loss=0.52012, acc=0.73438
# [4/100] training 5.4% loss=0.35157, acc=0.89062
# [4/100] training 5.5% loss=0.36753, acc=0.84375
# [4/100] training 5.7% loss=0.40902, acc=0.82812
# [4/100] training 5.9% loss=0.47269, acc=0.73438
# [4/100] training 6.0% loss=0.44071, acc=0.78125
# [4/100] training 6.3% loss=0.46901, acc=0.78125
# [4/100] training 6.4% loss=0.44240, acc=0.76562
# [4/100] training 6.6% loss=0.41780, acc=0.85938
# [4/100] training 6.7% loss=0.44816, acc=0.75000
# [4/100] training 6.9% loss=0.41569, acc=0.81250
# [4/100] training 7.1% loss=0.49454, acc=0.73438
# [4/100] training 7.2% loss=0.40102, acc=0.84375
# [4/100] training 7.5% loss=0.36678, acc=0.89062
# [4/100] training 7.6% loss=0.29877, acc=0.93750
# [4/100] training 7.8% loss=0.36637, acc=0.79688
# [4/100] training 7.9% loss=0.35681, acc=0.79688
# [4/100] training 8.1% loss=0.29961, acc=0.84375
# [4/100] training 8.2% loss=0.31520, acc=0.79688
# [4/100] training 8.4% loss=0.48642, acc=0.76562
# [4/100] training 8.7% loss=0.34413, acc=0.85938
# [4/100] training 8.8% loss=0.36483, acc=0.85938
# [4/100] training 9.0% loss=0.46755, acc=0.76562
# [4/100] training 9.1% loss=0.55346, acc=0.71875
# [4/100] training 9.3% loss=0.54674, acc=0.70312
# [4/100] training 9.4% loss=0.57398, acc=0.70312
# [4/100] training 9.7% loss=0.47724, acc=0.78125
# [4/100] training 9.9% loss=0.53172, acc=0.71875
# [4/100] training 10.0% loss=0.41360, acc=0.75000
# [4/100] training 10.2% loss=0.44778, acc=0.79688
# [4/100] training 10.3% loss=0.40109, acc=0.84375
# [4/100] training 10.5% loss=0.48386, acc=0.79688
# [4/100] training 10.6% loss=0.38938, acc=0.82812
# [4/100] training 10.9% loss=0.48593, acc=0.78125
# [4/100] training 11.0% loss=0.41334, acc=0.78125
# [4/100] training 11.2% loss=0.38333, acc=0.78125
# [4/100] training 11.4% loss=0.49071, acc=0.78125
# [4/100] training 11.5% loss=0.58782, acc=0.68750
# [4/100] training 11.7% loss=0.42683, acc=0.82812
# [4/100] training 11.8% loss=0.44025, acc=0.79688
# [4/100] training 12.1% loss=0.45779, acc=0.76562
# [4/100] training 12.2% loss=0.46868, acc=0.81250
# [4/100] training 12.4% loss=0.36527, acc=0.85938
# [4/100] training 12.6% loss=0.35798, acc=0.85938
# [4/100] training 12.7% loss=0.61814, acc=0.70312
# [4/100] training 12.9% loss=0.48204, acc=0.78125
# [4/100] training 13.0% loss=0.49217, acc=0.73438
# [4/100] training 13.3% loss=0.52219, acc=0.70312
# [4/100] training 13.4% loss=0.57271, acc=0.71875
# [4/100] training 13.6% loss=0.44713, acc=0.79688
# [4/100] training 13.7% loss=0.57908, acc=0.65625
# [4/100] training 13.9% loss=0.47045, acc=0.78125
# [4/100] training 14.1% loss=0.43276, acc=0.78125
# [4/100] training 14.3% loss=0.47951, acc=0.73438
# [4/100] training 14.5% loss=0.28664, acc=0.89062
# [4/100] training 14.6% loss=0.57794, acc=0.71875
# [4/100] training 14.8% loss=0.40374, acc=0.81250
# [4/100] training 14.9% loss=0.48690, acc=0.76562
# [4/100] training 15.1% loss=0.47798, acc=0.81250
# [4/100] training 15.4% loss=0.49120, acc=0.71875
# [4/100] training 15.5% loss=0.46674, acc=0.79688
# [4/100] training 15.7% loss=0.73126, acc=0.57812
# [4/100] training 15.8% loss=0.42320, acc=0.79688
# [4/100] training 16.0% loss=0.51407, acc=0.75000
# [4/100] training 16.1% loss=0.56846, acc=0.68750
# [4/100] training 16.3% loss=0.48629, acc=0.76562
# [4/100] training 16.4% loss=0.49274, acc=0.71875
# [4/100] training 16.7% loss=0.52472, acc=0.71875
# [4/100] training 16.9% loss=0.59270, acc=0.73438
# [4/100] training 17.0% loss=0.44065, acc=0.78125
# [4/100] training 17.2% loss=0.35476, acc=0.87500
# [4/100] training 17.3% loss=0.40016, acc=0.81250
# [4/100] training 17.5% loss=0.52653, acc=0.76562
# [4/100] training 17.7% loss=0.51981, acc=0.73438
# [4/100] training 17.9% loss=0.44688, acc=0.81250
# [4/100] training 18.1% loss=0.51875, acc=0.73438
# [4/100] training 18.2% loss=0.55572, acc=0.71875
# [4/100] training 18.4% loss=0.51516, acc=0.76562
# [4/100] training 18.5% loss=0.45546, acc=0.82812
# [4/100] training 18.8% loss=0.42322, acc=0.84375
# [4/100] training 18.9% loss=0.46339, acc=0.82812
# [4/100] training 19.1% loss=0.51650, acc=0.70312
# [4/100] training 19.2% loss=0.35552, acc=0.84375
# [4/100] training 19.4% loss=0.34012, acc=0.89062
# [4/100] training 19.6% loss=0.51826, acc=0.78125
# [4/100] training 19.7% loss=0.52678, acc=0.73438
# [4/100] training 20.0% loss=0.37832, acc=0.79688
# [4/100] training 20.1% loss=0.42142, acc=0.85938
# [4/100] training 20.3% loss=0.50000, acc=0.75000
# [4/100] training 20.4% loss=0.47824, acc=0.75000
# [4/100] training 20.6% loss=0.46765, acc=0.71875
# [4/100] training 20.8% loss=0.43174, acc=0.82812
# [4/100] training 20.9% loss=0.42759, acc=0.79688
# [4/100] training 21.2% loss=0.42313, acc=0.78125
# [4/100] training 21.3% loss=0.44760, acc=0.79688
# [4/100] training 21.5% loss=0.40134, acc=0.84375
# [4/100] training 21.6% loss=0.30039, acc=0.87500
# [4/100] training 21.8% loss=0.36591, acc=0.81250
# [4/100] training 21.9% loss=0.51185, acc=0.73438
# [4/100] training 22.2% loss=0.38612, acc=0.76562
# [4/100] training 22.4% loss=0.50801, acc=0.79688
# [4/100] training 22.5% loss=0.36686, acc=0.82812
# [4/100] training 22.7% loss=0.42950, acc=0.76562
# [4/100] training 22.8% loss=0.49725, acc=0.81250
# [4/100] training 23.0% loss=0.36689, acc=0.79688
# [4/100] training 23.1% loss=0.49592, acc=0.70312
# [4/100] training 23.4% loss=0.52451, acc=0.81250
# [4/100] training 23.6% loss=0.36842, acc=0.84375
# [4/100] training 23.7% loss=0.49250, acc=0.73438
# [4/100] training 23.9% loss=0.43462, acc=0.79688
# [4/100] training 24.0% loss=0.43417, acc=0.81250
# [4/100] training 24.2% loss=0.50454, acc=0.76562
# [4/100] training 24.3% loss=0.47916, acc=0.82812
# [4/100] training 24.6% loss=0.37293, acc=0.78125
# [4/100] training 24.7% loss=0.51123, acc=0.71875
# [4/100] training 24.9% loss=0.42576, acc=0.82812
# [4/100] training 25.1% loss=0.53526, acc=0.75000
# [4/100] training 25.2% loss=0.36185, acc=0.93750
# [4/100] training 25.4% loss=0.42705, acc=0.79688
# [4/100] training 25.6% loss=0.41763, acc=0.82812
# [4/100] training 25.8% loss=0.48993, acc=0.76562
# [4/100] training 25.9% loss=0.36787, acc=0.84375
# [4/100] training 26.1% loss=0.51276, acc=0.76562
# [4/100] training 26.3% loss=0.36177, acc=0.84375
# [4/100] training 26.4% loss=0.29556, acc=0.87500
# [4/100] training 26.6% loss=0.44222, acc=0.81250
# [4/100] training 26.8% loss=0.43814, acc=0.84375
# [4/100] training 27.0% loss=0.42663, acc=0.81250
# [4/100] training 27.1% loss=0.27779, acc=0.92188
# [4/100] training 27.3% loss=0.42923, acc=0.81250
# [4/100] training 27.4% loss=0.36596, acc=0.85938
# [4/100] training 27.6% loss=0.50875, acc=0.75000
# [4/100] training 27.9% loss=0.45662, acc=0.81250
# [4/100] training 28.0% loss=0.52300, acc=0.78125
# [4/100] training 28.2% loss=0.44477, acc=0.78125
# [4/100] training 28.3% loss=0.34744, acc=0.89062
# [4/100] training 28.5% loss=0.47200, acc=0.79688
# [4/100] training 28.6% loss=0.52240, acc=0.75000
# [4/100] training 28.8% loss=0.31972, acc=0.84375
# [4/100] training 29.1% loss=0.44887, acc=0.75000
# [4/100] training 29.2% loss=0.40660, acc=0.84375
# [4/100] training 29.4% loss=0.49967, acc=0.71875
# [4/100] training 29.5% loss=0.37128, acc=0.79688
# [4/100] training 29.7% loss=0.47843, acc=0.76562
# [4/100] training 29.8% loss=0.34695, acc=0.84375
# [4/100] training 30.0% loss=0.50483, acc=0.68750
# [4/100] training 30.2% loss=0.47926, acc=0.73438
# [4/100] training 30.4% loss=0.44338, acc=0.79688
# [4/100] training 30.6% loss=0.43110, acc=0.81250
# [4/100] training 30.7% loss=0.37867, acc=0.79688
# [4/100] training 30.9% loss=0.57732, acc=0.67188
# [4/100] training 31.0% loss=0.38166, acc=0.81250
# [4/100] training 31.3% loss=0.42511, acc=0.78125
# [4/100] training 31.4% loss=0.50947, acc=0.78125
# [4/100] training 31.6% loss=0.57882, acc=0.71875
# [4/100] training 31.8% loss=0.34384, acc=0.82812
# [4/100] training 31.9% loss=0.42298, acc=0.81250
# [4/100] training 32.1% loss=0.46875, acc=0.75000
# [4/100] training 32.2% loss=0.34798, acc=0.89062
# [4/100] training 32.5% loss=0.43546, acc=0.78125
# [4/100] training 32.6% loss=0.43401, acc=0.81250
# [4/100] training 32.8% loss=0.35157, acc=0.87500
# [4/100] training 32.9% loss=0.50330, acc=0.76562
# [4/100] training 33.1% loss=0.44138, acc=0.73438
# [4/100] training 33.3% loss=0.39193, acc=0.78125
# [4/100] training 33.4% loss=0.42830, acc=0.82812
# [4/100] training 33.7% loss=0.57552, acc=0.71875
# [4/100] training 33.8% loss=0.50860, acc=0.68750
# [4/100] training 34.0% loss=0.45407, acc=0.82812
# [4/100] training 34.1% loss=0.38724, acc=0.84375
# [4/100] training 34.3% loss=0.40017, acc=0.76562
# [4/100] training 34.5% loss=0.54224, acc=0.73438
# [4/100] training 34.7% loss=0.30449, acc=0.84375
# [4/100] training 34.9% loss=0.39712, acc=0.73438
# [4/100] training 35.0% loss=0.45859, acc=0.78125
# [4/100] training 35.2% loss=0.44816, acc=0.78125
# [4/100] training 35.3% loss=0.48597, acc=0.78125
# [4/100] training 35.5% loss=0.47177, acc=0.76562
# [4/100] training 35.6% loss=0.39437, acc=0.82812
# [4/100] training 35.9% loss=0.54982, acc=0.71875
# [4/100] training 36.1% loss=0.38795, acc=0.76562
# [4/100] training 36.2% loss=0.50807, acc=0.78125
# [4/100] training 36.4% loss=0.44283, acc=0.71875
# [4/100] training 36.5% loss=0.44000, acc=0.84375
# [4/100] training 36.7% loss=0.33608, acc=0.82812
# [4/100] training 36.8% loss=0.43403, acc=0.78125
# [4/100] training 37.1% loss=0.44346, acc=0.81250
# [4/100] training 37.3% loss=0.43559, acc=0.76562
# [4/100] training 37.4% loss=0.39944, acc=0.84375
# [4/100] training 37.6% loss=0.40510, acc=0.79688
# [4/100] training 37.7% loss=0.40495, acc=0.87500
# [4/100] training 37.9% loss=0.33700, acc=0.82812
# [4/100] training 38.1% loss=0.49159, acc=0.70312
# [4/100] training 38.3% loss=0.45574, acc=0.76562
# [4/100] training 38.4% loss=0.38502, acc=0.84375
# [4/100] training 38.6% loss=0.40838, acc=0.81250
# [4/100] training 38.8% loss=0.47033, acc=0.81250
# [4/100] training 38.9% loss=0.40052, acc=0.85938
# [4/100] training 39.1% loss=0.42709, acc=0.82812
# [4/100] training 39.3% loss=0.35980, acc=0.84375
# [4/100] training 39.5% loss=0.49990, acc=0.79688
# [4/100] training 39.6% loss=0.46874, acc=0.79688
# [4/100] training 39.8% loss=0.37654, acc=0.82812
# [4/100] training 40.0% loss=0.41936, acc=0.79688
# [4/100] training 40.1% loss=0.46316, acc=0.76562
# [4/100] training 40.4% loss=0.43659, acc=0.75000
# [4/100] training 40.5% loss=0.30202, acc=0.85938
# [4/100] training 40.7% loss=0.32789, acc=0.82812
# [4/100] training 40.8% loss=0.40381, acc=0.81250
# [4/100] training 41.0% loss=0.37789, acc=0.79688
# [4/100] training 41.1% loss=0.50760, acc=0.71875
# [4/100] training 41.3% loss=0.48728, acc=0.78125
# [4/100] training 41.6% loss=0.40055, acc=0.79688
# [4/100] training 41.7% loss=0.63182, acc=0.73438
# [4/100] training 41.9% loss=0.37972, acc=0.82812
# [4/100] training 42.0% loss=0.40994, acc=0.73438
# [4/100] training 42.2% loss=0.49235, acc=0.81250
# [4/100] training 42.3% loss=0.38661, acc=0.82812
# [4/100] training 42.5% loss=0.33445, acc=0.87500
# [4/100] training 42.8% loss=0.36514, acc=0.89062
# [4/100] training 42.9% loss=0.31819, acc=0.90625
# [4/100] training 43.1% loss=0.44412, acc=0.76562
# [4/100] training 43.2% loss=0.49353, acc=0.73438
# [4/100] training 43.4% loss=0.43579, acc=0.81250
# [4/100] training 43.5% loss=0.37014, acc=0.79688
# [4/100] training 43.8% loss=0.47267, acc=0.73438
# [4/100] training 43.9% loss=0.44926, acc=0.73438
# [4/100] training 44.1% loss=0.34287, acc=0.90625
# [4/100] training 44.3% loss=0.48874, acc=0.79688
# [4/100] training 44.4% loss=0.46831, acc=0.78125
# [4/100] training 44.6% loss=0.48409, acc=0.75000
# [4/100] training 44.7% loss=0.51500, acc=0.68750
# [4/100] training 45.0% loss=0.42460, acc=0.79688
# [4/100] training 45.1% loss=0.57335, acc=0.73438
# [4/100] training 45.3% loss=0.58131, acc=0.73438
# [4/100] training 45.5% loss=0.41075, acc=0.85938
# [4/100] training 45.6% loss=0.47829, acc=0.73438
# [4/100] training 45.8% loss=0.41796, acc=0.81250
# [4/100] training 45.9% loss=0.36004, acc=0.84375
# [4/100] training 46.2% loss=0.35463, acc=0.84375
# [4/100] training 46.3% loss=0.37897, acc=0.79688
# [4/100] training 46.5% loss=0.54949, acc=0.71875
# [4/100] training 46.6% loss=0.39058, acc=0.78125
# [4/100] training 46.8% loss=0.44547, acc=0.81250
# [4/100] training 47.0% loss=0.36449, acc=0.85938
# [4/100] training 47.2% loss=0.48876, acc=0.79688
# [4/100] training 47.4% loss=0.40470, acc=0.76562
# [4/100] training 47.5% loss=0.38261, acc=0.82812
# [4/100] training 47.7% loss=0.34814, acc=0.85938
# [4/100] training 47.8% loss=0.35269, acc=0.81250
# [4/100] training 48.0% loss=0.59363, acc=0.78125
# [4/100] training 48.3% loss=0.33419, acc=0.85938
# [4/100] training 48.4% loss=0.40525, acc=0.82812
# [4/100] training 48.6% loss=0.39140, acc=0.85938
# [4/100] training 48.7% loss=0.53138, acc=0.68750
# [4/100] training 48.9% loss=0.55577, acc=0.71875
# [4/100] training 49.0% loss=0.39920, acc=0.82812
# [4/100] training 49.2% loss=0.37663, acc=0.79688
# [4/100] training 49.3% loss=0.41190, acc=0.79688
# [4/100] training 49.6% loss=0.39820, acc=0.79688
# [4/100] training 49.8% loss=0.36765, acc=0.79688
# [4/100] training 49.9% loss=0.41221, acc=0.78125
# [4/100] training 50.1% loss=0.35853, acc=0.81250
# [4/100] training 50.2% loss=0.38065, acc=0.81250
# [4/100] training 50.4% loss=0.44854, acc=0.78125
# [4/100] training 50.6% loss=0.54719, acc=0.73438
# [4/100] training 50.8% loss=0.49543, acc=0.70312
# [4/100] training 51.0% loss=0.48031, acc=0.76562
# [4/100] training 51.1% loss=0.39165, acc=0.84375
# [4/100] training 51.3% loss=0.40595, acc=0.81250
# [4/100] training 51.4% loss=0.42357, acc=0.75000
# [4/100] training 51.7% loss=0.53640, acc=0.71875
# [4/100] training 51.8% loss=0.33062, acc=0.85938
# [4/100] training 52.0% loss=0.48318, acc=0.70312
# [4/100] training 52.1% loss=0.55100, acc=0.76562
# [4/100] training 52.3% loss=0.47256, acc=0.75000
# [4/100] training 52.5% loss=0.35751, acc=0.90625
# [4/100] training 52.6% loss=0.43914, acc=0.82812
# [4/100] training 52.9% loss=0.55566, acc=0.68750
# [4/100] training 53.0% loss=0.42341, acc=0.81250
# [4/100] training 53.2% loss=0.50892, acc=0.81250
# [4/100] training 53.3% loss=0.25385, acc=0.89062
# [4/100] training 53.5% loss=0.53210, acc=0.70312
# [4/100] training 53.7% loss=0.49526, acc=0.73438
# [4/100] training 53.8% loss=0.51502, acc=0.75000
# [4/100] training 54.1% loss=0.44078, acc=0.79688
# [4/100] training 54.2% loss=0.40849, acc=0.82812
# [4/100] training 54.4% loss=0.48272, acc=0.81250
# [4/100] training 54.5% loss=0.38209, acc=0.84375
# [4/100] training 54.7% loss=0.45411, acc=0.81250
# [4/100] training 54.8% loss=0.29666, acc=0.85938
# [4/100] training 55.1% loss=0.34964, acc=0.85938
# [4/100] training 55.3% loss=0.29431, acc=0.92188
# [4/100] training 55.4% loss=0.47318, acc=0.73438
# [4/100] training 55.6% loss=0.36989, acc=0.87500
# [4/100] training 55.7% loss=0.41443, acc=0.84375
# [4/100] training 55.9% loss=0.40501, acc=0.79688
# [4/100] training 56.0% loss=0.42765, acc=0.76562
# [4/100] training 56.3% loss=0.60514, acc=0.70312
# [4/100] training 56.5% loss=0.50120, acc=0.76562
# [4/100] training 56.6% loss=0.45878, acc=0.76562
# [4/100] training 56.8% loss=0.51808, acc=0.73438
# [4/100] training 56.9% loss=0.46575, acc=0.75000
# [4/100] training 57.1% loss=0.46207, acc=0.82812
# [4/100] training 57.2% loss=0.45364, acc=0.73438
# [4/100] training 57.5% loss=0.47320, acc=0.78125
# [4/100] training 57.6% loss=0.47120, acc=0.71875
# [4/100] training 57.8% loss=0.32473, acc=0.84375
# [4/100] training 58.0% loss=0.36164, acc=0.84375
# [4/100] training 58.1% loss=0.37452, acc=0.82812
# [4/100] training 58.3% loss=0.37134, acc=0.82812
# [4/100] training 58.4% loss=0.44339, acc=0.82812
# [4/100] training 58.7% loss=0.45097, acc=0.78125
# [4/100] training 58.8% loss=0.54344, acc=0.70312
# [4/100] training 59.0% loss=0.38586, acc=0.82812
# [4/100] training 59.2% loss=0.51515, acc=0.75000
# [4/100] training 59.3% loss=0.43325, acc=0.76562
# [4/100] training 59.5% loss=0.51167, acc=0.68750
# [4/100] training 59.7% loss=0.51749, acc=0.67188
# [4/100] training 59.9% loss=0.39031, acc=0.82812
# [4/100] training 60.0% loss=0.36986, acc=0.84375
# [4/100] training 60.2% loss=0.46549, acc=0.76562
# [4/100] training 60.3% loss=0.49773, acc=0.71875
# [4/100] training 60.5% loss=0.41998, acc=0.76562
# [4/100] training 60.8% loss=0.47715, acc=0.75000
# [4/100] training 60.9% loss=0.45947, acc=0.82812
# [4/100] training 61.1% loss=0.58760, acc=0.68750
# [4/100] training 61.2% loss=0.46163, acc=0.78125
# [4/100] training 61.4% loss=0.50047, acc=0.75000
# [4/100] training 61.5% loss=0.52059, acc=0.75000
# [4/100] training 61.7% loss=0.52877, acc=0.73438
# [4/100] training 62.0% loss=0.48351, acc=0.78125
# [4/100] training 62.1% loss=0.51059, acc=0.73438
# [4/100] training 62.3% loss=0.41573, acc=0.82812
# [4/100] training 62.4% loss=0.47047, acc=0.73438
# [4/100] training 62.6% loss=0.44380, acc=0.81250
# [4/100] training 62.7% loss=0.31026, acc=0.87500
# [4/100] training 62.9% loss=0.56079, acc=0.68750
# [4/100] training 63.1% loss=0.40294, acc=0.79688
# [4/100] training 63.3% loss=0.52448, acc=0.76562
# [4/100] training 63.5% loss=0.46590, acc=0.75000
# [4/100] training 63.6% loss=0.47896, acc=0.79688
# [4/100] training 63.8% loss=0.41776, acc=0.79688
# [4/100] training 63.9% loss=0.36005, acc=0.87500
# [4/100] training 64.2% loss=0.35294, acc=0.87500
# [4/100] training 64.3% loss=0.42422, acc=0.78125
# [4/100] training 64.5% loss=0.50027, acc=0.75000
# [4/100] training 64.7% loss=0.41966, acc=0.76562
# [4/100] training 64.8% loss=0.68645, acc=0.67188
# [4/100] training 65.0% loss=0.42569, acc=0.84375
# [4/100] training 65.1% loss=0.51186, acc=0.71875
# [4/100] training 65.4% loss=0.43489, acc=0.78125
# [4/100] training 65.5% loss=0.36413, acc=0.87500
# [4/100] training 65.7% loss=0.37141, acc=0.76562
# [4/100] training 65.8% loss=0.57581, acc=0.71875
# [4/100] training 66.0% loss=0.50946, acc=0.76562
# [4/100] training 66.2% loss=0.40611, acc=0.81250
# [4/100] training 66.3% loss=0.47832, acc=0.78125
# [4/100] training 66.6% loss=0.42178, acc=0.79688
# [4/100] training 66.7% loss=0.34629, acc=0.84375
# [4/100] training 66.9% loss=0.41915, acc=0.81250
# [4/100] training 67.0% loss=0.48038, acc=0.71875
# [4/100] training 67.2% loss=0.35435, acc=0.85938
# [4/100] training 67.4% loss=0.39623, acc=0.81250
# [4/100] training 67.6% loss=0.38496, acc=0.84375
# [4/100] training 67.8% loss=0.44493, acc=0.78125
# [4/100] training 67.9% loss=0.37132, acc=0.81250
# [4/100] training 68.1% loss=0.40269, acc=0.79688
# [4/100] training 68.2% loss=0.32587, acc=0.84375
# [4/100] training 68.4% loss=0.26449, acc=0.89062
# [4/100] training 68.5% loss=0.60557, acc=0.70312
# [4/100] training 68.8% loss=0.51896, acc=0.73438
# [4/100] training 69.0% loss=0.42939, acc=0.81250
# [4/100] training 69.1% loss=0.33578, acc=0.85938
# [4/100] training 69.3% loss=0.36205, acc=0.89062
# [4/100] training 69.4% loss=0.37820, acc=0.82812
# [4/100] training 69.6% loss=0.34754, acc=0.82812
# [4/100] training 69.7% loss=0.41671, acc=0.85938
# [4/100] training 70.0% loss=0.48319, acc=0.78125
# [4/100] training 70.2% loss=0.41625, acc=0.81250
# [4/100] training 70.3% loss=0.42375, acc=0.81250
# [4/100] training 70.5% loss=0.47096, acc=0.71875
# [4/100] training 70.6% loss=0.44051, acc=0.82812
# [4/100] training 70.8% loss=0.46767, acc=0.78125
# [4/100] training 71.0% loss=0.42963, acc=0.75000
# [4/100] training 71.2% loss=0.34094, acc=0.87500
# [4/100] training 71.3% loss=0.36070, acc=0.81250
# [4/100] training 71.5% loss=0.59154, acc=0.73438
# [4/100] training 71.7% loss=0.51730, acc=0.73438
# [4/100] training 71.8% loss=0.41267, acc=0.85938
# [4/100] training 72.0% loss=0.36152, acc=0.82812
# [4/100] training 72.2% loss=0.40743, acc=0.78125
# [4/100] training 72.4% loss=0.51557, acc=0.79688
# [4/100] training 72.5% loss=0.44061, acc=0.78125
# [4/100] training 72.7% loss=0.46366, acc=0.75000
# [4/100] training 72.9% loss=0.40470, acc=0.81250
# [4/100] training 73.0% loss=0.37872, acc=0.79688
# [4/100] training 73.3% loss=0.41487, acc=0.75000
# [4/100] training 73.4% loss=0.36235, acc=0.84375
# [4/100] training 73.6% loss=0.32966, acc=0.82812
# [4/100] training 73.7% loss=0.34660, acc=0.82812
# [4/100] training 73.9% loss=0.33872, acc=0.84375
# [4/100] training 74.0% loss=0.45644, acc=0.79688
# [4/100] training 74.2% loss=0.38568, acc=0.78125
# [4/100] training 74.5% loss=0.38037, acc=0.76562
# [4/100] training 74.6% loss=0.54188, acc=0.71875
# [4/100] training 74.8% loss=0.54449, acc=0.75000
# [4/100] training 74.9% loss=0.50910, acc=0.76562
# [4/100] training 75.1% loss=0.47498, acc=0.75000
# [4/100] training 75.2% loss=0.40782, acc=0.85938
# [4/100] training 75.4% loss=0.37200, acc=0.84375
# [4/100] training 75.7% loss=0.39517, acc=0.84375
# [4/100] training 75.8% loss=0.50674, acc=0.70312
# [4/100] training 76.0% loss=0.33298, acc=0.87500
# [4/100] training 76.1% loss=0.42461, acc=0.78125
# [4/100] training 76.3% loss=0.43635, acc=0.82812
# [4/100] training 76.4% loss=0.45571, acc=0.79688
# [4/100] training 76.7% loss=0.43912, acc=0.82812
# [4/100] training 76.8% loss=0.36693, acc=0.87500
# [4/100] training 77.0% loss=0.38803, acc=0.81250
# [4/100] training 77.2% loss=0.44409, acc=0.82812
# [4/100] training 77.3% loss=0.33528, acc=0.84375
# [4/100] training 77.5% loss=0.39605, acc=0.84375
# [4/100] training 77.6% loss=0.42146, acc=0.78125
# [4/100] training 77.9% loss=0.42519, acc=0.76562
# [4/100] training 78.0% loss=0.50406, acc=0.76562
# [4/100] training 78.2% loss=0.44228, acc=0.75000
# [4/100] training 78.4% loss=0.27316, acc=0.93750
# [4/100] training 78.5% loss=0.45964, acc=0.81250
# [4/100] training 78.7% loss=0.32759, acc=0.85938
# [4/100] training 78.8% loss=0.33444, acc=0.87500
# [4/100] training 79.1% loss=0.32023, acc=0.87500
# [4/100] training 79.2% loss=0.36003, acc=0.85938
# [4/100] training 79.4% loss=0.58426, acc=0.68750
# [4/100] training 79.5% loss=0.36320, acc=0.82812
# [4/100] training 79.7% loss=0.28875, acc=0.92188
# [4/100] training 79.9% loss=0.36420, acc=0.84375
# [4/100] training 80.1% loss=0.41609, acc=0.79688
# [4/100] training 80.3% loss=0.39227, acc=0.79688
# [4/100] training 80.4% loss=0.38787, acc=0.84375
# [4/100] training 80.6% loss=0.46071, acc=0.75000
# [4/100] training 80.7% loss=0.39620, acc=0.78125
# [4/100] training 80.9% loss=0.43981, acc=0.82812
# [4/100] training 81.2% loss=0.41080, acc=0.79688
# [4/100] training 81.3% loss=0.48351, acc=0.76562
# [4/100] training 81.5% loss=0.33420, acc=0.84375
# [4/100] training 81.6% loss=0.47312, acc=0.73438
# [4/100] training 81.8% loss=0.37456, acc=0.84375
# [4/100] training 81.9% loss=0.53762, acc=0.76562
# [4/100] training 82.1% loss=0.33324, acc=0.82812
# [4/100] training 82.2% loss=0.56319, acc=0.70312
# [4/100] training 82.5% loss=0.40260, acc=0.78125
# [4/100] training 82.7% loss=0.36606, acc=0.85938
# [4/100] training 82.8% loss=0.46241, acc=0.75000
# [4/100] training 83.0% loss=0.44955, acc=0.75000
# [4/100] training 83.1% loss=0.39807, acc=0.76562
# [4/100] training 83.3% loss=0.39673, acc=0.76562
# [4/100] training 83.5% loss=0.52702, acc=0.71875
# [4/100] training 83.7% loss=0.51084, acc=0.79688
# [4/100] training 83.9% loss=0.42405, acc=0.79688
# [4/100] training 84.0% loss=0.41535, acc=0.82812
# [4/100] training 84.2% loss=0.37886, acc=0.81250
# [4/100] training 84.3% loss=0.34497, acc=0.85938
# [4/100] training 84.5% loss=0.37689, acc=0.84375
# [4/100] training 84.7% loss=0.46187, acc=0.73438
# [4/100] training 84.9% loss=0.42151, acc=0.78125
# [4/100] training 85.0% loss=0.38100, acc=0.78125
# [4/100] training 85.2% loss=0.44396, acc=0.73438
# [4/100] training 85.4% loss=0.48523, acc=0.78125
# [4/100] training 85.5% loss=0.31979, acc=0.84375
# [4/100] training 85.8% loss=0.42383, acc=0.78125
# [4/100] training 85.9% loss=0.42489, acc=0.78125
# [4/100] training 86.1% loss=0.36546, acc=0.76562
# [4/100] training 86.2% loss=0.44637, acc=0.73438
# [4/100] training 86.4% loss=0.43865, acc=0.81250
# [4/100] training 86.6% loss=0.51492, acc=0.78125
# [4/100] training 86.7% loss=0.50528, acc=0.71875
# [4/100] training 87.0% loss=0.41451, acc=0.78125
# [4/100] training 87.1% loss=0.43499, acc=0.81250
# [4/100] training 87.3% loss=0.39017, acc=0.79688
# [4/100] training 87.4% loss=0.42780, acc=0.82812
# [4/100] training 87.6% loss=0.47925, acc=0.75000
# [4/100] training 87.7% loss=0.40802, acc=0.81250
# [4/100] training 87.9% loss=0.38960, acc=0.84375
# [4/100] training 88.2% loss=0.24944, acc=0.89062
# [4/100] training 88.3% loss=0.34258, acc=0.82812
# [4/100] training 88.5% loss=0.41845, acc=0.81250
# [4/100] training 88.6% loss=0.22363, acc=0.89062
# [4/100] training 88.8% loss=0.39810, acc=0.84375
# [4/100] training 88.9% loss=0.40869, acc=0.79688
# [4/100] training 89.2% loss=0.31503, acc=0.90625
# [4/100] training 89.4% loss=0.41010, acc=0.81250
# [4/100] training 89.5% loss=0.45804, acc=0.75000
# [4/100] training 89.7% loss=0.50953, acc=0.76562
# [4/100] training 89.8% loss=0.35492, acc=0.87500
# [4/100] training 90.0% loss=0.35138, acc=0.82812
# [4/100] training 90.1% loss=0.38800, acc=0.79688
# [4/100] training 90.4% loss=0.32350, acc=0.85938
# [4/100] training 90.5% loss=0.31486, acc=0.81250
# [4/100] training 90.7% loss=0.43113, acc=0.82812
# [4/100] training 90.9% loss=0.39131, acc=0.84375
# [4/100] training 91.0% loss=0.35742, acc=0.82812
# [4/100] training 91.2% loss=0.51062, acc=0.73438
# [4/100] training 91.3% loss=0.59150, acc=0.68750
# [4/100] training 91.6% loss=0.59706, acc=0.64062
# [4/100] training 91.7% loss=0.39234, acc=0.79688
# [4/100] training 91.9% loss=0.44246, acc=0.78125
# [4/100] training 92.1% loss=0.43087, acc=0.79688
# [4/100] training 92.2% loss=0.30802, acc=0.90625
# [4/100] training 92.4% loss=0.36277, acc=0.85938
# [4/100] training 92.6% loss=0.43934, acc=0.81250
# [4/100] training 92.8% loss=0.55229, acc=0.70312
# [4/100] training 92.9% loss=0.35575, acc=0.82812
# [4/100] training 93.1% loss=0.50734, acc=0.75000
# [4/100] training 93.2% loss=0.44612, acc=0.78125
# [4/100] training 93.4% loss=0.37343, acc=0.87500
# [4/100] training 93.7% loss=0.43029, acc=0.76562
# [4/100] training 93.8% loss=0.38134, acc=0.78125
# [4/100] training 94.0% loss=0.31954, acc=0.82812
# [4/100] training 94.1% loss=0.41401, acc=0.76562
# [4/100] training 94.3% loss=0.30573, acc=0.87500
# [4/100] training 94.4% loss=0.37684, acc=0.78125
# [4/100] training 94.6% loss=0.45716, acc=0.79688
# [4/100] training 94.9% loss=0.41763, acc=0.79688
# [4/100] training 95.0% loss=0.58572, acc=0.76562
# [4/100] training 95.2% loss=0.38643, acc=0.81250
# [4/100] training 95.3% loss=0.45502, acc=0.81250
# [4/100] training 95.5% loss=0.37867, acc=0.85938
# [4/100] training 95.6% loss=0.49224, acc=0.76562
# [4/100] training 95.8% loss=0.51253, acc=0.68750
# [4/100] training 96.0% loss=0.40729, acc=0.79688
# [4/100] training 96.2% loss=0.27148, acc=0.90625
# [4/100] training 96.4% loss=0.29780, acc=0.87500
# [4/100] training 96.5% loss=0.39484, acc=0.84375
# [4/100] training 96.7% loss=0.30385, acc=0.84375
# [4/100] training 96.8% loss=0.39179, acc=0.84375
# [4/100] training 97.1% loss=0.45059, acc=0.78125
# [4/100] training 97.2% loss=0.45489, acc=0.79688
# [4/100] training 97.4% loss=0.40657, acc=0.81250
# [4/100] training 97.6% loss=0.38510, acc=0.84375
# [4/100] training 97.7% loss=0.34913, acc=0.87500
# [4/100] training 97.9% loss=0.32503, acc=0.85938
# [4/100] training 98.0% loss=0.47345, acc=0.81250
# [4/100] training 98.3% loss=0.37476, acc=0.78125
# [4/100] training 98.4% loss=0.37761, acc=0.84375
# [4/100] training 98.6% loss=0.57043, acc=0.75000
# [4/100] training 98.7% loss=0.49025, acc=0.73438
# [4/100] training 98.9% loss=0.41212, acc=0.79688
# [4/100] training 99.1% loss=0.35526, acc=0.82812
# [4/100] training 99.2% loss=0.39085, acc=0.81250
# [4/100] training 99.5% loss=0.41797, acc=0.76562
# [4/100] training 99.6% loss=0.43693, acc=0.81250
# [4/100] training 99.8% loss=0.30111, acc=0.89062
# [4/100] training 99.9% loss=0.30027, acc=0.84375
# [4/100] testing 0.9% loss=0.27302, acc=0.82812
# [4/100] testing 1.8% loss=0.41919, acc=0.79688
# [4/100] testing 2.2% loss=0.41328, acc=0.71875
# [4/100] testing 3.1% loss=0.36930, acc=0.85938
# [4/100] testing 3.5% loss=0.38475, acc=0.82812
# [4/100] testing 4.4% loss=0.38418, acc=0.82812
# [4/100] testing 4.8% loss=0.45393, acc=0.78125
# [4/100] testing 5.7% loss=0.41981, acc=0.84375
# [4/100] testing 6.6% loss=0.40446, acc=0.79688
# [4/100] testing 7.0% loss=0.37777, acc=0.82812
# [4/100] testing 7.9% loss=0.52341, acc=0.70312
# [4/100] testing 8.3% loss=0.29243, acc=0.87500
# [4/100] testing 9.2% loss=0.38912, acc=0.78125
# [4/100] testing 9.7% loss=0.39673, acc=0.81250
# [4/100] testing 10.5% loss=0.51039, acc=0.71875
# [4/100] testing 11.0% loss=0.33705, acc=0.84375
# [4/100] testing 11.8% loss=0.43653, acc=0.81250
# [4/100] testing 12.7% loss=0.51829, acc=0.70312
# [4/100] testing 13.2% loss=0.33756, acc=0.84375
# [4/100] testing 14.0% loss=0.42864, acc=0.79688
# [4/100] testing 14.5% loss=0.47850, acc=0.71875
# [4/100] testing 15.4% loss=0.37422, acc=0.82812
# [4/100] testing 15.8% loss=0.31902, acc=0.89062
# [4/100] testing 16.7% loss=0.42531, acc=0.81250
# [4/100] testing 17.5% loss=0.43717, acc=0.79688
# [4/100] testing 18.0% loss=0.33918, acc=0.84375
# [4/100] testing 18.9% loss=0.31109, acc=0.90625
# [4/100] testing 19.3% loss=0.36167, acc=0.90625
# [4/100] testing 20.2% loss=0.54285, acc=0.76562
# [4/100] testing 20.6% loss=0.44607, acc=0.78125
# [4/100] testing 21.5% loss=0.41331, acc=0.81250
# [4/100] testing 21.9% loss=0.49134, acc=0.73438
# [4/100] testing 22.8% loss=0.44782, acc=0.81250
# [4/100] testing 23.7% loss=0.42084, acc=0.82812
# [4/100] testing 24.1% loss=0.42185, acc=0.78125
# [4/100] testing 25.0% loss=0.41042, acc=0.76562
# [4/100] testing 25.4% loss=0.33371, acc=0.79688
# [4/100] testing 26.3% loss=0.45464, acc=0.75000
# [4/100] testing 26.8% loss=0.41650, acc=0.81250
# [4/100] testing 27.6% loss=0.51876, acc=0.78125
# [4/100] testing 28.5% loss=0.39844, acc=0.81250
# [4/100] testing 29.0% loss=0.36946, acc=0.76562
# [4/100] testing 29.8% loss=0.45190, acc=0.81250
# [4/100] testing 30.3% loss=0.47508, acc=0.78125
# [4/100] testing 31.1% loss=0.51750, acc=0.75000
# [4/100] testing 31.6% loss=0.31285, acc=0.81250
# [4/100] testing 32.5% loss=0.41459, acc=0.79688
# [4/100] testing 32.9% loss=0.41416, acc=0.81250
# [4/100] testing 33.8% loss=0.45601, acc=0.73438
# [4/100] testing 34.7% loss=0.47319, acc=0.78125
# [4/100] testing 35.1% loss=0.40818, acc=0.81250
# [4/100] testing 36.0% loss=0.53870, acc=0.75000
# [4/100] testing 36.4% loss=0.44912, acc=0.79688
# [4/100] testing 37.3% loss=0.39591, acc=0.85938
# [4/100] testing 37.7% loss=0.50814, acc=0.76562
# [4/100] testing 38.6% loss=0.32858, acc=0.90625
# [4/100] testing 39.5% loss=0.53628, acc=0.78125
# [4/100] testing 39.9% loss=0.35212, acc=0.84375
# [4/100] testing 40.8% loss=0.36505, acc=0.78125
# [4/100] testing 41.2% loss=0.35133, acc=0.89062
# [4/100] testing 42.1% loss=0.51108, acc=0.76562
# [4/100] testing 42.5% loss=0.44348, acc=0.79688
# [4/100] testing 43.4% loss=0.38140, acc=0.79688
# [4/100] testing 43.9% loss=0.38390, acc=0.87500
# [4/100] testing 44.7% loss=0.50896, acc=0.78125
# [4/100] testing 45.6% loss=0.40091, acc=0.85938
# [4/100] testing 46.1% loss=0.35337, acc=0.89062
# [4/100] testing 46.9% loss=0.35541, acc=0.76562
# [4/100] testing 47.4% loss=0.34862, acc=0.79688
# [4/100] testing 48.3% loss=0.40584, acc=0.78125
# [4/100] testing 48.7% loss=0.65583, acc=0.73438
# [4/100] testing 49.6% loss=0.38485, acc=0.84375
# [4/100] testing 50.4% loss=0.34225, acc=0.82812
# [4/100] testing 50.9% loss=0.36255, acc=0.81250
# [4/100] testing 51.8% loss=0.40191, acc=0.81250
# [4/100] testing 52.2% loss=0.46165, acc=0.82812
# [4/100] testing 53.1% loss=0.42304, acc=0.78125
# [4/100] testing 53.5% loss=0.33630, acc=0.84375
# [4/100] testing 54.4% loss=0.48406, acc=0.70312
# [4/100] testing 54.8% loss=0.46431, acc=0.73438
# [4/100] testing 55.7% loss=0.30977, acc=0.89062
# [4/100] testing 56.6% loss=0.46421, acc=0.81250
# [4/100] testing 57.0% loss=0.52570, acc=0.71875
# [4/100] testing 57.9% loss=0.50831, acc=0.76562
# [4/100] testing 58.3% loss=0.48539, acc=0.71875
# [4/100] testing 59.2% loss=0.45217, acc=0.71875
# [4/100] testing 59.7% loss=0.34708, acc=0.84375
# [4/100] testing 60.5% loss=0.44380, acc=0.75000
# [4/100] testing 61.4% loss=0.25906, acc=0.89062
# [4/100] testing 61.9% loss=0.36369, acc=0.82812
# [4/100] testing 62.7% loss=0.37783, acc=0.82812
# [4/100] testing 63.2% loss=0.39898, acc=0.78125
# [4/100] testing 64.0% loss=0.42023, acc=0.73438
# [4/100] testing 64.5% loss=0.37982, acc=0.85938
# [4/100] testing 65.4% loss=0.39256, acc=0.79688
# [4/100] testing 65.8% loss=0.42285, acc=0.79688
# [4/100] testing 66.7% loss=0.34245, acc=0.84375
# [4/100] testing 67.6% loss=0.42731, acc=0.81250
# [4/100] testing 68.0% loss=0.42815, acc=0.84375
# [4/100] testing 68.9% loss=0.34587, acc=0.84375
# [4/100] testing 69.3% loss=0.47930, acc=0.76562
# [4/100] testing 70.2% loss=0.48810, acc=0.73438
# [4/100] testing 70.6% loss=0.42390, acc=0.81250
# [4/100] testing 71.5% loss=0.46013, acc=0.81250
# [4/100] testing 72.4% loss=0.37050, acc=0.79688
# [4/100] testing 72.8% loss=0.42445, acc=0.78125
# [4/100] testing 73.7% loss=0.33354, acc=0.85938
# [4/100] testing 74.1% loss=0.41216, acc=0.81250
# [4/100] testing 75.0% loss=0.31301, acc=0.85938
# [4/100] testing 75.4% loss=0.35896, acc=0.87500
# [4/100] testing 76.3% loss=0.25442, acc=0.92188
# [4/100] testing 76.8% loss=0.39727, acc=0.78125
# [4/100] testing 77.6% loss=0.36899, acc=0.79688
# [4/100] testing 78.5% loss=0.58746, acc=0.67188
# [4/100] testing 79.0% loss=0.43340, acc=0.79688
# [4/100] testing 79.8% loss=0.40774, acc=0.79688
# [4/100] testing 80.3% loss=0.34150, acc=0.82812
# [4/100] testing 81.2% loss=0.40761, acc=0.82812
# [4/100] testing 81.6% loss=0.38540, acc=0.84375
# [4/100] testing 82.5% loss=0.33519, acc=0.81250
# [4/100] testing 83.3% loss=0.33638, acc=0.81250
# [4/100] testing 83.8% loss=0.33533, acc=0.90625
# [4/100] testing 84.7% loss=0.37219, acc=0.82812
# [4/100] testing 85.1% loss=0.48565, acc=0.76562
# [4/100] testing 86.0% loss=0.42605, acc=0.78125
# [4/100] testing 86.4% loss=0.42126, acc=0.76562
# [4/100] testing 87.3% loss=0.50230, acc=0.78125
# [4/100] testing 87.7% loss=0.35530, acc=0.81250
# [4/100] testing 88.6% loss=0.32649, acc=0.84375
# [4/100] testing 89.5% loss=0.48546, acc=0.75000
# [4/100] testing 89.9% loss=0.46277, acc=0.73438
# [4/100] testing 90.8% loss=0.34189, acc=0.81250
# [4/100] testing 91.2% loss=0.27975, acc=0.89062
# [4/100] testing 92.1% loss=0.42069, acc=0.76562
# [4/100] testing 92.6% loss=0.45097, acc=0.79688
# [4/100] testing 93.4% loss=0.47456, acc=0.76562
# [4/100] testing 94.3% loss=0.26879, acc=0.87500
# [4/100] testing 94.7% loss=0.41563, acc=0.76562
# [4/100] testing 95.6% loss=0.37738, acc=0.82812
# [4/100] testing 96.1% loss=0.38520, acc=0.78125
# [4/100] testing 96.9% loss=0.37586, acc=0.82812
# [4/100] testing 97.4% loss=0.33349, acc=0.85938
# [4/100] testing 98.3% loss=0.44201, acc=0.78125
# [4/100] testing 98.7% loss=0.35077, acc=0.81250
# [4/100] testing 99.6% loss=0.34475, acc=0.85938
# [5/100] training 0.2% loss=0.45816, acc=0.78125
# [5/100] training 0.4% loss=0.50767, acc=0.75000
# [5/100] training 0.5% loss=0.43378, acc=0.78125
# [5/100] training 0.8% loss=0.48381, acc=0.75000
# [5/100] training 0.9% loss=0.43684, acc=0.79688
# [5/100] training 1.1% loss=0.44784, acc=0.84375
# [5/100] training 1.2% loss=0.48206, acc=0.75000
# [5/100] training 1.4% loss=0.44141, acc=0.79688
# [5/100] training 1.6% loss=0.24137, acc=0.90625
# [5/100] training 1.8% loss=0.34670, acc=0.89062
# [5/100] training 2.0% loss=0.38267, acc=0.82812
# [5/100] training 2.1% loss=0.45442, acc=0.79688
# [5/100] training 2.3% loss=0.33005, acc=0.76562
# [5/100] training 2.4% loss=0.51776, acc=0.68750
# [5/100] training 2.6% loss=0.43394, acc=0.82812
# [5/100] training 2.7% loss=0.44952, acc=0.76562
# [5/100] training 3.0% loss=0.61404, acc=0.60938
# [5/100] training 3.2% loss=0.41839, acc=0.89062
# [5/100] training 3.3% loss=0.44345, acc=0.81250
# [5/100] training 3.5% loss=0.46110, acc=0.73438
# [5/100] training 3.6% loss=0.65044, acc=0.70312
# [5/100] training 3.8% loss=0.35145, acc=0.82812
# [5/100] training 3.9% loss=0.52639, acc=0.67188
# [5/100] training 4.2% loss=0.40014, acc=0.81250
# [5/100] training 4.4% loss=0.35121, acc=0.87500
# [5/100] training 4.5% loss=0.34851, acc=0.84375
# [5/100] training 4.7% loss=0.48600, acc=0.81250
# [5/100] training 4.8% loss=0.42941, acc=0.79688
# [5/100] training 5.0% loss=0.35212, acc=0.82812
# [5/100] training 5.2% loss=0.46466, acc=0.79688
# [5/100] training 5.4% loss=0.31516, acc=0.87500
# [5/100] training 5.5% loss=0.34192, acc=0.82812
# [5/100] training 5.7% loss=0.31588, acc=0.82812
# [5/100] training 5.9% loss=0.51648, acc=0.75000
# [5/100] training 6.0% loss=0.41080, acc=0.82812
# [5/100] training 6.3% loss=0.46910, acc=0.81250
# [5/100] training 6.4% loss=0.40283, acc=0.79688
# [5/100] training 6.6% loss=0.38762, acc=0.81250
# [5/100] training 6.7% loss=0.41745, acc=0.76562
# [5/100] training 6.9% loss=0.35993, acc=0.85938
# [5/100] training 7.1% loss=0.41472, acc=0.79688
# [5/100] training 7.2% loss=0.35999, acc=0.85938
# [5/100] training 7.5% loss=0.37243, acc=0.84375
# [5/100] training 7.6% loss=0.32260, acc=0.84375
# [5/100] training 7.8% loss=0.33461, acc=0.85938
# [5/100] training 7.9% loss=0.41837, acc=0.79688
# [5/100] training 8.1% loss=0.35937, acc=0.81250
# [5/100] training 8.2% loss=0.28573, acc=0.90625
# [5/100] training 8.4% loss=0.46013, acc=0.78125
# [5/100] training 8.7% loss=0.33883, acc=0.82812
# [5/100] training 8.8% loss=0.30536, acc=0.82812
# [5/100] training 9.0% loss=0.53931, acc=0.79688
# [5/100] training 9.1% loss=0.47186, acc=0.78125
# [5/100] training 9.3% loss=0.51996, acc=0.71875
# [5/100] training 9.4% loss=0.56626, acc=0.70312
# [5/100] training 9.7% loss=0.43404, acc=0.81250
# [5/100] training 9.9% loss=0.51604, acc=0.76562
# [5/100] training 10.0% loss=0.41691, acc=0.78125
# [5/100] training 10.2% loss=0.46371, acc=0.75000
# [5/100] training 10.3% loss=0.37775, acc=0.76562
# [5/100] training 10.5% loss=0.45225, acc=0.82812
# [5/100] training 10.6% loss=0.37016, acc=0.79688
# [5/100] training 10.9% loss=0.47307, acc=0.78125
# [5/100] training 11.0% loss=0.36747, acc=0.82812
# [5/100] training 11.2% loss=0.34102, acc=0.79688
# [5/100] training 11.4% loss=0.56961, acc=0.73438
# [5/100] training 11.5% loss=0.56953, acc=0.75000
# [5/100] training 11.7% loss=0.39069, acc=0.84375
# [5/100] training 11.8% loss=0.39399, acc=0.79688
# [5/100] training 12.1% loss=0.40255, acc=0.81250
# [5/100] training 12.2% loss=0.42728, acc=0.81250
# [5/100] training 12.4% loss=0.34161, acc=0.89062
# [5/100] training 12.6% loss=0.33974, acc=0.87500
# [5/100] training 12.7% loss=0.52714, acc=0.75000
# [5/100] training 12.9% loss=0.52016, acc=0.76562
# [5/100] training 13.0% loss=0.38260, acc=0.82812
# [5/100] training 13.3% loss=0.46760, acc=0.76562
# [5/100] training 13.4% loss=0.56697, acc=0.68750
# [5/100] training 13.6% loss=0.38779, acc=0.85938
# [5/100] training 13.7% loss=0.55490, acc=0.73438
# [5/100] training 13.9% loss=0.45728, acc=0.82812
# [5/100] training 14.1% loss=0.43134, acc=0.79688
# [5/100] training 14.3% loss=0.40458, acc=0.81250
# [5/100] training 14.5% loss=0.27896, acc=0.92188
# [5/100] training 14.6% loss=0.46415, acc=0.81250
# [5/100] training 14.8% loss=0.35774, acc=0.85938
# [5/100] training 14.9% loss=0.38860, acc=0.82812
# [5/100] training 15.1% loss=0.41558, acc=0.81250
# [5/100] training 15.4% loss=0.41882, acc=0.79688
# [5/100] training 15.5% loss=0.50209, acc=0.70312
# [5/100] training 15.7% loss=0.59459, acc=0.75000
# [5/100] training 15.8% loss=0.35542, acc=0.84375
# [5/100] training 16.0% loss=0.52864, acc=0.75000
# [5/100] training 16.1% loss=0.63748, acc=0.70312
# [5/100] training 16.3% loss=0.51814, acc=0.75000
# [5/100] training 16.4% loss=0.41411, acc=0.84375
# [5/100] training 16.7% loss=0.49508, acc=0.70312
# [5/100] training 16.9% loss=0.50826, acc=0.76562
# [5/100] training 17.0% loss=0.40402, acc=0.81250
# [5/100] training 17.2% loss=0.35275, acc=0.84375
# [5/100] training 17.3% loss=0.42806, acc=0.79688
# [5/100] training 17.5% loss=0.41856, acc=0.81250
# [5/100] training 17.7% loss=0.45490, acc=0.73438
# [5/100] training 17.9% loss=0.44410, acc=0.82812
# [5/100] training 18.1% loss=0.43505, acc=0.81250
# [5/100] training 18.2% loss=0.51432, acc=0.76562
# [5/100] training 18.4% loss=0.40174, acc=0.82812
# [5/100] training 18.5% loss=0.37972, acc=0.82812
# [5/100] training 18.8% loss=0.34919, acc=0.87500
# [5/100] training 18.9% loss=0.33077, acc=0.85938
# [5/100] training 19.1% loss=0.41428, acc=0.81250
# [5/100] training 19.2% loss=0.32523, acc=0.85938
# [5/100] training 19.4% loss=0.27314, acc=0.90625
# [5/100] training 19.6% loss=0.43874, acc=0.81250
# [5/100] training 19.7% loss=0.48461, acc=0.75000
# [5/100] training 20.0% loss=0.29385, acc=0.90625
# [5/100] training 20.1% loss=0.33215, acc=0.84375
# [5/100] training 20.3% loss=0.42858, acc=0.81250
# [5/100] training 20.4% loss=0.48225, acc=0.76562
# [5/100] training 20.6% loss=0.38948, acc=0.85938
# [5/100] training 20.8% loss=0.40990, acc=0.78125
# [5/100] training 20.9% loss=0.39695, acc=0.81250
# [5/100] training 21.2% loss=0.41642, acc=0.78125
# [5/100] training 21.3% loss=0.42818, acc=0.79688
# [5/100] training 21.5% loss=0.38028, acc=0.82812
# [5/100] training 21.6% loss=0.26390, acc=0.89062
# [5/100] training 21.8% loss=0.42394, acc=0.78125
# [5/100] training 21.9% loss=0.47273, acc=0.78125
# [5/100] training 22.2% loss=0.36836, acc=0.84375
# [5/100] training 22.4% loss=0.54317, acc=0.75000
# [5/100] training 22.5% loss=0.38813, acc=0.85938
# [5/100] training 22.7% loss=0.45151, acc=0.75000
# [5/100] training 22.8% loss=0.54079, acc=0.78125
# [5/100] training 23.0% loss=0.31219, acc=0.85938
# [5/100] training 23.1% loss=0.45885, acc=0.81250
# [5/100] training 23.4% loss=0.39323, acc=0.82812
# [5/100] training 23.6% loss=0.41388, acc=0.84375
# [5/100] training 23.7% loss=0.44943, acc=0.81250
# [5/100] training 23.9% loss=0.40482, acc=0.81250
# [5/100] training 24.0% loss=0.50523, acc=0.73438
# [5/100] training 24.2% loss=0.43875, acc=0.78125
# [5/100] training 24.3% loss=0.40433, acc=0.82812
# [5/100] training 24.6% loss=0.34451, acc=0.79688
# [5/100] training 24.7% loss=0.45273, acc=0.75000
# [5/100] training 24.9% loss=0.44392, acc=0.81250
# [5/100] training 25.1% loss=0.52020, acc=0.73438
# [5/100] training 25.2% loss=0.36932, acc=0.87500
# [5/100] training 25.4% loss=0.48579, acc=0.76562
# [5/100] training 25.6% loss=0.38086, acc=0.84375
# [5/100] training 25.8% loss=0.47232, acc=0.75000
# [5/100] training 25.9% loss=0.33501, acc=0.87500
# [5/100] training 26.1% loss=0.46638, acc=0.78125
# [5/100] training 26.3% loss=0.34509, acc=0.89062
# [5/100] training 26.4% loss=0.28636, acc=0.92188
# [5/100] training 26.6% loss=0.34737, acc=0.84375
# [5/100] training 26.8% loss=0.43940, acc=0.84375
# [5/100] training 27.0% loss=0.41995, acc=0.81250
# [5/100] training 27.1% loss=0.31974, acc=0.84375
# [5/100] training 27.3% loss=0.51954, acc=0.68750
# [5/100] training 27.4% loss=0.28286, acc=0.90625
# [5/100] training 27.6% loss=0.46378, acc=0.81250
# [5/100] training 27.9% loss=0.41725, acc=0.79688
# [5/100] training 28.0% loss=0.48476, acc=0.79688
# [5/100] training 28.2% loss=0.42391, acc=0.79688
# [5/100] training 28.3% loss=0.30929, acc=0.87500
# [5/100] training 28.5% loss=0.40620, acc=0.79688
# [5/100] training 28.6% loss=0.46115, acc=0.76562
# [5/100] training 28.8% loss=0.34184, acc=0.85938
# [5/100] training 29.1% loss=0.44432, acc=0.76562
# [5/100] training 29.2% loss=0.46074, acc=0.76562
# [5/100] training 29.4% loss=0.46536, acc=0.76562
# [5/100] training 29.5% loss=0.40062, acc=0.79688
# [5/100] training 29.7% loss=0.49491, acc=0.76562
# [5/100] training 29.8% loss=0.30340, acc=0.89062
# [5/100] training 30.0% loss=0.48643, acc=0.71875
# [5/100] training 30.2% loss=0.42563, acc=0.79688
# [5/100] training 30.4% loss=0.37734, acc=0.82812
# [5/100] training 30.6% loss=0.44528, acc=0.82812
# [5/100] training 30.7% loss=0.35795, acc=0.84375
# [5/100] training 30.9% loss=0.51516, acc=0.78125
# [5/100] training 31.0% loss=0.39077, acc=0.82812
# [5/100] training 31.3% loss=0.38487, acc=0.82812
# [5/100] training 31.4% loss=0.41916, acc=0.76562
# [5/100] training 31.6% loss=0.55699, acc=0.75000
# [5/100] training 31.8% loss=0.30073, acc=0.92188
# [5/100] training 31.9% loss=0.37976, acc=0.81250
# [5/100] training 32.1% loss=0.42864, acc=0.81250
# [5/100] training 32.2% loss=0.35506, acc=0.82812
# [5/100] training 32.5% loss=0.39707, acc=0.85938
# [5/100] training 32.6% loss=0.40914, acc=0.82812
# [5/100] training 32.8% loss=0.34834, acc=0.87500
# [5/100] training 32.9% loss=0.45384, acc=0.76562
# [5/100] training 33.1% loss=0.44106, acc=0.78125
# [5/100] training 33.3% loss=0.35977, acc=0.81250
# [5/100] training 33.4% loss=0.39803, acc=0.89062
# [5/100] training 33.7% loss=0.49240, acc=0.81250
# [5/100] training 33.8% loss=0.56124, acc=0.78125
# [5/100] training 34.0% loss=0.42625, acc=0.82812
# [5/100] training 34.1% loss=0.34453, acc=0.87500
# [5/100] training 34.3% loss=0.41774, acc=0.81250
# [5/100] training 34.5% loss=0.45584, acc=0.79688
# [5/100] training 34.7% loss=0.29429, acc=0.89062
# [5/100] training 34.9% loss=0.38853, acc=0.79688
# [5/100] training 35.0% loss=0.40809, acc=0.79688
# [5/100] training 35.2% loss=0.44880, acc=0.81250
# [5/100] training 35.3% loss=0.46003, acc=0.75000
# [5/100] training 35.5% loss=0.50045, acc=0.71875
# [5/100] training 35.6% loss=0.34018, acc=0.85938
# [5/100] training 35.9% loss=0.50140, acc=0.76562
# [5/100] training 36.1% loss=0.37444, acc=0.82812
# [5/100] training 36.2% loss=0.44420, acc=0.84375
# [5/100] training 36.4% loss=0.36850, acc=0.85938
# [5/100] training 36.5% loss=0.42381, acc=0.79688
# [5/100] training 36.7% loss=0.30091, acc=0.90625
# [5/100] training 36.8% loss=0.34316, acc=0.81250
# [5/100] training 37.1% loss=0.44439, acc=0.79688
# [5/100] training 37.3% loss=0.49278, acc=0.78125
# [5/100] training 37.4% loss=0.45952, acc=0.75000
# [5/100] training 37.6% loss=0.37806, acc=0.79688
# [5/100] training 37.7% loss=0.35501, acc=0.87500
# [5/100] training 37.9% loss=0.35252, acc=0.87500
# [5/100] training 38.1% loss=0.47973, acc=0.71875
# [5/100] training 38.3% loss=0.47027, acc=0.78125
# [5/100] training 38.4% loss=0.30364, acc=0.82812
# [5/100] training 38.6% loss=0.36778, acc=0.81250
# [5/100] training 38.8% loss=0.53158, acc=0.71875
# [5/100] training 38.9% loss=0.39288, acc=0.84375
# [5/100] training 39.1% loss=0.36515, acc=0.85938
# [5/100] training 39.3% loss=0.34800, acc=0.81250
# [5/100] training 39.5% loss=0.51419, acc=0.76562
# [5/100] training 39.6% loss=0.48237, acc=0.76562
# [5/100] training 39.8% loss=0.32815, acc=0.82812
# [5/100] training 40.0% loss=0.44621, acc=0.76562
# [5/100] training 40.1% loss=0.46436, acc=0.78125
# [5/100] training 40.4% loss=0.37706, acc=0.79688
# [5/100] training 40.5% loss=0.31277, acc=0.87500
# [5/100] training 40.7% loss=0.31546, acc=0.84375
# [5/100] training 40.8% loss=0.40382, acc=0.79688
# [5/100] training 41.0% loss=0.36165, acc=0.87500
# [5/100] training 41.1% loss=0.49025, acc=0.70312
# [5/100] training 41.3% loss=0.51416, acc=0.71875
# [5/100] training 41.6% loss=0.40091, acc=0.82812
# [5/100] training 41.7% loss=0.59671, acc=0.73438
# [5/100] training 41.9% loss=0.36778, acc=0.85938
# [5/100] training 42.0% loss=0.39101, acc=0.79688
# [5/100] training 42.2% loss=0.44410, acc=0.79688
# [5/100] training 42.3% loss=0.39550, acc=0.85938
# [5/100] training 42.5% loss=0.26319, acc=0.89062
# [5/100] training 42.8% loss=0.33050, acc=0.84375
# [5/100] training 42.9% loss=0.29964, acc=0.87500
# [5/100] training 43.1% loss=0.39796, acc=0.81250
# [5/100] training 43.2% loss=0.46099, acc=0.73438
# [5/100] training 43.4% loss=0.42406, acc=0.81250
# [5/100] training 43.5% loss=0.30512, acc=0.89062
# [5/100] training 43.8% loss=0.47923, acc=0.71875
# [5/100] training 43.9% loss=0.40439, acc=0.79688
# [5/100] training 44.1% loss=0.27211, acc=0.89062
# [5/100] training 44.3% loss=0.42584, acc=0.79688
# [5/100] training 44.4% loss=0.43332, acc=0.79688
# [5/100] training 44.6% loss=0.50276, acc=0.76562
# [5/100] training 44.7% loss=0.46583, acc=0.76562
# [5/100] training 45.0% loss=0.38284, acc=0.81250
# [5/100] training 45.1% loss=0.48947, acc=0.76562
# [5/100] training 45.3% loss=0.52616, acc=0.73438
# [5/100] training 45.5% loss=0.31834, acc=0.87500
# [5/100] training 45.6% loss=0.42093, acc=0.81250
# [5/100] training 45.8% loss=0.46054, acc=0.76562
# [5/100] training 45.9% loss=0.34393, acc=0.81250
# [5/100] training 46.2% loss=0.32907, acc=0.84375
# [5/100] training 46.3% loss=0.35674, acc=0.84375
# [5/100] training 46.5% loss=0.50110, acc=0.79688
# [5/100] training 46.6% loss=0.33931, acc=0.79688
# [5/100] training 46.8% loss=0.39076, acc=0.82812
# [5/100] training 47.0% loss=0.31060, acc=0.84375
# [5/100] training 47.2% loss=0.47361, acc=0.78125
# [5/100] training 47.4% loss=0.32955, acc=0.82812
# [5/100] training 47.5% loss=0.36946, acc=0.84375
# [5/100] training 47.7% loss=0.36031, acc=0.82812
# [5/100] training 47.8% loss=0.37361, acc=0.84375
# [5/100] training 48.0% loss=0.47802, acc=0.81250
# [5/100] training 48.3% loss=0.33693, acc=0.81250
# [5/100] training 48.4% loss=0.27813, acc=0.87500
# [5/100] training 48.6% loss=0.38094, acc=0.87500
# [5/100] training 48.7% loss=0.52003, acc=0.76562
# [5/100] training 48.9% loss=0.48168, acc=0.78125
# [5/100] training 49.0% loss=0.42904, acc=0.81250
# [5/100] training 49.2% loss=0.37558, acc=0.79688
# [5/100] training 49.3% loss=0.41342, acc=0.79688
# [5/100] training 49.6% loss=0.29009, acc=0.84375
# [5/100] training 49.8% loss=0.33386, acc=0.82812
# [5/100] training 49.9% loss=0.46167, acc=0.81250
# [5/100] training 50.1% loss=0.38512, acc=0.85938
# [5/100] training 50.2% loss=0.33513, acc=0.82812
# [5/100] training 50.4% loss=0.46606, acc=0.78125
# [5/100] training 50.6% loss=0.51433, acc=0.75000
# [5/100] training 50.8% loss=0.56264, acc=0.68750
# [5/100] training 51.0% loss=0.44335, acc=0.75000
# [5/100] training 51.1% loss=0.31120, acc=0.87500
# [5/100] training 51.3% loss=0.46342, acc=0.78125
# [5/100] training 51.4% loss=0.44087, acc=0.79688
# [5/100] training 51.7% loss=0.50242, acc=0.75000
# [5/100] training 51.8% loss=0.36195, acc=0.81250
# [5/100] training 52.0% loss=0.43482, acc=0.81250
# [5/100] training 52.1% loss=0.50417, acc=0.73438
# [5/100] training 52.3% loss=0.48768, acc=0.75000
# [5/100] training 52.5% loss=0.37436, acc=0.87500
# [5/100] training 52.6% loss=0.42859, acc=0.84375
# [5/100] training 52.9% loss=0.50011, acc=0.76562
# [5/100] training 53.0% loss=0.45149, acc=0.73438
# [5/100] training 53.2% loss=0.47061, acc=0.82812
# [5/100] training 53.3% loss=0.26775, acc=0.90625
# [5/100] training 53.5% loss=0.55487, acc=0.71875
# [5/100] training 53.7% loss=0.43486, acc=0.78125
# [5/100] training 53.8% loss=0.48467, acc=0.75000
# [5/100] training 54.1% loss=0.45836, acc=0.78125
# [5/100] training 54.2% loss=0.31247, acc=0.89062
# [5/100] training 54.4% loss=0.38137, acc=0.84375
# [5/100] training 54.5% loss=0.32849, acc=0.84375
# [5/100] training 54.7% loss=0.48642, acc=0.73438
# [5/100] training 54.8% loss=0.24150, acc=0.89062
# [5/100] training 55.1% loss=0.26932, acc=0.89062
# [5/100] training 55.3% loss=0.23942, acc=0.92188
# [5/100] training 55.4% loss=0.43028, acc=0.81250
# [5/100] training 55.6% loss=0.38310, acc=0.84375
# [5/100] training 55.7% loss=0.34968, acc=0.84375
# [5/100] training 55.9% loss=0.34708, acc=0.81250
# [5/100] training 56.0% loss=0.40107, acc=0.81250
# [5/100] training 56.3% loss=0.55461, acc=0.75000
# [5/100] training 56.5% loss=0.44312, acc=0.73438
# [5/100] training 56.6% loss=0.43760, acc=0.79688
# [5/100] training 56.8% loss=0.47709, acc=0.75000
# [5/100] training 56.9% loss=0.44098, acc=0.78125
# [5/100] training 57.1% loss=0.48925, acc=0.78125
# [5/100] training 57.2% loss=0.39217, acc=0.76562
# [5/100] training 57.5% loss=0.42146, acc=0.81250
# [5/100] training 57.6% loss=0.50365, acc=0.67188
# [5/100] training 57.8% loss=0.38421, acc=0.75000
# [5/100] training 58.0% loss=0.32202, acc=0.87500
# [5/100] training 58.1% loss=0.45612, acc=0.79688
# [5/100] training 58.3% loss=0.35548, acc=0.85938
# [5/100] training 58.4% loss=0.37327, acc=0.85938
# [5/100] training 58.7% loss=0.44240, acc=0.78125
# [5/100] training 58.8% loss=0.52469, acc=0.78125
# [5/100] training 59.0% loss=0.40124, acc=0.85938
# [5/100] training 59.2% loss=0.49955, acc=0.71875
# [5/100] training 59.3% loss=0.45831, acc=0.75000
# [5/100] training 59.5% loss=0.47235, acc=0.79688
# [5/100] training 59.7% loss=0.49250, acc=0.75000
# [5/100] training 59.9% loss=0.41310, acc=0.81250
# [5/100] training 60.0% loss=0.40546, acc=0.81250
# [5/100] training 60.2% loss=0.44916, acc=0.73438
# [5/100] training 60.3% loss=0.47337, acc=0.71875
# [5/100] training 60.5% loss=0.40959, acc=0.82812
# [5/100] training 60.8% loss=0.44183, acc=0.78125
# [5/100] training 60.9% loss=0.42761, acc=0.81250
# [5/100] training 61.1% loss=0.52011, acc=0.73438
# [5/100] training 61.2% loss=0.40478, acc=0.81250
# [5/100] training 61.4% loss=0.46629, acc=0.71875
# [5/100] training 61.5% loss=0.44674, acc=0.76562
# [5/100] training 61.7% loss=0.53236, acc=0.78125
# [5/100] training 62.0% loss=0.46176, acc=0.75000
# [5/100] training 62.1% loss=0.48132, acc=0.73438
# [5/100] training 62.3% loss=0.38247, acc=0.81250
# [5/100] training 62.4% loss=0.41339, acc=0.76562
# [5/100] training 62.6% loss=0.51093, acc=0.76562
# [5/100] training 62.7% loss=0.37894, acc=0.87500
# [5/100] training 62.9% loss=0.49948, acc=0.75000
# [5/100] training 63.1% loss=0.34408, acc=0.85938
# [5/100] training 63.3% loss=0.51832, acc=0.79688
# [5/100] training 63.5% loss=0.44251, acc=0.78125
# [5/100] training 63.6% loss=0.39733, acc=0.89062
# [5/100] training 63.8% loss=0.43585, acc=0.84375
# [5/100] training 63.9% loss=0.38918, acc=0.82812
# [5/100] training 64.2% loss=0.35027, acc=0.81250
# [5/100] training 64.3% loss=0.44687, acc=0.78125
# [5/100] training 64.5% loss=0.41840, acc=0.82812
# [5/100] training 64.7% loss=0.46008, acc=0.71875
# [5/100] training 64.8% loss=0.52340, acc=0.75000
# [5/100] training 65.0% loss=0.44187, acc=0.76562
# [5/100] training 65.1% loss=0.46234, acc=0.73438
# [5/100] training 65.4% loss=0.44510, acc=0.78125
# [5/100] training 65.5% loss=0.38690, acc=0.87500
# [5/100] training 65.7% loss=0.34023, acc=0.84375
# [5/100] training 65.8% loss=0.51381, acc=0.73438
# [5/100] training 66.0% loss=0.43344, acc=0.76562
# [5/100] training 66.2% loss=0.29769, acc=0.89062
# [5/100] training 66.3% loss=0.57189, acc=0.71875
# [5/100] training 66.6% loss=0.41588, acc=0.81250
# [5/100] training 66.7% loss=0.31726, acc=0.84375
# [5/100] training 66.9% loss=0.39591, acc=0.87500
# [5/100] training 67.0% loss=0.51603, acc=0.73438
# [5/100] training 67.2% loss=0.29983, acc=0.87500
# [5/100] training 67.4% loss=0.51672, acc=0.75000
# [5/100] training 67.6% loss=0.38830, acc=0.82812
# [5/100] training 67.8% loss=0.43745, acc=0.78125
# [5/100] training 67.9% loss=0.34447, acc=0.87500
# [5/100] training 68.1% loss=0.33900, acc=0.84375
# [5/100] training 68.2% loss=0.33772, acc=0.87500
# [5/100] training 68.4% loss=0.29723, acc=0.89062
# [5/100] training 68.5% loss=0.55074, acc=0.73438
# [5/100] training 68.8% loss=0.43059, acc=0.81250
# [5/100] training 69.0% loss=0.49512, acc=0.73438
# [5/100] training 69.1% loss=0.29393, acc=0.89062
# [5/100] training 69.3% loss=0.31096, acc=0.90625
# [5/100] training 69.4% loss=0.33655, acc=0.92188
# [5/100] training 69.6% loss=0.45086, acc=0.76562
# [5/100] training 69.7% loss=0.45959, acc=0.76562
# [5/100] training 70.0% loss=0.52629, acc=0.68750
# [5/100] training 70.2% loss=0.40815, acc=0.82812
# [5/100] training 70.3% loss=0.37644, acc=0.84375
# [5/100] training 70.5% loss=0.41638, acc=0.81250
# [5/100] training 70.6% loss=0.36985, acc=0.85938
# [5/100] training 70.8% loss=0.40343, acc=0.73438
# [5/100] training 71.0% loss=0.42037, acc=0.76562
# [5/100] training 71.2% loss=0.31897, acc=0.81250
# [5/100] training 71.3% loss=0.32715, acc=0.82812
# [5/100] training 71.5% loss=0.60677, acc=0.73438
# [5/100] training 71.7% loss=0.47822, acc=0.79688
# [5/100] training 71.8% loss=0.47564, acc=0.76562
# [5/100] training 72.0% loss=0.33420, acc=0.84375
# [5/100] training 72.2% loss=0.38250, acc=0.78125
# [5/100] training 72.4% loss=0.48945, acc=0.81250
# [5/100] training 72.5% loss=0.47180, acc=0.82812
# [5/100] training 72.7% loss=0.50178, acc=0.71875
# [5/100] training 72.9% loss=0.37689, acc=0.82812
# [5/100] training 73.0% loss=0.38038, acc=0.81250
# [5/100] training 73.3% loss=0.38361, acc=0.81250
# [5/100] training 73.4% loss=0.39283, acc=0.90625
# [5/100] training 73.6% loss=0.32583, acc=0.85938
# [5/100] training 73.7% loss=0.37352, acc=0.82812
# [5/100] training 73.9% loss=0.36519, acc=0.82812
# [5/100] training 74.0% loss=0.37681, acc=0.79688
# [5/100] training 74.2% loss=0.38396, acc=0.84375
# [5/100] training 74.5% loss=0.39896, acc=0.78125
# [5/100] training 74.6% loss=0.45948, acc=0.79688
# [5/100] training 74.8% loss=0.50352, acc=0.75000
# [5/100] training 74.9% loss=0.47087, acc=0.76562
# [5/100] training 75.1% loss=0.38447, acc=0.79688
# [5/100] training 75.2% loss=0.34597, acc=0.85938
# [5/100] training 75.4% loss=0.38498, acc=0.81250
# [5/100] training 75.7% loss=0.41004, acc=0.76562
# [5/100] training 75.8% loss=0.44876, acc=0.75000
# [5/100] training 76.0% loss=0.34897, acc=0.84375
# [5/100] training 76.1% loss=0.41842, acc=0.82812
# [5/100] training 76.3% loss=0.56507, acc=0.75000
# [5/100] training 76.4% loss=0.39681, acc=0.79688
# [5/100] training 76.7% loss=0.30393, acc=0.85938
# [5/100] training 76.8% loss=0.38398, acc=0.79688
# [5/100] training 77.0% loss=0.35131, acc=0.81250
# [5/100] training 77.2% loss=0.38570, acc=0.82812
# [5/100] training 77.3% loss=0.37541, acc=0.84375
# [5/100] training 77.5% loss=0.32746, acc=0.87500
# [5/100] training 77.6% loss=0.39900, acc=0.84375
# [5/100] training 77.9% loss=0.43978, acc=0.70312
# [5/100] training 78.0% loss=0.42659, acc=0.75000
# [5/100] training 78.2% loss=0.39196, acc=0.87500
# [5/100] training 78.4% loss=0.28264, acc=0.89062
# [5/100] training 78.5% loss=0.50510, acc=0.76562
# [5/100] training 78.7% loss=0.35766, acc=0.85938
# [5/100] training 78.8% loss=0.34771, acc=0.82812
# [5/100] training 79.1% loss=0.30846, acc=0.84375
# [5/100] training 79.2% loss=0.42913, acc=0.81250
# [5/100] training 79.4% loss=0.50566, acc=0.73438
# [5/100] training 79.5% loss=0.37949, acc=0.82812
# [5/100] training 79.7% loss=0.38726, acc=0.84375
# [5/100] training 79.9% loss=0.36369, acc=0.82812
# [5/100] training 80.1% loss=0.35833, acc=0.78125
# [5/100] training 80.3% loss=0.47797, acc=0.76562
# [5/100] training 80.4% loss=0.39615, acc=0.87500
# [5/100] training 80.6% loss=0.45280, acc=0.76562
# [5/100] training 80.7% loss=0.38179, acc=0.81250
# [5/100] training 80.9% loss=0.42051, acc=0.81250
# [5/100] training 81.2% loss=0.42363, acc=0.76562
# [5/100] training 81.3% loss=0.41983, acc=0.82812
# [5/100] training 81.5% loss=0.29006, acc=0.85938
# [5/100] training 81.6% loss=0.44867, acc=0.79688
# [5/100] training 81.8% loss=0.44717, acc=0.79688
# [5/100] training 81.9% loss=0.46676, acc=0.81250
# [5/100] training 82.1% loss=0.29458, acc=0.81250
# [5/100] training 82.2% loss=0.48055, acc=0.75000
# [5/100] training 82.5% loss=0.37659, acc=0.84375
# [5/100] training 82.7% loss=0.40399, acc=0.84375
# [5/100] training 82.8% loss=0.40695, acc=0.81250
# [5/100] training 83.0% loss=0.40052, acc=0.76562
# [5/100] training 83.1% loss=0.42022, acc=0.90625
# [5/100] training 83.3% loss=0.43332, acc=0.76562
# [5/100] training 83.5% loss=0.36087, acc=0.81250
# [5/100] training 83.7% loss=0.42817, acc=0.84375
# [5/100] training 83.9% loss=0.39826, acc=0.84375
# [5/100] training 84.0% loss=0.39331, acc=0.76562
# [5/100] training 84.2% loss=0.34605, acc=0.81250
# [5/100] training 84.3% loss=0.32447, acc=0.85938
# [5/100] training 84.5% loss=0.33834, acc=0.84375
# [5/100] training 84.7% loss=0.53392, acc=0.75000
# [5/100] training 84.9% loss=0.40871, acc=0.76562
# [5/100] training 85.0% loss=0.35622, acc=0.81250
# [5/100] training 85.2% loss=0.47121, acc=0.78125
# [5/100] training 85.4% loss=0.40103, acc=0.90625
# [5/100] training 85.5% loss=0.32667, acc=0.81250
# [5/100] training 85.8% loss=0.45410, acc=0.76562
# [5/100] training 85.9% loss=0.44635, acc=0.81250
# [5/100] training 86.1% loss=0.33591, acc=0.81250
# [5/100] training 86.2% loss=0.36292, acc=0.84375
# [5/100] training 86.4% loss=0.53514, acc=0.71875
# [5/100] training 86.6% loss=0.50331, acc=0.75000
# [5/100] training 86.7% loss=0.43480, acc=0.79688
# [5/100] training 87.0% loss=0.38522, acc=0.82812
# [5/100] training 87.1% loss=0.38683, acc=0.76562
# [5/100] training 87.3% loss=0.39123, acc=0.84375
# [5/100] training 87.4% loss=0.34866, acc=0.85938
# [5/100] training 87.6% loss=0.44393, acc=0.84375
# [5/100] training 87.7% loss=0.49439, acc=0.79688
# [5/100] training 87.9% loss=0.34142, acc=0.89062
# [5/100] training 88.2% loss=0.22900, acc=0.90625
# [5/100] training 88.3% loss=0.46186, acc=0.75000
# [5/100] training 88.5% loss=0.31614, acc=0.87500
# [5/100] training 88.6% loss=0.23021, acc=0.90625
# [5/100] training 88.8% loss=0.37720, acc=0.82812
# [5/100] training 88.9% loss=0.39263, acc=0.84375
# [5/100] training 89.2% loss=0.26923, acc=0.89062
# [5/100] training 89.4% loss=0.35050, acc=0.81250
# [5/100] training 89.5% loss=0.47506, acc=0.79688
# [5/100] training 89.7% loss=0.53065, acc=0.76562
# [5/100] training 89.8% loss=0.36969, acc=0.87500
# [5/100] training 90.0% loss=0.33777, acc=0.85938
# [5/100] training 90.1% loss=0.36944, acc=0.81250
# [5/100] training 90.4% loss=0.39059, acc=0.87500
# [5/100] training 90.5% loss=0.31394, acc=0.84375
# [5/100] training 90.7% loss=0.43462, acc=0.81250
# [5/100] training 90.9% loss=0.32924, acc=0.90625
# [5/100] training 91.0% loss=0.38762, acc=0.84375
# [5/100] training 91.2% loss=0.47562, acc=0.76562
# [5/100] training 91.3% loss=0.51543, acc=0.76562
# [5/100] training 91.6% loss=0.63447, acc=0.67188
# [5/100] training 91.7% loss=0.37989, acc=0.81250
# [5/100] training 91.9% loss=0.38146, acc=0.81250
# [5/100] training 92.1% loss=0.44781, acc=0.81250
# [5/100] training 92.2% loss=0.37335, acc=0.81250
# [5/100] training 92.4% loss=0.39815, acc=0.82812
# [5/100] training 92.6% loss=0.45437, acc=0.76562
# [5/100] training 92.8% loss=0.50948, acc=0.76562
# [5/100] training 92.9% loss=0.32281, acc=0.90625
# [5/100] training 93.1% loss=0.49179, acc=0.78125
# [5/100] training 93.2% loss=0.38780, acc=0.78125
# [5/100] training 93.4% loss=0.30002, acc=0.87500
# [5/100] training 93.7% loss=0.40201, acc=0.82812
# [5/100] training 93.8% loss=0.37456, acc=0.79688
# [5/100] training 94.0% loss=0.29661, acc=0.87500
# [5/100] training 94.1% loss=0.50502, acc=0.71875
# [5/100] training 94.3% loss=0.29066, acc=0.85938
# [5/100] training 94.4% loss=0.43473, acc=0.76562
# [5/100] training 94.6% loss=0.34748, acc=0.84375
# [5/100] training 94.9% loss=0.39070, acc=0.82812
# [5/100] training 95.0% loss=0.41222, acc=0.82812
# [5/100] training 95.2% loss=0.45173, acc=0.75000
# [5/100] training 95.3% loss=0.38506, acc=0.85938
# [5/100] training 95.5% loss=0.33008, acc=0.90625
# [5/100] training 95.6% loss=0.50153, acc=0.79688
# [5/100] training 95.8% loss=0.47811, acc=0.70312
# [5/100] training 96.0% loss=0.36905, acc=0.82812
# [5/100] training 96.2% loss=0.25900, acc=0.87500
# [5/100] training 96.4% loss=0.26557, acc=0.87500
# [5/100] training 96.5% loss=0.43554, acc=0.81250
# [5/100] training 96.7% loss=0.33860, acc=0.84375
# [5/100] training 96.8% loss=0.35145, acc=0.87500
# [5/100] training 97.1% loss=0.41742, acc=0.75000
# [5/100] training 97.2% loss=0.40972, acc=0.81250
# [5/100] training 97.4% loss=0.42376, acc=0.78125
# [5/100] training 97.6% loss=0.42616, acc=0.81250
# [5/100] training 97.7% loss=0.38133, acc=0.81250
# [5/100] training 97.9% loss=0.27583, acc=0.90625
# [5/100] training 98.0% loss=0.41599, acc=0.76562
# [5/100] training 98.3% loss=0.35817, acc=0.85938
# [5/100] training 98.4% loss=0.32928, acc=0.84375
# [5/100] training 98.6% loss=0.59759, acc=0.71875
# [5/100] training 98.7% loss=0.42213, acc=0.75000
# [5/100] training 98.9% loss=0.42981, acc=0.79688
# [5/100] training 99.1% loss=0.26487, acc=0.89062
# [5/100] training 99.2% loss=0.25199, acc=0.89062
# [5/100] training 99.5% loss=0.34618, acc=0.84375
# [5/100] training 99.6% loss=0.37097, acc=0.84375
# [5/100] training 99.8% loss=0.31330, acc=0.82812
# [5/100] training 99.9% loss=0.28211, acc=0.90625
# [5/100] testing 0.9% loss=0.30418, acc=0.82812
# [5/100] testing 1.8% loss=0.47353, acc=0.81250
# [5/100] testing 2.2% loss=0.41669, acc=0.81250
# [5/100] testing 3.1% loss=0.38584, acc=0.81250
# [5/100] testing 3.5% loss=0.35000, acc=0.85938
# [5/100] testing 4.4% loss=0.45817, acc=0.78125
# [5/100] testing 4.8% loss=0.54747, acc=0.71875
# [5/100] testing 5.7% loss=0.34462, acc=0.87500
# [5/100] testing 6.6% loss=0.43214, acc=0.78125
# [5/100] testing 7.0% loss=0.31759, acc=0.89062
# [5/100] testing 7.9% loss=0.45361, acc=0.78125
# [5/100] testing 8.3% loss=0.35359, acc=0.84375
# [5/100] testing 9.2% loss=0.40559, acc=0.76562
# [5/100] testing 9.7% loss=0.41048, acc=0.82812
# [5/100] testing 10.5% loss=0.58973, acc=0.76562
# [5/100] testing 11.0% loss=0.38877, acc=0.76562
# [5/100] testing 11.8% loss=0.47080, acc=0.81250
# [5/100] testing 12.7% loss=0.46982, acc=0.75000
# [5/100] testing 13.2% loss=0.30135, acc=0.85938
# [5/100] testing 14.0% loss=0.43983, acc=0.76562
# [5/100] testing 14.5% loss=0.48944, acc=0.75000
# [5/100] testing 15.4% loss=0.42672, acc=0.84375
# [5/100] testing 15.8% loss=0.33255, acc=0.82812
# [5/100] testing 16.7% loss=0.43417, acc=0.85938
# [5/100] testing 17.5% loss=0.49167, acc=0.79688
# [5/100] testing 18.0% loss=0.32777, acc=0.82812
# [5/100] testing 18.9% loss=0.37866, acc=0.78125
# [5/100] testing 19.3% loss=0.36332, acc=0.81250
# [5/100] testing 20.2% loss=0.52393, acc=0.76562
# [5/100] testing 20.6% loss=0.56072, acc=0.70312
# [5/100] testing 21.5% loss=0.42016, acc=0.79688
# [5/100] testing 21.9% loss=0.54220, acc=0.64062
# [5/100] testing 22.8% loss=0.51098, acc=0.75000
# [5/100] testing 23.7% loss=0.39945, acc=0.81250
# [5/100] testing 24.1% loss=0.36799, acc=0.85938
# [5/100] testing 25.0% loss=0.43028, acc=0.76562
# [5/100] testing 25.4% loss=0.31503, acc=0.84375
# [5/100] testing 26.3% loss=0.49360, acc=0.68750
# [5/100] testing 26.8% loss=0.41233, acc=0.81250
# [5/100] testing 27.6% loss=0.53213, acc=0.76562
# [5/100] testing 28.5% loss=0.48486, acc=0.78125
# [5/100] testing 29.0% loss=0.39663, acc=0.82812
# [5/100] testing 29.8% loss=0.46041, acc=0.82812
# [5/100] testing 30.3% loss=0.48568, acc=0.73438
# [5/100] testing 31.1% loss=0.53401, acc=0.78125
# [5/100] testing 31.6% loss=0.30024, acc=0.84375
# [5/100] testing 32.5% loss=0.37905, acc=0.81250
# [5/100] testing 32.9% loss=0.41297, acc=0.82812
# [5/100] testing 33.8% loss=0.52919, acc=0.75000
# [5/100] testing 34.7% loss=0.49273, acc=0.76562
# [5/100] testing 35.1% loss=0.40860, acc=0.84375
# [5/100] testing 36.0% loss=0.57860, acc=0.71875
# [5/100] testing 36.4% loss=0.49231, acc=0.76562
# [5/100] testing 37.3% loss=0.43501, acc=0.82812
# [5/100] testing 37.7% loss=0.51118, acc=0.75000
# [5/100] testing 38.6% loss=0.32550, acc=0.84375
# [5/100] testing 39.5% loss=0.54046, acc=0.79688
# [5/100] testing 39.9% loss=0.35558, acc=0.85938
# [5/100] testing 40.8% loss=0.41675, acc=0.81250
# [5/100] testing 41.2% loss=0.40496, acc=0.85938
# [5/100] testing 42.1% loss=0.44912, acc=0.78125
# [5/100] testing 42.5% loss=0.40868, acc=0.82812
# [5/100] testing 43.4% loss=0.44541, acc=0.78125
# [5/100] testing 43.9% loss=0.38687, acc=0.78125
# [5/100] testing 44.7% loss=0.52711, acc=0.76562
# [5/100] testing 45.6% loss=0.41553, acc=0.81250
# [5/100] testing 46.1% loss=0.31817, acc=0.89062
# [5/100] testing 46.9% loss=0.35155, acc=0.81250
# [5/100] testing 47.4% loss=0.34414, acc=0.79688
# [5/100] testing 48.3% loss=0.40869, acc=0.82812
# [5/100] testing 48.7% loss=0.62958, acc=0.75000
# [5/100] testing 49.6% loss=0.38207, acc=0.81250
# [5/100] testing 50.4% loss=0.28860, acc=0.89062
# [5/100] testing 50.9% loss=0.44037, acc=0.78125
# [5/100] testing 51.8% loss=0.47859, acc=0.76562
# [5/100] testing 52.2% loss=0.46859, acc=0.81250
# [5/100] testing 53.1% loss=0.36268, acc=0.81250
# [5/100] testing 53.5% loss=0.38286, acc=0.84375
# [5/100] testing 54.4% loss=0.64374, acc=0.67188
# [5/100] testing 54.8% loss=0.47270, acc=0.75000
# [5/100] testing 55.7% loss=0.34151, acc=0.82812
# [5/100] testing 56.6% loss=0.44296, acc=0.89062
# [5/100] testing 57.0% loss=0.60891, acc=0.70312
# [5/100] testing 57.9% loss=0.52262, acc=0.73438
# [5/100] testing 58.3% loss=0.60211, acc=0.64062
# [5/100] testing 59.2% loss=0.43660, acc=0.76562
# [5/100] testing 59.7% loss=0.44374, acc=0.78125
# [5/100] testing 60.5% loss=0.48774, acc=0.78125
# [5/100] testing 61.4% loss=0.27917, acc=0.89062
# [5/100] testing 61.9% loss=0.34211, acc=0.82812
# [5/100] testing 62.7% loss=0.35712, acc=0.87500
# [5/100] testing 63.2% loss=0.49793, acc=0.71875
# [5/100] testing 64.0% loss=0.43847, acc=0.70312
# [5/100] testing 64.5% loss=0.39805, acc=0.73438
# [5/100] testing 65.4% loss=0.41499, acc=0.78125
# [5/100] testing 65.8% loss=0.39322, acc=0.79688
# [5/100] testing 66.7% loss=0.30422, acc=0.84375
# [5/100] testing 67.6% loss=0.46778, acc=0.76562
# [5/100] testing 68.0% loss=0.41525, acc=0.82812
# [5/100] testing 68.9% loss=0.37793, acc=0.85938
# [5/100] testing 69.3% loss=0.44789, acc=0.78125
# [5/100] testing 70.2% loss=0.49022, acc=0.76562
# [5/100] testing 70.6% loss=0.45175, acc=0.75000
# [5/100] testing 71.5% loss=0.43148, acc=0.81250
# [5/100] testing 72.4% loss=0.34640, acc=0.82812
# [5/100] testing 72.8% loss=0.35343, acc=0.82812
# [5/100] testing 73.7% loss=0.35193, acc=0.84375
# [5/100] testing 74.1% loss=0.51535, acc=0.76562
# [5/100] testing 75.0% loss=0.29163, acc=0.84375
# [5/100] testing 75.4% loss=0.36829, acc=0.87500
# [5/100] testing 76.3% loss=0.21715, acc=0.90625
# [5/100] testing 76.8% loss=0.32011, acc=0.82812
# [5/100] testing 77.6% loss=0.33459, acc=0.82812
# [5/100] testing 78.5% loss=0.58553, acc=0.73438
# [5/100] testing 79.0% loss=0.45023, acc=0.78125
# [5/100] testing 79.8% loss=0.47422, acc=0.73438
# [5/100] testing 80.3% loss=0.36251, acc=0.81250
# [5/100] testing 81.2% loss=0.45192, acc=0.81250
# [5/100] testing 81.6% loss=0.37469, acc=0.82812
# [5/100] testing 82.5% loss=0.35121, acc=0.82812
# [5/100] testing 83.3% loss=0.29875, acc=0.84375
# [5/100] testing 83.8% loss=0.36919, acc=0.82812
# [5/100] testing 84.7% loss=0.35332, acc=0.85938
# [5/100] testing 85.1% loss=0.48969, acc=0.75000
# [5/100] testing 86.0% loss=0.43227, acc=0.78125
# [5/100] testing 86.4% loss=0.51545, acc=0.75000
# [5/100] testing 87.3% loss=0.49343, acc=0.81250
# [5/100] testing 87.7% loss=0.39531, acc=0.82812
# [5/100] testing 88.6% loss=0.31897, acc=0.84375
# [5/100] testing 89.5% loss=0.55165, acc=0.73438
# [5/100] testing 89.9% loss=0.55154, acc=0.71875
# [5/100] testing 90.8% loss=0.30887, acc=0.87500
# [5/100] testing 91.2% loss=0.28238, acc=0.82812
# [5/100] testing 92.1% loss=0.38815, acc=0.79688
# [5/100] testing 92.6% loss=0.47327, acc=0.79688
# [5/100] testing 93.4% loss=0.51358, acc=0.75000
# [5/100] testing 94.3% loss=0.30802, acc=0.89062
# [5/100] testing 94.7% loss=0.39837, acc=0.78125
# [5/100] testing 95.6% loss=0.40646, acc=0.76562
# [5/100] testing 96.1% loss=0.39731, acc=0.81250
# [5/100] testing 96.9% loss=0.37694, acc=0.81250
# [5/100] testing 97.4% loss=0.31199, acc=0.87500
# [5/100] testing 98.3% loss=0.49979, acc=0.73438
# [5/100] testing 98.7% loss=0.42537, acc=0.79688
# [5/100] testing 99.6% loss=0.34840, acc=0.82812
# [6/100] training 0.2% loss=0.45615, acc=0.81250
# [6/100] training 0.4% loss=0.52791, acc=0.75000
# [6/100] training 0.5% loss=0.39704, acc=0.78125
# [6/100] training 0.8% loss=0.45850, acc=0.82812
# [6/100] training 0.9% loss=0.42672, acc=0.82812
# [6/100] training 1.1% loss=0.33876, acc=0.85938
# [6/100] training 1.2% loss=0.49563, acc=0.73438
# [6/100] training 1.4% loss=0.42283, acc=0.82812
# [6/100] training 1.6% loss=0.31930, acc=0.84375
# [6/100] training 1.8% loss=0.40370, acc=0.82812
# [6/100] training 2.0% loss=0.39550, acc=0.82812
# [6/100] training 2.1% loss=0.40566, acc=0.76562
# [6/100] training 2.3% loss=0.31535, acc=0.82812
# [6/100] training 2.4% loss=0.49284, acc=0.75000
# [6/100] training 2.6% loss=0.43923, acc=0.84375
# [6/100] training 2.7% loss=0.41924, acc=0.79688
# [6/100] training 3.0% loss=0.53123, acc=0.73438
# [6/100] training 3.2% loss=0.35329, acc=0.90625
# [6/100] training 3.3% loss=0.36037, acc=0.85938
# [6/100] training 3.5% loss=0.51092, acc=0.75000
# [6/100] training 3.6% loss=0.59689, acc=0.75000
# [6/100] training 3.8% loss=0.31043, acc=0.85938
# [6/100] training 3.9% loss=0.42769, acc=0.78125
# [6/100] training 4.2% loss=0.33943, acc=0.84375
# [6/100] training 4.4% loss=0.37749, acc=0.82812
# [6/100] training 4.5% loss=0.33402, acc=0.79688
# [6/100] training 4.7% loss=0.45875, acc=0.81250
# [6/100] training 4.8% loss=0.44457, acc=0.75000
# [6/100] training 5.0% loss=0.32741, acc=0.87500
# [6/100] training 5.2% loss=0.40363, acc=0.82812
# [6/100] training 5.4% loss=0.32403, acc=0.85938
# [6/100] training 5.5% loss=0.31109, acc=0.85938
# [6/100] training 5.7% loss=0.27698, acc=0.87500
# [6/100] training 5.9% loss=0.52702, acc=0.73438
# [6/100] training 6.0% loss=0.42648, acc=0.82812
# [6/100] training 6.3% loss=0.45502, acc=0.78125
# [6/100] training 6.4% loss=0.40857, acc=0.79688
# [6/100] training 6.6% loss=0.39538, acc=0.84375
# [6/100] training 6.7% loss=0.43892, acc=0.79688
# [6/100] training 6.9% loss=0.42788, acc=0.79688
# [6/100] training 7.1% loss=0.43447, acc=0.78125
# [6/100] training 7.2% loss=0.41083, acc=0.79688
# [6/100] training 7.5% loss=0.37405, acc=0.81250
# [6/100] training 7.6% loss=0.40115, acc=0.82812
# [6/100] training 7.8% loss=0.39263, acc=0.81250
# [6/100] training 7.9% loss=0.41310, acc=0.84375
# [6/100] training 8.1% loss=0.25998, acc=0.90625
# [6/100] training 8.2% loss=0.30926, acc=0.85938
# [6/100] training 8.4% loss=0.43664, acc=0.81250
# [6/100] training 8.7% loss=0.33714, acc=0.87500
# [6/100] training 8.8% loss=0.37438, acc=0.78125
# [6/100] training 9.0% loss=0.52215, acc=0.71875
# [6/100] training 9.1% loss=0.47748, acc=0.73438
# [6/100] training 9.3% loss=0.55201, acc=0.68750
# [6/100] training 9.4% loss=0.40734, acc=0.81250
# [6/100] training 9.7% loss=0.37809, acc=0.84375
# [6/100] training 9.9% loss=0.41862, acc=0.81250
# [6/100] training 10.0% loss=0.38142, acc=0.78125
# [6/100] training 10.2% loss=0.36030, acc=0.87500
# [6/100] training 10.3% loss=0.37354, acc=0.79688
# [6/100] training 10.5% loss=0.42327, acc=0.85938
# [6/100] training 10.6% loss=0.40302, acc=0.81250
# [6/100] training 10.9% loss=0.41568, acc=0.84375
# [6/100] training 11.0% loss=0.42403, acc=0.76562
# [6/100] training 11.2% loss=0.29799, acc=0.84375
# [6/100] training 11.4% loss=0.49712, acc=0.81250
# [6/100] training 11.5% loss=0.50387, acc=0.76562
# [6/100] training 11.7% loss=0.41964, acc=0.81250
# [6/100] training 11.8% loss=0.33426, acc=0.85938
# [6/100] training 12.1% loss=0.43487, acc=0.79688
# [6/100] training 12.2% loss=0.35991, acc=0.84375
# [6/100] training 12.4% loss=0.38563, acc=0.82812
# [6/100] training 12.6% loss=0.39017, acc=0.82812
# [6/100] training 12.7% loss=0.45830, acc=0.75000
# [6/100] training 12.9% loss=0.40847, acc=0.84375
# [6/100] training 13.0% loss=0.44816, acc=0.76562
# [6/100] training 13.3% loss=0.42194, acc=0.81250
# [6/100] training 13.4% loss=0.45201, acc=0.79688
# [6/100] training 13.6% loss=0.39902, acc=0.78125
# [6/100] training 13.7% loss=0.53966, acc=0.79688
# [6/100] training 13.9% loss=0.42795, acc=0.81250
# [6/100] training 14.1% loss=0.38488, acc=0.82812
# [6/100] training 14.3% loss=0.40749, acc=0.81250
# [6/100] training 14.5% loss=0.29097, acc=0.87500
# [6/100] training 14.6% loss=0.30063, acc=0.84375
# [6/100] training 14.8% loss=0.42747, acc=0.82812
# [6/100] training 14.9% loss=0.38290, acc=0.78125
# [6/100] training 15.1% loss=0.44188, acc=0.81250
# [6/100] training 15.4% loss=0.43798, acc=0.73438
# [6/100] training 15.5% loss=0.47241, acc=0.75000
# [6/100] training 15.7% loss=0.59066, acc=0.65625
# [6/100] training 15.8% loss=0.33441, acc=0.87500
# [6/100] training 16.0% loss=0.52167, acc=0.73438
# [6/100] training 16.1% loss=0.63085, acc=0.68750
# [6/100] training 16.3% loss=0.42009, acc=0.82812
# [6/100] training 16.4% loss=0.37999, acc=0.87500
# [6/100] training 16.7% loss=0.49942, acc=0.70312
# [6/100] training 16.9% loss=0.47998, acc=0.76562
# [6/100] training 17.0% loss=0.42537, acc=0.75000
# [6/100] training 17.2% loss=0.30804, acc=0.84375
# [6/100] training 17.3% loss=0.38942, acc=0.76562
# [6/100] training 17.5% loss=0.45660, acc=0.78125
# [6/100] training 17.7% loss=0.47237, acc=0.75000
# [6/100] training 17.9% loss=0.46814, acc=0.84375
# [6/100] training 18.1% loss=0.43784, acc=0.79688
# [6/100] training 18.2% loss=0.51002, acc=0.73438
# [6/100] training 18.4% loss=0.42065, acc=0.79688
# [6/100] training 18.5% loss=0.33865, acc=0.87500
# [6/100] training 18.8% loss=0.33251, acc=0.87500
# [6/100] training 18.9% loss=0.35306, acc=0.87500
# [6/100] training 19.1% loss=0.46937, acc=0.73438
# [6/100] training 19.2% loss=0.31957, acc=0.89062
# [6/100] training 19.4% loss=0.29788, acc=0.84375
# [6/100] training 19.6% loss=0.53638, acc=0.79688
# [6/100] training 19.7% loss=0.40023, acc=0.78125
# [6/100] training 20.0% loss=0.27455, acc=0.87500
# [6/100] training 20.1% loss=0.37006, acc=0.87500
# [6/100] training 20.3% loss=0.40120, acc=0.78125
# [6/100] training 20.4% loss=0.44847, acc=0.76562
# [6/100] training 20.6% loss=0.38439, acc=0.84375
# [6/100] training 20.8% loss=0.37811, acc=0.84375
# [6/100] training 20.9% loss=0.37710, acc=0.85938
# [6/100] training 21.2% loss=0.35456, acc=0.87500
# [6/100] training 21.3% loss=0.37684, acc=0.84375
# [6/100] training 21.5% loss=0.36922, acc=0.81250
# [6/100] training 21.6% loss=0.27383, acc=0.90625
# [6/100] training 21.8% loss=0.33385, acc=0.82812
# [6/100] training 21.9% loss=0.45334, acc=0.81250
# [6/100] training 22.2% loss=0.39565, acc=0.81250
# [6/100] training 22.4% loss=0.40728, acc=0.81250
# [6/100] training 22.5% loss=0.30674, acc=0.93750
# [6/100] training 22.7% loss=0.36716, acc=0.84375
# [6/100] training 22.8% loss=0.51029, acc=0.75000
# [6/100] training 23.0% loss=0.28888, acc=0.82812
# [6/100] training 23.1% loss=0.41788, acc=0.76562
# [6/100] training 23.4% loss=0.51937, acc=0.81250
# [6/100] training 23.6% loss=0.38662, acc=0.89062
# [6/100] training 23.7% loss=0.51821, acc=0.79688
# [6/100] training 23.9% loss=0.39605, acc=0.78125
# [6/100] training 24.0% loss=0.39614, acc=0.82812
# [6/100] training 24.2% loss=0.42463, acc=0.78125
# [6/100] training 24.3% loss=0.54927, acc=0.78125
# [6/100] training 24.6% loss=0.30183, acc=0.87500
# [6/100] training 24.7% loss=0.50028, acc=0.75000
# [6/100] training 24.9% loss=0.36928, acc=0.84375
# [6/100] training 25.1% loss=0.48361, acc=0.73438
# [6/100] training 25.2% loss=0.29490, acc=0.90625
# [6/100] training 25.4% loss=0.36235, acc=0.85938
# [6/100] training 25.6% loss=0.40750, acc=0.82812
# [6/100] training 25.8% loss=0.48315, acc=0.78125
# [6/100] training 25.9% loss=0.33238, acc=0.85938
# [6/100] training 26.1% loss=0.48291, acc=0.78125
# [6/100] training 26.3% loss=0.34419, acc=0.84375
# [6/100] training 26.4% loss=0.27687, acc=0.87500
# [6/100] training 26.6% loss=0.38932, acc=0.82812
# [6/100] training 26.8% loss=0.44086, acc=0.79688
# [6/100] training 27.0% loss=0.39419, acc=0.85938
# [6/100] training 27.1% loss=0.32488, acc=0.87500
# [6/100] training 27.3% loss=0.48309, acc=0.78125
# [6/100] training 27.4% loss=0.39810, acc=0.81250
# [6/100] training 27.6% loss=0.42766, acc=0.76562
# [6/100] training 27.9% loss=0.43412, acc=0.79688
# [6/100] training 28.0% loss=0.49297, acc=0.75000
# [6/100] training 28.2% loss=0.41662, acc=0.82812
# [6/100] training 28.3% loss=0.29531, acc=0.87500
# [6/100] training 28.5% loss=0.46825, acc=0.75000
# [6/100] training 28.6% loss=0.45415, acc=0.82812
# [6/100] training 28.8% loss=0.29687, acc=0.92188
# [6/100] training 29.1% loss=0.39642, acc=0.79688
# [6/100] training 29.2% loss=0.36738, acc=0.84375
# [6/100] training 29.4% loss=0.38968, acc=0.78125
# [6/100] training 29.5% loss=0.40677, acc=0.82812
# [6/100] training 29.7% loss=0.44377, acc=0.78125
# [6/100] training 29.8% loss=0.27772, acc=0.89062
# [6/100] training 30.0% loss=0.51630, acc=0.73438
# [6/100] training 30.2% loss=0.42611, acc=0.82812
# [6/100] training 30.4% loss=0.36374, acc=0.82812
# [6/100] training 30.6% loss=0.45915, acc=0.82812
# [6/100] training 30.7% loss=0.30950, acc=0.82812
# [6/100] training 30.9% loss=0.51032, acc=0.73438
# [6/100] training 31.0% loss=0.36581, acc=0.82812
# [6/100] training 31.3% loss=0.34579, acc=0.90625
# [6/100] training 31.4% loss=0.42514, acc=0.78125
# [6/100] training 31.6% loss=0.47849, acc=0.73438
# [6/100] training 31.8% loss=0.32642, acc=0.84375
# [6/100] training 31.9% loss=0.35583, acc=0.87500
# [6/100] training 32.1% loss=0.44307, acc=0.81250
# [6/100] training 32.2% loss=0.38822, acc=0.79688
# [6/100] training 32.5% loss=0.34841, acc=0.82812
# [6/100] training 32.6% loss=0.33746, acc=0.87500
# [6/100] training 32.8% loss=0.36732, acc=0.82812
# [6/100] training 32.9% loss=0.43072, acc=0.81250
# [6/100] training 33.1% loss=0.45633, acc=0.76562
# [6/100] training 33.3% loss=0.35702, acc=0.81250
# [6/100] training 33.4% loss=0.39444, acc=0.84375
# [6/100] training 33.7% loss=0.48376, acc=0.73438
# [6/100] training 33.8% loss=0.46802, acc=0.79688
# [6/100] training 34.0% loss=0.40019, acc=0.84375
# [6/100] training 34.1% loss=0.33128, acc=0.82812
# [6/100] training 34.3% loss=0.35621, acc=0.84375
# [6/100] training 34.5% loss=0.44598, acc=0.79688
# [6/100] training 34.7% loss=0.23038, acc=0.92188
# [6/100] training 34.9% loss=0.39212, acc=0.82812
# [6/100] training 35.0% loss=0.38555, acc=0.79688
# [6/100] training 35.2% loss=0.48968, acc=0.78125
# [6/100] training 35.3% loss=0.47347, acc=0.75000
# [6/100] training 35.5% loss=0.41085, acc=0.76562
# [6/100] training 35.6% loss=0.31330, acc=0.87500
# [6/100] training 35.9% loss=0.51913, acc=0.71875
# [6/100] training 36.1% loss=0.38401, acc=0.81250
# [6/100] training 36.2% loss=0.42571, acc=0.82812
# [6/100] training 36.4% loss=0.34890, acc=0.84375
# [6/100] training 36.5% loss=0.45226, acc=0.81250
# [6/100] training 36.7% loss=0.37010, acc=0.89062
# [6/100] training 36.8% loss=0.31059, acc=0.89062
# [6/100] training 37.1% loss=0.43321, acc=0.84375
# [6/100] training 37.3% loss=0.36727, acc=0.81250
# [6/100] training 37.4% loss=0.30053, acc=0.89062
# [6/100] training 37.6% loss=0.40837, acc=0.84375
# [6/100] training 37.7% loss=0.34464, acc=0.85938
# [6/100] training 37.9% loss=0.32656, acc=0.87500
# [6/100] training 38.1% loss=0.52427, acc=0.71875
# [6/100] training 38.3% loss=0.44366, acc=0.78125
# [6/100] training 38.4% loss=0.37327, acc=0.81250
# [6/100] training 38.6% loss=0.27246, acc=0.92188
# [6/100] training 38.8% loss=0.40407, acc=0.81250
# [6/100] training 38.9% loss=0.50679, acc=0.79688
# [6/100] training 39.1% loss=0.34602, acc=0.87500
# [6/100] training 39.3% loss=0.35057, acc=0.78125
# [6/100] training 39.5% loss=0.48996, acc=0.78125
# [6/100] training 39.6% loss=0.46055, acc=0.79688
# [6/100] training 39.8% loss=0.35257, acc=0.81250
# [6/100] training 40.0% loss=0.37575, acc=0.82812
# [6/100] training 40.1% loss=0.44569, acc=0.84375
# [6/100] training 40.4% loss=0.42744, acc=0.82812
# [6/100] training 40.5% loss=0.32174, acc=0.84375
# [6/100] training 40.7% loss=0.28027, acc=0.87500
# [6/100] training 40.8% loss=0.37657, acc=0.76562
# [6/100] training 41.0% loss=0.29793, acc=0.92188
# [6/100] training 41.1% loss=0.44110, acc=0.78125
# [6/100] training 41.3% loss=0.46000, acc=0.76562
# [6/100] training 41.6% loss=0.32427, acc=0.87500
# [6/100] training 41.7% loss=0.52611, acc=0.76562
# [6/100] training 41.9% loss=0.35534, acc=0.85938
# [6/100] training 42.0% loss=0.39412, acc=0.81250
# [6/100] training 42.2% loss=0.35565, acc=0.85938
# [6/100] training 42.3% loss=0.33550, acc=0.87500
# [6/100] training 42.5% loss=0.30965, acc=0.87500
# [6/100] training 42.8% loss=0.32789, acc=0.85938
# [6/100] training 42.9% loss=0.24633, acc=0.90625
# [6/100] training 43.1% loss=0.41730, acc=0.81250
# [6/100] training 43.2% loss=0.38759, acc=0.78125
# [6/100] training 43.4% loss=0.44740, acc=0.79688
# [6/100] training 43.5% loss=0.34863, acc=0.82812
# [6/100] training 43.8% loss=0.51745, acc=0.70312
# [6/100] training 43.9% loss=0.40093, acc=0.79688
# [6/100] training 44.1% loss=0.28570, acc=0.89062
# [6/100] training 44.3% loss=0.42213, acc=0.76562
# [6/100] training 44.4% loss=0.41402, acc=0.76562
# [6/100] training 44.6% loss=0.41224, acc=0.81250
# [6/100] training 44.7% loss=0.44292, acc=0.79688
# [6/100] training 45.0% loss=0.34822, acc=0.87500
# [6/100] training 45.1% loss=0.62901, acc=0.76562
# [6/100] training 45.3% loss=0.47127, acc=0.79688
# [6/100] training 45.5% loss=0.34293, acc=0.89062
# [6/100] training 45.6% loss=0.35458, acc=0.79688
# [6/100] training 45.8% loss=0.29985, acc=0.85938
# [6/100] training 45.9% loss=0.33970, acc=0.81250
# [6/100] training 46.2% loss=0.27992, acc=0.89062
# [6/100] training 46.3% loss=0.33849, acc=0.82812
# [6/100] training 46.5% loss=0.50845, acc=0.82812
# [6/100] training 46.6% loss=0.35511, acc=0.82812
# [6/100] training 46.8% loss=0.39829, acc=0.81250
# [6/100] training 47.0% loss=0.39042, acc=0.82812
# [6/100] training 47.2% loss=0.41382, acc=0.87500
# [6/100] training 47.4% loss=0.35374, acc=0.82812
# [6/100] training 47.5% loss=0.38350, acc=0.79688
# [6/100] training 47.7% loss=0.25298, acc=0.89062
# [6/100] training 47.8% loss=0.35564, acc=0.87500
# [6/100] training 48.0% loss=0.50558, acc=0.82812
# [6/100] training 48.3% loss=0.23248, acc=0.90625
# [6/100] training 48.4% loss=0.31388, acc=0.89062
# [6/100] training 48.6% loss=0.32118, acc=0.87500
# [6/100] training 48.7% loss=0.47404, acc=0.79688
# [6/100] training 48.9% loss=0.47784, acc=0.75000
# [6/100] training 49.0% loss=0.37166, acc=0.79688
# [6/100] training 49.2% loss=0.35870, acc=0.79688
# [6/100] training 49.3% loss=0.43194, acc=0.78125
# [6/100] training 49.6% loss=0.36152, acc=0.79688
# [6/100] training 49.8% loss=0.33411, acc=0.84375
# [6/100] training 49.9% loss=0.38783, acc=0.79688
# [6/100] training 50.1% loss=0.40480, acc=0.85938
# [6/100] training 50.2% loss=0.35097, acc=0.82812
# [6/100] training 50.4% loss=0.50346, acc=0.82812
# [6/100] training 50.6% loss=0.45852, acc=0.81250
# [6/100] training 50.8% loss=0.46152, acc=0.82812
# [6/100] training 51.0% loss=0.55606, acc=0.78125
# [6/100] training 51.1% loss=0.29593, acc=0.89062
# [6/100] training 51.3% loss=0.51451, acc=0.73438
# [6/100] training 51.4% loss=0.42314, acc=0.81250
# [6/100] training 51.7% loss=0.52029, acc=0.76562
# [6/100] training 51.8% loss=0.35442, acc=0.85938
# [6/100] training 52.0% loss=0.44305, acc=0.81250
# [6/100] training 52.1% loss=0.49280, acc=0.82812
# [6/100] training 52.3% loss=0.40112, acc=0.81250
# [6/100] training 52.5% loss=0.28581, acc=0.90625
# [6/100] training 52.6% loss=0.33418, acc=0.87500
# [6/100] training 52.9% loss=0.54335, acc=0.75000
# [6/100] training 53.0% loss=0.42123, acc=0.81250
# [6/100] training 53.2% loss=0.37746, acc=0.84375
# [6/100] training 53.3% loss=0.23655, acc=0.90625
# [6/100] training 53.5% loss=0.45369, acc=0.81250
# [6/100] training 53.7% loss=0.39086, acc=0.81250
# [6/100] training 53.8% loss=0.39050, acc=0.78125
# [6/100] training 54.1% loss=0.45708, acc=0.81250
# [6/100] training 54.2% loss=0.39414, acc=0.82812
# [6/100] training 54.4% loss=0.43152, acc=0.85938
# [6/100] training 54.5% loss=0.37719, acc=0.82812
# [6/100] training 54.7% loss=0.45346, acc=0.75000
# [6/100] training 54.8% loss=0.27640, acc=0.89062
# [6/100] training 55.1% loss=0.30033, acc=0.89062
# [6/100] training 55.3% loss=0.27218, acc=0.89062
# [6/100] training 55.4% loss=0.50194, acc=0.75000
# [6/100] training 55.6% loss=0.36086, acc=0.84375
# [6/100] training 55.7% loss=0.35472, acc=0.85938
# [6/100] training 55.9% loss=0.27977, acc=0.85938
# [6/100] training 56.0% loss=0.45729, acc=0.79688
# [6/100] training 56.3% loss=0.60374, acc=0.76562
# [6/100] training 56.5% loss=0.42036, acc=0.82812
# [6/100] training 56.6% loss=0.39138, acc=0.75000
# [6/100] training 56.8% loss=0.47528, acc=0.73438
# [6/100] training 56.9% loss=0.42373, acc=0.82812
# [6/100] training 57.1% loss=0.43165, acc=0.84375
# [6/100] training 57.2% loss=0.40078, acc=0.82812
# [6/100] training 57.5% loss=0.37982, acc=0.84375
# [6/100] training 57.6% loss=0.45718, acc=0.79688
# [6/100] training 57.8% loss=0.36967, acc=0.81250
# [6/100] training 58.0% loss=0.28947, acc=0.85938
# [6/100] training 58.1% loss=0.34667, acc=0.87500
# [6/100] training 58.3% loss=0.30788, acc=0.85938
# [6/100] training 58.4% loss=0.39241, acc=0.85938
# [6/100] training 58.7% loss=0.42026, acc=0.78125
# [6/100] training 58.8% loss=0.53623, acc=0.78125
# [6/100] training 59.0% loss=0.38407, acc=0.85938
# [6/100] training 59.2% loss=0.42045, acc=0.78125
# [6/100] training 59.3% loss=0.40303, acc=0.82812
# [6/100] training 59.5% loss=0.46488, acc=0.84375
# [6/100] training 59.7% loss=0.41110, acc=0.78125
# [6/100] training 59.9% loss=0.31314, acc=0.92188
# [6/100] training 60.0% loss=0.33309, acc=0.87500
# [6/100] training 60.2% loss=0.45555, acc=0.75000
# [6/100] training 60.3% loss=0.37380, acc=0.78125
# [6/100] training 60.5% loss=0.36072, acc=0.79688
# [6/100] training 60.8% loss=0.46816, acc=0.75000
# [6/100] training 60.9% loss=0.35203, acc=0.78125
# [6/100] training 61.1% loss=0.55274, acc=0.71875
# [6/100] training 61.2% loss=0.40388, acc=0.78125
# [6/100] training 61.4% loss=0.42030, acc=0.82812
# [6/100] training 61.5% loss=0.51318, acc=0.79688
# [6/100] training 61.7% loss=0.56337, acc=0.68750
# [6/100] training 62.0% loss=0.50648, acc=0.71875
# [6/100] training 62.1% loss=0.45212, acc=0.78125
# [6/100] training 62.3% loss=0.37722, acc=0.89062
# [6/100] training 62.4% loss=0.33694, acc=0.84375
# [6/100] training 62.6% loss=0.48212, acc=0.78125
# [6/100] training 62.7% loss=0.43098, acc=0.82812
# [6/100] training 62.9% loss=0.49634, acc=0.73438
# [6/100] training 63.1% loss=0.32000, acc=0.82812
# [6/100] training 63.3% loss=0.43434, acc=0.79688
# [6/100] training 63.5% loss=0.43058, acc=0.79688
# [6/100] training 63.6% loss=0.38951, acc=0.79688
# [6/100] training 63.8% loss=0.38166, acc=0.84375
# [6/100] training 63.9% loss=0.38668, acc=0.85938
# [6/100] training 64.2% loss=0.30279, acc=0.84375
# [6/100] training 64.3% loss=0.42907, acc=0.81250
# [6/100] training 64.5% loss=0.40908, acc=0.78125
# [6/100] training 64.7% loss=0.48455, acc=0.70312
# [6/100] training 64.8% loss=0.71334, acc=0.67188
# [6/100] training 65.0% loss=0.42978, acc=0.81250
# [6/100] training 65.1% loss=0.55047, acc=0.71875
# [6/100] training 65.4% loss=0.42539, acc=0.84375
# [6/100] training 65.5% loss=0.35148, acc=0.84375
# [6/100] training 65.7% loss=0.35109, acc=0.84375
# [6/100] training 65.8% loss=0.53489, acc=0.75000
# [6/100] training 66.0% loss=0.43279, acc=0.76562
# [6/100] training 66.2% loss=0.31132, acc=0.85938
# [6/100] training 66.3% loss=0.47340, acc=0.73438
# [6/100] training 66.6% loss=0.45119, acc=0.78125
# [6/100] training 66.7% loss=0.28520, acc=0.87500
# [6/100] training 66.9% loss=0.37308, acc=0.81250
# [6/100] training 67.0% loss=0.46872, acc=0.78125
# [6/100] training 67.2% loss=0.29577, acc=0.87500
# [6/100] training 67.4% loss=0.35769, acc=0.82812
# [6/100] training 67.6% loss=0.30943, acc=0.85938
# [6/100] training 67.8% loss=0.43218, acc=0.81250
# [6/100] training 67.9% loss=0.34970, acc=0.85938
# [6/100] training 68.1% loss=0.35174, acc=0.84375
# [6/100] training 68.2% loss=0.30973, acc=0.82812
# [6/100] training 68.4% loss=0.36761, acc=0.81250
# [6/100] training 68.5% loss=0.58709, acc=0.70312
# [6/100] training 68.8% loss=0.44358, acc=0.81250
# [6/100] training 69.0% loss=0.45079, acc=0.82812
# [6/100] training 69.1% loss=0.31718, acc=0.87500
# [6/100] training 69.3% loss=0.34859, acc=0.85938
# [6/100] training 69.4% loss=0.34588, acc=0.87500
# [6/100] training 69.6% loss=0.40033, acc=0.84375
# [6/100] training 69.7% loss=0.38499, acc=0.81250
# [6/100] training 70.0% loss=0.51230, acc=0.71875
# [6/100] training 70.2% loss=0.37718, acc=0.85938
# [6/100] training 70.3% loss=0.38327, acc=0.78125
# [6/100] training 70.5% loss=0.40135, acc=0.81250
# [6/100] training 70.6% loss=0.40375, acc=0.85938
# [6/100] training 70.8% loss=0.39893, acc=0.79688
# [6/100] training 71.0% loss=0.40810, acc=0.82812
# [6/100] training 71.2% loss=0.33031, acc=0.79688
# [6/100] training 71.3% loss=0.31429, acc=0.89062
# [6/100] training 71.5% loss=0.55242, acc=0.81250
# [6/100] training 71.7% loss=0.48948, acc=0.78125
# [6/100] training 71.8% loss=0.45093, acc=0.84375
# [6/100] training 72.0% loss=0.28121, acc=0.87500
# [6/100] training 72.2% loss=0.35657, acc=0.81250
# [6/100] training 72.4% loss=0.45799, acc=0.84375
# [6/100] training 72.5% loss=0.50777, acc=0.79688
# [6/100] training 72.7% loss=0.46587, acc=0.76562
# [6/100] training 72.9% loss=0.36516, acc=0.82812
# [6/100] training 73.0% loss=0.36900, acc=0.87500
# [6/100] training 73.3% loss=0.41947, acc=0.82812
# [6/100] training 73.4% loss=0.34175, acc=0.87500
# [6/100] training 73.6% loss=0.27976, acc=0.87500
# [6/100] training 73.7% loss=0.27971, acc=0.85938
# [6/100] training 73.9% loss=0.40675, acc=0.84375
# [6/100] training 74.0% loss=0.48159, acc=0.81250
# [6/100] training 74.2% loss=0.47841, acc=0.76562
# [6/100] training 74.5% loss=0.31649, acc=0.85938
# [6/100] training 74.6% loss=0.46995, acc=0.78125
# [6/100] training 74.8% loss=0.44773, acc=0.71875
# [6/100] training 74.9% loss=0.48465, acc=0.75000
# [6/100] training 75.1% loss=0.41334, acc=0.81250
# [6/100] training 75.2% loss=0.35984, acc=0.82812
# [6/100] training 75.4% loss=0.36306, acc=0.82812
# [6/100] training 75.7% loss=0.39127, acc=0.82812
# [6/100] training 75.8% loss=0.50334, acc=0.71875
# [6/100] training 76.0% loss=0.26500, acc=0.84375
# [6/100] training 76.1% loss=0.36600, acc=0.81250
# [6/100] training 76.3% loss=0.27868, acc=0.92188
# [6/100] training 76.4% loss=0.36269, acc=0.82812
# [6/100] training 76.7% loss=0.38496, acc=0.84375
# [6/100] training 76.8% loss=0.34496, acc=0.87500
# [6/100] training 77.0% loss=0.38337, acc=0.75000
# [6/100] training 77.2% loss=0.42158, acc=0.84375
# [6/100] training 77.3% loss=0.26703, acc=0.90625
# [6/100] training 77.5% loss=0.44227, acc=0.78125
# [6/100] training 77.6% loss=0.41216, acc=0.79688
# [6/100] training 77.9% loss=0.36712, acc=0.84375
# [6/100] training 78.0% loss=0.38436, acc=0.79688
# [6/100] training 78.2% loss=0.48137, acc=0.81250
# [6/100] training 78.4% loss=0.27914, acc=0.89062
# [6/100] training 78.5% loss=0.44060, acc=0.84375
# [6/100] training 78.7% loss=0.38637, acc=0.81250
# [6/100] training 78.8% loss=0.38415, acc=0.81250
# [6/100] training 79.1% loss=0.26690, acc=0.90625
# [6/100] training 79.2% loss=0.31990, acc=0.90625
# [6/100] training 79.4% loss=0.60664, acc=0.71875
# [6/100] training 79.5% loss=0.31476, acc=0.84375
# [6/100] training 79.7% loss=0.25898, acc=0.95312
# [6/100] training 79.9% loss=0.41586, acc=0.78125
# [6/100] training 80.1% loss=0.36186, acc=0.87500
# [6/100] training 80.3% loss=0.43946, acc=0.78125
# [6/100] training 80.4% loss=0.44160, acc=0.81250
# [6/100] training 80.6% loss=0.42640, acc=0.79688
# [6/100] training 80.7% loss=0.31771, acc=0.85938
# [6/100] training 80.9% loss=0.38839, acc=0.81250
# [6/100] training 81.2% loss=0.36901, acc=0.84375
# [6/100] training 81.3% loss=0.49054, acc=0.70312
# [6/100] training 81.5% loss=0.29368, acc=0.87500
# [6/100] training 81.6% loss=0.47544, acc=0.76562
# [6/100] training 81.8% loss=0.37803, acc=0.85938
# [6/100] training 81.9% loss=0.46925, acc=0.84375
# [6/100] training 82.1% loss=0.27874, acc=0.89062
# [6/100] training 82.2% loss=0.52287, acc=0.75000
# [6/100] training 82.5% loss=0.33819, acc=0.85938
# [6/100] training 82.7% loss=0.34132, acc=0.85938
# [6/100] training 82.8% loss=0.40942, acc=0.81250
# [6/100] training 83.0% loss=0.34364, acc=0.89062
# [6/100] training 83.1% loss=0.38329, acc=0.84375
# [6/100] training 83.3% loss=0.36690, acc=0.79688
# [6/100] training 83.5% loss=0.43616, acc=0.78125
# [6/100] training 83.7% loss=0.44587, acc=0.79688
# [6/100] training 83.9% loss=0.34537, acc=0.84375
# [6/100] training 84.0% loss=0.28787, acc=0.87500
# [6/100] training 84.2% loss=0.31282, acc=0.85938
# [6/100] training 84.3% loss=0.36863, acc=0.81250
# [6/100] training 84.5% loss=0.34171, acc=0.82812
# [6/100] training 84.7% loss=0.50196, acc=0.79688
# [6/100] training 84.9% loss=0.41440, acc=0.79688
# [6/100] training 85.0% loss=0.30524, acc=0.89062
# [6/100] training 85.2% loss=0.43882, acc=0.79688
# [6/100] training 85.4% loss=0.33059, acc=0.90625
# [6/100] training 85.5% loss=0.28568, acc=0.84375
# [6/100] training 85.8% loss=0.32350, acc=0.79688
# [6/100] training 85.9% loss=0.32679, acc=0.89062
# [6/100] training 86.1% loss=0.26318, acc=0.92188
# [6/100] training 86.2% loss=0.35615, acc=0.84375
# [6/100] training 86.4% loss=0.58165, acc=0.73438
# [6/100] training 86.6% loss=0.58692, acc=0.73438
# [6/100] training 86.7% loss=0.41404, acc=0.82812
# [6/100] training 87.0% loss=0.35219, acc=0.81250
# [6/100] training 87.1% loss=0.32800, acc=0.85938
# [6/100] training 87.3% loss=0.38167, acc=0.82812
# [6/100] training 87.4% loss=0.36080, acc=0.84375
# [6/100] training 87.6% loss=0.40605, acc=0.82812
# [6/100] training 87.7% loss=0.39608, acc=0.81250
# [6/100] training 87.9% loss=0.45187, acc=0.76562
# [6/100] training 88.2% loss=0.23085, acc=0.90625
# [6/100] training 88.3% loss=0.43106, acc=0.76562
# [6/100] training 88.5% loss=0.36720, acc=0.79688
# [6/100] training 88.6% loss=0.20307, acc=0.92188
# [6/100] training 88.8% loss=0.38007, acc=0.85938
# [6/100] training 88.9% loss=0.34582, acc=0.87500
# [6/100] training 89.2% loss=0.32516, acc=0.85938
# [6/100] training 89.4% loss=0.31609, acc=0.87500
# [6/100] training 89.5% loss=0.47290, acc=0.76562
# [6/100] training 89.7% loss=0.50187, acc=0.79688
# [6/100] training 89.8% loss=0.32858, acc=0.87500
# [6/100] training 90.0% loss=0.30658, acc=0.82812
# [6/100] training 90.1% loss=0.36959, acc=0.82812
# [6/100] training 90.4% loss=0.33212, acc=0.84375
# [6/100] training 90.5% loss=0.27044, acc=0.85938
# [6/100] training 90.7% loss=0.37426, acc=0.81250
# [6/100] training 90.9% loss=0.25303, acc=0.93750
# [6/100] training 91.0% loss=0.36830, acc=0.87500
# [6/100] training 91.2% loss=0.54747, acc=0.76562
# [6/100] training 91.3% loss=0.56406, acc=0.71875
# [6/100] training 91.6% loss=0.53150, acc=0.75000
# [6/100] training 91.7% loss=0.34773, acc=0.82812
# [6/100] training 91.9% loss=0.41793, acc=0.81250
# [6/100] training 92.1% loss=0.33461, acc=0.82812
# [6/100] training 92.2% loss=0.28530, acc=0.92188
# [6/100] training 92.4% loss=0.37379, acc=0.81250
# [6/100] training 92.6% loss=0.42595, acc=0.76562
# [6/100] training 92.8% loss=0.52203, acc=0.71875
# [6/100] training 92.9% loss=0.31908, acc=0.82812
# [6/100] training 93.1% loss=0.43176, acc=0.82812
# [6/100] training 93.2% loss=0.34425, acc=0.85938
# [6/100] training 93.4% loss=0.31866, acc=0.87500
# [6/100] training 93.7% loss=0.34935, acc=0.90625
# [6/100] training 93.8% loss=0.40064, acc=0.78125
# [6/100] training 94.0% loss=0.28457, acc=0.87500
# [6/100] training 94.1% loss=0.41979, acc=0.78125
# [6/100] training 94.3% loss=0.27611, acc=0.87500
# [6/100] training 94.4% loss=0.38605, acc=0.79688
# [6/100] training 94.6% loss=0.29539, acc=0.89062
# [6/100] training 94.9% loss=0.45410, acc=0.76562
# [6/100] training 95.0% loss=0.45325, acc=0.79688
# [6/100] training 95.2% loss=0.52542, acc=0.75000
# [6/100] training 95.3% loss=0.33175, acc=0.85938
# [6/100] training 95.5% loss=0.28937, acc=0.92188
# [6/100] training 95.6% loss=0.43559, acc=0.84375
# [6/100] training 95.8% loss=0.42351, acc=0.84375
# [6/100] training 96.0% loss=0.30460, acc=0.81250
# [6/100] training 96.2% loss=0.25471, acc=0.89062
# [6/100] training 96.4% loss=0.37362, acc=0.79688
# [6/100] training 96.5% loss=0.48755, acc=0.79688
# [6/100] training 96.7% loss=0.30497, acc=0.90625
# [6/100] training 96.8% loss=0.35915, acc=0.85938
# [6/100] training 97.1% loss=0.37600, acc=0.78125
# [6/100] training 97.2% loss=0.42861, acc=0.81250
# [6/100] training 97.4% loss=0.38919, acc=0.79688
# [6/100] training 97.6% loss=0.38778, acc=0.82812
# [6/100] training 97.7% loss=0.32614, acc=0.84375
# [6/100] training 97.9% loss=0.25933, acc=0.89062
# [6/100] training 98.0% loss=0.40580, acc=0.81250
# [6/100] training 98.3% loss=0.32176, acc=0.87500
# [6/100] training 98.4% loss=0.30927, acc=0.85938
# [6/100] training 98.6% loss=0.58349, acc=0.75000
# [6/100] training 98.7% loss=0.47008, acc=0.76562
# [6/100] training 98.9% loss=0.38791, acc=0.79688
# [6/100] training 99.1% loss=0.29468, acc=0.85938
# [6/100] training 99.2% loss=0.24983, acc=0.89062
# [6/100] training 99.5% loss=0.41619, acc=0.82812
# [6/100] training 99.6% loss=0.35798, acc=0.82812
# [6/100] training 99.8% loss=0.37801, acc=0.84375
# [6/100] training 99.9% loss=0.23402, acc=0.93750
# [6/100] testing 0.9% loss=0.25929, acc=0.87500
# [6/100] testing 1.8% loss=0.56516, acc=0.76562
# [6/100] testing 2.2% loss=0.32902, acc=0.84375
# [6/100] testing 3.1% loss=0.33641, acc=0.85938
# [6/100] testing 3.5% loss=0.32689, acc=0.87500
# [6/100] testing 4.4% loss=0.38468, acc=0.81250
# [6/100] testing 4.8% loss=0.54220, acc=0.71875
# [6/100] testing 5.7% loss=0.42730, acc=0.81250
# [6/100] testing 6.6% loss=0.40986, acc=0.79688
# [6/100] testing 7.0% loss=0.29564, acc=0.89062
# [6/100] testing 7.9% loss=0.49790, acc=0.73438
# [6/100] testing 8.3% loss=0.28178, acc=0.85938
# [6/100] testing 9.2% loss=0.38132, acc=0.82812
# [6/100] testing 9.7% loss=0.36890, acc=0.81250
# [6/100] testing 10.5% loss=0.49954, acc=0.75000
# [6/100] testing 11.0% loss=0.32499, acc=0.89062
# [6/100] testing 11.8% loss=0.42552, acc=0.82812
# [6/100] testing 12.7% loss=0.58224, acc=0.78125
# [6/100] testing 13.2% loss=0.32608, acc=0.82812
# [6/100] testing 14.0% loss=0.51314, acc=0.78125
# [6/100] testing 14.5% loss=0.54767, acc=0.76562
# [6/100] testing 15.4% loss=0.40602, acc=0.85938
# [6/100] testing 15.8% loss=0.29905, acc=0.87500
# [6/100] testing 16.7% loss=0.38498, acc=0.85938
# [6/100] testing 17.5% loss=0.45318, acc=0.79688
# [6/100] testing 18.0% loss=0.30934, acc=0.82812
# [6/100] testing 18.9% loss=0.28240, acc=0.87500
# [6/100] testing 19.3% loss=0.36839, acc=0.85938
# [6/100] testing 20.2% loss=0.64513, acc=0.68750
# [6/100] testing 20.6% loss=0.48540, acc=0.76562
# [6/100] testing 21.5% loss=0.36151, acc=0.82812
# [6/100] testing 21.9% loss=0.56065, acc=0.73438
# [6/100] testing 22.8% loss=0.42417, acc=0.82812
# [6/100] testing 23.7% loss=0.39704, acc=0.85938
# [6/100] testing 24.1% loss=0.37249, acc=0.85938
# [6/100] testing 25.0% loss=0.41408, acc=0.81250
# [6/100] testing 25.4% loss=0.29943, acc=0.87500
# [6/100] testing 26.3% loss=0.44774, acc=0.73438
# [6/100] testing 26.8% loss=0.36424, acc=0.85938
# [6/100] testing 27.6% loss=0.54306, acc=0.78125
# [6/100] testing 28.5% loss=0.40507, acc=0.85938
# [6/100] testing 29.0% loss=0.37627, acc=0.82812
# [6/100] testing 29.8% loss=0.48666, acc=0.79688
# [6/100] testing 30.3% loss=0.54830, acc=0.75000
# [6/100] testing 31.1% loss=0.54710, acc=0.71875
# [6/100] testing 31.6% loss=0.33458, acc=0.82812
# [6/100] testing 32.5% loss=0.42713, acc=0.81250
# [6/100] testing 32.9% loss=0.49752, acc=0.79688
# [6/100] testing 33.8% loss=0.47387, acc=0.78125
# [6/100] testing 34.7% loss=0.47294, acc=0.81250
# [6/100] testing 35.1% loss=0.37708, acc=0.85938
# [6/100] testing 36.0% loss=0.58629, acc=0.70312
# [6/100] testing 36.4% loss=0.44461, acc=0.81250
# [6/100] testing 37.3% loss=0.38030, acc=0.82812
# [6/100] testing 37.7% loss=0.48591, acc=0.73438
# [6/100] testing 38.6% loss=0.31759, acc=0.87500
# [6/100] testing 39.5% loss=0.54661, acc=0.79688
# [6/100] testing 39.9% loss=0.29245, acc=0.87500
# [6/100] testing 40.8% loss=0.37082, acc=0.82812
# [6/100] testing 41.2% loss=0.31463, acc=0.90625
# [6/100] testing 42.1% loss=0.51038, acc=0.81250
# [6/100] testing 42.5% loss=0.37687, acc=0.81250
# [6/100] testing 43.4% loss=0.42842, acc=0.79688
# [6/100] testing 43.9% loss=0.33171, acc=0.90625
# [6/100] testing 44.7% loss=0.52322, acc=0.81250
# [6/100] testing 45.6% loss=0.36557, acc=0.82812
# [6/100] testing 46.1% loss=0.28782, acc=0.89062
# [6/100] testing 46.9% loss=0.34590, acc=0.85938
# [6/100] testing 47.4% loss=0.31230, acc=0.82812
# [6/100] testing 48.3% loss=0.45793, acc=0.76562
# [6/100] testing 48.7% loss=0.54364, acc=0.76562
# [6/100] testing 49.6% loss=0.42114, acc=0.79688
# [6/100] testing 50.4% loss=0.29073, acc=0.85938
# [6/100] testing 50.9% loss=0.38179, acc=0.84375
# [6/100] testing 51.8% loss=0.39511, acc=0.82812
# [6/100] testing 52.2% loss=0.43343, acc=0.84375
# [6/100] testing 53.1% loss=0.30765, acc=0.85938
# [6/100] testing 53.5% loss=0.27768, acc=0.90625
# [6/100] testing 54.4% loss=0.54653, acc=0.75000
# [6/100] testing 54.8% loss=0.42481, acc=0.76562
# [6/100] testing 55.7% loss=0.22636, acc=0.95312
# [6/100] testing 56.6% loss=0.46471, acc=0.76562
# [6/100] testing 57.0% loss=0.63401, acc=0.68750
# [6/100] testing 57.9% loss=0.43366, acc=0.82812
# [6/100] testing 58.3% loss=0.54084, acc=0.73438
# [6/100] testing 59.2% loss=0.42713, acc=0.82812
# [6/100] testing 59.7% loss=0.38731, acc=0.79688
# [6/100] testing 60.5% loss=0.45147, acc=0.79688
# [6/100] testing 61.4% loss=0.26879, acc=0.89062
# [6/100] testing 61.9% loss=0.40053, acc=0.82812
# [6/100] testing 62.7% loss=0.43533, acc=0.81250
# [6/100] testing 63.2% loss=0.46038, acc=0.76562
# [6/100] testing 64.0% loss=0.39373, acc=0.79688
# [6/100] testing 64.5% loss=0.35144, acc=0.87500
# [6/100] testing 65.4% loss=0.36335, acc=0.79688
# [6/100] testing 65.8% loss=0.45321, acc=0.82812
# [6/100] testing 66.7% loss=0.27819, acc=0.85938
# [6/100] testing 67.6% loss=0.41640, acc=0.78125
# [6/100] testing 68.0% loss=0.40988, acc=0.81250
# [6/100] testing 68.9% loss=0.37155, acc=0.84375
# [6/100] testing 69.3% loss=0.46490, acc=0.81250
# [6/100] testing 70.2% loss=0.39412, acc=0.81250
# [6/100] testing 70.6% loss=0.41318, acc=0.78125
# [6/100] testing 71.5% loss=0.48661, acc=0.79688
# [6/100] testing 72.4% loss=0.39478, acc=0.84375
# [6/100] testing 72.8% loss=0.46429, acc=0.78125
# [6/100] testing 73.7% loss=0.36524, acc=0.82812
# [6/100] testing 74.1% loss=0.44652, acc=0.79688
# [6/100] testing 75.0% loss=0.34180, acc=0.84375
# [6/100] testing 75.4% loss=0.30355, acc=0.90625
# [6/100] testing 76.3% loss=0.20987, acc=0.92188
# [6/100] testing 76.8% loss=0.37755, acc=0.82812
# [6/100] testing 77.6% loss=0.32336, acc=0.84375
# [6/100] testing 78.5% loss=0.61710, acc=0.71875
# [6/100] testing 79.0% loss=0.36446, acc=0.82812
# [6/100] testing 79.8% loss=0.41369, acc=0.75000
# [6/100] testing 80.3% loss=0.33357, acc=0.85938
# [6/100] testing 81.2% loss=0.44839, acc=0.81250
# [6/100] testing 81.6% loss=0.39802, acc=0.84375
# [6/100] testing 82.5% loss=0.30607, acc=0.82812
# [6/100] testing 83.3% loss=0.33129, acc=0.84375
# [6/100] testing 83.8% loss=0.26479, acc=0.89062
# [6/100] testing 84.7% loss=0.33484, acc=0.87500
# [6/100] testing 85.1% loss=0.47469, acc=0.75000
# [6/100] testing 86.0% loss=0.47515, acc=0.75000
# [6/100] testing 86.4% loss=0.49763, acc=0.79688
# [6/100] testing 87.3% loss=0.54034, acc=0.75000
# [6/100] testing 87.7% loss=0.36651, acc=0.84375
# [6/100] testing 88.6% loss=0.32221, acc=0.82812
# [6/100] testing 89.5% loss=0.56459, acc=0.68750
# [6/100] testing 89.9% loss=0.39696, acc=0.81250
# [6/100] testing 90.8% loss=0.39241, acc=0.79688
# [6/100] testing 91.2% loss=0.24940, acc=0.89062
# [6/100] testing 92.1% loss=0.44316, acc=0.81250
# [6/100] testing 92.6% loss=0.45922, acc=0.78125
# [6/100] testing 93.4% loss=0.42432, acc=0.82812
# [6/100] testing 94.3% loss=0.22997, acc=0.89062
# [6/100] testing 94.7% loss=0.37122, acc=0.79688
# [6/100] testing 95.6% loss=0.33420, acc=0.85938
# [6/100] testing 96.1% loss=0.32680, acc=0.79688
# [6/100] testing 96.9% loss=0.37672, acc=0.87500
# [6/100] testing 97.4% loss=0.22451, acc=0.92188
# [6/100] testing 98.3% loss=0.45944, acc=0.76562
# [6/100] testing 98.7% loss=0.40282, acc=0.81250
# [6/100] testing 99.6% loss=0.33593, acc=0.85938
# [7/100] training 0.2% loss=0.49631, acc=0.78125
# [7/100] training 0.4% loss=0.53714, acc=0.73438
# [7/100] training 0.5% loss=0.43909, acc=0.79688
# [7/100] training 0.8% loss=0.42146, acc=0.81250
# [7/100] training 0.9% loss=0.38891, acc=0.81250
# [7/100] training 1.1% loss=0.38847, acc=0.81250
# [7/100] training 1.2% loss=0.46135, acc=0.79688
# [7/100] training 1.4% loss=0.40665, acc=0.82812
# [7/100] training 1.6% loss=0.25004, acc=0.89062
# [7/100] training 1.8% loss=0.32900, acc=0.87500
# [7/100] training 2.0% loss=0.32442, acc=0.89062
# [7/100] training 2.1% loss=0.42825, acc=0.82812
# [7/100] training 2.3% loss=0.29685, acc=0.85938
# [7/100] training 2.4% loss=0.41164, acc=0.79688
# [7/100] training 2.6% loss=0.38786, acc=0.78125
# [7/100] training 2.7% loss=0.40578, acc=0.82812
# [7/100] training 3.0% loss=0.54285, acc=0.73438
# [7/100] training 3.2% loss=0.36535, acc=0.84375
# [7/100] training 3.3% loss=0.37597, acc=0.81250
# [7/100] training 3.5% loss=0.39589, acc=0.78125
# [7/100] training 3.6% loss=0.58662, acc=0.76562
# [7/100] training 3.8% loss=0.25258, acc=0.85938
# [7/100] training 3.9% loss=0.36048, acc=0.84375
# [7/100] training 4.2% loss=0.35916, acc=0.82812
# [7/100] training 4.4% loss=0.31795, acc=0.81250
# [7/100] training 4.5% loss=0.32735, acc=0.84375
# [7/100] training 4.7% loss=0.38185, acc=0.84375
# [7/100] training 4.8% loss=0.41770, acc=0.79688
# [7/100] training 5.0% loss=0.30565, acc=0.90625
# [7/100] training 5.2% loss=0.51240, acc=0.82812
# [7/100] training 5.4% loss=0.30422, acc=0.87500
# [7/100] training 5.5% loss=0.32570, acc=0.87500
# [7/100] training 5.7% loss=0.34543, acc=0.85938
# [7/100] training 5.9% loss=0.44568, acc=0.75000
# [7/100] training 6.0% loss=0.34147, acc=0.84375
# [7/100] training 6.3% loss=0.41528, acc=0.81250
# [7/100] training 6.4% loss=0.29704, acc=0.85938
# [7/100] training 6.6% loss=0.37282, acc=0.84375
# [7/100] training 6.7% loss=0.42126, acc=0.79688
# [7/100] training 6.9% loss=0.34649, acc=0.81250
# [7/100] training 7.1% loss=0.45882, acc=0.79688
# [7/100] training 7.2% loss=0.39942, acc=0.76562
# [7/100] training 7.5% loss=0.34717, acc=0.87500
# [7/100] training 7.6% loss=0.42477, acc=0.76562
# [7/100] training 7.8% loss=0.38998, acc=0.81250
# [7/100] training 7.9% loss=0.43644, acc=0.81250
# [7/100] training 8.1% loss=0.33461, acc=0.84375
# [7/100] training 8.2% loss=0.30898, acc=0.85938
# [7/100] training 8.4% loss=0.39721, acc=0.85938
# [7/100] training 8.7% loss=0.31685, acc=0.87500
# [7/100] training 8.8% loss=0.36310, acc=0.82812
# [7/100] training 9.0% loss=0.41964, acc=0.81250
# [7/100] training 9.1% loss=0.41894, acc=0.82812
# [7/100] training 9.3% loss=0.56364, acc=0.76562
# [7/100] training 9.4% loss=0.32982, acc=0.85938
# [7/100] training 9.7% loss=0.42705, acc=0.79688
# [7/100] training 9.9% loss=0.40509, acc=0.85938
# [7/100] training 10.0% loss=0.40172, acc=0.81250
# [7/100] training 10.2% loss=0.33643, acc=0.84375
# [7/100] training 10.3% loss=0.31044, acc=0.82812
# [7/100] training 10.5% loss=0.42504, acc=0.85938
# [7/100] training 10.6% loss=0.31431, acc=0.85938
# [7/100] training 10.9% loss=0.42741, acc=0.87500
# [7/100] training 11.0% loss=0.31709, acc=0.85938
# [7/100] training 11.2% loss=0.29174, acc=0.85938
# [7/100] training 11.4% loss=0.49748, acc=0.81250
# [7/100] training 11.5% loss=0.59054, acc=0.75000
# [7/100] training 11.7% loss=0.34544, acc=0.87500
# [7/100] training 11.8% loss=0.34637, acc=0.82812
# [7/100] training 12.1% loss=0.40540, acc=0.79688
# [7/100] training 12.2% loss=0.45794, acc=0.81250
# [7/100] training 12.4% loss=0.39062, acc=0.79688
# [7/100] training 12.6% loss=0.31913, acc=0.84375
# [7/100] training 12.7% loss=0.41411, acc=0.82812
# [7/100] training 12.9% loss=0.42704, acc=0.84375
# [7/100] training 13.0% loss=0.38452, acc=0.84375
# [7/100] training 13.3% loss=0.38200, acc=0.81250
# [7/100] training 13.4% loss=0.51527, acc=0.73438
# [7/100] training 13.6% loss=0.38860, acc=0.79688
# [7/100] training 13.7% loss=0.59266, acc=0.78125
# [7/100] training 13.9% loss=0.40000, acc=0.81250
# [7/100] training 14.1% loss=0.41662, acc=0.81250
# [7/100] training 14.3% loss=0.37663, acc=0.82812
# [7/100] training 14.5% loss=0.31253, acc=0.89062
# [7/100] training 14.6% loss=0.31886, acc=0.90625
# [7/100] training 14.8% loss=0.33892, acc=0.89062
# [7/100] training 14.9% loss=0.36939, acc=0.85938
# [7/100] training 15.1% loss=0.45448, acc=0.79688
# [7/100] training 15.4% loss=0.33722, acc=0.84375
# [7/100] training 15.5% loss=0.43087, acc=0.76562
# [7/100] training 15.7% loss=0.57401, acc=0.71875
# [7/100] training 15.8% loss=0.31009, acc=0.87500
# [7/100] training 16.0% loss=0.48203, acc=0.78125
# [7/100] training 16.1% loss=0.61255, acc=0.70312
# [7/100] training 16.3% loss=0.44072, acc=0.78125
# [7/100] training 16.4% loss=0.37183, acc=0.87500
# [7/100] training 16.7% loss=0.45877, acc=0.79688
# [7/100] training 16.9% loss=0.48764, acc=0.73438
# [7/100] training 17.0% loss=0.35106, acc=0.87500
# [7/100] training 17.2% loss=0.29704, acc=0.92188
# [7/100] training 17.3% loss=0.36509, acc=0.82812
# [7/100] training 17.5% loss=0.42327, acc=0.81250
# [7/100] training 17.7% loss=0.50030, acc=0.78125
# [7/100] training 17.9% loss=0.43384, acc=0.79688
# [7/100] training 18.1% loss=0.44110, acc=0.76562
# [7/100] training 18.2% loss=0.43140, acc=0.82812
# [7/100] training 18.4% loss=0.43844, acc=0.79688
# [7/100] training 18.5% loss=0.35700, acc=0.89062
# [7/100] training 18.8% loss=0.31495, acc=0.89062
# [7/100] training 18.9% loss=0.35680, acc=0.82812
# [7/100] training 19.1% loss=0.40358, acc=0.79688
# [7/100] training 19.2% loss=0.31079, acc=0.87500
# [7/100] training 19.4% loss=0.18987, acc=0.93750
# [7/100] training 19.6% loss=0.41264, acc=0.84375
# [7/100] training 19.7% loss=0.43754, acc=0.76562
# [7/100] training 20.0% loss=0.28298, acc=0.87500
# [7/100] training 20.1% loss=0.40440, acc=0.84375
# [7/100] training 20.3% loss=0.36289, acc=0.82812
# [7/100] training 20.4% loss=0.40584, acc=0.81250
# [7/100] training 20.6% loss=0.39023, acc=0.82812
# [7/100] training 20.8% loss=0.35113, acc=0.85938
# [7/100] training 20.9% loss=0.35885, acc=0.84375
# [7/100] training 21.2% loss=0.33408, acc=0.84375
# [7/100] training 21.3% loss=0.42885, acc=0.73438
# [7/100] training 21.5% loss=0.37769, acc=0.82812
# [7/100] training 21.6% loss=0.27778, acc=0.89062
# [7/100] training 21.8% loss=0.34790, acc=0.84375
# [7/100] training 21.9% loss=0.43407, acc=0.84375
# [7/100] training 22.2% loss=0.35254, acc=0.85938
# [7/100] training 22.4% loss=0.41929, acc=0.76562
# [7/100] training 22.5% loss=0.29371, acc=0.89062
# [7/100] training 22.7% loss=0.34586, acc=0.84375
# [7/100] training 22.8% loss=0.49183, acc=0.79688
# [7/100] training 23.0% loss=0.30177, acc=0.87500
# [7/100] training 23.1% loss=0.42338, acc=0.79688
# [7/100] training 23.4% loss=0.39842, acc=0.81250
# [7/100] training 23.6% loss=0.44258, acc=0.81250
# [7/100] training 23.7% loss=0.35652, acc=0.82812
# [7/100] training 23.9% loss=0.39896, acc=0.81250
# [7/100] training 24.0% loss=0.48774, acc=0.71875
# [7/100] training 24.2% loss=0.47955, acc=0.79688
# [7/100] training 24.3% loss=0.37610, acc=0.85938
# [7/100] training 24.6% loss=0.31789, acc=0.87500
# [7/100] training 24.7% loss=0.47653, acc=0.78125
# [7/100] training 24.9% loss=0.35779, acc=0.85938
# [7/100] training 25.1% loss=0.44828, acc=0.82812
# [7/100] training 25.2% loss=0.29636, acc=0.92188
# [7/100] training 25.4% loss=0.39211, acc=0.84375
# [7/100] training 25.6% loss=0.38665, acc=0.85938
# [7/100] training 25.8% loss=0.43212, acc=0.78125
# [7/100] training 25.9% loss=0.35811, acc=0.85938
# [7/100] training 26.1% loss=0.48951, acc=0.78125
# [7/100] training 26.3% loss=0.32894, acc=0.87500
# [7/100] training 26.4% loss=0.25438, acc=0.87500
# [7/100] training 26.6% loss=0.35710, acc=0.85938
# [7/100] training 26.8% loss=0.42197, acc=0.87500
# [7/100] training 27.0% loss=0.40749, acc=0.84375
# [7/100] training 27.1% loss=0.34132, acc=0.85938
# [7/100] training 27.3% loss=0.43783, acc=0.81250
# [7/100] training 27.4% loss=0.33876, acc=0.84375
# [7/100] training 27.6% loss=0.46888, acc=0.79688
# [7/100] training 27.9% loss=0.37219, acc=0.82812
# [7/100] training 28.0% loss=0.42839, acc=0.81250
# [7/100] training 28.2% loss=0.35546, acc=0.85938
# [7/100] training 28.3% loss=0.32626, acc=0.84375
# [7/100] training 28.5% loss=0.55149, acc=0.76562
# [7/100] training 28.6% loss=0.38916, acc=0.85938
# [7/100] training 28.8% loss=0.27783, acc=0.90625
# [7/100] training 29.1% loss=0.40080, acc=0.84375
# [7/100] training 29.2% loss=0.32639, acc=0.84375
# [7/100] training 29.4% loss=0.38021, acc=0.82812
# [7/100] training 29.5% loss=0.39682, acc=0.81250
# [7/100] training 29.7% loss=0.51649, acc=0.78125
# [7/100] training 29.8% loss=0.28631, acc=0.89062
# [7/100] training 30.0% loss=0.50684, acc=0.75000
# [7/100] training 30.2% loss=0.33182, acc=0.82812
# [7/100] training 30.4% loss=0.39070, acc=0.82812
# [7/100] training 30.6% loss=0.36853, acc=0.90625
# [7/100] training 30.7% loss=0.32149, acc=0.84375
# [7/100] training 30.9% loss=0.40423, acc=0.84375
# [7/100] training 31.0% loss=0.32843, acc=0.85938
# [7/100] training 31.3% loss=0.43059, acc=0.78125
# [7/100] training 31.4% loss=0.51714, acc=0.76562
# [7/100] training 31.6% loss=0.51291, acc=0.78125
# [7/100] training 31.8% loss=0.31387, acc=0.87500
# [7/100] training 31.9% loss=0.34979, acc=0.84375
# [7/100] training 32.1% loss=0.38476, acc=0.81250
# [7/100] training 32.2% loss=0.31565, acc=0.89062
# [7/100] training 32.5% loss=0.35837, acc=0.85938
# [7/100] training 32.6% loss=0.39647, acc=0.82812
# [7/100] training 32.8% loss=0.29778, acc=0.87500
# [7/100] training 32.9% loss=0.41893, acc=0.79688
# [7/100] training 33.1% loss=0.49213, acc=0.78125
# [7/100] training 33.3% loss=0.36135, acc=0.81250
# [7/100] training 33.4% loss=0.34405, acc=0.89062
# [7/100] training 33.7% loss=0.43837, acc=0.76562
# [7/100] training 33.8% loss=0.33798, acc=0.85938
# [7/100] training 34.0% loss=0.32627, acc=0.89062
# [7/100] training 34.1% loss=0.26151, acc=0.85938
# [7/100] training 34.3% loss=0.34684, acc=0.81250
# [7/100] training 34.5% loss=0.39672, acc=0.82812
# [7/100] training 34.7% loss=0.28768, acc=0.89062
# [7/100] training 34.9% loss=0.39325, acc=0.81250
# [7/100] training 35.0% loss=0.36308, acc=0.89062
# [7/100] training 35.2% loss=0.45440, acc=0.78125
# [7/100] training 35.3% loss=0.45115, acc=0.75000
# [7/100] training 35.5% loss=0.42136, acc=0.75000
# [7/100] training 35.6% loss=0.42154, acc=0.81250
# [7/100] training 35.9% loss=0.39077, acc=0.82812
# [7/100] training 36.1% loss=0.39412, acc=0.82812
# [7/100] training 36.2% loss=0.38466, acc=0.84375
# [7/100] training 36.4% loss=0.35237, acc=0.84375
# [7/100] training 36.5% loss=0.31808, acc=0.85938
# [7/100] training 36.7% loss=0.30876, acc=0.84375
# [7/100] training 36.8% loss=0.39407, acc=0.84375
# [7/100] training 37.1% loss=0.45855, acc=0.82812
# [7/100] training 37.3% loss=0.41205, acc=0.79688
# [7/100] training 37.4% loss=0.38136, acc=0.82812
# [7/100] training 37.6% loss=0.35245, acc=0.84375
# [7/100] training 37.7% loss=0.34665, acc=0.89062
# [7/100] training 37.9% loss=0.31019, acc=0.89062
# [7/100] training 38.1% loss=0.48414, acc=0.76562
# [7/100] training 38.3% loss=0.39410, acc=0.79688
# [7/100] training 38.4% loss=0.30027, acc=0.90625
# [7/100] training 38.6% loss=0.30794, acc=0.84375
# [7/100] training 38.8% loss=0.45055, acc=0.81250
# [7/100] training 38.9% loss=0.41458, acc=0.82812
# [7/100] training 39.1% loss=0.34118, acc=0.87500
# [7/100] training 39.3% loss=0.33349, acc=0.82812
# [7/100] training 39.5% loss=0.41690, acc=0.78125
# [7/100] training 39.6% loss=0.41346, acc=0.76562
# [7/100] training 39.8% loss=0.29712, acc=0.84375
# [7/100] training 40.0% loss=0.42393, acc=0.79688
# [7/100] training 40.1% loss=0.36294, acc=0.84375
# [7/100] training 40.4% loss=0.35247, acc=0.85938
# [7/100] training 40.5% loss=0.32504, acc=0.85938
# [7/100] training 40.7% loss=0.31342, acc=0.82812
# [7/100] training 40.8% loss=0.32808, acc=0.81250
# [7/100] training 41.0% loss=0.25492, acc=0.89062
# [7/100] training 41.1% loss=0.46840, acc=0.79688
# [7/100] training 41.3% loss=0.49219, acc=0.75000
# [7/100] training 41.6% loss=0.35550, acc=0.87500
# [7/100] training 41.7% loss=0.51972, acc=0.70312
# [7/100] training 41.9% loss=0.39587, acc=0.79688
# [7/100] training 42.0% loss=0.40502, acc=0.81250
# [7/100] training 42.2% loss=0.36590, acc=0.87500
# [7/100] training 42.3% loss=0.33278, acc=0.87500
# [7/100] training 42.5% loss=0.21156, acc=0.89062
# [7/100] training 42.8% loss=0.30579, acc=0.85938
# [7/100] training 42.9% loss=0.24890, acc=0.90625
# [7/100] training 43.1% loss=0.34840, acc=0.81250
# [7/100] training 43.2% loss=0.34363, acc=0.81250
# [7/100] training 43.4% loss=0.41025, acc=0.79688
# [7/100] training 43.5% loss=0.34416, acc=0.82812
# [7/100] training 43.8% loss=0.48474, acc=0.78125
# [7/100] training 43.9% loss=0.41189, acc=0.75000
# [7/100] training 44.1% loss=0.33732, acc=0.89062
# [7/100] training 44.3% loss=0.34736, acc=0.85938
# [7/100] training 44.4% loss=0.35225, acc=0.82812
# [7/100] training 44.6% loss=0.35497, acc=0.84375
# [7/100] training 44.7% loss=0.38249, acc=0.84375
# [7/100] training 45.0% loss=0.47022, acc=0.81250
# [7/100] training 45.1% loss=0.44508, acc=0.82812
# [7/100] training 45.3% loss=0.48464, acc=0.76562
# [7/100] training 45.5% loss=0.26766, acc=0.93750
# [7/100] training 45.6% loss=0.34834, acc=0.81250
# [7/100] training 45.8% loss=0.30638, acc=0.85938
# [7/100] training 45.9% loss=0.30443, acc=0.87500
# [7/100] training 46.2% loss=0.31270, acc=0.87500
# [7/100] training 46.3% loss=0.24768, acc=0.90625
# [7/100] training 46.5% loss=0.52634, acc=0.79688
# [7/100] training 46.6% loss=0.38883, acc=0.84375
# [7/100] training 46.8% loss=0.34470, acc=0.87500
# [7/100] training 47.0% loss=0.40911, acc=0.76562
# [7/100] training 47.2% loss=0.40750, acc=0.82812
# [7/100] training 47.4% loss=0.32661, acc=0.87500
# [7/100] training 47.5% loss=0.41452, acc=0.79688
# [7/100] training 47.7% loss=0.32412, acc=0.89062
# [7/100] training 47.8% loss=0.45062, acc=0.79688
# [7/100] training 48.0% loss=0.53932, acc=0.78125
# [7/100] training 48.3% loss=0.22145, acc=0.90625
# [7/100] training 48.4% loss=0.26686, acc=0.89062
# [7/100] training 48.6% loss=0.30849, acc=0.89062
# [7/100] training 48.7% loss=0.45775, acc=0.81250
# [7/100] training 48.9% loss=0.46464, acc=0.78125
# [7/100] training 49.0% loss=0.34221, acc=0.84375
# [7/100] training 49.2% loss=0.40687, acc=0.79688
# [7/100] training 49.3% loss=0.34251, acc=0.85938
# [7/100] training 49.6% loss=0.35043, acc=0.84375
# [7/100] training 49.8% loss=0.36146, acc=0.82812
# [7/100] training 49.9% loss=0.35565, acc=0.85938
# [7/100] training 50.1% loss=0.34542, acc=0.87500
# [7/100] training 50.2% loss=0.40939, acc=0.84375
# [7/100] training 50.4% loss=0.46581, acc=0.81250
# [7/100] training 50.6% loss=0.45699, acc=0.81250
# [7/100] training 50.8% loss=0.53969, acc=0.71875
# [7/100] training 51.0% loss=0.42321, acc=0.78125
# [7/100] training 51.1% loss=0.30434, acc=0.87500
# [7/100] training 51.3% loss=0.37288, acc=0.79688
# [7/100] training 51.4% loss=0.31231, acc=0.85938
# [7/100] training 51.7% loss=0.52281, acc=0.76562
# [7/100] training 51.8% loss=0.33996, acc=0.85938
# [7/100] training 52.0% loss=0.35091, acc=0.82812
# [7/100] training 52.1% loss=0.47288, acc=0.81250
# [7/100] training 52.3% loss=0.41128, acc=0.84375
# [7/100] training 52.5% loss=0.29222, acc=0.93750
# [7/100] training 52.6% loss=0.29988, acc=0.90625
# [7/100] training 52.9% loss=0.47703, acc=0.81250
# [7/100] training 53.0% loss=0.40564, acc=0.81250
# [7/100] training 53.2% loss=0.52875, acc=0.84375
# [7/100] training 53.3% loss=0.24459, acc=0.90625
# [7/100] training 53.5% loss=0.41813, acc=0.82812
# [7/100] training 53.7% loss=0.44122, acc=0.76562
# [7/100] training 53.8% loss=0.51882, acc=0.75000
# [7/100] training 54.1% loss=0.46251, acc=0.79688
# [7/100] training 54.2% loss=0.36939, acc=0.81250
# [7/100] training 54.4% loss=0.36216, acc=0.87500
# [7/100] training 54.5% loss=0.38459, acc=0.81250
# [7/100] training 54.7% loss=0.44425, acc=0.78125
# [7/100] training 54.8% loss=0.26861, acc=0.85938
# [7/100] training 55.1% loss=0.29847, acc=0.89062
# [7/100] training 55.3% loss=0.27048, acc=0.89062
# [7/100] training 55.4% loss=0.32194, acc=0.84375
# [7/100] training 55.6% loss=0.34584, acc=0.81250
# [7/100] training 55.7% loss=0.29699, acc=0.89062
# [7/100] training 55.9% loss=0.36068, acc=0.84375
# [7/100] training 56.0% loss=0.43748, acc=0.81250
# [7/100] training 56.3% loss=0.62084, acc=0.78125
# [7/100] training 56.5% loss=0.34422, acc=0.87500
# [7/100] training 56.6% loss=0.40709, acc=0.76562
# [7/100] training 56.8% loss=0.40047, acc=0.82812
# [7/100] training 56.9% loss=0.36897, acc=0.82812
# [7/100] training 57.1% loss=0.44782, acc=0.82812
# [7/100] training 57.2% loss=0.38003, acc=0.82812
# [7/100] training 57.5% loss=0.36373, acc=0.85938
# [7/100] training 57.6% loss=0.40970, acc=0.79688
# [7/100] training 57.8% loss=0.35818, acc=0.84375
# [7/100] training 58.0% loss=0.32508, acc=0.84375
# [7/100] training 58.1% loss=0.36582, acc=0.82812
# [7/100] training 58.3% loss=0.31916, acc=0.84375
# [7/100] training 58.4% loss=0.39478, acc=0.87500
# [7/100] training 58.7% loss=0.46999, acc=0.81250
# [7/100] training 58.8% loss=0.49431, acc=0.79688
# [7/100] training 59.0% loss=0.36777, acc=0.87500
# [7/100] training 59.2% loss=0.42309, acc=0.81250
# [7/100] training 59.3% loss=0.40042, acc=0.76562
# [7/100] training 59.5% loss=0.38379, acc=0.85938
# [7/100] training 59.7% loss=0.41824, acc=0.78125
# [7/100] training 59.9% loss=0.33485, acc=0.85938
# [7/100] training 60.0% loss=0.34098, acc=0.85938
# [7/100] training 60.2% loss=0.45749, acc=0.75000
# [7/100] training 60.3% loss=0.40875, acc=0.81250
# [7/100] training 60.5% loss=0.34458, acc=0.81250
# [7/100] training 60.8% loss=0.50317, acc=0.73438
# [7/100] training 60.9% loss=0.42110, acc=0.75000
# [7/100] training 61.1% loss=0.48052, acc=0.76562
# [7/100] training 61.2% loss=0.33372, acc=0.78125
# [7/100] training 61.4% loss=0.44178, acc=0.76562
# [7/100] training 61.5% loss=0.45815, acc=0.78125
# [7/100] training 61.7% loss=0.48658, acc=0.81250
# [7/100] training 62.0% loss=0.45686, acc=0.79688
# [7/100] training 62.1% loss=0.40472, acc=0.81250
# [7/100] training 62.3% loss=0.29434, acc=0.89062
# [7/100] training 62.4% loss=0.33543, acc=0.85938
# [7/100] training 62.6% loss=0.54198, acc=0.70312
# [7/100] training 62.7% loss=0.27299, acc=0.85938
# [7/100] training 62.9% loss=0.52357, acc=0.81250
# [7/100] training 63.1% loss=0.34291, acc=0.82812
# [7/100] training 63.3% loss=0.36708, acc=0.85938
# [7/100] training 63.5% loss=0.36453, acc=0.81250
# [7/100] training 63.6% loss=0.32638, acc=0.82812
# [7/100] training 63.8% loss=0.42285, acc=0.82812
# [7/100] training 63.9% loss=0.35629, acc=0.82812
# [7/100] training 64.2% loss=0.26561, acc=0.89062
# [7/100] training 64.3% loss=0.29682, acc=0.82812
# [7/100] training 64.5% loss=0.43183, acc=0.81250
# [7/100] training 64.7% loss=0.35948, acc=0.79688
# [7/100] training 64.8% loss=0.71870, acc=0.68750
# [7/100] training 65.0% loss=0.33265, acc=0.82812
# [7/100] training 65.1% loss=0.45053, acc=0.81250
# [7/100] training 65.4% loss=0.38324, acc=0.81250
# [7/100] training 65.5% loss=0.38532, acc=0.87500
# [7/100] training 65.7% loss=0.28557, acc=0.89062
# [7/100] training 65.8% loss=0.45638, acc=0.73438
# [7/100] training 66.0% loss=0.37329, acc=0.76562
# [7/100] training 66.2% loss=0.29640, acc=0.85938
# [7/100] training 66.3% loss=0.49423, acc=0.76562
# [7/100] training 66.6% loss=0.41782, acc=0.79688
# [7/100] training 66.7% loss=0.30685, acc=0.84375
# [7/100] training 66.9% loss=0.38166, acc=0.84375
# [7/100] training 67.0% loss=0.49930, acc=0.81250
# [7/100] training 67.2% loss=0.29619, acc=0.85938
# [7/100] training 67.4% loss=0.52278, acc=0.76562
# [7/100] training 67.6% loss=0.32201, acc=0.85938
# [7/100] training 67.8% loss=0.45508, acc=0.75000
# [7/100] training 67.9% loss=0.39507, acc=0.78125
# [7/100] training 68.1% loss=0.27453, acc=0.87500
# [7/100] training 68.2% loss=0.33620, acc=0.82812
# [7/100] training 68.4% loss=0.25266, acc=0.89062
# [7/100] training 68.5% loss=0.50111, acc=0.73438
# [7/100] training 68.8% loss=0.44375, acc=0.75000
# [7/100] training 69.0% loss=0.43492, acc=0.78125
# [7/100] training 69.1% loss=0.28550, acc=0.89062
# [7/100] training 69.3% loss=0.32874, acc=0.82812
# [7/100] training 69.4% loss=0.34198, acc=0.79688
# [7/100] training 69.6% loss=0.45743, acc=0.79688
# [7/100] training 69.7% loss=0.38870, acc=0.79688
# [7/100] training 70.0% loss=0.46242, acc=0.73438
# [7/100] training 70.2% loss=0.41663, acc=0.76562
# [7/100] training 70.3% loss=0.39536, acc=0.79688
# [7/100] training 70.5% loss=0.43594, acc=0.85938
# [7/100] training 70.6% loss=0.29318, acc=0.90625
# [7/100] training 70.8% loss=0.39862, acc=0.76562
# [7/100] training 71.0% loss=0.40579, acc=0.81250
# [7/100] training 71.2% loss=0.26849, acc=0.90625
# [7/100] training 71.3% loss=0.34632, acc=0.81250
# [7/100] training 71.5% loss=0.57475, acc=0.76562
# [7/100] training 71.7% loss=0.41425, acc=0.76562
# [7/100] training 71.8% loss=0.38197, acc=0.81250
# [7/100] training 72.0% loss=0.30415, acc=0.89062
# [7/100] training 72.2% loss=0.34531, acc=0.84375
# [7/100] training 72.4% loss=0.52838, acc=0.82812
# [7/100] training 72.5% loss=0.62271, acc=0.78125
# [7/100] training 72.7% loss=0.49097, acc=0.67188
# [7/100] training 72.9% loss=0.37823, acc=0.84375
# [7/100] training 73.0% loss=0.39216, acc=0.79688
# [7/100] training 73.3% loss=0.38022, acc=0.82812
# [7/100] training 73.4% loss=0.29413, acc=0.90625
# [7/100] training 73.6% loss=0.29556, acc=0.81250
# [7/100] training 73.7% loss=0.31396, acc=0.84375
# [7/100] training 73.9% loss=0.36628, acc=0.85938
# [7/100] training 74.0% loss=0.39671, acc=0.82812
# [7/100] training 74.2% loss=0.49405, acc=0.73438
# [7/100] training 74.5% loss=0.32202, acc=0.84375
# [7/100] training 74.6% loss=0.48032, acc=0.82812
# [7/100] training 74.8% loss=0.49316, acc=0.76562
# [7/100] training 74.9% loss=0.45103, acc=0.81250
# [7/100] training 75.1% loss=0.43392, acc=0.76562
# [7/100] training 75.2% loss=0.37701, acc=0.82812
# [7/100] training 75.4% loss=0.38785, acc=0.79688
# [7/100] training 75.7% loss=0.37359, acc=0.81250
# [7/100] training 75.8% loss=0.46126, acc=0.73438
# [7/100] training 76.0% loss=0.22828, acc=0.92188
# [7/100] training 76.1% loss=0.36155, acc=0.76562
# [7/100] training 76.3% loss=0.31446, acc=0.90625
# [7/100] training 76.4% loss=0.41802, acc=0.82812
# [7/100] training 76.7% loss=0.41609, acc=0.82812
# [7/100] training 76.8% loss=0.31895, acc=0.90625
# [7/100] training 77.0% loss=0.29536, acc=0.85938
# [7/100] training 77.2% loss=0.47633, acc=0.82812
# [7/100] training 77.3% loss=0.30096, acc=0.89062
# [7/100] training 77.5% loss=0.39905, acc=0.81250
# [7/100] training 77.6% loss=0.37319, acc=0.81250
# [7/100] training 77.9% loss=0.34546, acc=0.82812
# [7/100] training 78.0% loss=0.34585, acc=0.84375
# [7/100] training 78.2% loss=0.42038, acc=0.79688
# [7/100] training 78.4% loss=0.21474, acc=0.90625
# [7/100] training 78.5% loss=0.44118, acc=0.81250
# [7/100] training 78.7% loss=0.37510, acc=0.84375
# [7/100] training 78.8% loss=0.35156, acc=0.79688
# [7/100] training 79.1% loss=0.27437, acc=0.92188
# [7/100] training 79.2% loss=0.28772, acc=0.90625
# [7/100] training 79.4% loss=0.57469, acc=0.75000
# [7/100] training 79.5% loss=0.35063, acc=0.85938
# [7/100] training 79.7% loss=0.25496, acc=0.93750
# [7/100] training 79.9% loss=0.37708, acc=0.84375
# [7/100] training 80.1% loss=0.42643, acc=0.84375
# [7/100] training 80.3% loss=0.36520, acc=0.76562
# [7/100] training 80.4% loss=0.37644, acc=0.85938
# [7/100] training 80.6% loss=0.40771, acc=0.78125
# [7/100] training 80.7% loss=0.26667, acc=0.89062
# [7/100] training 80.9% loss=0.40096, acc=0.81250
# [7/100] training 81.2% loss=0.39169, acc=0.82812
# [7/100] training 81.3% loss=0.40924, acc=0.79688
# [7/100] training 81.5% loss=0.30524, acc=0.87500
# [7/100] training 81.6% loss=0.43592, acc=0.81250
# [7/100] training 81.8% loss=0.35810, acc=0.84375
# [7/100] training 81.9% loss=0.45123, acc=0.82812
# [7/100] training 82.1% loss=0.28057, acc=0.87500
# [7/100] training 82.2% loss=0.43764, acc=0.81250
# [7/100] training 82.5% loss=0.33433, acc=0.82812
# [7/100] training 82.7% loss=0.32024, acc=0.89062
# [7/100] training 82.8% loss=0.38600, acc=0.82812
# [7/100] training 83.0% loss=0.39189, acc=0.82812
# [7/100] training 83.1% loss=0.34373, acc=0.87500
# [7/100] training 83.3% loss=0.41272, acc=0.84375
# [7/100] training 83.5% loss=0.43846, acc=0.78125
# [7/100] training 83.7% loss=0.41296, acc=0.84375
# [7/100] training 83.9% loss=0.40741, acc=0.81250
# [7/100] training 84.0% loss=0.32934, acc=0.84375
# [7/100] training 84.2% loss=0.26253, acc=0.87500
# [7/100] training 84.3% loss=0.38163, acc=0.84375
# [7/100] training 84.5% loss=0.30033, acc=0.84375
# [7/100] training 84.7% loss=0.36004, acc=0.81250
# [7/100] training 84.9% loss=0.37899, acc=0.81250
# [7/100] training 85.0% loss=0.29293, acc=0.85938
# [7/100] training 85.2% loss=0.36764, acc=0.89062
# [7/100] training 85.4% loss=0.37592, acc=0.85938
# [7/100] training 85.5% loss=0.28002, acc=0.89062
# [7/100] training 85.8% loss=0.30526, acc=0.84375
# [7/100] training 85.9% loss=0.32159, acc=0.87500
# [7/100] training 86.1% loss=0.24148, acc=0.89062
# [7/100] training 86.2% loss=0.41454, acc=0.82812
# [7/100] training 86.4% loss=0.46685, acc=0.81250
# [7/100] training 86.6% loss=0.52985, acc=0.73438
# [7/100] training 86.7% loss=0.42464, acc=0.79688
# [7/100] training 87.0% loss=0.36703, acc=0.84375
# [7/100] training 87.1% loss=0.32516, acc=0.82812
# [7/100] training 87.3% loss=0.38618, acc=0.84375
# [7/100] training 87.4% loss=0.33016, acc=0.89062
# [7/100] training 87.6% loss=0.31264, acc=0.84375
# [7/100] training 87.7% loss=0.34178, acc=0.85938
# [7/100] training 87.9% loss=0.36350, acc=0.89062
# [7/100] training 88.2% loss=0.19901, acc=0.92188
# [7/100] training 88.3% loss=0.41636, acc=0.82812
# [7/100] training 88.5% loss=0.32990, acc=0.89062
# [7/100] training 88.6% loss=0.20544, acc=0.90625
# [7/100] training 88.8% loss=0.31923, acc=0.85938
# [7/100] training 88.9% loss=0.34159, acc=0.79688
# [7/100] training 89.2% loss=0.27536, acc=0.89062
# [7/100] training 89.4% loss=0.33429, acc=0.84375
# [7/100] training 89.5% loss=0.37458, acc=0.85938
# [7/100] training 89.7% loss=0.50852, acc=0.78125
# [7/100] training 89.8% loss=0.29499, acc=0.89062
# [7/100] training 90.0% loss=0.25599, acc=0.89062
# [7/100] training 90.1% loss=0.34156, acc=0.82812
# [7/100] training 90.4% loss=0.31132, acc=0.85938
# [7/100] training 90.5% loss=0.23648, acc=0.89062
# [7/100] training 90.7% loss=0.40283, acc=0.82812
# [7/100] training 90.9% loss=0.28827, acc=0.89062
# [7/100] training 91.0% loss=0.31037, acc=0.85938
# [7/100] training 91.2% loss=0.37030, acc=0.81250
# [7/100] training 91.3% loss=0.56164, acc=0.78125
# [7/100] training 91.6% loss=0.45517, acc=0.76562
# [7/100] training 91.7% loss=0.36007, acc=0.84375
# [7/100] training 91.9% loss=0.43786, acc=0.81250
# [7/100] training 92.1% loss=0.31624, acc=0.87500
# [7/100] training 92.2% loss=0.25700, acc=0.90625
# [7/100] training 92.4% loss=0.26340, acc=0.90625
# [7/100] training 92.6% loss=0.40818, acc=0.82812
# [7/100] training 92.8% loss=0.48079, acc=0.78125
# [7/100] training 92.9% loss=0.33767, acc=0.85938
# [7/100] training 93.1% loss=0.46147, acc=0.79688
# [7/100] training 93.2% loss=0.35282, acc=0.85938
# [7/100] training 93.4% loss=0.30202, acc=0.92188
# [7/100] training 93.7% loss=0.25541, acc=0.93750
# [7/100] training 93.8% loss=0.32047, acc=0.84375
# [7/100] training 94.0% loss=0.32020, acc=0.85938
# [7/100] training 94.1% loss=0.41562, acc=0.76562
# [7/100] training 94.3% loss=0.22243, acc=0.92188
# [7/100] training 94.4% loss=0.36327, acc=0.87500
# [7/100] training 94.6% loss=0.22925, acc=0.92188
# [7/100] training 94.9% loss=0.38714, acc=0.79688
# [7/100] training 95.0% loss=0.49946, acc=0.82812
# [7/100] training 95.2% loss=0.47697, acc=0.78125
# [7/100] training 95.3% loss=0.32138, acc=0.84375
# [7/100] training 95.5% loss=0.31717, acc=0.92188
# [7/100] training 95.6% loss=0.49743, acc=0.76562
# [7/100] training 95.8% loss=0.41555, acc=0.78125
# [7/100] training 96.0% loss=0.29137, acc=0.87500
# [7/100] training 96.2% loss=0.29144, acc=0.87500
# [7/100] training 96.4% loss=0.25376, acc=0.92188
# [7/100] training 96.5% loss=0.42973, acc=0.84375
# [7/100] training 96.7% loss=0.31605, acc=0.87500
# [7/100] training 96.8% loss=0.36003, acc=0.84375
# [7/100] training 97.1% loss=0.33585, acc=0.82812
# [7/100] training 97.2% loss=0.36696, acc=0.82812
# [7/100] training 97.4% loss=0.35004, acc=0.84375
# [7/100] training 97.6% loss=0.42900, acc=0.81250
# [7/100] training 97.7% loss=0.34528, acc=0.82812
# [7/100] training 97.9% loss=0.23513, acc=0.92188
# [7/100] training 98.0% loss=0.45094, acc=0.78125
# [7/100] training 98.3% loss=0.26401, acc=0.89062
# [7/100] training 98.4% loss=0.34241, acc=0.85938
# [7/100] training 98.6% loss=0.59279, acc=0.73438
# [7/100] training 98.7% loss=0.37349, acc=0.84375
# [7/100] training 98.9% loss=0.35119, acc=0.82812
# [7/100] training 99.1% loss=0.27282, acc=0.84375
# [7/100] training 99.2% loss=0.28505, acc=0.87500
# [7/100] training 99.5% loss=0.41856, acc=0.81250
# [7/100] training 99.6% loss=0.37405, acc=0.82812
# [7/100] training 99.8% loss=0.35240, acc=0.84375
# [7/100] training 99.9% loss=0.20198, acc=0.92188
# [7/100] testing 0.9% loss=0.21264, acc=0.89062
# [7/100] testing 1.8% loss=0.56340, acc=0.71875
# [7/100] testing 2.2% loss=0.36722, acc=0.81250
# [7/100] testing 3.1% loss=0.34333, acc=0.82812
# [7/100] testing 3.5% loss=0.30650, acc=0.84375
# [7/100] testing 4.4% loss=0.36762, acc=0.82812
# [7/100] testing 4.8% loss=0.56679, acc=0.75000
# [7/100] testing 5.7% loss=0.36988, acc=0.89062
# [7/100] testing 6.6% loss=0.40416, acc=0.82812
# [7/100] testing 7.0% loss=0.29713, acc=0.92188
# [7/100] testing 7.9% loss=0.44266, acc=0.79688
# [7/100] testing 8.3% loss=0.24833, acc=0.87500
# [7/100] testing 9.2% loss=0.38911, acc=0.82812
# [7/100] testing 9.7% loss=0.26115, acc=0.85938
# [7/100] testing 10.5% loss=0.54204, acc=0.76562
# [7/100] testing 11.0% loss=0.34539, acc=0.79688
# [7/100] testing 11.8% loss=0.40508, acc=0.87500
# [7/100] testing 12.7% loss=0.54364, acc=0.78125
# [7/100] testing 13.2% loss=0.29402, acc=0.87500
# [7/100] testing 14.0% loss=0.50945, acc=0.78125
# [7/100] testing 14.5% loss=0.54366, acc=0.75000
# [7/100] testing 15.4% loss=0.38451, acc=0.87500
# [7/100] testing 15.8% loss=0.28245, acc=0.87500
# [7/100] testing 16.7% loss=0.41927, acc=0.79688
# [7/100] testing 17.5% loss=0.50101, acc=0.76562
# [7/100] testing 18.0% loss=0.28918, acc=0.87500
# [7/100] testing 18.9% loss=0.25068, acc=0.92188
# [7/100] testing 19.3% loss=0.34691, acc=0.85938
# [7/100] testing 20.2% loss=0.55289, acc=0.75000
# [7/100] testing 20.6% loss=0.44457, acc=0.76562
# [7/100] testing 21.5% loss=0.37006, acc=0.84375
# [7/100] testing 21.9% loss=0.57529, acc=0.73438
# [7/100] testing 22.8% loss=0.44737, acc=0.81250
# [7/100] testing 23.7% loss=0.44922, acc=0.79688
# [7/100] testing 24.1% loss=0.42325, acc=0.82812
# [7/100] testing 25.0% loss=0.44439, acc=0.85938
# [7/100] testing 25.4% loss=0.26481, acc=0.89062
# [7/100] testing 26.3% loss=0.41433, acc=0.78125
# [7/100] testing 26.8% loss=0.34551, acc=0.85938
# [7/100] testing 27.6% loss=0.50152, acc=0.76562
# [7/100] testing 28.5% loss=0.38474, acc=0.87500
# [7/100] testing 29.0% loss=0.36872, acc=0.85938
# [7/100] testing 29.8% loss=0.47401, acc=0.81250
# [7/100] testing 30.3% loss=0.49742, acc=0.78125
# [7/100] testing 31.1% loss=0.57949, acc=0.75000
# [7/100] testing 31.6% loss=0.34181, acc=0.84375
# [7/100] testing 32.5% loss=0.41349, acc=0.84375
# [7/100] testing 32.9% loss=0.39535, acc=0.81250
# [7/100] testing 33.8% loss=0.50589, acc=0.78125
# [7/100] testing 34.7% loss=0.47745, acc=0.79688
# [7/100] testing 35.1% loss=0.40763, acc=0.76562
# [7/100] testing 36.0% loss=0.56932, acc=0.68750
# [7/100] testing 36.4% loss=0.39718, acc=0.87500
# [7/100] testing 37.3% loss=0.36347, acc=0.82812
# [7/100] testing 37.7% loss=0.53773, acc=0.75000
# [7/100] testing 38.6% loss=0.28113, acc=0.90625
# [7/100] testing 39.5% loss=0.53913, acc=0.81250
# [7/100] testing 39.9% loss=0.32551, acc=0.87500
# [7/100] testing 40.8% loss=0.35549, acc=0.84375
# [7/100] testing 41.2% loss=0.34066, acc=0.89062
# [7/100] testing 42.1% loss=0.43442, acc=0.81250
# [7/100] testing 42.5% loss=0.34434, acc=0.85938
# [7/100] testing 43.4% loss=0.43425, acc=0.81250
# [7/100] testing 43.9% loss=0.38034, acc=0.90625
# [7/100] testing 44.7% loss=0.52957, acc=0.78125
# [7/100] testing 45.6% loss=0.41700, acc=0.82812
# [7/100] testing 46.1% loss=0.25191, acc=0.90625
# [7/100] testing 46.9% loss=0.30543, acc=0.84375
# [7/100] testing 47.4% loss=0.27930, acc=0.85938
# [7/100] testing 48.3% loss=0.47264, acc=0.76562
# [7/100] testing 48.7% loss=0.62006, acc=0.78125
# [7/100] testing 49.6% loss=0.44168, acc=0.73438
# [7/100] testing 50.4% loss=0.27215, acc=0.92188
# [7/100] testing 50.9% loss=0.43346, acc=0.78125
# [7/100] testing 51.8% loss=0.39978, acc=0.78125
# [7/100] testing 52.2% loss=0.40894, acc=0.87500
# [7/100] testing 53.1% loss=0.34351, acc=0.84375
# [7/100] testing 53.5% loss=0.30473, acc=0.87500
# [7/100] testing 54.4% loss=0.50257, acc=0.81250
# [7/100] testing 54.8% loss=0.42138, acc=0.78125
# [7/100] testing 55.7% loss=0.25082, acc=0.89062
# [7/100] testing 56.6% loss=0.45977, acc=0.82812
# [7/100] testing 57.0% loss=0.55507, acc=0.82812
# [7/100] testing 57.9% loss=0.48441, acc=0.79688
# [7/100] testing 58.3% loss=0.53610, acc=0.71875
# [7/100] testing 59.2% loss=0.38704, acc=0.79688
# [7/100] testing 59.7% loss=0.36824, acc=0.84375
# [7/100] testing 60.5% loss=0.49439, acc=0.76562
# [7/100] testing 61.4% loss=0.30871, acc=0.85938
# [7/100] testing 61.9% loss=0.38777, acc=0.84375
# [7/100] testing 62.7% loss=0.39230, acc=0.82812
# [7/100] testing 63.2% loss=0.40408, acc=0.79688
# [7/100] testing 64.0% loss=0.43955, acc=0.81250
# [7/100] testing 64.5% loss=0.37225, acc=0.79688
# [7/100] testing 65.4% loss=0.37482, acc=0.82812
# [7/100] testing 65.8% loss=0.41453, acc=0.82812
# [7/100] testing 66.7% loss=0.27763, acc=0.87500
# [7/100] testing 67.6% loss=0.39476, acc=0.82812
# [7/100] testing 68.0% loss=0.46317, acc=0.78125
# [7/100] testing 68.9% loss=0.35359, acc=0.85938
# [7/100] testing 69.3% loss=0.40205, acc=0.81250
# [7/100] testing 70.2% loss=0.43528, acc=0.81250
# [7/100] testing 70.6% loss=0.40040, acc=0.76562
# [7/100] testing 71.5% loss=0.46275, acc=0.81250
# [7/100] testing 72.4% loss=0.38666, acc=0.82812
# [7/100] testing 72.8% loss=0.44316, acc=0.79688
# [7/100] testing 73.7% loss=0.32768, acc=0.89062
# [7/100] testing 74.1% loss=0.50436, acc=0.79688
# [7/100] testing 75.0% loss=0.32540, acc=0.82812
# [7/100] testing 75.4% loss=0.32038, acc=0.85938
# [7/100] testing 76.3% loss=0.21042, acc=0.92188
# [7/100] testing 76.8% loss=0.24159, acc=0.87500
# [7/100] testing 77.6% loss=0.30048, acc=0.89062
# [7/100] testing 78.5% loss=0.68181, acc=0.75000
# [7/100] testing 79.0% loss=0.33858, acc=0.82812
# [7/100] testing 79.8% loss=0.41224, acc=0.76562
# [7/100] testing 80.3% loss=0.38140, acc=0.82812
# [7/100] testing 81.2% loss=0.47771, acc=0.82812
# [7/100] testing 81.6% loss=0.47896, acc=0.81250
# [7/100] testing 82.5% loss=0.29971, acc=0.84375
# [7/100] testing 83.3% loss=0.30039, acc=0.85938
# [7/100] testing 83.8% loss=0.24724, acc=0.90625
# [7/100] testing 84.7% loss=0.30592, acc=0.85938
# [7/100] testing 85.1% loss=0.50362, acc=0.78125
# [7/100] testing 86.0% loss=0.36968, acc=0.82812
# [7/100] testing 86.4% loss=0.52371, acc=0.76562
# [7/100] testing 87.3% loss=0.49899, acc=0.81250
# [7/100] testing 87.7% loss=0.33675, acc=0.82812
# [7/100] testing 88.6% loss=0.33421, acc=0.84375
# [7/100] testing 89.5% loss=0.55074, acc=0.73438
# [7/100] testing 89.9% loss=0.42925, acc=0.78125
# [7/100] testing 90.8% loss=0.33988, acc=0.89062
# [7/100] testing 91.2% loss=0.23201, acc=0.90625
# [7/100] testing 92.1% loss=0.43150, acc=0.82812
# [7/100] testing 92.6% loss=0.44621, acc=0.79688
# [7/100] testing 93.4% loss=0.39449, acc=0.81250
# [7/100] testing 94.3% loss=0.26012, acc=0.87500
# [7/100] testing 94.7% loss=0.39361, acc=0.81250
# [7/100] testing 95.6% loss=0.42875, acc=0.76562
# [7/100] testing 96.1% loss=0.32942, acc=0.82812
# [7/100] testing 96.9% loss=0.38268, acc=0.81250
# [7/100] testing 97.4% loss=0.20802, acc=0.92188
# [7/100] testing 98.3% loss=0.44523, acc=0.76562
# [7/100] testing 98.7% loss=0.38890, acc=0.81250
# [7/100] testing 99.6% loss=0.34134, acc=0.84375
# [8/100] training 0.2% loss=0.48280, acc=0.78125
# [8/100] training 0.4% loss=0.54006, acc=0.73438
# [8/100] training 0.5% loss=0.38026, acc=0.79688
# [8/100] training 0.8% loss=0.44812, acc=0.76562
# [8/100] training 0.9% loss=0.37681, acc=0.82812
# [8/100] training 1.1% loss=0.33863, acc=0.87500
# [8/100] training 1.2% loss=0.39958, acc=0.81250
# [8/100] training 1.4% loss=0.37218, acc=0.82812
# [8/100] training 1.6% loss=0.22828, acc=0.89062
# [8/100] training 1.8% loss=0.30399, acc=0.89062
# [8/100] training 2.0% loss=0.39815, acc=0.85938
# [8/100] training 2.1% loss=0.41508, acc=0.79688
# [8/100] training 2.3% loss=0.27299, acc=0.89062
# [8/100] training 2.4% loss=0.37805, acc=0.85938
# [8/100] training 2.6% loss=0.36856, acc=0.79688
# [8/100] training 2.7% loss=0.45184, acc=0.81250
# [8/100] training 3.0% loss=0.46161, acc=0.73438
# [8/100] training 3.2% loss=0.37182, acc=0.82812
# [8/100] training 3.3% loss=0.39796, acc=0.84375
# [8/100] training 3.5% loss=0.40102, acc=0.81250
# [8/100] training 3.6% loss=0.58784, acc=0.78125
# [8/100] training 3.8% loss=0.23626, acc=0.89062
# [8/100] training 3.9% loss=0.33936, acc=0.85938
# [8/100] training 4.2% loss=0.33341, acc=0.84375
# [8/100] training 4.4% loss=0.33509, acc=0.85938
# [8/100] training 4.5% loss=0.30453, acc=0.85938
# [8/100] training 4.7% loss=0.44737, acc=0.87500
# [8/100] training 4.8% loss=0.48092, acc=0.79688
# [8/100] training 5.0% loss=0.34998, acc=0.84375
# [8/100] training 5.2% loss=0.48782, acc=0.76562
# [8/100] training 5.4% loss=0.30753, acc=0.85938
# [8/100] training 5.5% loss=0.35589, acc=0.89062
# [8/100] training 5.7% loss=0.26886, acc=0.89062
# [8/100] training 5.9% loss=0.41733, acc=0.81250
# [8/100] training 6.0% loss=0.37112, acc=0.81250
# [8/100] training 6.3% loss=0.39119, acc=0.79688
# [8/100] training 6.4% loss=0.30799, acc=0.84375
# [8/100] training 6.6% loss=0.34675, acc=0.82812
# [8/100] training 6.7% loss=0.44076, acc=0.76562
# [8/100] training 6.9% loss=0.24803, acc=0.92188
# [8/100] training 7.1% loss=0.39569, acc=0.79688
# [8/100] training 7.2% loss=0.32577, acc=0.84375
# [8/100] training 7.5% loss=0.30519, acc=0.87500
# [8/100] training 7.6% loss=0.38268, acc=0.79688
# [8/100] training 7.8% loss=0.39717, acc=0.75000
# [8/100] training 7.9% loss=0.42024, acc=0.81250
# [8/100] training 8.1% loss=0.29615, acc=0.90625
# [8/100] training 8.2% loss=0.31490, acc=0.82812
# [8/100] training 8.4% loss=0.32045, acc=0.87500
# [8/100] training 8.7% loss=0.30852, acc=0.84375
# [8/100] training 8.8% loss=0.36198, acc=0.81250
# [8/100] training 9.0% loss=0.37198, acc=0.84375
# [8/100] training 9.1% loss=0.42618, acc=0.79688
# [8/100] training 9.3% loss=0.48187, acc=0.78125
# [8/100] training 9.4% loss=0.37763, acc=0.76562
# [8/100] training 9.7% loss=0.39084, acc=0.82812
# [8/100] training 9.9% loss=0.41473, acc=0.87500
# [8/100] training 10.0% loss=0.34669, acc=0.82812
# [8/100] training 10.2% loss=0.31342, acc=0.84375
# [8/100] training 10.3% loss=0.30261, acc=0.82812
# [8/100] training 10.5% loss=0.36165, acc=0.85938
# [8/100] training 10.6% loss=0.31611, acc=0.85938
# [8/100] training 10.9% loss=0.44532, acc=0.81250
# [8/100] training 11.0% loss=0.37701, acc=0.82812
# [8/100] training 11.2% loss=0.28017, acc=0.90625
# [8/100] training 11.4% loss=0.52121, acc=0.78125
# [8/100] training 11.5% loss=0.54059, acc=0.73438
# [8/100] training 11.7% loss=0.36384, acc=0.81250
# [8/100] training 11.8% loss=0.36763, acc=0.82812
# [8/100] training 12.1% loss=0.42294, acc=0.75000
# [8/100] training 12.2% loss=0.39813, acc=0.82812
# [8/100] training 12.4% loss=0.34863, acc=0.85938
# [8/100] training 12.6% loss=0.31399, acc=0.90625
# [8/100] training 12.7% loss=0.46686, acc=0.81250
# [8/100] training 12.9% loss=0.44831, acc=0.84375
# [8/100] training 13.0% loss=0.40751, acc=0.81250
# [8/100] training 13.3% loss=0.36465, acc=0.85938
# [8/100] training 13.4% loss=0.47329, acc=0.75000
# [8/100] training 13.6% loss=0.35663, acc=0.85938
# [8/100] training 13.7% loss=0.53556, acc=0.78125
# [8/100] training 13.9% loss=0.45385, acc=0.84375
# [8/100] training 14.1% loss=0.38252, acc=0.81250
# [8/100] training 14.3% loss=0.34277, acc=0.89062
# [8/100] training 14.5% loss=0.30082, acc=0.87500
# [8/100] training 14.6% loss=0.32316, acc=0.85938
# [8/100] training 14.8% loss=0.36729, acc=0.87500
# [8/100] training 14.9% loss=0.44015, acc=0.75000
# [8/100] training 15.1% loss=0.37877, acc=0.85938
# [8/100] training 15.4% loss=0.33081, acc=0.87500
# [8/100] training 15.5% loss=0.37559, acc=0.79688
# [8/100] training 15.7% loss=0.49894, acc=0.75000
# [8/100] training 15.8% loss=0.31360, acc=0.89062
# [8/100] training 16.0% loss=0.46674, acc=0.76562
# [8/100] training 16.1% loss=0.62900, acc=0.71875
# [8/100] training 16.3% loss=0.34030, acc=0.82812
# [8/100] training 16.4% loss=0.32007, acc=0.84375
# [8/100] training 16.7% loss=0.40015, acc=0.81250
# [8/100] training 16.9% loss=0.46154, acc=0.79688
# [8/100] training 17.0% loss=0.33864, acc=0.85938
# [8/100] training 17.2% loss=0.27855, acc=0.85938
# [8/100] training 17.3% loss=0.37848, acc=0.84375
# [8/100] training 17.5% loss=0.34724, acc=0.85938
# [8/100] training 17.7% loss=0.40670, acc=0.82812
# [8/100] training 17.9% loss=0.53738, acc=0.75000
# [8/100] training 18.1% loss=0.42516, acc=0.81250
# [8/100] training 18.2% loss=0.40487, acc=0.82812
# [8/100] training 18.4% loss=0.41399, acc=0.84375
# [8/100] training 18.5% loss=0.34952, acc=0.90625
# [8/100] training 18.8% loss=0.30993, acc=0.89062
# [8/100] training 18.9% loss=0.30545, acc=0.89062
# [8/100] training 19.1% loss=0.41976, acc=0.79688
# [8/100] training 19.2% loss=0.25056, acc=0.90625
# [8/100] training 19.4% loss=0.18091, acc=0.95312
# [8/100] training 19.6% loss=0.42438, acc=0.81250
# [8/100] training 19.7% loss=0.48796, acc=0.78125
# [8/100] training 20.0% loss=0.24425, acc=0.92188
# [8/100] training 20.1% loss=0.36940, acc=0.82812
# [8/100] training 20.3% loss=0.36586, acc=0.85938
# [8/100] training 20.4% loss=0.46060, acc=0.73438
# [8/100] training 20.6% loss=0.40642, acc=0.81250
# [8/100] training 20.8% loss=0.31870, acc=0.87500
# [8/100] training 20.9% loss=0.34780, acc=0.90625
# [8/100] training 21.2% loss=0.31230, acc=0.87500
# [8/100] training 21.3% loss=0.37395, acc=0.82812
# [8/100] training 21.5% loss=0.31817, acc=0.87500
# [8/100] training 21.6% loss=0.21433, acc=0.90625
# [8/100] training 21.8% loss=0.27590, acc=0.89062
# [8/100] training 21.9% loss=0.39146, acc=0.76562
# [8/100] training 22.2% loss=0.32604, acc=0.84375
# [8/100] training 22.4% loss=0.45189, acc=0.82812
# [8/100] training 22.5% loss=0.36148, acc=0.78125
# [8/100] training 22.7% loss=0.34053, acc=0.84375
# [8/100] training 22.8% loss=0.45552, acc=0.82812
# [8/100] training 23.0% loss=0.26041, acc=0.84375
# [8/100] training 23.1% loss=0.31330, acc=0.89062
# [8/100] training 23.4% loss=0.35111, acc=0.84375
# [8/100] training 23.6% loss=0.48271, acc=0.82812
# [8/100] training 23.7% loss=0.36439, acc=0.79688
# [8/100] training 23.9% loss=0.33224, acc=0.84375
# [8/100] training 24.0% loss=0.44467, acc=0.76562
# [8/100] training 24.2% loss=0.35282, acc=0.87500
# [8/100] training 24.3% loss=0.37895, acc=0.82812
# [8/100] training 24.6% loss=0.26394, acc=0.89062
# [8/100] training 24.7% loss=0.44802, acc=0.76562
# [8/100] training 24.9% loss=0.31408, acc=0.85938
# [8/100] training 25.1% loss=0.44456, acc=0.79688
# [8/100] training 25.2% loss=0.20238, acc=0.95312
# [8/100] training 25.4% loss=0.33544, acc=0.85938
# [8/100] training 25.6% loss=0.35016, acc=0.85938
# [8/100] training 25.8% loss=0.36451, acc=0.87500
# [8/100] training 25.9% loss=0.31282, acc=0.89062
# [8/100] training 26.1% loss=0.44204, acc=0.84375
# [8/100] training 26.3% loss=0.35146, acc=0.85938
# [8/100] training 26.4% loss=0.21895, acc=0.93750
# [8/100] training 26.6% loss=0.34301, acc=0.84375
# [8/100] training 26.8% loss=0.41382, acc=0.84375
# [8/100] training 27.0% loss=0.35673, acc=0.90625
# [8/100] training 27.1% loss=0.35751, acc=0.84375
# [8/100] training 27.3% loss=0.35359, acc=0.82812
# [8/100] training 27.4% loss=0.25424, acc=0.87500
# [8/100] training 27.6% loss=0.36966, acc=0.82812
# [8/100] training 27.9% loss=0.33829, acc=0.79688
# [8/100] training 28.0% loss=0.42780, acc=0.81250
# [8/100] training 28.2% loss=0.41227, acc=0.82812
# [8/100] training 28.3% loss=0.22783, acc=0.93750
# [8/100] training 28.5% loss=0.38921, acc=0.78125
# [8/100] training 28.6% loss=0.40943, acc=0.78125
# [8/100] training 28.8% loss=0.23053, acc=0.92188
# [8/100] training 29.1% loss=0.49207, acc=0.78125
# [8/100] training 29.2% loss=0.33012, acc=0.90625
# [8/100] training 29.4% loss=0.45488, acc=0.81250
# [8/100] training 29.5% loss=0.23779, acc=0.87500
# [8/100] training 29.7% loss=0.56414, acc=0.73438
# [8/100] training 29.8% loss=0.31428, acc=0.82812
# [8/100] training 30.0% loss=0.49672, acc=0.75000
# [8/100] training 30.2% loss=0.34623, acc=0.87500
# [8/100] training 30.4% loss=0.32056, acc=0.82812
# [8/100] training 30.6% loss=0.45896, acc=0.78125
# [8/100] training 30.7% loss=0.31075, acc=0.84375
# [8/100] training 30.9% loss=0.35860, acc=0.85938
# [8/100] training 31.0% loss=0.29319, acc=0.89062
# [8/100] training 31.3% loss=0.36644, acc=0.82812
# [8/100] training 31.4% loss=0.45388, acc=0.76562
# [8/100] training 31.6% loss=0.54727, acc=0.78125
# [8/100] training 31.8% loss=0.27271, acc=0.89062
# [8/100] training 31.9% loss=0.36025, acc=0.84375
# [8/100] training 32.1% loss=0.36130, acc=0.79688
# [8/100] training 32.2% loss=0.30910, acc=0.89062
# [8/100] training 32.5% loss=0.29300, acc=0.87500
# [8/100] training 32.6% loss=0.35434, acc=0.85938
# [8/100] training 32.8% loss=0.23102, acc=0.90625
# [8/100] training 32.9% loss=0.41500, acc=0.81250
# [8/100] training 33.1% loss=0.40723, acc=0.84375
# [8/100] training 33.3% loss=0.36232, acc=0.81250
# [8/100] training 33.4% loss=0.37756, acc=0.87500
# [8/100] training 33.7% loss=0.39330, acc=0.84375
# [8/100] training 33.8% loss=0.43482, acc=0.84375
# [8/100] training 34.0% loss=0.37011, acc=0.84375
# [8/100] training 34.1% loss=0.32804, acc=0.87500
# [8/100] training 34.3% loss=0.30982, acc=0.90625
# [8/100] training 34.5% loss=0.42931, acc=0.78125
# [8/100] training 34.7% loss=0.27971, acc=0.93750
# [8/100] training 34.9% loss=0.27334, acc=0.87500
# [8/100] training 35.0% loss=0.39394, acc=0.82812
# [8/100] training 35.2% loss=0.38983, acc=0.84375
# [8/100] training 35.3% loss=0.32052, acc=0.85938
# [8/100] training 35.5% loss=0.37194, acc=0.84375
# [8/100] training 35.6% loss=0.41746, acc=0.79688
# [8/100] training 35.9% loss=0.29863, acc=0.82812
# [8/100] training 36.1% loss=0.31781, acc=0.85938
# [8/100] training 36.2% loss=0.43126, acc=0.78125
# [8/100] training 36.4% loss=0.38263, acc=0.87500
# [8/100] training 36.5% loss=0.34657, acc=0.87500
# [8/100] training 36.7% loss=0.26056, acc=0.90625
# [8/100] training 36.8% loss=0.30622, acc=0.85938
# [8/100] training 37.1% loss=0.44681, acc=0.84375
# [8/100] training 37.3% loss=0.33571, acc=0.85938
# [8/100] training 37.4% loss=0.38732, acc=0.81250
# [8/100] training 37.6% loss=0.32400, acc=0.87500
# [8/100] training 37.7% loss=0.26446, acc=0.92188
# [8/100] training 37.9% loss=0.32349, acc=0.85938
# [8/100] training 38.1% loss=0.38728, acc=0.78125
# [8/100] training 38.3% loss=0.40921, acc=0.82812
# [8/100] training 38.4% loss=0.36152, acc=0.78125
# [8/100] training 38.6% loss=0.35002, acc=0.82812
# [8/100] training 38.8% loss=0.38365, acc=0.84375
# [8/100] training 38.9% loss=0.39121, acc=0.82812
# [8/100] training 39.1% loss=0.31264, acc=0.89062
# [8/100] training 39.3% loss=0.39650, acc=0.75000
# [8/100] training 39.5% loss=0.36980, acc=0.82812
# [8/100] training 39.6% loss=0.31762, acc=0.89062
# [8/100] training 39.8% loss=0.26552, acc=0.89062
# [8/100] training 40.0% loss=0.36318, acc=0.79688
# [8/100] training 40.1% loss=0.42186, acc=0.81250
# [8/100] training 40.4% loss=0.37192, acc=0.79688
# [8/100] training 40.5% loss=0.27622, acc=0.90625
# [8/100] training 40.7% loss=0.29953, acc=0.84375
# [8/100] training 40.8% loss=0.26912, acc=0.89062
# [8/100] training 41.0% loss=0.23503, acc=0.92188
# [8/100] training 41.1% loss=0.37114, acc=0.82812
# [8/100] training 41.3% loss=0.50477, acc=0.73438
# [8/100] training 41.6% loss=0.35195, acc=0.87500
# [8/100] training 41.7% loss=0.49481, acc=0.75000
# [8/100] training 41.9% loss=0.33883, acc=0.85938
# [8/100] training 42.0% loss=0.27422, acc=0.89062
# [8/100] training 42.2% loss=0.30658, acc=0.90625
# [8/100] training 42.3% loss=0.32677, acc=0.87500
# [8/100] training 42.5% loss=0.21606, acc=0.93750
# [8/100] training 42.8% loss=0.29938, acc=0.82812
# [8/100] training 42.9% loss=0.23988, acc=0.90625
# [8/100] training 43.1% loss=0.30439, acc=0.87500
# [8/100] training 43.2% loss=0.35188, acc=0.82812
# [8/100] training 43.4% loss=0.36719, acc=0.82812
# [8/100] training 43.5% loss=0.30604, acc=0.85938
# [8/100] training 43.8% loss=0.49060, acc=0.75000
# [8/100] training 43.9% loss=0.34860, acc=0.89062
# [8/100] training 44.1% loss=0.28222, acc=0.82812
# [8/100] training 44.3% loss=0.39270, acc=0.82812
# [8/100] training 44.4% loss=0.32004, acc=0.85938
# [8/100] training 44.6% loss=0.51240, acc=0.78125
# [8/100] training 44.7% loss=0.39161, acc=0.79688
# [8/100] training 45.0% loss=0.30631, acc=0.89062
# [8/100] training 45.1% loss=0.44136, acc=0.82812
# [8/100] training 45.3% loss=0.39864, acc=0.79688
# [8/100] training 45.5% loss=0.24109, acc=0.92188
# [8/100] training 45.6% loss=0.30015, acc=0.87500
# [8/100] training 45.8% loss=0.39040, acc=0.82812
# [8/100] training 45.9% loss=0.25860, acc=0.89062
# [8/100] training 46.2% loss=0.27105, acc=0.89062
# [8/100] training 46.3% loss=0.26944, acc=0.85938
# [8/100] training 46.5% loss=0.49396, acc=0.81250
# [8/100] training 46.6% loss=0.34331, acc=0.85938
# [8/100] training 46.8% loss=0.34935, acc=0.87500
# [8/100] training 47.0% loss=0.39586, acc=0.78125
# [8/100] training 47.2% loss=0.34769, acc=0.85938
# [8/100] training 47.4% loss=0.31959, acc=0.87500
# [8/100] training 47.5% loss=0.35678, acc=0.82812
# [8/100] training 47.7% loss=0.30093, acc=0.89062
# [8/100] training 47.8% loss=0.45579, acc=0.76562
# [8/100] training 48.0% loss=0.48490, acc=0.79688
# [8/100] training 48.3% loss=0.18519, acc=0.90625
# [8/100] training 48.4% loss=0.21181, acc=0.92188
# [8/100] training 48.6% loss=0.26868, acc=0.89062
# [8/100] training 48.7% loss=0.44640, acc=0.82812
# [8/100] training 48.9% loss=0.50546, acc=0.76562
# [8/100] training 49.0% loss=0.39447, acc=0.79688
# [8/100] training 49.2% loss=0.32945, acc=0.81250
# [8/100] training 49.3% loss=0.30246, acc=0.92188
# [8/100] training 49.6% loss=0.36620, acc=0.84375
# [8/100] training 49.8% loss=0.33503, acc=0.79688
# [8/100] training 49.9% loss=0.34764, acc=0.85938
# [8/100] training 50.1% loss=0.34620, acc=0.85938
# [8/100] training 50.2% loss=0.39039, acc=0.78125
# [8/100] training 50.4% loss=0.48363, acc=0.82812
# [8/100] training 50.6% loss=0.50009, acc=0.76562
# [8/100] training 50.8% loss=0.49225, acc=0.73438
# [8/100] training 51.0% loss=0.43494, acc=0.78125
# [8/100] training 51.1% loss=0.32564, acc=0.81250
# [8/100] training 51.3% loss=0.37864, acc=0.78125
# [8/100] training 51.4% loss=0.36628, acc=0.84375
# [8/100] training 51.7% loss=0.51815, acc=0.75000
# [8/100] training 51.8% loss=0.36453, acc=0.81250
# [8/100] training 52.0% loss=0.34463, acc=0.87500
# [8/100] training 52.1% loss=0.45142, acc=0.82812
# [8/100] training 52.3% loss=0.42037, acc=0.84375
# [8/100] training 52.5% loss=0.23551, acc=0.92188
# [8/100] training 52.6% loss=0.36848, acc=0.85938
# [8/100] training 52.9% loss=0.52367, acc=0.79688
# [8/100] training 53.0% loss=0.32819, acc=0.84375
# [8/100] training 53.2% loss=0.38332, acc=0.84375
# [8/100] training 53.3% loss=0.20361, acc=0.85938
# [8/100] training 53.5% loss=0.53878, acc=0.78125
# [8/100] training 53.7% loss=0.40832, acc=0.84375
# [8/100] training 53.8% loss=0.44560, acc=0.79688
# [8/100] training 54.1% loss=0.43687, acc=0.81250
# [8/100] training 54.2% loss=0.33982, acc=0.85938
# [8/100] training 54.4% loss=0.35571, acc=0.89062
# [8/100] training 54.5% loss=0.39422, acc=0.84375
# [8/100] training 54.7% loss=0.47051, acc=0.78125
# [8/100] training 54.8% loss=0.27860, acc=0.89062
# [8/100] training 55.1% loss=0.29305, acc=0.89062
# [8/100] training 55.3% loss=0.22857, acc=0.93750
# [8/100] training 55.4% loss=0.40023, acc=0.82812
# [8/100] training 55.6% loss=0.38911, acc=0.82812
# [8/100] training 55.7% loss=0.34647, acc=0.82812
# [8/100] training 55.9% loss=0.31451, acc=0.82812
# [8/100] training 56.0% loss=0.41257, acc=0.78125
# [8/100] training 56.3% loss=0.57444, acc=0.73438
# [8/100] training 56.5% loss=0.37905, acc=0.84375
# [8/100] training 56.6% loss=0.39399, acc=0.79688
# [8/100] training 56.8% loss=0.42605, acc=0.79688
# [8/100] training 56.9% loss=0.40343, acc=0.81250
# [8/100] training 57.1% loss=0.38807, acc=0.84375
# [8/100] training 57.2% loss=0.31727, acc=0.85938
# [8/100] training 57.5% loss=0.34728, acc=0.85938
# [8/100] training 57.6% loss=0.36135, acc=0.78125
# [8/100] training 57.8% loss=0.30430, acc=0.85938
# [8/100] training 58.0% loss=0.26521, acc=0.87500
# [8/100] training 58.1% loss=0.35070, acc=0.82812
# [8/100] training 58.3% loss=0.21051, acc=0.89062
# [8/100] training 58.4% loss=0.36705, acc=0.81250
# [8/100] training 58.7% loss=0.39515, acc=0.79688
# [8/100] training 58.8% loss=0.47405, acc=0.79688
# [8/100] training 59.0% loss=0.30936, acc=0.89062
# [8/100] training 59.2% loss=0.50612, acc=0.73438
# [8/100] training 59.3% loss=0.32256, acc=0.84375
# [8/100] training 59.5% loss=0.45015, acc=0.82812
# [8/100] training 59.7% loss=0.41762, acc=0.78125
# [8/100] training 59.9% loss=0.30933, acc=0.92188
# [8/100] training 60.0% loss=0.27733, acc=0.90625
# [8/100] training 60.2% loss=0.45761, acc=0.76562
# [8/100] training 60.3% loss=0.44759, acc=0.81250
# [8/100] training 60.5% loss=0.32889, acc=0.79688
# [8/100] training 60.8% loss=0.40598, acc=0.78125
# [8/100] training 60.9% loss=0.39452, acc=0.81250
# [8/100] training 61.1% loss=0.44802, acc=0.71875
# [8/100] training 61.2% loss=0.33846, acc=0.82812
# [8/100] training 61.4% loss=0.38530, acc=0.78125
# [8/100] training 61.5% loss=0.49590, acc=0.76562
# [8/100] training 61.7% loss=0.49224, acc=0.76562
# [8/100] training 62.0% loss=0.44706, acc=0.82812
# [8/100] training 62.1% loss=0.42007, acc=0.76562
# [8/100] training 62.3% loss=0.28562, acc=0.96875
# [8/100] training 62.4% loss=0.32247, acc=0.85938
# [8/100] training 62.6% loss=0.47327, acc=0.78125
# [8/100] training 62.7% loss=0.25720, acc=0.89062
# [8/100] training 62.9% loss=0.63157, acc=0.76562
# [8/100] training 63.1% loss=0.36821, acc=0.85938
# [8/100] training 63.3% loss=0.38846, acc=0.82812
# [8/100] training 63.5% loss=0.41574, acc=0.78125
# [8/100] training 63.6% loss=0.31199, acc=0.85938
# [8/100] training 63.8% loss=0.39440, acc=0.82812
# [8/100] training 63.9% loss=0.34887, acc=0.85938
# [8/100] training 64.2% loss=0.31556, acc=0.87500
# [8/100] training 64.3% loss=0.32710, acc=0.84375
# [8/100] training 64.5% loss=0.41043, acc=0.81250
# [8/100] training 64.7% loss=0.32749, acc=0.82812
# [8/100] training 64.8% loss=0.55356, acc=0.81250
# [8/100] training 65.0% loss=0.42747, acc=0.84375
# [8/100] training 65.1% loss=0.48446, acc=0.75000
# [8/100] training 65.4% loss=0.34100, acc=0.82812
# [8/100] training 65.5% loss=0.32718, acc=0.82812
# [8/100] training 65.7% loss=0.30818, acc=0.81250
# [8/100] training 65.8% loss=0.43945, acc=0.78125
# [8/100] training 66.0% loss=0.38429, acc=0.81250
# [8/100] training 66.2% loss=0.24943, acc=0.90625
# [8/100] training 66.3% loss=0.49485, acc=0.75000
# [8/100] training 66.6% loss=0.37606, acc=0.81250
# [8/100] training 66.7% loss=0.26221, acc=0.84375
# [8/100] training 66.9% loss=0.34863, acc=0.85938
# [8/100] training 67.0% loss=0.44582, acc=0.76562
# [8/100] training 67.2% loss=0.22224, acc=0.92188
# [8/100] training 67.4% loss=0.44636, acc=0.82812
# [8/100] training 67.6% loss=0.27310, acc=0.89062
# [8/100] training 67.8% loss=0.48263, acc=0.78125
# [8/100] training 67.9% loss=0.28978, acc=0.87500
# [8/100] training 68.1% loss=0.19719, acc=0.95312
# [8/100] training 68.2% loss=0.28760, acc=0.85938
# [8/100] training 68.4% loss=0.26709, acc=0.84375
# [8/100] training 68.5% loss=0.53403, acc=0.70312
# [8/100] training 68.8% loss=0.40680, acc=0.81250
# [8/100] training 69.0% loss=0.50085, acc=0.75000
# [8/100] training 69.1% loss=0.24537, acc=0.92188
# [8/100] training 69.3% loss=0.34144, acc=0.82812
# [8/100] training 69.4% loss=0.33431, acc=0.87500
# [8/100] training 69.6% loss=0.29307, acc=0.85938
# [8/100] training 69.7% loss=0.36004, acc=0.84375
# [8/100] training 70.0% loss=0.40914, acc=0.75000
# [8/100] training 70.2% loss=0.33724, acc=0.85938
# [8/100] training 70.3% loss=0.41252, acc=0.81250
# [8/100] training 70.5% loss=0.40886, acc=0.87500
# [8/100] training 70.6% loss=0.33081, acc=0.84375
# [8/100] training 70.8% loss=0.38263, acc=0.81250
# [8/100] training 71.0% loss=0.37082, acc=0.82812
# [8/100] training 71.2% loss=0.32412, acc=0.84375
# [8/100] training 71.3% loss=0.35171, acc=0.82812
# [8/100] training 71.5% loss=0.60475, acc=0.76562
# [8/100] training 71.7% loss=0.41972, acc=0.78125
# [8/100] training 71.8% loss=0.39665, acc=0.81250
# [8/100] training 72.0% loss=0.25319, acc=0.87500
# [8/100] training 72.2% loss=0.31520, acc=0.81250
# [8/100] training 72.4% loss=0.47974, acc=0.79688
# [8/100] training 72.5% loss=0.43970, acc=0.79688
# [8/100] training 72.7% loss=0.43450, acc=0.76562
# [8/100] training 72.9% loss=0.37998, acc=0.84375
# [8/100] training 73.0% loss=0.26284, acc=0.92188
# [8/100] training 73.3% loss=0.40298, acc=0.81250
# [8/100] training 73.4% loss=0.22374, acc=0.92188
# [8/100] training 73.6% loss=0.23504, acc=0.89062
# [8/100] training 73.7% loss=0.26281, acc=0.87500
# [8/100] training 73.9% loss=0.36756, acc=0.82812
# [8/100] training 74.0% loss=0.38849, acc=0.84375
# [8/100] training 74.2% loss=0.37097, acc=0.84375
# [8/100] training 74.5% loss=0.36895, acc=0.84375
# [8/100] training 74.6% loss=0.56061, acc=0.78125
# [8/100] training 74.8% loss=0.47414, acc=0.78125
# [8/100] training 74.9% loss=0.43554, acc=0.82812
# [8/100] training 75.1% loss=0.41108, acc=0.82812
# [8/100] training 75.2% loss=0.28980, acc=0.92188
# [8/100] training 75.4% loss=0.38322, acc=0.79688
# [8/100] training 75.7% loss=0.31382, acc=0.87500
# [8/100] training 75.8% loss=0.50733, acc=0.81250
# [8/100] training 76.0% loss=0.23683, acc=0.89062
# [8/100] training 76.1% loss=0.38098, acc=0.84375
# [8/100] training 76.3% loss=0.27129, acc=0.90625
# [8/100] training 76.4% loss=0.37421, acc=0.84375
# [8/100] training 76.7% loss=0.23322, acc=0.89062
# [8/100] training 76.8% loss=0.29459, acc=0.87500
# [8/100] training 77.0% loss=0.31540, acc=0.82812
# [8/100] training 77.2% loss=0.37536, acc=0.85938
# [8/100] training 77.3% loss=0.29327, acc=0.87500
# [8/100] training 77.5% loss=0.34505, acc=0.81250
# [8/100] training 77.6% loss=0.30310, acc=0.89062
# [8/100] training 77.9% loss=0.51634, acc=0.67188
# [8/100] training 78.0% loss=0.36912, acc=0.85938
# [8/100] training 78.2% loss=0.32949, acc=0.87500
# [8/100] training 78.4% loss=0.25074, acc=0.90625
# [8/100] training 78.5% loss=0.40079, acc=0.84375
# [8/100] training 78.7% loss=0.30632, acc=0.90625
# [8/100] training 78.8% loss=0.30217, acc=0.89062
# [8/100] training 79.1% loss=0.24603, acc=0.89062
# [8/100] training 79.2% loss=0.29701, acc=0.92188
# [8/100] training 79.4% loss=0.54193, acc=0.75000
# [8/100] training 79.5% loss=0.32533, acc=0.81250
# [8/100] training 79.7% loss=0.22668, acc=0.90625
# [8/100] training 79.9% loss=0.36196, acc=0.81250
# [8/100] training 80.1% loss=0.29633, acc=0.87500
# [8/100] training 80.3% loss=0.47813, acc=0.78125
# [8/100] training 80.4% loss=0.38194, acc=0.82812
# [8/100] training 80.6% loss=0.42214, acc=0.79688
# [8/100] training 80.7% loss=0.30067, acc=0.89062
# [8/100] training 80.9% loss=0.43753, acc=0.79688
# [8/100] training 81.2% loss=0.37622, acc=0.82812
# [8/100] training 81.3% loss=0.44279, acc=0.75000
# [8/100] training 81.5% loss=0.31955, acc=0.87500
# [8/100] training 81.6% loss=0.42889, acc=0.79688
# [8/100] training 81.8% loss=0.34916, acc=0.84375
# [8/100] training 81.9% loss=0.46831, acc=0.87500
# [8/100] training 82.1% loss=0.25836, acc=0.85938
# [8/100] training 82.2% loss=0.44875, acc=0.82812
# [8/100] training 82.5% loss=0.30557, acc=0.89062
# [8/100] training 82.7% loss=0.34181, acc=0.87500
# [8/100] training 82.8% loss=0.34004, acc=0.87500
# [8/100] training 83.0% loss=0.35561, acc=0.84375
# [8/100] training 83.1% loss=0.33521, acc=0.89062
# [8/100] training 83.3% loss=0.33918, acc=0.79688
# [8/100] training 83.5% loss=0.33982, acc=0.84375
# [8/100] training 83.7% loss=0.42634, acc=0.85938
# [8/100] training 83.9% loss=0.37806, acc=0.82812
# [8/100] training 84.0% loss=0.27533, acc=0.85938
# [8/100] training 84.2% loss=0.23281, acc=0.93750
# [8/100] training 84.3% loss=0.39650, acc=0.84375
# [8/100] training 84.5% loss=0.33072, acc=0.84375
# [8/100] training 84.7% loss=0.46612, acc=0.78125
# [8/100] training 84.9% loss=0.36124, acc=0.81250
# [8/100] training 85.0% loss=0.34503, acc=0.82812
# [8/100] training 85.2% loss=0.39168, acc=0.84375
# [8/100] training 85.4% loss=0.35644, acc=0.90625
# [8/100] training 85.5% loss=0.26861, acc=0.84375
# [8/100] training 85.8% loss=0.34241, acc=0.79688
# [8/100] training 85.9% loss=0.34312, acc=0.85938
# [8/100] training 86.1% loss=0.35887, acc=0.82812
# [8/100] training 86.2% loss=0.28945, acc=0.89062
# [8/100] training 86.4% loss=0.39743, acc=0.81250
# [8/100] training 86.6% loss=0.46716, acc=0.79688
# [8/100] training 86.7% loss=0.36281, acc=0.87500
# [8/100] training 87.0% loss=0.31297, acc=0.84375
# [8/100] training 87.1% loss=0.31728, acc=0.82812
# [8/100] training 87.3% loss=0.35842, acc=0.85938
# [8/100] training 87.4% loss=0.32609, acc=0.89062
# [8/100] training 87.6% loss=0.33519, acc=0.85938
# [8/100] training 87.7% loss=0.37528, acc=0.84375
# [8/100] training 87.9% loss=0.34762, acc=0.85938
# [8/100] training 88.2% loss=0.21300, acc=0.90625
# [8/100] training 88.3% loss=0.39473, acc=0.82812
# [8/100] training 88.5% loss=0.34346, acc=0.85938
# [8/100] training 88.6% loss=0.17891, acc=0.96875
# [8/100] training 88.8% loss=0.37279, acc=0.85938
# [8/100] training 88.9% loss=0.35740, acc=0.81250
# [8/100] training 89.2% loss=0.30066, acc=0.90625
# [8/100] training 89.4% loss=0.33358, acc=0.84375
# [8/100] training 89.5% loss=0.32600, acc=0.85938
# [8/100] training 89.7% loss=0.43687, acc=0.79688
# [8/100] training 89.8% loss=0.32463, acc=0.85938
# [8/100] training 90.0% loss=0.27037, acc=0.87500
# [8/100] training 90.1% loss=0.29563, acc=0.87500
# [8/100] training 90.4% loss=0.37042, acc=0.89062
# [8/100] training 90.5% loss=0.21404, acc=0.90625
# [8/100] training 90.7% loss=0.39536, acc=0.81250
# [8/100] training 90.9% loss=0.31633, acc=0.89062
# [8/100] training 91.0% loss=0.25585, acc=0.90625
# [8/100] training 91.2% loss=0.46702, acc=0.75000
# [8/100] training 91.3% loss=0.54416, acc=0.81250
# [8/100] training 91.6% loss=0.43766, acc=0.76562
# [8/100] training 91.7% loss=0.37859, acc=0.82812
# [8/100] training 91.9% loss=0.33556, acc=0.81250
# [8/100] training 92.1% loss=0.28301, acc=0.87500
# [8/100] training 92.2% loss=0.26377, acc=0.92188
# [8/100] training 92.4% loss=0.35620, acc=0.79688
# [8/100] training 92.6% loss=0.42827, acc=0.82812
# [8/100] training 92.8% loss=0.52597, acc=0.75000
# [8/100] training 92.9% loss=0.54717, acc=0.71875
# [8/100] training 93.1% loss=0.43858, acc=0.81250
# [8/100] training 93.2% loss=0.32742, acc=0.82812
# [8/100] training 93.4% loss=0.25397, acc=0.93750
# [8/100] training 93.7% loss=0.33161, acc=0.84375
# [8/100] training 93.8% loss=0.35400, acc=0.81250
# [8/100] training 94.0% loss=0.31328, acc=0.85938
# [8/100] training 94.1% loss=0.50205, acc=0.76562
# [8/100] training 94.3% loss=0.25760, acc=0.89062
# [8/100] training 94.4% loss=0.41698, acc=0.73438
# [8/100] training 94.6% loss=0.27073, acc=0.87500
# [8/100] training 94.9% loss=0.38728, acc=0.81250
# [8/100] training 95.0% loss=0.41630, acc=0.82812
# [8/100] training 95.2% loss=0.45332, acc=0.78125
# [8/100] training 95.3% loss=0.31477, acc=0.84375
# [8/100] training 95.5% loss=0.24983, acc=0.93750
# [8/100] training 95.6% loss=0.43547, acc=0.84375
# [8/100] training 95.8% loss=0.41610, acc=0.79688
# [8/100] training 96.0% loss=0.25669, acc=0.89062
# [8/100] training 96.2% loss=0.24169, acc=0.92188
# [8/100] training 96.4% loss=0.27489, acc=0.90625
# [8/100] training 96.5% loss=0.39130, acc=0.85938
# [8/100] training 96.7% loss=0.31235, acc=0.85938
# [8/100] training 96.8% loss=0.32605, acc=0.85938
# [8/100] training 97.1% loss=0.34087, acc=0.81250
# [8/100] training 97.2% loss=0.37636, acc=0.82812
# [8/100] training 97.4% loss=0.33919, acc=0.87500
# [8/100] training 97.6% loss=0.35381, acc=0.82812
# [8/100] training 97.7% loss=0.24320, acc=0.93750
# [8/100] training 97.9% loss=0.19977, acc=0.90625
# [8/100] training 98.0% loss=0.34493, acc=0.79688
# [8/100] training 98.3% loss=0.38530, acc=0.85938
# [8/100] training 98.4% loss=0.22020, acc=0.93750
# [8/100] training 98.6% loss=0.45438, acc=0.81250
# [8/100] training 98.7% loss=0.42558, acc=0.76562
# [8/100] training 98.9% loss=0.26145, acc=0.85938
# [8/100] training 99.1% loss=0.28315, acc=0.89062
# [8/100] training 99.2% loss=0.27126, acc=0.84375
# [8/100] training 99.5% loss=0.36132, acc=0.81250
# [8/100] training 99.6% loss=0.36598, acc=0.84375
# [8/100] training 99.8% loss=0.27329, acc=0.85938
# [8/100] training 99.9% loss=0.25440, acc=0.90625
# [8/100] testing 0.9% loss=0.20644, acc=0.87500
# [8/100] testing 1.8% loss=0.61296, acc=0.73438
# [8/100] testing 2.2% loss=0.26869, acc=0.92188
# [8/100] testing 3.1% loss=0.31886, acc=0.87500
# [8/100] testing 3.5% loss=0.34422, acc=0.89062
# [8/100] testing 4.4% loss=0.33640, acc=0.85938
# [8/100] testing 4.8% loss=0.53024, acc=0.75000
# [8/100] testing 5.7% loss=0.42457, acc=0.82812
# [8/100] testing 6.6% loss=0.43777, acc=0.76562
# [8/100] testing 7.0% loss=0.33926, acc=0.82812
# [8/100] testing 7.9% loss=0.47978, acc=0.81250
# [8/100] testing 8.3% loss=0.27479, acc=0.90625
# [8/100] testing 9.2% loss=0.44480, acc=0.85938
# [8/100] testing 9.7% loss=0.34769, acc=0.84375
# [8/100] testing 10.5% loss=0.56102, acc=0.76562
# [8/100] testing 11.0% loss=0.38003, acc=0.84375
# [8/100] testing 11.8% loss=0.47539, acc=0.81250
# [8/100] testing 12.7% loss=0.64941, acc=0.75000
# [8/100] testing 13.2% loss=0.34408, acc=0.82812
# [8/100] testing 14.0% loss=0.57082, acc=0.76562
# [8/100] testing 14.5% loss=0.44337, acc=0.81250
# [8/100] testing 15.4% loss=0.41441, acc=0.81250
# [8/100] testing 15.8% loss=0.26121, acc=0.89062
# [8/100] testing 16.7% loss=0.43337, acc=0.78125
# [8/100] testing 17.5% loss=0.38963, acc=0.78125
# [8/100] testing 18.0% loss=0.35493, acc=0.79688
# [8/100] testing 18.9% loss=0.25453, acc=0.92188
# [8/100] testing 19.3% loss=0.31718, acc=0.87500
# [8/100] testing 20.2% loss=0.56331, acc=0.76562
# [8/100] testing 20.6% loss=0.52351, acc=0.76562
# [8/100] testing 21.5% loss=0.45787, acc=0.82812
# [8/100] testing 21.9% loss=0.56318, acc=0.71875
# [8/100] testing 22.8% loss=0.50026, acc=0.79688
# [8/100] testing 23.7% loss=0.45350, acc=0.82812
# [8/100] testing 24.1% loss=0.43026, acc=0.84375
# [8/100] testing 25.0% loss=0.49938, acc=0.82812
# [8/100] testing 25.4% loss=0.26760, acc=0.85938
# [8/100] testing 26.3% loss=0.52679, acc=0.71875
# [8/100] testing 26.8% loss=0.35252, acc=0.85938
# [8/100] testing 27.6% loss=0.44990, acc=0.82812
# [8/100] testing 28.5% loss=0.42825, acc=0.82812
# [8/100] testing 29.0% loss=0.39256, acc=0.79688
# [8/100] testing 29.8% loss=0.40107, acc=0.81250
# [8/100] testing 30.3% loss=0.48557, acc=0.81250
# [8/100] testing 31.1% loss=0.63243, acc=0.78125
# [8/100] testing 31.6% loss=0.36897, acc=0.81250
# [8/100] testing 32.5% loss=0.33514, acc=0.82812
# [8/100] testing 32.9% loss=0.43994, acc=0.79688
# [8/100] testing 33.8% loss=0.44325, acc=0.78125
# [8/100] testing 34.7% loss=0.49109, acc=0.81250
# [8/100] testing 35.1% loss=0.38591, acc=0.85938
# [8/100] testing 36.0% loss=0.52496, acc=0.79688
# [8/100] testing 36.4% loss=0.41917, acc=0.81250
# [8/100] testing 37.3% loss=0.39344, acc=0.84375
# [8/100] testing 37.7% loss=0.53507, acc=0.78125
# [8/100] testing 38.6% loss=0.33315, acc=0.89062
# [8/100] testing 39.5% loss=0.55804, acc=0.81250
# [8/100] testing 39.9% loss=0.33624, acc=0.82812
# [8/100] testing 40.8% loss=0.32346, acc=0.89062
# [8/100] testing 41.2% loss=0.32796, acc=0.90625
# [8/100] testing 42.1% loss=0.47662, acc=0.84375
# [8/100] testing 42.5% loss=0.35251, acc=0.84375
# [8/100] testing 43.4% loss=0.54160, acc=0.82812
# [8/100] testing 43.9% loss=0.35995, acc=0.87500
# [8/100] testing 44.7% loss=0.56302, acc=0.79688
# [8/100] testing 45.6% loss=0.46259, acc=0.81250
# [8/100] testing 46.1% loss=0.31310, acc=0.89062
# [8/100] testing 46.9% loss=0.25804, acc=0.92188
# [8/100] testing 47.4% loss=0.28120, acc=0.85938
# [8/100] testing 48.3% loss=0.49342, acc=0.81250
# [8/100] testing 48.7% loss=0.54494, acc=0.82812
# [8/100] testing 49.6% loss=0.48489, acc=0.76562
# [8/100] testing 50.4% loss=0.32370, acc=0.87500
# [8/100] testing 50.9% loss=0.48622, acc=0.75000
# [8/100] testing 51.8% loss=0.49845, acc=0.81250
# [8/100] testing 52.2% loss=0.39851, acc=0.89062
# [8/100] testing 53.1% loss=0.30561, acc=0.85938
# [8/100] testing 53.5% loss=0.31283, acc=0.92188
# [8/100] testing 54.4% loss=0.60234, acc=0.75000
# [8/100] testing 54.8% loss=0.47754, acc=0.75000
# [8/100] testing 55.7% loss=0.27653, acc=0.90625
# [8/100] testing 56.6% loss=0.49375, acc=0.85938
# [8/100] testing 57.0% loss=0.59996, acc=0.79688
# [8/100] testing 57.9% loss=0.47065, acc=0.82812
# [8/100] testing 58.3% loss=0.54878, acc=0.68750
# [8/100] testing 59.2% loss=0.43607, acc=0.79688
# [8/100] testing 59.7% loss=0.35326, acc=0.84375
# [8/100] testing 60.5% loss=0.50618, acc=0.75000
# [8/100] testing 61.4% loss=0.21923, acc=0.92188
# [8/100] testing 61.9% loss=0.44923, acc=0.85938
# [8/100] testing 62.7% loss=0.39354, acc=0.85938
# [8/100] testing 63.2% loss=0.49833, acc=0.78125
# [8/100] testing 64.0% loss=0.40542, acc=0.81250
# [8/100] testing 64.5% loss=0.44266, acc=0.82812
# [8/100] testing 65.4% loss=0.36311, acc=0.81250
# [8/100] testing 65.8% loss=0.57715, acc=0.76562
# [8/100] testing 66.7% loss=0.31999, acc=0.85938
# [8/100] testing 67.6% loss=0.44795, acc=0.79688
# [8/100] testing 68.0% loss=0.41845, acc=0.79688
# [8/100] testing 68.9% loss=0.43285, acc=0.84375
# [8/100] testing 69.3% loss=0.48632, acc=0.76562
# [8/100] testing 70.2% loss=0.44826, acc=0.81250
# [8/100] testing 70.6% loss=0.45268, acc=0.82812
# [8/100] testing 71.5% loss=0.49906, acc=0.81250
# [8/100] testing 72.4% loss=0.38653, acc=0.84375
# [8/100] testing 72.8% loss=0.40386, acc=0.78125
# [8/100] testing 73.7% loss=0.28950, acc=0.89062
# [8/100] testing 74.1% loss=0.58330, acc=0.79688
# [8/100] testing 75.0% loss=0.37494, acc=0.85938
# [8/100] testing 75.4% loss=0.32333, acc=0.82812
# [8/100] testing 76.3% loss=0.18751, acc=0.92188
# [8/100] testing 76.8% loss=0.39356, acc=0.84375
# [8/100] testing 77.6% loss=0.33680, acc=0.85938
# [8/100] testing 78.5% loss=0.63154, acc=0.71875
# [8/100] testing 79.0% loss=0.45066, acc=0.81250
# [8/100] testing 79.8% loss=0.43618, acc=0.71875
# [8/100] testing 80.3% loss=0.40329, acc=0.82812
# [8/100] testing 81.2% loss=0.52789, acc=0.82812
# [8/100] testing 81.6% loss=0.51428, acc=0.81250
# [8/100] testing 82.5% loss=0.29319, acc=0.85938
# [8/100] testing 83.3% loss=0.22876, acc=0.87500
# [8/100] testing 83.8% loss=0.31971, acc=0.85938
# [8/100] testing 84.7% loss=0.39159, acc=0.84375
# [8/100] testing 85.1% loss=0.47851, acc=0.78125
# [8/100] testing 86.0% loss=0.44160, acc=0.79688
# [8/100] testing 86.4% loss=0.52995, acc=0.75000
# [8/100] testing 87.3% loss=0.55636, acc=0.78125
# [8/100] testing 87.7% loss=0.32175, acc=0.85938
# [8/100] testing 88.6% loss=0.33060, acc=0.79688
# [8/100] testing 89.5% loss=0.65712, acc=0.64062
# [8/100] testing 89.9% loss=0.40401, acc=0.79688
# [8/100] testing 90.8% loss=0.30864, acc=0.81250
# [8/100] testing 91.2% loss=0.30351, acc=0.84375
# [8/100] testing 92.1% loss=0.36365, acc=0.84375
# [8/100] testing 92.6% loss=0.41586, acc=0.79688
# [8/100] testing 93.4% loss=0.49411, acc=0.75000
# [8/100] testing 94.3% loss=0.29402, acc=0.90625
# [8/100] testing 94.7% loss=0.29854, acc=0.87500
# [8/100] testing 95.6% loss=0.40977, acc=0.79688
# [8/100] testing 96.1% loss=0.39753, acc=0.76562
# [8/100] testing 96.9% loss=0.39493, acc=0.76562
# [8/100] testing 97.4% loss=0.26516, acc=0.89062
# [8/100] testing 98.3% loss=0.42211, acc=0.82812
# [8/100] testing 98.7% loss=0.44328, acc=0.82812
# [8/100] testing 99.6% loss=0.40602, acc=0.84375
# [9/100] training 0.2% loss=0.45817, acc=0.79688
# [9/100] training 0.4% loss=0.46061, acc=0.78125
# [9/100] training 0.5% loss=0.32612, acc=0.85938
# [9/100] training 0.8% loss=0.44664, acc=0.85938
# [9/100] training 0.9% loss=0.32137, acc=0.84375
# [9/100] training 1.1% loss=0.30023, acc=0.93750
# [9/100] training 1.2% loss=0.44560, acc=0.78125
# [9/100] training 1.4% loss=0.47113, acc=0.76562
# [9/100] training 1.6% loss=0.24379, acc=0.89062
# [9/100] training 1.8% loss=0.34216, acc=0.79688
# [9/100] training 2.0% loss=0.34872, acc=0.82812
# [9/100] training 2.1% loss=0.33776, acc=0.85938
# [9/100] training 2.3% loss=0.31913, acc=0.84375
# [9/100] training 2.4% loss=0.43960, acc=0.79688
# [9/100] training 2.6% loss=0.32878, acc=0.84375
# [9/100] training 2.7% loss=0.37032, acc=0.82812
# [9/100] training 3.0% loss=0.41310, acc=0.78125
# [9/100] training 3.2% loss=0.27190, acc=0.89062
# [9/100] training 3.3% loss=0.37107, acc=0.87500
# [9/100] training 3.5% loss=0.39129, acc=0.84375
# [9/100] training 3.6% loss=0.61141, acc=0.76562
# [9/100] training 3.8% loss=0.22086, acc=0.92188
# [9/100] training 3.9% loss=0.38700, acc=0.82812
# [9/100] training 4.2% loss=0.30012, acc=0.92188
# [9/100] training 4.4% loss=0.28118, acc=0.87500
# [9/100] training 4.5% loss=0.35178, acc=0.84375
# [9/100] training 4.7% loss=0.42046, acc=0.82812
# [9/100] training 4.8% loss=0.40719, acc=0.81250
# [9/100] training 5.0% loss=0.30714, acc=0.87500
# [9/100] training 5.2% loss=0.41385, acc=0.82812
# [9/100] training 5.4% loss=0.27901, acc=0.90625
# [9/100] training 5.5% loss=0.39398, acc=0.84375
# [9/100] training 5.7% loss=0.30344, acc=0.87500
# [9/100] training 5.9% loss=0.44767, acc=0.79688
# [9/100] training 6.0% loss=0.32260, acc=0.84375
# [9/100] training 6.3% loss=0.33911, acc=0.81250
# [9/100] training 6.4% loss=0.31057, acc=0.87500
# [9/100] training 6.6% loss=0.35807, acc=0.81250
# [9/100] training 6.7% loss=0.38027, acc=0.78125
# [9/100] training 6.9% loss=0.29738, acc=0.84375
# [9/100] training 7.1% loss=0.37926, acc=0.81250
# [9/100] training 7.2% loss=0.34549, acc=0.85938
# [9/100] training 7.5% loss=0.29665, acc=0.82812
# [9/100] training 7.6% loss=0.30910, acc=0.89062
# [9/100] training 7.8% loss=0.32294, acc=0.85938
# [9/100] training 7.9% loss=0.45003, acc=0.85938
# [9/100] training 8.1% loss=0.19790, acc=0.92188
# [9/100] training 8.2% loss=0.29776, acc=0.87500
# [9/100] training 8.4% loss=0.43833, acc=0.85938
# [9/100] training 8.7% loss=0.30814, acc=0.87500
# [9/100] training 8.8% loss=0.31227, acc=0.82812
# [9/100] training 9.0% loss=0.32030, acc=0.87500
# [9/100] training 9.1% loss=0.39709, acc=0.84375
# [9/100] training 9.3% loss=0.50405, acc=0.79688
# [9/100] training 9.4% loss=0.29160, acc=0.87500
# [9/100] training 9.7% loss=0.40897, acc=0.79688
# [9/100] training 9.9% loss=0.36991, acc=0.82812
# [9/100] training 10.0% loss=0.34957, acc=0.82812
# [9/100] training 10.2% loss=0.34884, acc=0.87500
# [9/100] training 10.3% loss=0.31159, acc=0.85938
# [9/100] training 10.5% loss=0.41834, acc=0.85938
# [9/100] training 10.6% loss=0.33019, acc=0.84375
# [9/100] training 10.9% loss=0.30388, acc=0.85938
# [9/100] training 11.0% loss=0.30319, acc=0.89062
# [9/100] training 11.2% loss=0.25814, acc=0.92188
# [9/100] training 11.4% loss=0.48125, acc=0.79688
# [9/100] training 11.5% loss=0.47033, acc=0.78125
# [9/100] training 11.7% loss=0.35241, acc=0.84375
# [9/100] training 11.8% loss=0.32693, acc=0.84375
# [9/100] training 12.1% loss=0.39170, acc=0.81250
# [9/100] training 12.2% loss=0.31203, acc=0.84375
# [9/100] training 12.4% loss=0.35839, acc=0.81250
# [9/100] training 12.6% loss=0.32985, acc=0.87500
# [9/100] training 12.7% loss=0.39395, acc=0.79688
# [9/100] training 12.9% loss=0.39060, acc=0.89062
# [9/100] training 13.0% loss=0.35497, acc=0.85938
# [9/100] training 13.3% loss=0.37117, acc=0.85938
# [9/100] training 13.4% loss=0.36043, acc=0.78125
# [9/100] training 13.6% loss=0.33271, acc=0.87500
# [9/100] training 13.7% loss=0.54821, acc=0.82812
# [9/100] training 13.9% loss=0.50073, acc=0.81250
# [9/100] training 14.1% loss=0.36686, acc=0.82812
# [9/100] training 14.3% loss=0.40307, acc=0.82812
# [9/100] training 14.5% loss=0.33686, acc=0.87500
# [9/100] training 14.6% loss=0.36864, acc=0.84375
# [9/100] training 14.8% loss=0.42657, acc=0.85938
# [9/100] training 14.9% loss=0.34831, acc=0.85938
# [9/100] training 15.1% loss=0.39630, acc=0.84375
# [9/100] training 15.4% loss=0.33908, acc=0.84375
# [9/100] training 15.5% loss=0.42649, acc=0.84375
# [9/100] training 15.7% loss=0.45585, acc=0.82812
# [9/100] training 15.8% loss=0.29882, acc=0.87500
# [9/100] training 16.0% loss=0.45734, acc=0.79688
# [9/100] training 16.1% loss=0.56908, acc=0.68750
# [9/100] training 16.3% loss=0.36586, acc=0.84375
# [9/100] training 16.4% loss=0.32423, acc=0.89062
# [9/100] training 16.7% loss=0.40997, acc=0.81250
# [9/100] training 16.9% loss=0.47082, acc=0.81250
# [9/100] training 17.0% loss=0.36382, acc=0.82812
# [9/100] training 17.2% loss=0.25470, acc=0.90625
# [9/100] training 17.3% loss=0.42364, acc=0.78125
# [9/100] training 17.5% loss=0.32423, acc=0.84375
# [9/100] training 17.7% loss=0.36582, acc=0.84375
# [9/100] training 17.9% loss=0.43414, acc=0.84375
# [9/100] training 18.1% loss=0.39766, acc=0.82812
# [9/100] training 18.2% loss=0.35776, acc=0.84375
# [9/100] training 18.4% loss=0.36994, acc=0.84375
# [9/100] training 18.5% loss=0.30075, acc=0.90625
# [9/100] training 18.8% loss=0.31035, acc=0.87500
# [9/100] training 18.9% loss=0.26402, acc=0.87500
# [9/100] training 19.1% loss=0.43716, acc=0.79688
# [9/100] training 19.2% loss=0.27450, acc=0.90625
# [9/100] training 19.4% loss=0.22449, acc=0.92188
# [9/100] training 19.6% loss=0.44992, acc=0.81250
# [9/100] training 19.7% loss=0.33683, acc=0.81250
# [9/100] training 20.0% loss=0.31903, acc=0.87500
# [9/100] training 20.1% loss=0.30684, acc=0.87500
# [9/100] training 20.3% loss=0.35702, acc=0.89062
# [9/100] training 20.4% loss=0.49827, acc=0.73438
# [9/100] training 20.6% loss=0.35876, acc=0.85938
# [9/100] training 20.8% loss=0.29219, acc=0.89062
# [9/100] training 20.9% loss=0.31840, acc=0.90625
# [9/100] training 21.2% loss=0.28823, acc=0.89062
# [9/100] training 21.3% loss=0.36892, acc=0.81250
# [9/100] training 21.5% loss=0.32662, acc=0.85938
# [9/100] training 21.6% loss=0.32854, acc=0.85938
# [9/100] training 21.8% loss=0.35178, acc=0.82812
# [9/100] training 21.9% loss=0.38574, acc=0.84375
# [9/100] training 22.2% loss=0.32030, acc=0.85938
# [9/100] training 22.4% loss=0.42261, acc=0.82812
# [9/100] training 22.5% loss=0.32161, acc=0.89062
# [9/100] training 22.7% loss=0.31022, acc=0.85938
# [9/100] training 22.8% loss=0.40228, acc=0.82812
# [9/100] training 23.0% loss=0.20807, acc=0.93750
# [9/100] training 23.1% loss=0.40299, acc=0.85938
# [9/100] training 23.4% loss=0.42017, acc=0.84375
# [9/100] training 23.6% loss=0.43976, acc=0.87500
# [9/100] training 23.7% loss=0.33484, acc=0.82812
# [9/100] training 23.9% loss=0.31565, acc=0.84375
# [9/100] training 24.0% loss=0.36029, acc=0.84375
# [9/100] training 24.2% loss=0.37241, acc=0.84375
# [9/100] training 24.3% loss=0.39572, acc=0.81250
# [9/100] training 24.6% loss=0.28744, acc=0.87500
# [9/100] training 24.7% loss=0.45983, acc=0.76562
# [9/100] training 24.9% loss=0.29105, acc=0.90625
# [9/100] training 25.1% loss=0.43162, acc=0.81250
# [9/100] training 25.2% loss=0.22483, acc=0.93750
# [9/100] training 25.4% loss=0.34131, acc=0.87500
# [9/100] training 25.6% loss=0.38085, acc=0.85938
# [9/100] training 25.8% loss=0.39245, acc=0.84375
# [9/100] training 25.9% loss=0.31084, acc=0.89062
# [9/100] training 26.1% loss=0.42999, acc=0.81250
# [9/100] training 26.3% loss=0.37933, acc=0.89062
# [9/100] training 26.4% loss=0.25238, acc=0.90625
# [9/100] training 26.6% loss=0.28249, acc=0.90625
# [9/100] training 26.8% loss=0.39535, acc=0.81250
# [9/100] training 27.0% loss=0.34529, acc=0.85938
# [9/100] training 27.1% loss=0.27441, acc=0.85938
# [9/100] training 27.3% loss=0.32239, acc=0.85938
# [9/100] training 27.4% loss=0.27499, acc=0.90625
# [9/100] training 27.6% loss=0.40691, acc=0.82812
# [9/100] training 27.9% loss=0.38871, acc=0.79688
# [9/100] training 28.0% loss=0.42731, acc=0.84375
# [9/100] training 28.2% loss=0.34765, acc=0.84375
# [9/100] training 28.3% loss=0.22268, acc=0.89062
# [9/100] training 28.5% loss=0.40537, acc=0.82812
# [9/100] training 28.6% loss=0.35492, acc=0.85938
# [9/100] training 28.8% loss=0.23444, acc=0.90625
# [9/100] training 29.1% loss=0.32265, acc=0.84375
# [9/100] training 29.2% loss=0.25164, acc=0.90625
# [9/100] training 29.4% loss=0.37448, acc=0.81250
# [9/100] training 29.5% loss=0.32046, acc=0.87500
# [9/100] training 29.7% loss=0.43585, acc=0.82812
# [9/100] training 29.8% loss=0.32486, acc=0.84375
# [9/100] training 30.0% loss=0.50871, acc=0.65625
# [9/100] training 30.2% loss=0.30735, acc=0.85938
# [9/100] training 30.4% loss=0.32183, acc=0.87500
# [9/100] training 30.6% loss=0.46667, acc=0.81250
# [9/100] training 30.7% loss=0.27877, acc=0.89062
# [9/100] training 30.9% loss=0.48997, acc=0.79688
# [9/100] training 31.0% loss=0.32489, acc=0.81250
# [9/100] training 31.3% loss=0.33533, acc=0.84375
# [9/100] training 31.4% loss=0.39849, acc=0.85938
# [9/100] training 31.6% loss=0.47118, acc=0.82812
# [9/100] training 31.8% loss=0.29122, acc=0.89062
# [9/100] training 31.9% loss=0.32713, acc=0.85938
# [9/100] training 32.1% loss=0.35729, acc=0.85938
# [9/100] training 32.2% loss=0.32900, acc=0.85938
# [9/100] training 32.5% loss=0.34973, acc=0.92188
# [9/100] training 32.6% loss=0.30915, acc=0.87500
# [9/100] training 32.8% loss=0.25561, acc=0.92188
# [9/100] training 32.9% loss=0.35334, acc=0.89062
# [9/100] training 33.1% loss=0.44502, acc=0.79688
# [9/100] training 33.3% loss=0.36262, acc=0.84375
# [9/100] training 33.4% loss=0.39894, acc=0.85938
# [9/100] training 33.7% loss=0.39289, acc=0.85938
# [9/100] training 33.8% loss=0.46682, acc=0.78125
# [9/100] training 34.0% loss=0.39023, acc=0.87500
# [9/100] training 34.1% loss=0.30209, acc=0.85938
# [9/100] training 34.3% loss=0.30357, acc=0.84375
# [9/100] training 34.5% loss=0.40035, acc=0.82812
# [9/100] training 34.7% loss=0.22649, acc=0.87500
# [9/100] training 34.9% loss=0.31260, acc=0.85938
# [9/100] training 35.0% loss=0.31457, acc=0.87500
# [9/100] training 35.2% loss=0.38233, acc=0.82812
# [9/100] training 35.3% loss=0.36850, acc=0.81250
# [9/100] training 35.5% loss=0.40494, acc=0.81250
# [9/100] training 35.6% loss=0.37152, acc=0.85938
# [9/100] training 35.9% loss=0.37351, acc=0.79688
# [9/100] training 36.1% loss=0.33453, acc=0.81250
# [9/100] training 36.2% loss=0.39114, acc=0.84375
# [9/100] training 36.4% loss=0.35717, acc=0.85938
# [9/100] training 36.5% loss=0.27534, acc=0.87500
# [9/100] training 36.7% loss=0.28991, acc=0.87500
# [9/100] training 36.8% loss=0.24282, acc=0.87500
# [9/100] training 37.1% loss=0.46170, acc=0.85938
# [9/100] training 37.3% loss=0.56440, acc=0.79688
# [9/100] training 37.4% loss=0.38381, acc=0.79688
# [9/100] training 37.6% loss=0.34783, acc=0.85938
# [9/100] training 37.7% loss=0.26876, acc=0.89062
# [9/100] training 37.9% loss=0.27293, acc=0.90625
# [9/100] training 38.1% loss=0.37433, acc=0.78125
# [9/100] training 38.3% loss=0.38466, acc=0.82812
# [9/100] training 38.4% loss=0.35126, acc=0.84375
# [9/100] training 38.6% loss=0.29603, acc=0.85938
# [9/100] training 38.8% loss=0.35970, acc=0.85938
# [9/100] training 38.9% loss=0.43846, acc=0.81250
# [9/100] training 39.1% loss=0.33233, acc=0.82812
# [9/100] training 39.3% loss=0.33283, acc=0.82812
# [9/100] training 39.5% loss=0.42695, acc=0.82812
# [9/100] training 39.6% loss=0.28964, acc=0.87500
# [9/100] training 39.8% loss=0.28776, acc=0.85938
# [9/100] training 40.0% loss=0.37785, acc=0.79688
# [9/100] training 40.1% loss=0.40744, acc=0.85938
# [9/100] training 40.4% loss=0.32283, acc=0.85938
# [9/100] training 40.5% loss=0.30267, acc=0.85938
# [9/100] training 40.7% loss=0.31673, acc=0.82812
# [9/100] training 40.8% loss=0.25581, acc=0.89062
# [9/100] training 41.0% loss=0.22374, acc=0.90625
# [9/100] training 41.1% loss=0.46823, acc=0.76562
# [9/100] training 41.3% loss=0.46500, acc=0.78125
# [9/100] training 41.6% loss=0.37268, acc=0.87500
# [9/100] training 41.7% loss=0.47222, acc=0.75000
# [9/100] training 41.9% loss=0.35327, acc=0.82812
# [9/100] training 42.0% loss=0.33773, acc=0.81250
# [9/100] training 42.2% loss=0.36033, acc=0.89062
# [9/100] training 42.3% loss=0.35124, acc=0.89062
# [9/100] training 42.5% loss=0.22388, acc=0.92188
# [9/100] training 42.8% loss=0.24732, acc=0.92188
# [9/100] training 42.9% loss=0.24060, acc=0.89062
# [9/100] training 43.1% loss=0.30796, acc=0.89062
# [9/100] training 43.2% loss=0.29513, acc=0.87500
# [9/100] training 43.4% loss=0.35531, acc=0.84375
# [9/100] training 43.5% loss=0.28601, acc=0.87500
# [9/100] training 43.8% loss=0.38954, acc=0.81250
# [9/100] training 43.9% loss=0.32608, acc=0.92188
# [9/100] training 44.1% loss=0.24292, acc=0.90625
# [9/100] training 44.3% loss=0.28770, acc=0.85938
# [9/100] training 44.4% loss=0.34106, acc=0.82812
# [9/100] training 44.6% loss=0.39508, acc=0.81250
# [9/100] training 44.7% loss=0.41135, acc=0.79688
# [9/100] training 45.0% loss=0.28970, acc=0.90625
# [9/100] training 45.1% loss=0.60684, acc=0.82812
# [9/100] training 45.3% loss=0.37174, acc=0.82812
# [9/100] training 45.5% loss=0.22939, acc=0.90625
# [9/100] training 45.6% loss=0.32915, acc=0.84375
# [9/100] training 45.8% loss=0.29438, acc=0.85938
# [9/100] training 45.9% loss=0.29154, acc=0.89062
# [9/100] training 46.2% loss=0.21425, acc=0.92188
# [9/100] training 46.3% loss=0.20782, acc=0.90625
# [9/100] training 46.5% loss=0.53558, acc=0.79688
# [9/100] training 46.6% loss=0.30281, acc=0.87500
# [9/100] training 46.8% loss=0.33107, acc=0.89062
# [9/100] training 47.0% loss=0.39483, acc=0.78125
# [9/100] training 47.2% loss=0.33221, acc=0.84375
# [9/100] training 47.4% loss=0.39224, acc=0.76562
# [9/100] training 47.5% loss=0.36533, acc=0.84375
# [9/100] training 47.7% loss=0.28075, acc=0.84375
# [9/100] training 47.8% loss=0.38812, acc=0.84375
# [9/100] training 48.0% loss=0.46499, acc=0.85938
# [9/100] training 48.3% loss=0.17676, acc=0.92188
# [9/100] training 48.4% loss=0.22592, acc=0.85938
# [9/100] training 48.6% loss=0.31579, acc=0.85938
# [9/100] training 48.7% loss=0.38561, acc=0.82812
# [9/100] training 48.9% loss=0.44661, acc=0.82812
# [9/100] training 49.0% loss=0.36910, acc=0.85938
# [9/100] training 49.2% loss=0.27084, acc=0.84375
# [9/100] training 49.3% loss=0.26193, acc=0.87500
# [9/100] training 49.6% loss=0.39136, acc=0.78125
# [9/100] training 49.8% loss=0.30887, acc=0.87500
# [9/100] training 49.9% loss=0.38108, acc=0.82812
# [9/100] training 50.1% loss=0.31749, acc=0.85938
# [9/100] training 50.2% loss=0.30829, acc=0.84375
# [9/100] training 50.4% loss=0.47437, acc=0.81250
# [9/100] training 50.6% loss=0.42993, acc=0.79688
# [9/100] training 50.8% loss=0.50550, acc=0.79688
# [9/100] training 51.0% loss=0.42395, acc=0.78125
# [9/100] training 51.1% loss=0.33643, acc=0.82812
# [9/100] training 51.3% loss=0.31426, acc=0.85938
# [9/100] training 51.4% loss=0.34537, acc=0.87500
# [9/100] training 51.7% loss=0.52238, acc=0.76562
# [9/100] training 51.8% loss=0.35508, acc=0.79688
# [9/100] training 52.0% loss=0.37985, acc=0.84375
# [9/100] training 52.1% loss=0.47178, acc=0.79688
# [9/100] training 52.3% loss=0.40999, acc=0.84375
# [9/100] training 52.5% loss=0.24037, acc=0.89062
# [9/100] training 52.6% loss=0.39084, acc=0.84375
# [9/100] training 52.9% loss=0.49149, acc=0.75000
# [9/100] training 53.0% loss=0.29612, acc=0.82812
# [9/100] training 53.2% loss=0.36522, acc=0.85938
# [9/100] training 53.3% loss=0.19401, acc=0.93750
# [9/100] training 53.5% loss=0.43671, acc=0.78125
# [9/100] training 53.7% loss=0.35215, acc=0.87500
# [9/100] training 53.8% loss=0.38173, acc=0.81250
# [9/100] training 54.1% loss=0.40408, acc=0.84375
# [9/100] training 54.2% loss=0.36468, acc=0.84375
# [9/100] training 54.4% loss=0.32620, acc=0.87500
# [9/100] training 54.5% loss=0.38665, acc=0.82812
# [9/100] training 54.7% loss=0.43984, acc=0.79688
# [9/100] training 54.8% loss=0.23000, acc=0.92188
# [9/100] training 55.1% loss=0.23565, acc=0.92188
# [9/100] training 55.3% loss=0.20713, acc=0.93750
# [9/100] training 55.4% loss=0.30771, acc=0.87500
# [9/100] training 55.6% loss=0.35544, acc=0.81250
# [9/100] training 55.7% loss=0.29687, acc=0.90625
# [9/100] training 55.9% loss=0.33212, acc=0.87500
# [9/100] training 56.0% loss=0.37980, acc=0.82812
# [9/100] training 56.3% loss=0.56571, acc=0.75000
# [9/100] training 56.5% loss=0.37309, acc=0.84375
# [9/100] training 56.6% loss=0.34317, acc=0.84375
# [9/100] training 56.8% loss=0.45287, acc=0.76562
# [9/100] training 56.9% loss=0.36726, acc=0.79688
# [9/100] training 57.1% loss=0.39086, acc=0.87500
# [9/100] training 57.2% loss=0.27789, acc=0.90625
# [9/100] training 57.5% loss=0.33942, acc=0.79688
# [9/100] training 57.6% loss=0.37205, acc=0.82812
# [9/100] training 57.8% loss=0.31494, acc=0.89062
# [9/100] training 58.0% loss=0.23486, acc=0.90625
# [9/100] training 58.1% loss=0.34343, acc=0.89062
# [9/100] training 58.3% loss=0.24601, acc=0.92188
# [9/100] training 58.4% loss=0.26055, acc=0.90625
# [9/100] training 58.7% loss=0.38072, acc=0.79688
# [9/100] training 58.8% loss=0.40459, acc=0.81250
# [9/100] training 59.0% loss=0.34122, acc=0.84375
# [9/100] training 59.2% loss=0.38759, acc=0.81250
# [9/100] training 59.3% loss=0.37197, acc=0.76562
# [9/100] training 59.5% loss=0.39995, acc=0.84375
# [9/100] training 59.7% loss=0.40506, acc=0.81250
# [9/100] training 59.9% loss=0.32833, acc=0.82812
# [9/100] training 60.0% loss=0.35017, acc=0.82812
# [9/100] training 60.2% loss=0.33930, acc=0.85938
# [9/100] training 60.3% loss=0.34699, acc=0.81250
# [9/100] training 60.5% loss=0.27886, acc=0.89062
# [9/100] training 60.8% loss=0.42195, acc=0.82812
# [9/100] training 60.9% loss=0.43445, acc=0.76562
# [9/100] training 61.1% loss=0.48396, acc=0.79688
# [9/100] training 61.2% loss=0.35218, acc=0.79688
# [9/100] training 61.4% loss=0.40534, acc=0.78125
# [9/100] training 61.5% loss=0.45103, acc=0.73438
# [9/100] training 61.7% loss=0.41295, acc=0.81250
# [9/100] training 62.0% loss=0.41664, acc=0.82812
# [9/100] training 62.1% loss=0.41805, acc=0.81250
# [9/100] training 62.3% loss=0.25368, acc=0.89062
# [9/100] training 62.4% loss=0.34643, acc=0.82812
# [9/100] training 62.6% loss=0.45893, acc=0.79688
# [9/100] training 62.7% loss=0.31549, acc=0.84375
# [9/100] training 62.9% loss=0.44851, acc=0.79688
# [9/100] training 63.1% loss=0.33665, acc=0.81250
# [9/100] training 63.3% loss=0.39249, acc=0.84375
# [9/100] training 63.5% loss=0.41511, acc=0.79688
# [9/100] training 63.6% loss=0.28635, acc=0.87500
# [9/100] training 63.8% loss=0.41202, acc=0.79688
# [9/100] training 63.9% loss=0.33612, acc=0.84375
# [9/100] training 64.2% loss=0.26051, acc=0.89062
# [9/100] training 64.3% loss=0.40627, acc=0.82812
# [9/100] training 64.5% loss=0.35981, acc=0.79688
# [9/100] training 64.7% loss=0.31465, acc=0.82812
# [9/100] training 64.8% loss=0.58190, acc=0.73438
# [9/100] training 65.0% loss=0.39501, acc=0.81250
# [9/100] training 65.1% loss=0.40272, acc=0.81250
# [9/100] training 65.4% loss=0.36444, acc=0.78125
# [9/100] training 65.5% loss=0.36652, acc=0.81250
# [9/100] training 65.7% loss=0.26554, acc=0.87500
# [9/100] training 65.8% loss=0.45715, acc=0.76562
# [9/100] training 66.0% loss=0.42347, acc=0.76562
# [9/100] training 66.2% loss=0.22199, acc=0.92188
# [9/100] training 66.3% loss=0.43867, acc=0.84375
# [9/100] training 66.6% loss=0.39408, acc=0.81250
# [9/100] training 66.7% loss=0.30255, acc=0.85938
# [9/100] training 66.9% loss=0.43582, acc=0.81250
# [9/100] training 67.0% loss=0.44351, acc=0.79688
# [9/100] training 67.2% loss=0.24968, acc=0.89062
# [9/100] training 67.4% loss=0.35323, acc=0.84375
# [9/100] training 67.6% loss=0.30006, acc=0.87500
# [9/100] training 67.8% loss=0.34942, acc=0.81250
# [9/100] training 67.9% loss=0.25893, acc=0.92188
# [9/100] training 68.1% loss=0.29750, acc=0.87500
# [9/100] training 68.2% loss=0.25808, acc=0.90625
# [9/100] training 68.4% loss=0.19833, acc=0.93750
# [9/100] training 68.5% loss=0.53824, acc=0.76562
# [9/100] training 68.8% loss=0.41595, acc=0.79688
# [9/100] training 69.0% loss=0.44279, acc=0.84375
# [9/100] training 69.1% loss=0.27314, acc=0.84375
# [9/100] training 69.3% loss=0.26903, acc=0.87500
# [9/100] training 69.4% loss=0.27671, acc=0.85938
# [9/100] training 69.6% loss=0.38924, acc=0.87500
# [9/100] training 69.7% loss=0.39147, acc=0.81250
# [9/100] training 70.0% loss=0.40214, acc=0.78125
# [9/100] training 70.2% loss=0.34269, acc=0.81250
# [9/100] training 70.3% loss=0.33174, acc=0.84375
# [9/100] training 70.5% loss=0.37937, acc=0.87500
# [9/100] training 70.6% loss=0.27987, acc=0.89062
# [9/100] training 70.8% loss=0.33219, acc=0.84375
# [9/100] training 71.0% loss=0.38359, acc=0.82812
# [9/100] training 71.2% loss=0.29456, acc=0.82812
# [9/100] training 71.3% loss=0.29951, acc=0.87500
# [9/100] training 71.5% loss=0.48500, acc=0.82812
# [9/100] training 71.7% loss=0.43002, acc=0.79688
# [9/100] training 71.8% loss=0.33313, acc=0.85938
# [9/100] training 72.0% loss=0.23266, acc=0.93750
# [9/100] training 72.2% loss=0.28690, acc=0.82812
# [9/100] training 72.4% loss=0.45019, acc=0.79688
# [9/100] training 72.5% loss=0.44200, acc=0.84375
# [9/100] training 72.7% loss=0.39424, acc=0.79688
# [9/100] training 72.9% loss=0.31455, acc=0.82812
# [9/100] training 73.0% loss=0.26507, acc=0.89062
# [9/100] training 73.3% loss=0.28181, acc=0.89062
# [9/100] training 73.4% loss=0.29979, acc=0.85938
# [9/100] training 73.6% loss=0.25718, acc=0.89062
# [9/100] training 73.7% loss=0.28825, acc=0.89062
# [9/100] training 73.9% loss=0.42413, acc=0.84375
# [9/100] training 74.0% loss=0.50222, acc=0.81250
# [9/100] training 74.2% loss=0.38998, acc=0.76562
# [9/100] training 74.5% loss=0.36196, acc=0.82812
# [9/100] training 74.6% loss=0.46016, acc=0.81250
# [9/100] training 74.8% loss=0.45761, acc=0.76562
# [9/100] training 74.9% loss=0.45702, acc=0.81250
# [9/100] training 75.1% loss=0.35477, acc=0.85938
# [9/100] training 75.2% loss=0.26119, acc=0.93750
# [9/100] training 75.4% loss=0.35026, acc=0.84375
# [9/100] training 75.7% loss=0.44194, acc=0.73438
# [9/100] training 75.8% loss=0.42993, acc=0.82812
# [9/100] training 76.0% loss=0.16992, acc=0.95312
# [9/100] training 76.1% loss=0.33755, acc=0.79688
# [9/100] training 76.3% loss=0.27404, acc=0.90625
# [9/100] training 76.4% loss=0.29668, acc=0.85938
# [9/100] training 76.7% loss=0.26415, acc=0.89062
# [9/100] training 76.8% loss=0.19993, acc=0.96875
# [9/100] training 77.0% loss=0.27329, acc=0.89062
# [9/100] training 77.2% loss=0.35641, acc=0.90625
# [9/100] training 77.3% loss=0.20636, acc=0.95312
# [9/100] training 77.5% loss=0.34401, acc=0.82812
# [9/100] training 77.6% loss=0.26592, acc=0.85938
# [9/100] training 77.9% loss=0.34734, acc=0.81250
# [9/100] training 78.0% loss=0.37123, acc=0.82812
# [9/100] training 78.2% loss=0.36973, acc=0.85938
# [9/100] training 78.4% loss=0.19270, acc=0.89062
# [9/100] training 78.5% loss=0.41614, acc=0.85938
# [9/100] training 78.7% loss=0.28059, acc=0.89062
# [9/100] training 78.8% loss=0.30033, acc=0.87500
# [9/100] training 79.1% loss=0.27607, acc=0.87500
# [9/100] training 79.2% loss=0.25777, acc=0.92188
# [9/100] training 79.4% loss=0.48375, acc=0.76562
# [9/100] training 79.5% loss=0.36260, acc=0.82812
# [9/100] training 79.7% loss=0.22493, acc=0.93750
# [9/100] training 79.9% loss=0.28123, acc=0.89062
# [9/100] training 80.1% loss=0.34589, acc=0.84375
# [9/100] training 80.3% loss=0.50357, acc=0.78125
# [9/100] training 80.4% loss=0.34580, acc=0.85938
# [9/100] training 80.6% loss=0.52434, acc=0.81250
# [9/100] training 80.7% loss=0.34943, acc=0.81250
# [9/100] training 80.9% loss=0.44609, acc=0.78125
# [9/100] training 81.2% loss=0.43016, acc=0.76562
# [9/100] training 81.3% loss=0.36752, acc=0.82812
# [9/100] training 81.5% loss=0.36495, acc=0.84375
# [9/100] training 81.6% loss=0.44556, acc=0.76562
# [9/100] training 81.8% loss=0.32127, acc=0.85938
# [9/100] training 81.9% loss=0.43228, acc=0.87500
# [9/100] training 82.1% loss=0.23182, acc=0.89062
# [9/100] training 82.2% loss=0.39982, acc=0.87500
# [9/100] training 82.5% loss=0.29119, acc=0.87500
# [9/100] training 82.7% loss=0.34821, acc=0.87500
# [9/100] training 82.8% loss=0.35171, acc=0.84375
# [9/100] training 83.0% loss=0.40731, acc=0.79688
# [9/100] training 83.1% loss=0.39053, acc=0.87500
# [9/100] training 83.3% loss=0.35902, acc=0.87500
# [9/100] training 83.5% loss=0.32641, acc=0.89062
# [9/100] training 83.7% loss=0.37744, acc=0.87500
# [9/100] training 83.9% loss=0.36610, acc=0.87500
# [9/100] training 84.0% loss=0.41010, acc=0.81250
# [9/100] training 84.2% loss=0.23118, acc=0.93750
# [9/100] training 84.3% loss=0.32554, acc=0.87500
# [9/100] training 84.5% loss=0.29886, acc=0.84375
# [9/100] training 84.7% loss=0.43778, acc=0.81250
# [9/100] training 84.9% loss=0.34302, acc=0.84375
# [9/100] training 85.0% loss=0.33088, acc=0.85938
# [9/100] training 85.2% loss=0.39111, acc=0.79688
# [9/100] training 85.4% loss=0.28621, acc=0.93750
# [9/100] training 85.5% loss=0.27194, acc=0.87500
# [9/100] training 85.8% loss=0.31720, acc=0.79688
# [9/100] training 85.9% loss=0.40045, acc=0.85938
# [9/100] training 86.1% loss=0.29998, acc=0.82812
# [9/100] training 86.2% loss=0.25872, acc=0.90625
# [9/100] training 86.4% loss=0.41756, acc=0.81250
# [9/100] training 86.6% loss=0.44888, acc=0.82812
# [9/100] training 86.7% loss=0.34403, acc=0.82812
# [9/100] training 87.0% loss=0.34098, acc=0.82812
# [9/100] training 87.1% loss=0.30405, acc=0.85938
# [9/100] training 87.3% loss=0.32251, acc=0.89062
# [9/100] training 87.4% loss=0.30501, acc=0.89062
# [9/100] training 87.6% loss=0.28782, acc=0.89062
# [9/100] training 87.7% loss=0.40284, acc=0.84375
# [9/100] training 87.9% loss=0.32798, acc=0.89062
# [9/100] training 88.2% loss=0.16712, acc=0.92188
# [9/100] training 88.3% loss=0.27078, acc=0.89062
# [9/100] training 88.5% loss=0.30736, acc=0.90625
# [9/100] training 88.6% loss=0.15847, acc=0.93750
# [9/100] training 88.8% loss=0.35698, acc=0.92188
# [9/100] training 88.9% loss=0.37093, acc=0.84375
# [9/100] training 89.2% loss=0.24652, acc=0.92188
# [9/100] training 89.4% loss=0.32669, acc=0.84375
# [9/100] training 89.5% loss=0.35756, acc=0.82812
# [9/100] training 89.7% loss=0.39935, acc=0.79688
# [9/100] training 89.8% loss=0.29999, acc=0.90625
# [9/100] training 90.0% loss=0.32737, acc=0.81250
# [9/100] training 90.1% loss=0.32991, acc=0.87500
# [9/100] training 90.4% loss=0.38705, acc=0.79688
# [9/100] training 90.5% loss=0.24617, acc=0.87500
# [9/100] training 90.7% loss=0.42089, acc=0.79688
# [9/100] training 90.9% loss=0.38236, acc=0.90625
# [9/100] training 91.0% loss=0.31999, acc=0.85938
# [9/100] training 91.2% loss=0.32598, acc=0.82812
# [9/100] training 91.3% loss=0.49331, acc=0.78125
# [9/100] training 91.6% loss=0.45148, acc=0.81250
# [9/100] training 91.7% loss=0.31092, acc=0.85938
# [9/100] training 91.9% loss=0.35864, acc=0.85938
# [9/100] training 92.1% loss=0.33037, acc=0.84375
# [9/100] training 92.2% loss=0.26119, acc=0.90625
# [9/100] training 92.4% loss=0.32252, acc=0.85938
# [9/100] training 92.6% loss=0.49602, acc=0.78125
# [9/100] training 92.8% loss=0.41774, acc=0.79688
# [9/100] training 92.9% loss=0.40521, acc=0.79688
# [9/100] training 93.1% loss=0.41493, acc=0.78125
# [9/100] training 93.2% loss=0.32840, acc=0.85938
# [9/100] training 93.4% loss=0.23706, acc=0.90625
# [9/100] training 93.7% loss=0.31966, acc=0.85938
# [9/100] training 93.8% loss=0.37475, acc=0.79688
# [9/100] training 94.0% loss=0.25543, acc=0.87500
# [9/100] training 94.1% loss=0.36751, acc=0.85938
# [9/100] training 94.3% loss=0.28862, acc=0.82812
# [9/100] training 94.4% loss=0.33970, acc=0.81250
# [9/100] training 94.6% loss=0.24063, acc=0.90625
# [9/100] training 94.9% loss=0.42701, acc=0.76562
# [9/100] training 95.0% loss=0.39261, acc=0.84375
# [9/100] training 95.2% loss=0.46976, acc=0.76562
# [9/100] training 95.3% loss=0.28384, acc=0.89062
# [9/100] training 95.5% loss=0.27186, acc=0.90625
# [9/100] training 95.6% loss=0.41742, acc=0.84375
# [9/100] training 95.8% loss=0.40043, acc=0.82812
# [9/100] training 96.0% loss=0.36345, acc=0.82812
# [9/100] training 96.2% loss=0.26216, acc=0.89062
# [9/100] training 96.4% loss=0.25652, acc=0.93750
# [9/100] training 96.5% loss=0.32136, acc=0.89062
# [9/100] training 96.7% loss=0.27594, acc=0.89062
# [9/100] training 96.8% loss=0.34322, acc=0.85938
# [9/100] training 97.1% loss=0.30402, acc=0.84375
# [9/100] training 97.2% loss=0.36602, acc=0.84375
# [9/100] training 97.4% loss=0.35270, acc=0.84375
# [9/100] training 97.6% loss=0.35510, acc=0.84375
# [9/100] training 97.7% loss=0.30381, acc=0.87500
# [9/100] training 97.9% loss=0.18779, acc=0.90625
# [9/100] training 98.0% loss=0.39266, acc=0.81250
# [9/100] training 98.3% loss=0.36909, acc=0.89062
# [9/100] training 98.4% loss=0.26416, acc=0.90625
# [9/100] training 98.6% loss=0.49609, acc=0.78125
# [9/100] training 98.7% loss=0.39739, acc=0.84375
# [9/100] training 98.9% loss=0.28677, acc=0.89062
# [9/100] training 99.1% loss=0.21782, acc=0.89062
# [9/100] training 99.2% loss=0.24268, acc=0.85938
# [9/100] training 99.5% loss=0.40335, acc=0.84375
# [9/100] training 99.6% loss=0.31666, acc=0.84375
# [9/100] training 99.8% loss=0.31468, acc=0.85938
# [9/100] training 99.9% loss=0.21705, acc=0.90625
# [9/100] testing 0.9% loss=0.18284, acc=0.90625
# [9/100] testing 1.8% loss=0.60209, acc=0.71875
# [9/100] testing 2.2% loss=0.27678, acc=0.87500
# [9/100] testing 3.1% loss=0.36538, acc=0.81250
# [9/100] testing 3.5% loss=0.24848, acc=0.85938
# [9/100] testing 4.4% loss=0.35094, acc=0.89062
# [9/100] testing 4.8% loss=0.42661, acc=0.81250
# [9/100] testing 5.7% loss=0.34565, acc=0.81250
# [9/100] testing 6.6% loss=0.37440, acc=0.81250
# [9/100] testing 7.0% loss=0.22429, acc=0.90625
# [9/100] testing 7.9% loss=0.39453, acc=0.81250
# [9/100] testing 8.3% loss=0.25701, acc=0.90625
# [9/100] testing 9.2% loss=0.33699, acc=0.85938
# [9/100] testing 9.7% loss=0.25976, acc=0.89062
# [9/100] testing 10.5% loss=0.42477, acc=0.78125
# [9/100] testing 11.0% loss=0.26837, acc=0.84375
# [9/100] testing 11.8% loss=0.32262, acc=0.87500
# [9/100] testing 12.7% loss=0.51949, acc=0.76562
# [9/100] testing 13.2% loss=0.27740, acc=0.85938
# [9/100] testing 14.0% loss=0.48879, acc=0.82812
# [9/100] testing 14.5% loss=0.35756, acc=0.81250
# [9/100] testing 15.4% loss=0.37712, acc=0.82812
# [9/100] testing 15.8% loss=0.27659, acc=0.87500
# [9/100] testing 16.7% loss=0.31287, acc=0.87500
# [9/100] testing 17.5% loss=0.49263, acc=0.82812
# [9/100] testing 18.0% loss=0.27023, acc=0.85938
# [9/100] testing 18.9% loss=0.19946, acc=0.95312
# [9/100] testing 19.3% loss=0.28085, acc=0.87500
# [9/100] testing 20.2% loss=0.45842, acc=0.78125
# [9/100] testing 20.6% loss=0.41943, acc=0.79688
# [9/100] testing 21.5% loss=0.31934, acc=0.85938
# [9/100] testing 21.9% loss=0.48282, acc=0.75000
# [9/100] testing 22.8% loss=0.42172, acc=0.82812
# [9/100] testing 23.7% loss=0.34961, acc=0.85938
# [9/100] testing 24.1% loss=0.28497, acc=0.89062
# [9/100] testing 25.0% loss=0.42617, acc=0.82812
# [9/100] testing 25.4% loss=0.30548, acc=0.89062
# [9/100] testing 26.3% loss=0.37321, acc=0.84375
# [9/100] testing 26.8% loss=0.33481, acc=0.81250
# [9/100] testing 27.6% loss=0.38238, acc=0.84375
# [9/100] testing 28.5% loss=0.43818, acc=0.81250
# [9/100] testing 29.0% loss=0.25238, acc=0.87500
# [9/100] testing 29.8% loss=0.44716, acc=0.84375
# [9/100] testing 30.3% loss=0.34138, acc=0.89062
# [9/100] testing 31.1% loss=0.46945, acc=0.81250
# [9/100] testing 31.6% loss=0.24079, acc=0.84375
# [9/100] testing 32.5% loss=0.31054, acc=0.84375
# [9/100] testing 32.9% loss=0.38420, acc=0.87500
# [9/100] testing 33.8% loss=0.40492, acc=0.82812
# [9/100] testing 34.7% loss=0.36945, acc=0.84375
# [9/100] testing 35.1% loss=0.34128, acc=0.87500
# [9/100] testing 36.0% loss=0.35937, acc=0.84375
# [9/100] testing 36.4% loss=0.31445, acc=0.84375
# [9/100] testing 37.3% loss=0.43641, acc=0.78125
# [9/100] testing 37.7% loss=0.42822, acc=0.81250
# [9/100] testing 38.6% loss=0.22098, acc=0.90625
# [9/100] testing 39.5% loss=0.45001, acc=0.84375
# [9/100] testing 39.9% loss=0.30360, acc=0.87500
# [9/100] testing 40.8% loss=0.32394, acc=0.89062
# [9/100] testing 41.2% loss=0.25097, acc=0.90625
# [9/100] testing 42.1% loss=0.43513, acc=0.81250
# [9/100] testing 42.5% loss=0.31791, acc=0.82812
# [9/100] testing 43.4% loss=0.42577, acc=0.85938
# [9/100] testing 43.9% loss=0.23810, acc=0.92188
# [9/100] testing 44.7% loss=0.50379, acc=0.81250
# [9/100] testing 45.6% loss=0.31935, acc=0.85938
# [9/100] testing 46.1% loss=0.32930, acc=0.90625
# [9/100] testing 46.9% loss=0.30464, acc=0.87500
# [9/100] testing 47.4% loss=0.16658, acc=0.93750
# [9/100] testing 48.3% loss=0.43225, acc=0.84375
# [9/100] testing 48.7% loss=0.54010, acc=0.81250
# [9/100] testing 49.6% loss=0.43495, acc=0.78125
# [9/100] testing 50.4% loss=0.25766, acc=0.87500
# [9/100] testing 50.9% loss=0.37045, acc=0.78125
# [9/100] testing 51.8% loss=0.32486, acc=0.84375
# [9/100] testing 52.2% loss=0.41713, acc=0.87500
# [9/100] testing 53.1% loss=0.34427, acc=0.84375
# [9/100] testing 53.5% loss=0.29225, acc=0.93750
# [9/100] testing 54.4% loss=0.47057, acc=0.81250
# [9/100] testing 54.8% loss=0.37545, acc=0.82812
# [9/100] testing 55.7% loss=0.23591, acc=0.90625
# [9/100] testing 56.6% loss=0.42093, acc=0.84375
# [9/100] testing 57.0% loss=0.56010, acc=0.78125
# [9/100] testing 57.9% loss=0.45782, acc=0.79688
# [9/100] testing 58.3% loss=0.38370, acc=0.82812
# [9/100] testing 59.2% loss=0.37121, acc=0.79688
# [9/100] testing 59.7% loss=0.30018, acc=0.82812
# [9/100] testing 60.5% loss=0.52829, acc=0.79688
# [9/100] testing 61.4% loss=0.25054, acc=0.90625
# [9/100] testing 61.9% loss=0.31293, acc=0.87500
# [9/100] testing 62.7% loss=0.33548, acc=0.84375
# [9/100] testing 63.2% loss=0.45898, acc=0.76562
# [9/100] testing 64.0% loss=0.44808, acc=0.79688
# [9/100] testing 64.5% loss=0.32243, acc=0.82812
# [9/100] testing 65.4% loss=0.28366, acc=0.87500
# [9/100] testing 65.8% loss=0.38647, acc=0.85938
# [9/100] testing 66.7% loss=0.24792, acc=0.90625
# [9/100] testing 67.6% loss=0.34260, acc=0.85938
# [9/100] testing 68.0% loss=0.27675, acc=0.87500
# [9/100] testing 68.9% loss=0.35124, acc=0.85938
# [9/100] testing 69.3% loss=0.45595, acc=0.79688
# [9/100] testing 70.2% loss=0.36102, acc=0.79688
# [9/100] testing 70.6% loss=0.42289, acc=0.79688
# [9/100] testing 71.5% loss=0.37381, acc=0.84375
# [9/100] testing 72.4% loss=0.24994, acc=0.93750
# [9/100] testing 72.8% loss=0.38554, acc=0.81250
# [9/100] testing 73.7% loss=0.27706, acc=0.90625
# [9/100] testing 74.1% loss=0.45786, acc=0.81250
# [9/100] testing 75.0% loss=0.26373, acc=0.87500
# [9/100] testing 75.4% loss=0.36400, acc=0.90625
# [9/100] testing 76.3% loss=0.16772, acc=0.93750
# [9/100] testing 76.8% loss=0.37524, acc=0.81250
# [9/100] testing 77.6% loss=0.21404, acc=0.92188
# [9/100] testing 78.5% loss=0.67163, acc=0.75000
# [9/100] testing 79.0% loss=0.38937, acc=0.82812
# [9/100] testing 79.8% loss=0.37701, acc=0.76562
# [9/100] testing 80.3% loss=0.30434, acc=0.87500
# [9/100] testing 81.2% loss=0.41709, acc=0.82812
# [9/100] testing 81.6% loss=0.30036, acc=0.87500
# [9/100] testing 82.5% loss=0.23150, acc=0.90625
# [9/100] testing 83.3% loss=0.24878, acc=0.90625
# [9/100] testing 83.8% loss=0.19892, acc=0.92188
# [9/100] testing 84.7% loss=0.34095, acc=0.85938
# [9/100] testing 85.1% loss=0.44594, acc=0.81250
# [9/100] testing 86.0% loss=0.36275, acc=0.82812
# [9/100] testing 86.4% loss=0.50521, acc=0.81250
# [9/100] testing 87.3% loss=0.47125, acc=0.82812
# [9/100] testing 87.7% loss=0.29987, acc=0.84375
# [9/100] testing 88.6% loss=0.39749, acc=0.82812
# [9/100] testing 89.5% loss=0.57481, acc=0.73438
# [9/100] testing 89.9% loss=0.30577, acc=0.82812
# [9/100] testing 90.8% loss=0.28936, acc=0.92188
# [9/100] testing 91.2% loss=0.21020, acc=0.90625
# [9/100] testing 92.1% loss=0.43038, acc=0.78125
# [9/100] testing 92.6% loss=0.34626, acc=0.81250
# [9/100] testing 93.4% loss=0.42108, acc=0.79688
# [9/100] testing 94.3% loss=0.23164, acc=0.89062
# [9/100] testing 94.7% loss=0.31980, acc=0.85938
# [9/100] testing 95.6% loss=0.33415, acc=0.85938
# [9/100] testing 96.1% loss=0.27958, acc=0.85938
# [9/100] testing 96.9% loss=0.34820, acc=0.81250
# [9/100] testing 97.4% loss=0.19738, acc=0.89062
# [9/100] testing 98.3% loss=0.36193, acc=0.84375
# [9/100] testing 98.7% loss=0.30625, acc=0.82812
# [9/100] testing 99.6% loss=0.31355, acc=0.84375
# [10/100] training 0.2% loss=0.50516, acc=0.78125
# [10/100] training 0.4% loss=0.45192, acc=0.82812
# [10/100] training 0.5% loss=0.30269, acc=0.89062
# [10/100] training 0.8% loss=0.43514, acc=0.82812
# [10/100] training 0.9% loss=0.37261, acc=0.79688
# [10/100] training 1.1% loss=0.33258, acc=0.90625
# [10/100] training 1.2% loss=0.44114, acc=0.79688
# [10/100] training 1.4% loss=0.35514, acc=0.85938
# [10/100] training 1.6% loss=0.20054, acc=0.90625
# [10/100] training 1.8% loss=0.38129, acc=0.87500
# [10/100] training 2.0% loss=0.37249, acc=0.81250
# [10/100] training 2.1% loss=0.34652, acc=0.82812
# [10/100] training 2.3% loss=0.35084, acc=0.84375
# [10/100] training 2.4% loss=0.34439, acc=0.85938
# [10/100] training 2.6% loss=0.39355, acc=0.82812
# [10/100] training 2.7% loss=0.35616, acc=0.85938
# [10/100] training 3.0% loss=0.40104, acc=0.76562
# [10/100] training 3.2% loss=0.25045, acc=0.90625
# [10/100] training 3.3% loss=0.33100, acc=0.84375
# [10/100] training 3.5% loss=0.43306, acc=0.85938
# [10/100] training 3.6% loss=0.58724, acc=0.76562
# [10/100] training 3.8% loss=0.23602, acc=0.89062
# [10/100] training 3.9% loss=0.36924, acc=0.79688
# [10/100] training 4.2% loss=0.31351, acc=0.89062
# [10/100] training 4.4% loss=0.23328, acc=0.95312
# [10/100] training 4.5% loss=0.27833, acc=0.87500
# [10/100] training 4.7% loss=0.37866, acc=0.89062
# [10/100] training 4.8% loss=0.39127, acc=0.81250
# [10/100] training 5.0% loss=0.28380, acc=0.89062
# [10/100] training 5.2% loss=0.38905, acc=0.89062
# [10/100] training 5.4% loss=0.31459, acc=0.89062
# [10/100] training 5.5% loss=0.32971, acc=0.85938
# [10/100] training 5.7% loss=0.27239, acc=0.89062
# [10/100] training 5.9% loss=0.36829, acc=0.82812
# [10/100] training 6.0% loss=0.28716, acc=0.87500
# [10/100] training 6.3% loss=0.38206, acc=0.81250
# [10/100] training 6.4% loss=0.24512, acc=0.85938
# [10/100] training 6.6% loss=0.37664, acc=0.82812
# [10/100] training 6.7% loss=0.39787, acc=0.82812
# [10/100] training 6.9% loss=0.27092, acc=0.90625
# [10/100] training 7.1% loss=0.40854, acc=0.79688
# [10/100] training 7.2% loss=0.33505, acc=0.89062
# [10/100] training 7.5% loss=0.25941, acc=0.93750
# [10/100] training 7.6% loss=0.31812, acc=0.89062
# [10/100] training 7.8% loss=0.35777, acc=0.81250
# [10/100] training 7.9% loss=0.41032, acc=0.82812
# [10/100] training 8.1% loss=0.31830, acc=0.87500
# [10/100] training 8.2% loss=0.26316, acc=0.92188
# [10/100] training 8.4% loss=0.32572, acc=0.85938
# [10/100] training 8.7% loss=0.30220, acc=0.85938
# [10/100] training 8.8% loss=0.35870, acc=0.84375
# [10/100] training 9.0% loss=0.31810, acc=0.82812
# [10/100] training 9.1% loss=0.37539, acc=0.81250
# [10/100] training 9.3% loss=0.52277, acc=0.81250
# [10/100] training 9.4% loss=0.32649, acc=0.89062
# [10/100] training 9.7% loss=0.31314, acc=0.84375
# [10/100] training 9.9% loss=0.34461, acc=0.82812
# [10/100] training 10.0% loss=0.40135, acc=0.78125
# [10/100] training 10.2% loss=0.32504, acc=0.85938
# [10/100] training 10.3% loss=0.29097, acc=0.85938
# [10/100] training 10.5% loss=0.37575, acc=0.87500
# [10/100] training 10.6% loss=0.29551, acc=0.84375
# [10/100] training 10.9% loss=0.29140, acc=0.85938
# [10/100] training 11.0% loss=0.33093, acc=0.85938
# [10/100] training 11.2% loss=0.27063, acc=0.90625
# [10/100] training 11.4% loss=0.39932, acc=0.84375
# [10/100] training 11.5% loss=0.46439, acc=0.81250
# [10/100] training 11.7% loss=0.22515, acc=0.93750
# [10/100] training 11.8% loss=0.26914, acc=0.89062
# [10/100] training 12.1% loss=0.40806, acc=0.84375
# [10/100] training 12.2% loss=0.30783, acc=0.85938
# [10/100] training 12.4% loss=0.31272, acc=0.87500
# [10/100] training 12.6% loss=0.26356, acc=0.89062
# [10/100] training 12.7% loss=0.41594, acc=0.84375
# [10/100] training 12.9% loss=0.36351, acc=0.89062
# [10/100] training 13.0% loss=0.31286, acc=0.87500
# [10/100] training 13.3% loss=0.30586, acc=0.87500
# [10/100] training 13.4% loss=0.36176, acc=0.75000
# [10/100] training 13.6% loss=0.29930, acc=0.90625
# [10/100] training 13.7% loss=0.54128, acc=0.78125
# [10/100] training 13.9% loss=0.40229, acc=0.85938
# [10/100] training 14.1% loss=0.29765, acc=0.87500
# [10/100] training 14.3% loss=0.34866, acc=0.85938
# [10/100] training 14.5% loss=0.32214, acc=0.85938
# [10/100] training 14.6% loss=0.30336, acc=0.89062
# [10/100] training 14.8% loss=0.29081, acc=0.90625
# [10/100] training 14.9% loss=0.32357, acc=0.84375
# [10/100] training 15.1% loss=0.38973, acc=0.84375
# [10/100] training 15.4% loss=0.35949, acc=0.78125
# [10/100] training 15.5% loss=0.35902, acc=0.89062
# [10/100] training 15.7% loss=0.47070, acc=0.84375
# [10/100] training 15.8% loss=0.28992, acc=0.85938
# [10/100] training 16.0% loss=0.41965, acc=0.82812
# [10/100] training 16.1% loss=0.55010, acc=0.78125
# [10/100] training 16.3% loss=0.36810, acc=0.84375
# [10/100] training 16.4% loss=0.27780, acc=0.90625
# [10/100] training 16.7% loss=0.38343, acc=0.82812
# [10/100] training 16.9% loss=0.42616, acc=0.82812
# [10/100] training 17.0% loss=0.36449, acc=0.87500
# [10/100] training 17.2% loss=0.26316, acc=0.90625
# [10/100] training 17.3% loss=0.26735, acc=0.84375
# [10/100] training 17.5% loss=0.29133, acc=0.87500
# [10/100] training 17.7% loss=0.34515, acc=0.84375
# [10/100] training 17.9% loss=0.39277, acc=0.87500
# [10/100] training 18.1% loss=0.42956, acc=0.81250
# [10/100] training 18.2% loss=0.33152, acc=0.85938
# [10/100] training 18.4% loss=0.43457, acc=0.78125
# [10/100] training 18.5% loss=0.31174, acc=0.92188
# [10/100] training 18.8% loss=0.28266, acc=0.85938
# [10/100] training 18.9% loss=0.27564, acc=0.89062
# [10/100] training 19.1% loss=0.40750, acc=0.81250
# [10/100] training 19.2% loss=0.23230, acc=0.90625
# [10/100] training 19.4% loss=0.20902, acc=0.90625
# [10/100] training 19.6% loss=0.39199, acc=0.82812
# [10/100] training 19.7% loss=0.31105, acc=0.82812
# [10/100] training 20.0% loss=0.26320, acc=0.85938
# [10/100] training 20.1% loss=0.31602, acc=0.89062
# [10/100] training 20.3% loss=0.38276, acc=0.90625
# [10/100] training 20.4% loss=0.44681, acc=0.78125
# [10/100] training 20.6% loss=0.32977, acc=0.89062
# [10/100] training 20.8% loss=0.29678, acc=0.89062
# [10/100] training 20.9% loss=0.35449, acc=0.81250
# [10/100] training 21.2% loss=0.27993, acc=0.89062
# [10/100] training 21.3% loss=0.32094, acc=0.85938
# [10/100] training 21.5% loss=0.37612, acc=0.82812
# [10/100] training 21.6% loss=0.27232, acc=0.90625
# [10/100] training 21.8% loss=0.24713, acc=0.87500
# [10/100] training 21.9% loss=0.41150, acc=0.79688
# [10/100] training 22.2% loss=0.27431, acc=0.84375
# [10/100] training 22.4% loss=0.37556, acc=0.85938
# [10/100] training 22.5% loss=0.24856, acc=0.90625
# [10/100] training 22.7% loss=0.29926, acc=0.89062
# [10/100] training 22.8% loss=0.38702, acc=0.82812
# [10/100] training 23.0% loss=0.20527, acc=0.92188
# [10/100] training 23.1% loss=0.36990, acc=0.85938
# [10/100] training 23.4% loss=0.35013, acc=0.87500
# [10/100] training 23.6% loss=0.37428, acc=0.89062
# [10/100] training 23.7% loss=0.31098, acc=0.84375
# [10/100] training 23.9% loss=0.31188, acc=0.81250
# [10/100] training 24.0% loss=0.39896, acc=0.79688
# [10/100] training 24.2% loss=0.46354, acc=0.79688
# [10/100] training 24.3% loss=0.36012, acc=0.87500
# [10/100] training 24.6% loss=0.34348, acc=0.84375
# [10/100] training 24.7% loss=0.52196, acc=0.68750
# [10/100] training 24.9% loss=0.37008, acc=0.85938
# [10/100] training 25.1% loss=0.38926, acc=0.82812
# [10/100] training 25.2% loss=0.24630, acc=0.90625
# [10/100] training 25.4% loss=0.32188, acc=0.85938
# [10/100] training 25.6% loss=0.30789, acc=0.90625
# [10/100] training 25.8% loss=0.38631, acc=0.84375
# [10/100] training 25.9% loss=0.26473, acc=0.90625
# [10/100] training 26.1% loss=0.45791, acc=0.81250
# [10/100] training 26.3% loss=0.37141, acc=0.90625
# [10/100] training 26.4% loss=0.29749, acc=0.85938
# [10/100] training 26.6% loss=0.29451, acc=0.89062
# [10/100] training 26.8% loss=0.41466, acc=0.79688
# [10/100] training 27.0% loss=0.29468, acc=0.87500
# [10/100] training 27.1% loss=0.23480, acc=0.92188
# [10/100] training 27.3% loss=0.30942, acc=0.85938
# [10/100] training 27.4% loss=0.27749, acc=0.84375
# [10/100] training 27.6% loss=0.37108, acc=0.87500
# [10/100] training 27.9% loss=0.33037, acc=0.85938
# [10/100] training 28.0% loss=0.34333, acc=0.82812
# [10/100] training 28.2% loss=0.34257, acc=0.89062
# [10/100] training 28.3% loss=0.20381, acc=0.96875
# [10/100] training 28.5% loss=0.36282, acc=0.87500
# [10/100] training 28.6% loss=0.33248, acc=0.87500
# [10/100] training 28.8% loss=0.24365, acc=0.89062
# [10/100] training 29.1% loss=0.36755, acc=0.85938
# [10/100] training 29.2% loss=0.22409, acc=0.92188
# [10/100] training 29.4% loss=0.39257, acc=0.82812
# [10/100] training 29.5% loss=0.28491, acc=0.84375
# [10/100] training 29.7% loss=0.41092, acc=0.82812
# [10/100] training 29.8% loss=0.28429, acc=0.84375
# [10/100] training 30.0% loss=0.47234, acc=0.76562
# [10/100] training 30.2% loss=0.28107, acc=0.87500
# [10/100] training 30.4% loss=0.30565, acc=0.87500
# [10/100] training 30.6% loss=0.34275, acc=0.85938
# [10/100] training 30.7% loss=0.28406, acc=0.85938
# [10/100] training 30.9% loss=0.37006, acc=0.81250
# [10/100] training 31.0% loss=0.32809, acc=0.84375
# [10/100] training 31.3% loss=0.27102, acc=0.89062
# [10/100] training 31.4% loss=0.46350, acc=0.70312
# [10/100] training 31.6% loss=0.41023, acc=0.85938
# [10/100] training 31.8% loss=0.28395, acc=0.84375
# [10/100] training 31.9% loss=0.31652, acc=0.89062
# [10/100] training 32.1% loss=0.40567, acc=0.84375
# [10/100] training 32.2% loss=0.30754, acc=0.84375
# [10/100] training 32.5% loss=0.26827, acc=0.92188
# [10/100] training 32.6% loss=0.30499, acc=0.84375
# [10/100] training 32.8% loss=0.24160, acc=0.90625
# [10/100] training 32.9% loss=0.38360, acc=0.84375
# [10/100] training 33.1% loss=0.40348, acc=0.85938
# [10/100] training 33.3% loss=0.32586, acc=0.84375
# [10/100] training 33.4% loss=0.34015, acc=0.89062
# [10/100] training 33.7% loss=0.40045, acc=0.82812
# [10/100] training 33.8% loss=0.33306, acc=0.82812
# [10/100] training 34.0% loss=0.31970, acc=0.85938
# [10/100] training 34.1% loss=0.30012, acc=0.89062
# [10/100] training 34.3% loss=0.22900, acc=0.93750
# [10/100] training 34.5% loss=0.43112, acc=0.76562
# [10/100] training 34.7% loss=0.25339, acc=0.90625
# [10/100] training 34.9% loss=0.33777, acc=0.87500
# [10/100] training 35.0% loss=0.30199, acc=0.85938
# [10/100] training 35.2% loss=0.39623, acc=0.79688
# [10/100] training 35.3% loss=0.32697, acc=0.78125
# [10/100] training 35.5% loss=0.41362, acc=0.79688
# [10/100] training 35.6% loss=0.35901, acc=0.84375
# [10/100] training 35.9% loss=0.37159, acc=0.81250
# [10/100] training 36.1% loss=0.42138, acc=0.81250
# [10/100] training 36.2% loss=0.33987, acc=0.87500
# [10/100] training 36.4% loss=0.34998, acc=0.87500
# [10/100] training 36.5% loss=0.32382, acc=0.85938
# [10/100] training 36.7% loss=0.29498, acc=0.85938
# [10/100] training 36.8% loss=0.30251, acc=0.85938
# [10/100] training 37.1% loss=0.41777, acc=0.84375
# [10/100] training 37.3% loss=0.36760, acc=0.89062
# [10/100] training 37.4% loss=0.31920, acc=0.82812
# [10/100] training 37.6% loss=0.31923, acc=0.90625
# [10/100] training 37.7% loss=0.33286, acc=0.89062
# [10/100] training 37.9% loss=0.28286, acc=0.89062
# [10/100] training 38.1% loss=0.44081, acc=0.75000
# [10/100] training 38.3% loss=0.29993, acc=0.84375
# [10/100] training 38.4% loss=0.24117, acc=0.85938
# [10/100] training 38.6% loss=0.25344, acc=0.89062
# [10/100] training 38.8% loss=0.37469, acc=0.85938
# [10/100] training 38.9% loss=0.30201, acc=0.92188
# [10/100] training 39.1% loss=0.30015, acc=0.89062
# [10/100] training 39.3% loss=0.31099, acc=0.85938
# [10/100] training 39.5% loss=0.39072, acc=0.85938
# [10/100] training 39.6% loss=0.31733, acc=0.89062
# [10/100] training 39.8% loss=0.25946, acc=0.87500
# [10/100] training 40.0% loss=0.28505, acc=0.85938
# [10/100] training 40.1% loss=0.35647, acc=0.85938
# [10/100] training 40.4% loss=0.31935, acc=0.85938
# [10/100] training 40.5% loss=0.21031, acc=0.92188
# [10/100] training 40.7% loss=0.30943, acc=0.89062
# [10/100] training 40.8% loss=0.23437, acc=0.89062
# [10/100] training 41.0% loss=0.22178, acc=0.92188
# [10/100] training 41.1% loss=0.51497, acc=0.81250
# [10/100] training 41.3% loss=0.64237, acc=0.75000
# [10/100] training 41.6% loss=0.33703, acc=0.87500
# [10/100] training 41.7% loss=0.40618, acc=0.79688
# [10/100] training 41.9% loss=0.27798, acc=0.90625
# [10/100] training 42.0% loss=0.36285, acc=0.81250
# [10/100] training 42.2% loss=0.28102, acc=0.90625
# [10/100] training 42.3% loss=0.30408, acc=0.89062
# [10/100] training 42.5% loss=0.25557, acc=0.87500
# [10/100] training 42.8% loss=0.28032, acc=0.85938
# [10/100] training 42.9% loss=0.24054, acc=0.90625
# [10/100] training 43.1% loss=0.29355, acc=0.84375
# [10/100] training 43.2% loss=0.28232, acc=0.90625
# [10/100] training 43.4% loss=0.33646, acc=0.87500
# [10/100] training 43.5% loss=0.37259, acc=0.89062
# [10/100] training 43.8% loss=0.37789, acc=0.79688
# [10/100] training 43.9% loss=0.33095, acc=0.89062
# [10/100] training 44.1% loss=0.26611, acc=0.90625
# [10/100] training 44.3% loss=0.24911, acc=0.93750
# [10/100] training 44.4% loss=0.31130, acc=0.85938
# [10/100] training 44.6% loss=0.35329, acc=0.85938
# [10/100] training 44.7% loss=0.37532, acc=0.82812
# [10/100] training 45.0% loss=0.32397, acc=0.87500
# [10/100] training 45.1% loss=0.55665, acc=0.78125
# [10/100] training 45.3% loss=0.40119, acc=0.79688
# [10/100] training 45.5% loss=0.21810, acc=0.90625
# [10/100] training 45.6% loss=0.26857, acc=0.92188
# [10/100] training 45.8% loss=0.34733, acc=0.87500
# [10/100] training 45.9% loss=0.34407, acc=0.89062
# [10/100] training 46.2% loss=0.30708, acc=0.89062
# [10/100] training 46.3% loss=0.24713, acc=0.90625
# [10/100] training 46.5% loss=0.45514, acc=0.82812
# [10/100] training 46.6% loss=0.31459, acc=0.87500
# [10/100] training 46.8% loss=0.34690, acc=0.87500
# [10/100] training 47.0% loss=0.41029, acc=0.81250
# [10/100] training 47.2% loss=0.34838, acc=0.79688
# [10/100] training 47.4% loss=0.31642, acc=0.84375
# [10/100] training 47.5% loss=0.38546, acc=0.82812
# [10/100] training 47.7% loss=0.29045, acc=0.85938
# [10/100] training 47.8% loss=0.37198, acc=0.84375
# [10/100] training 48.0% loss=0.47336, acc=0.78125
# [10/100] training 48.3% loss=0.16525, acc=0.93750
# [10/100] training 48.4% loss=0.16713, acc=0.93750
# [10/100] training 48.6% loss=0.28355, acc=0.89062
# [10/100] training 48.7% loss=0.45872, acc=0.82812
# [10/100] training 48.9% loss=0.54536, acc=0.75000
# [10/100] training 49.0% loss=0.37741, acc=0.81250
# [10/100] training 49.2% loss=0.47865, acc=0.75000
# [10/100] training 49.3% loss=0.35573, acc=0.85938
# [10/100] training 49.6% loss=0.36162, acc=0.81250
# [10/100] training 49.8% loss=0.35694, acc=0.78125
# [10/100] training 49.9% loss=0.32296, acc=0.82812
# [10/100] training 50.1% loss=0.28837, acc=0.89062
# [10/100] training 50.2% loss=0.37194, acc=0.79688
# [10/100] training 50.4% loss=0.43744, acc=0.84375
# [10/100] training 50.6% loss=0.45620, acc=0.76562
# [10/100] training 50.8% loss=0.42035, acc=0.79688
# [10/100] training 51.0% loss=0.43212, acc=0.82812
# [10/100] training 51.1% loss=0.32834, acc=0.85938
# [10/100] training 51.3% loss=0.45474, acc=0.76562
# [10/100] training 51.4% loss=0.31566, acc=0.87500
# [10/100] training 51.7% loss=0.38046, acc=0.81250
# [10/100] training 51.8% loss=0.38100, acc=0.82812
# [10/100] training 52.0% loss=0.31638, acc=0.85938
# [10/100] training 52.1% loss=0.45491, acc=0.81250
# [10/100] training 52.3% loss=0.39748, acc=0.82812
# [10/100] training 52.5% loss=0.28900, acc=0.87500
# [10/100] training 52.6% loss=0.32254, acc=0.87500
# [10/100] training 52.9% loss=0.46843, acc=0.79688
# [10/100] training 53.0% loss=0.39726, acc=0.79688
# [10/100] training 53.2% loss=0.39518, acc=0.85938
# [10/100] training 53.3% loss=0.22141, acc=0.95312
# [10/100] training 53.5% loss=0.39978, acc=0.81250
# [10/100] training 53.7% loss=0.33720, acc=0.90625
# [10/100] training 53.8% loss=0.41795, acc=0.78125
# [10/100] training 54.1% loss=0.43839, acc=0.85938
# [10/100] training 54.2% loss=0.37941, acc=0.79688
# [10/100] training 54.4% loss=0.35569, acc=0.90625
# [10/100] training 54.5% loss=0.44533, acc=0.81250
# [10/100] training 54.7% loss=0.38962, acc=0.79688
# [10/100] training 54.8% loss=0.22244, acc=0.87500
# [10/100] training 55.1% loss=0.24686, acc=0.90625
# [10/100] training 55.3% loss=0.21463, acc=0.90625
# [10/100] training 55.4% loss=0.35446, acc=0.81250
# [10/100] training 55.6% loss=0.32970, acc=0.84375
# [10/100] training 55.7% loss=0.28128, acc=0.85938
# [10/100] training 55.9% loss=0.26577, acc=0.87500
# [10/100] training 56.0% loss=0.32733, acc=0.82812
# [10/100] training 56.3% loss=0.54873, acc=0.73438
# [10/100] training 56.5% loss=0.26913, acc=0.92188
# [10/100] training 56.6% loss=0.37501, acc=0.82812
# [10/100] training 56.8% loss=0.39339, acc=0.84375
# [10/100] training 56.9% loss=0.38933, acc=0.79688
# [10/100] training 57.1% loss=0.36700, acc=0.90625
# [10/100] training 57.2% loss=0.25888, acc=0.90625
# [10/100] training 57.5% loss=0.29588, acc=0.87500
# [10/100] training 57.6% loss=0.41138, acc=0.79688
# [10/100] training 57.8% loss=0.34274, acc=0.84375
# [10/100] training 58.0% loss=0.26279, acc=0.89062
# [10/100] training 58.1% loss=0.31043, acc=0.87500
# [10/100] training 58.3% loss=0.24701, acc=0.82812
# [10/100] training 58.4% loss=0.34376, acc=0.89062
# [10/100] training 58.7% loss=0.40768, acc=0.84375
# [10/100] training 58.8% loss=0.38834, acc=0.84375
# [10/100] training 59.0% loss=0.30913, acc=0.87500
# [10/100] training 59.2% loss=0.45097, acc=0.78125
# [10/100] training 59.3% loss=0.34361, acc=0.84375
# [10/100] training 59.5% loss=0.37692, acc=0.81250
# [10/100] training 59.7% loss=0.46736, acc=0.76562
# [10/100] training 59.9% loss=0.26066, acc=0.92188
# [10/100] training 60.0% loss=0.25903, acc=0.92188
# [10/100] training 60.2% loss=0.45865, acc=0.73438
# [10/100] training 60.3% loss=0.30664, acc=0.84375
# [10/100] training 60.5% loss=0.25058, acc=0.85938
# [10/100] training 60.8% loss=0.43822, acc=0.81250
# [10/100] training 60.9% loss=0.33472, acc=0.84375
# [10/100] training 61.1% loss=0.55745, acc=0.71875
# [10/100] training 61.2% loss=0.34675, acc=0.81250
# [10/100] training 61.4% loss=0.36192, acc=0.85938
# [10/100] training 61.5% loss=0.47420, acc=0.78125
# [10/100] training 61.7% loss=0.47775, acc=0.78125
# [10/100] training 62.0% loss=0.37639, acc=0.84375
# [10/100] training 62.1% loss=0.40988, acc=0.79688
# [10/100] training 62.3% loss=0.24937, acc=0.92188
# [10/100] training 62.4% loss=0.29497, acc=0.84375
# [10/100] training 62.6% loss=0.47629, acc=0.75000
# [10/100] training 62.7% loss=0.29857, acc=0.85938
# [10/100] training 62.9% loss=0.40113, acc=0.82812
# [10/100] training 63.1% loss=0.23913, acc=0.92188
# [10/100] training 63.3% loss=0.48611, acc=0.79688
# [10/100] training 63.5% loss=0.36497, acc=0.79688
# [10/100] training 63.6% loss=0.35530, acc=0.81250
# [10/100] training 63.8% loss=0.42812, acc=0.82812
# [10/100] training 63.9% loss=0.32660, acc=0.85938
# [10/100] training 64.2% loss=0.30760, acc=0.82812
# [10/100] training 64.3% loss=0.35811, acc=0.82812
# [10/100] training 64.5% loss=0.42639, acc=0.82812
# [10/100] training 64.7% loss=0.34895, acc=0.79688
# [10/100] training 64.8% loss=0.62504, acc=0.73438
# [10/100] training 65.0% loss=0.37169, acc=0.85938
# [10/100] training 65.1% loss=0.45309, acc=0.81250
# [10/100] training 65.4% loss=0.34888, acc=0.87500
# [10/100] training 65.5% loss=0.26678, acc=0.93750
# [10/100] training 65.7% loss=0.26780, acc=0.90625
# [10/100] training 65.8% loss=0.41822, acc=0.82812
# [10/100] training 66.0% loss=0.36895, acc=0.81250
# [10/100] training 66.2% loss=0.24469, acc=0.92188
# [10/100] training 66.3% loss=0.41253, acc=0.84375
# [10/100] training 66.6% loss=0.38676, acc=0.81250
# [10/100] training 66.7% loss=0.19748, acc=0.92188
# [10/100] training 66.9% loss=0.34885, acc=0.85938
# [10/100] training 67.0% loss=0.42261, acc=0.81250
# [10/100] training 67.2% loss=0.19695, acc=0.96875
# [10/100] training 67.4% loss=0.32789, acc=0.89062
# [10/100] training 67.6% loss=0.28591, acc=0.89062
# [10/100] training 67.8% loss=0.31485, acc=0.90625
# [10/100] training 67.9% loss=0.19831, acc=0.93750
# [10/100] training 68.1% loss=0.23204, acc=0.90625
# [10/100] training 68.2% loss=0.28227, acc=0.85938
# [10/100] training 68.4% loss=0.23680, acc=0.90625
# [10/100] training 68.5% loss=0.50326, acc=0.76562
# [10/100] training 68.8% loss=0.29136, acc=0.84375
# [10/100] training 69.0% loss=0.46085, acc=0.76562
# [10/100] training 69.1% loss=0.24932, acc=0.92188
# [10/100] training 69.3% loss=0.23402, acc=0.90625
# [10/100] training 69.4% loss=0.26945, acc=0.87500
# [10/100] training 69.6% loss=0.34841, acc=0.85938
# [10/100] training 69.7% loss=0.31879, acc=0.89062
# [10/100] training 70.0% loss=0.38857, acc=0.82812
# [10/100] training 70.2% loss=0.35570, acc=0.82812
# [10/100] training 70.3% loss=0.30171, acc=0.87500
# [10/100] training 70.5% loss=0.36025, acc=0.82812
# [10/100] training 70.6% loss=0.31392, acc=0.87500
# [10/100] training 70.8% loss=0.34586, acc=0.82812
# [10/100] training 71.0% loss=0.39339, acc=0.79688
# [10/100] training 71.2% loss=0.25622, acc=0.89062
# [10/100] training 71.3% loss=0.29540, acc=0.84375
# [10/100] training 71.5% loss=0.44617, acc=0.84375
# [10/100] training 71.7% loss=0.41936, acc=0.81250
# [10/100] training 71.8% loss=0.32895, acc=0.87500
# [10/100] training 72.0% loss=0.20643, acc=0.93750
# [10/100] training 72.2% loss=0.26908, acc=0.90625
# [10/100] training 72.4% loss=0.43394, acc=0.81250
# [10/100] training 72.5% loss=0.50192, acc=0.84375
# [10/100] training 72.7% loss=0.51739, acc=0.78125
# [10/100] training 72.9% loss=0.32206, acc=0.85938
# [10/100] training 73.0% loss=0.24206, acc=0.92188
# [10/100] training 73.3% loss=0.34253, acc=0.84375
# [10/100] training 73.4% loss=0.25242, acc=0.89062
# [10/100] training 73.6% loss=0.23254, acc=0.93750
# [10/100] training 73.7% loss=0.20639, acc=0.93750
# [10/100] training 73.9% loss=0.33012, acc=0.89062
# [10/100] training 74.0% loss=0.48500, acc=0.82812
# [10/100] training 74.2% loss=0.41243, acc=0.84375
# [10/100] training 74.5% loss=0.35217, acc=0.85938
# [10/100] training 74.6% loss=0.45670, acc=0.76562
# [10/100] training 74.8% loss=0.44943, acc=0.81250
# [10/100] training 74.9% loss=0.43253, acc=0.85938
# [10/100] training 75.1% loss=0.32529, acc=0.82812
# [10/100] training 75.2% loss=0.28511, acc=0.85938
# [10/100] training 75.4% loss=0.31193, acc=0.85938
# [10/100] training 75.7% loss=0.36713, acc=0.82812
# [10/100] training 75.8% loss=0.43826, acc=0.79688
# [10/100] training 76.0% loss=0.17754, acc=0.93750
# [10/100] training 76.1% loss=0.36184, acc=0.79688
# [10/100] training 76.3% loss=0.26651, acc=0.90625
# [10/100] training 76.4% loss=0.31491, acc=0.87500
# [10/100] training 76.7% loss=0.30230, acc=0.87500
# [10/100] training 76.8% loss=0.22926, acc=0.92188
# [10/100] training 77.0% loss=0.26175, acc=0.89062
# [10/100] training 77.2% loss=0.44771, acc=0.82812
# [10/100] training 77.3% loss=0.22254, acc=0.89062
# [10/100] training 77.5% loss=0.31489, acc=0.85938
# [10/100] training 77.6% loss=0.37353, acc=0.79688
# [10/100] training 77.9% loss=0.25960, acc=0.95312
# [10/100] training 78.0% loss=0.31739, acc=0.87500
# [10/100] training 78.2% loss=0.42036, acc=0.82812
# [10/100] training 78.4% loss=0.21950, acc=0.90625
# [10/100] training 78.5% loss=0.36734, acc=0.90625
# [10/100] training 78.7% loss=0.28886, acc=0.90625
# [10/100] training 78.8% loss=0.31178, acc=0.84375
# [10/100] training 79.1% loss=0.28252, acc=0.84375
# [10/100] training 79.2% loss=0.28247, acc=0.89062
# [10/100] training 79.4% loss=0.43862, acc=0.82812
# [10/100] training 79.5% loss=0.33213, acc=0.84375
# [10/100] training 79.7% loss=0.22399, acc=0.95312
# [10/100] training 79.9% loss=0.32933, acc=0.89062
# [10/100] training 80.1% loss=0.29407, acc=0.85938
# [10/100] training 80.3% loss=0.45361, acc=0.84375
# [10/100] training 80.4% loss=0.46245, acc=0.76562
# [10/100] training 80.6% loss=0.34239, acc=0.89062
# [10/100] training 80.7% loss=0.28047, acc=0.90625
# [10/100] training 80.9% loss=0.42421, acc=0.84375
# [10/100] training 81.2% loss=0.36206, acc=0.84375
# [10/100] training 81.3% loss=0.42540, acc=0.79688
# [10/100] training 81.5% loss=0.28191, acc=0.92188
# [10/100] training 81.6% loss=0.37344, acc=0.82812
# [10/100] training 81.8% loss=0.27781, acc=0.89062
# [10/100] training 81.9% loss=0.50959, acc=0.85938
# [10/100] training 82.1% loss=0.25807, acc=0.84375
# [10/100] training 82.2% loss=0.46674, acc=0.81250
# [10/100] training 82.5% loss=0.29205, acc=0.89062
# [10/100] training 82.7% loss=0.33558, acc=0.87500
# [10/100] training 82.8% loss=0.27589, acc=0.89062
# [10/100] training 83.0% loss=0.34333, acc=0.84375
# [10/100] training 83.1% loss=0.33618, acc=0.87500
# [10/100] training 83.3% loss=0.30726, acc=0.85938
# [10/100] training 83.5% loss=0.33706, acc=0.84375
# [10/100] training 83.7% loss=0.40653, acc=0.85938
# [10/100] training 83.9% loss=0.38386, acc=0.78125
# [10/100] training 84.0% loss=0.24358, acc=0.89062
# [10/100] training 84.2% loss=0.25383, acc=0.90625
# [10/100] training 84.3% loss=0.29591, acc=0.87500
# [10/100] training 84.5% loss=0.34154, acc=0.84375
# [10/100] training 84.7% loss=0.33895, acc=0.81250
# [10/100] training 84.9% loss=0.31411, acc=0.87500
# [10/100] training 85.0% loss=0.27323, acc=0.87500
# [10/100] training 85.2% loss=0.34728, acc=0.82812
# [10/100] training 85.4% loss=0.29045, acc=0.93750
# [10/100] training 85.5% loss=0.27173, acc=0.87500
# [10/100] training 85.8% loss=0.21945, acc=0.85938
# [10/100] training 85.9% loss=0.36346, acc=0.84375
# [10/100] training 86.1% loss=0.30505, acc=0.85938
# [10/100] training 86.2% loss=0.17537, acc=0.93750
# [10/100] training 86.4% loss=0.50540, acc=0.78125
# [10/100] training 86.6% loss=0.40371, acc=0.81250
# [10/100] training 86.7% loss=0.31281, acc=0.90625
# [10/100] training 87.0% loss=0.28129, acc=0.90625
# [10/100] training 87.1% loss=0.25172, acc=0.89062
# [10/100] training 87.3% loss=0.37484, acc=0.82812
# [10/100] training 87.4% loss=0.38170, acc=0.81250
# [10/100] training 87.6% loss=0.26990, acc=0.90625
# [10/100] training 87.7% loss=0.32557, acc=0.87500
# [10/100] training 87.9% loss=0.30223, acc=0.87500
# [10/100] training 88.2% loss=0.20696, acc=0.87500
# [10/100] training 88.3% loss=0.37974, acc=0.84375
# [10/100] training 88.5% loss=0.25100, acc=0.92188
# [10/100] training 88.6% loss=0.18247, acc=0.92188
# [10/100] training 88.8% loss=0.26966, acc=0.87500
# [10/100] training 88.9% loss=0.38374, acc=0.81250
# [10/100] training 89.2% loss=0.20892, acc=0.92188
# [10/100] training 89.4% loss=0.29332, acc=0.89062
# [10/100] training 89.5% loss=0.44977, acc=0.76562
# [10/100] training 89.7% loss=0.41976, acc=0.79688
# [10/100] training 89.8% loss=0.34628, acc=0.81250
# [10/100] training 90.0% loss=0.31251, acc=0.87500
# [10/100] training 90.1% loss=0.33643, acc=0.84375
# [10/100] training 90.4% loss=0.29342, acc=0.92188
# [10/100] training 90.5% loss=0.20878, acc=0.93750
# [10/100] training 90.7% loss=0.26029, acc=0.89062
# [10/100] training 90.9% loss=0.29109, acc=0.92188
# [10/100] training 91.0% loss=0.22497, acc=0.93750
# [10/100] training 91.2% loss=0.33756, acc=0.81250
# [10/100] training 91.3% loss=0.50576, acc=0.85938
# [10/100] training 91.6% loss=0.40557, acc=0.82812
# [10/100] training 91.7% loss=0.29606, acc=0.87500
# [10/100] training 91.9% loss=0.45460, acc=0.79688
# [10/100] training 92.1% loss=0.27367, acc=0.90625
# [10/100] training 92.2% loss=0.21792, acc=0.90625
# [10/100] training 92.4% loss=0.30968, acc=0.87500
# [10/100] training 92.6% loss=0.36855, acc=0.81250
# [10/100] training 92.8% loss=0.49555, acc=0.79688
# [10/100] training 92.9% loss=0.37855, acc=0.81250
# [10/100] training 93.1% loss=0.34125, acc=0.84375
# [10/100] training 93.2% loss=0.32327, acc=0.84375
# [10/100] training 93.4% loss=0.24010, acc=0.92188
# [10/100] training 93.7% loss=0.24332, acc=0.89062
# [10/100] training 93.8% loss=0.28287, acc=0.87500
# [10/100] training 94.0% loss=0.26888, acc=0.87500
# [10/100] training 94.1% loss=0.41308, acc=0.84375
# [10/100] training 94.3% loss=0.23018, acc=0.93750
# [10/100] training 94.4% loss=0.37153, acc=0.87500
# [10/100] training 94.6% loss=0.24264, acc=0.93750
# [10/100] training 94.9% loss=0.25547, acc=0.89062
# [10/100] training 95.0% loss=0.41970, acc=0.84375
# [10/100] training 95.2% loss=0.43853, acc=0.85938
# [10/100] training 95.3% loss=0.24029, acc=0.89062
# [10/100] training 95.5% loss=0.26240, acc=0.89062
# [10/100] training 95.6% loss=0.40628, acc=0.87500
# [10/100] training 95.8% loss=0.36091, acc=0.84375
# [10/100] training 96.0% loss=0.32026, acc=0.89062
# [10/100] training 96.2% loss=0.24616, acc=0.89062
# [10/100] training 96.4% loss=0.30140, acc=0.87500
# [10/100] training 96.5% loss=0.36728, acc=0.81250
# [10/100] training 96.7% loss=0.33936, acc=0.84375
# [10/100] training 96.8% loss=0.32550, acc=0.87500
# [10/100] training 97.1% loss=0.33578, acc=0.84375
# [10/100] training 97.2% loss=0.34616, acc=0.85938
# [10/100] training 97.4% loss=0.30915, acc=0.89062
# [10/100] training 97.6% loss=0.29241, acc=0.82812
# [10/100] training 97.7% loss=0.18110, acc=0.93750
# [10/100] training 97.9% loss=0.21262, acc=0.90625
# [10/100] training 98.0% loss=0.38458, acc=0.82812
# [10/100] training 98.3% loss=0.30251, acc=0.89062
# [10/100] training 98.4% loss=0.23626, acc=0.90625
# [10/100] training 98.6% loss=0.51091, acc=0.79688
# [10/100] training 98.7% loss=0.35907, acc=0.84375
# [10/100] training 98.9% loss=0.23720, acc=0.92188
# [10/100] training 99.1% loss=0.18782, acc=0.93750
# [10/100] training 99.2% loss=0.32427, acc=0.84375
# [10/100] training 99.5% loss=0.48740, acc=0.81250
# [10/100] training 99.6% loss=0.28755, acc=0.85938
# [10/100] training 99.8% loss=0.26128, acc=0.89062
# [10/100] training 99.9% loss=0.16860, acc=0.90625
# [10/100] testing 0.9% loss=0.22537, acc=0.89062
# [10/100] testing 1.8% loss=0.52049, acc=0.79688
# [10/100] testing 2.2% loss=0.27348, acc=0.87500
# [10/100] testing 3.1% loss=0.27952, acc=0.82812
# [10/100] testing 3.5% loss=0.34445, acc=0.85938
# [10/100] testing 4.4% loss=0.30333, acc=0.90625
# [10/100] testing 4.8% loss=0.45860, acc=0.78125
# [10/100] testing 5.7% loss=0.37485, acc=0.81250
# [10/100] testing 6.6% loss=0.31598, acc=0.87500
# [10/100] testing 7.0% loss=0.24708, acc=0.90625
# [10/100] testing 7.9% loss=0.48541, acc=0.78125
# [10/100] testing 8.3% loss=0.21923, acc=0.92188
# [10/100] testing 9.2% loss=0.35751, acc=0.84375
# [10/100] testing 9.7% loss=0.28366, acc=0.82812
# [10/100] testing 10.5% loss=0.57076, acc=0.75000
# [10/100] testing 11.0% loss=0.38105, acc=0.84375
# [10/100] testing 11.8% loss=0.41074, acc=0.76562
# [10/100] testing 12.7% loss=0.61611, acc=0.78125
# [10/100] testing 13.2% loss=0.30842, acc=0.89062
# [10/100] testing 14.0% loss=0.49258, acc=0.81250
# [10/100] testing 14.5% loss=0.43464, acc=0.82812
# [10/100] testing 15.4% loss=0.38875, acc=0.81250
# [10/100] testing 15.8% loss=0.26297, acc=0.87500
# [10/100] testing 16.7% loss=0.30461, acc=0.90625
# [10/100] testing 17.5% loss=0.26902, acc=0.89062
# [10/100] testing 18.0% loss=0.31631, acc=0.87500
# [10/100] testing 18.9% loss=0.19952, acc=0.93750
# [10/100] testing 19.3% loss=0.28155, acc=0.90625
# [10/100] testing 20.2% loss=0.39805, acc=0.81250
# [10/100] testing 20.6% loss=0.38578, acc=0.85938
# [10/100] testing 21.5% loss=0.33396, acc=0.87500
# [10/100] testing 21.9% loss=0.52495, acc=0.73438
# [10/100] testing 22.8% loss=0.43182, acc=0.85938
# [10/100] testing 23.7% loss=0.33581, acc=0.89062
# [10/100] testing 24.1% loss=0.26795, acc=0.89062
# [10/100] testing 25.0% loss=0.41230, acc=0.82812
# [10/100] testing 25.4% loss=0.25290, acc=0.90625
# [10/100] testing 26.3% loss=0.44675, acc=0.78125
# [10/100] testing 26.8% loss=0.40894, acc=0.84375
# [10/100] testing 27.6% loss=0.37562, acc=0.84375
# [10/100] testing 28.5% loss=0.40970, acc=0.87500
# [10/100] testing 29.0% loss=0.26282, acc=0.87500
# [10/100] testing 29.8% loss=0.44649, acc=0.85938
# [10/100] testing 30.3% loss=0.35686, acc=0.87500
# [10/100] testing 31.1% loss=0.46981, acc=0.78125
# [10/100] testing 31.6% loss=0.38836, acc=0.84375
# [10/100] testing 32.5% loss=0.38899, acc=0.82812
# [10/100] testing 32.9% loss=0.41178, acc=0.82812
# [10/100] testing 33.8% loss=0.45527, acc=0.79688
# [10/100] testing 34.7% loss=0.36132, acc=0.85938
# [10/100] testing 35.1% loss=0.41225, acc=0.81250
# [10/100] testing 36.0% loss=0.38873, acc=0.81250
# [10/100] testing 36.4% loss=0.37782, acc=0.84375
# [10/100] testing 37.3% loss=0.49895, acc=0.79688
# [10/100] testing 37.7% loss=0.49462, acc=0.76562
# [10/100] testing 38.6% loss=0.26311, acc=0.90625
# [10/100] testing 39.5% loss=0.52180, acc=0.81250
# [10/100] testing 39.9% loss=0.34727, acc=0.85938
# [10/100] testing 40.8% loss=0.35356, acc=0.84375
# [10/100] testing 41.2% loss=0.30290, acc=0.92188
# [10/100] testing 42.1% loss=0.39142, acc=0.84375
# [10/100] testing 42.5% loss=0.34218, acc=0.82812
# [10/100] testing 43.4% loss=0.41447, acc=0.85938
# [10/100] testing 43.9% loss=0.33133, acc=0.84375
# [10/100] testing 44.7% loss=0.57164, acc=0.79688
# [10/100] testing 45.6% loss=0.49414, acc=0.82812
# [10/100] testing 46.1% loss=0.28929, acc=0.87500
# [10/100] testing 46.9% loss=0.36097, acc=0.84375
# [10/100] testing 47.4% loss=0.24252, acc=0.89062
# [10/100] testing 48.3% loss=0.39849, acc=0.84375
# [10/100] testing 48.7% loss=0.52661, acc=0.75000
# [10/100] testing 49.6% loss=0.45942, acc=0.81250
# [10/100] testing 50.4% loss=0.25809, acc=0.90625
# [10/100] testing 50.9% loss=0.41526, acc=0.79688
# [10/100] testing 51.8% loss=0.43512, acc=0.81250
# [10/100] testing 52.2% loss=0.36892, acc=0.89062
# [10/100] testing 53.1% loss=0.30453, acc=0.89062
# [10/100] testing 53.5% loss=0.33878, acc=0.90625
# [10/100] testing 54.4% loss=0.56814, acc=0.76562
# [10/100] testing 54.8% loss=0.46777, acc=0.84375
# [10/100] testing 55.7% loss=0.22384, acc=0.89062
# [10/100] testing 56.6% loss=0.41853, acc=0.85938
# [10/100] testing 57.0% loss=0.56194, acc=0.76562
# [10/100] testing 57.9% loss=0.44675, acc=0.76562
# [10/100] testing 58.3% loss=0.52518, acc=0.71875
# [10/100] testing 59.2% loss=0.37620, acc=0.85938
# [10/100] testing 59.7% loss=0.37704, acc=0.87500
# [10/100] testing 60.5% loss=0.33354, acc=0.87500
# [10/100] testing 61.4% loss=0.25851, acc=0.87500
# [10/100] testing 61.9% loss=0.38336, acc=0.87500
# [10/100] testing 62.7% loss=0.32989, acc=0.85938
# [10/100] testing 63.2% loss=0.50507, acc=0.79688
# [10/100] testing 64.0% loss=0.51697, acc=0.76562
# [10/100] testing 64.5% loss=0.29835, acc=0.87500
# [10/100] testing 65.4% loss=0.35002, acc=0.81250
# [10/100] testing 65.8% loss=0.43448, acc=0.82812
# [10/100] testing 66.7% loss=0.38655, acc=0.82812
# [10/100] testing 67.6% loss=0.41000, acc=0.81250
# [10/100] testing 68.0% loss=0.38510, acc=0.82812
# [10/100] testing 68.9% loss=0.36614, acc=0.84375
# [10/100] testing 69.3% loss=0.46783, acc=0.78125
# [10/100] testing 70.2% loss=0.41716, acc=0.79688
# [10/100] testing 70.6% loss=0.50029, acc=0.76562
# [10/100] testing 71.5% loss=0.49012, acc=0.87500
# [10/100] testing 72.4% loss=0.34123, acc=0.87500
# [10/100] testing 72.8% loss=0.32993, acc=0.79688
# [10/100] testing 73.7% loss=0.31739, acc=0.87500
# [10/100] testing 74.1% loss=0.50041, acc=0.82812
# [10/100] testing 75.0% loss=0.35543, acc=0.84375
# [10/100] testing 75.4% loss=0.33839, acc=0.84375
# [10/100] testing 76.3% loss=0.13317, acc=0.95312
# [10/100] testing 76.8% loss=0.34409, acc=0.85938
# [10/100] testing 77.6% loss=0.36608, acc=0.85938
# [10/100] testing 78.5% loss=0.63991, acc=0.76562
# [10/100] testing 79.0% loss=0.47185, acc=0.76562
# [10/100] testing 79.8% loss=0.37799, acc=0.78125
# [10/100] testing 80.3% loss=0.37347, acc=0.87500
# [10/100] testing 81.2% loss=0.51930, acc=0.79688
# [10/100] testing 81.6% loss=0.31853, acc=0.84375
# [10/100] testing 82.5% loss=0.33222, acc=0.85938
# [10/100] testing 83.3% loss=0.26578, acc=0.89062
# [10/100] testing 83.8% loss=0.22480, acc=0.92188
# [10/100] testing 84.7% loss=0.34438, acc=0.82812
# [10/100] testing 85.1% loss=0.34241, acc=0.87500
# [10/100] testing 86.0% loss=0.36878, acc=0.89062
# [10/100] testing 86.4% loss=0.49624, acc=0.75000
# [10/100] testing 87.3% loss=0.46602, acc=0.81250
# [10/100] testing 87.7% loss=0.27479, acc=0.89062
# [10/100] testing 88.6% loss=0.29082, acc=0.85938
# [10/100] testing 89.5% loss=0.56282, acc=0.70312
# [10/100] testing 89.9% loss=0.32843, acc=0.82812
# [10/100] testing 90.8% loss=0.23972, acc=0.92188
# [10/100] testing 91.2% loss=0.19944, acc=0.89062
# [10/100] testing 92.1% loss=0.50454, acc=0.78125
# [10/100] testing 92.6% loss=0.27981, acc=0.89062
# [10/100] testing 93.4% loss=0.52804, acc=0.78125
# [10/100] testing 94.3% loss=0.24439, acc=0.87500
# [10/100] testing 94.7% loss=0.32811, acc=0.85938
# [10/100] testing 95.6% loss=0.40604, acc=0.82812
# [10/100] testing 96.1% loss=0.30444, acc=0.84375
# [10/100] testing 96.9% loss=0.38419, acc=0.78125
# [10/100] testing 97.4% loss=0.24686, acc=0.89062
# [10/100] testing 98.3% loss=0.41902, acc=0.82812
# [10/100] testing 98.7% loss=0.39615, acc=0.84375
# [10/100] testing 99.6% loss=0.33800, acc=0.87500
# [11/100] training 0.2% loss=0.47868, acc=0.81250
# [11/100] training 0.4% loss=0.53368, acc=0.71875
# [11/100] training 0.5% loss=0.33192, acc=0.85938
# [11/100] training 0.8% loss=0.37039, acc=0.84375
# [11/100] training 0.9% loss=0.32205, acc=0.84375
# [11/100] training 1.1% loss=0.35966, acc=0.89062
# [11/100] training 1.2% loss=0.36869, acc=0.81250
# [11/100] training 1.4% loss=0.43082, acc=0.81250
# [11/100] training 1.6% loss=0.22101, acc=0.87500
# [11/100] training 1.8% loss=0.32089, acc=0.89062
# [11/100] training 2.0% loss=0.34226, acc=0.85938
# [11/100] training 2.1% loss=0.35400, acc=0.84375
# [11/100] training 2.3% loss=0.28043, acc=0.90625
# [11/100] training 2.4% loss=0.38596, acc=0.85938
# [11/100] training 2.6% loss=0.32377, acc=0.89062
# [11/100] training 2.7% loss=0.34526, acc=0.87500
# [11/100] training 3.0% loss=0.37009, acc=0.85938
# [11/100] training 3.2% loss=0.20192, acc=0.95312
# [11/100] training 3.3% loss=0.39867, acc=0.85938
# [11/100] training 3.5% loss=0.35505, acc=0.85938
# [11/100] training 3.6% loss=0.67772, acc=0.75000
# [11/100] training 3.8% loss=0.28346, acc=0.84375
# [11/100] training 3.9% loss=0.38058, acc=0.81250
# [11/100] training 4.2% loss=0.39943, acc=0.79688
# [11/100] training 4.4% loss=0.31017, acc=0.87500
# [11/100] training 4.5% loss=0.36712, acc=0.82812
# [11/100] training 4.7% loss=0.40902, acc=0.79688
# [11/100] training 4.8% loss=0.39311, acc=0.81250
# [11/100] training 5.0% loss=0.26119, acc=0.90625
# [11/100] training 5.2% loss=0.42314, acc=0.82812
# [11/100] training 5.4% loss=0.23278, acc=0.92188
# [11/100] training 5.5% loss=0.36685, acc=0.87500
# [11/100] training 5.7% loss=0.28200, acc=0.85938
# [11/100] training 5.9% loss=0.41857, acc=0.79688
# [11/100] training 6.0% loss=0.33301, acc=0.89062
# [11/100] training 6.3% loss=0.34303, acc=0.84375
# [11/100] training 6.4% loss=0.25334, acc=0.90625
# [11/100] training 6.6% loss=0.33699, acc=0.89062
# [11/100] training 6.7% loss=0.42533, acc=0.81250
# [11/100] training 6.9% loss=0.22029, acc=0.89062
# [11/100] training 7.1% loss=0.33702, acc=0.81250
# [11/100] training 7.2% loss=0.31123, acc=0.85938
# [11/100] training 7.5% loss=0.29481, acc=0.87500
# [11/100] training 7.6% loss=0.29566, acc=0.87500
# [11/100] training 7.8% loss=0.33642, acc=0.82812
# [11/100] training 7.9% loss=0.42804, acc=0.79688
# [11/100] training 8.1% loss=0.25449, acc=0.90625
# [11/100] training 8.2% loss=0.26797, acc=0.89062
# [11/100] training 8.4% loss=0.34293, acc=0.84375
# [11/100] training 8.7% loss=0.29739, acc=0.85938
# [11/100] training 8.8% loss=0.28085, acc=0.89062
# [11/100] training 9.0% loss=0.34979, acc=0.84375
# [11/100] training 9.1% loss=0.36871, acc=0.84375
# [11/100] training 9.3% loss=0.53109, acc=0.78125
# [11/100] training 9.4% loss=0.31957, acc=0.87500
# [11/100] training 9.7% loss=0.33119, acc=0.82812
# [11/100] training 9.9% loss=0.36925, acc=0.82812
# [11/100] training 10.0% loss=0.31262, acc=0.84375
# [11/100] training 10.2% loss=0.33133, acc=0.87500
# [11/100] training 10.3% loss=0.25761, acc=0.90625
# [11/100] training 10.5% loss=0.33150, acc=0.84375
# [11/100] training 10.6% loss=0.30215, acc=0.89062
# [11/100] training 10.9% loss=0.24695, acc=0.89062
# [11/100] training 11.0% loss=0.36201, acc=0.82812
# [11/100] training 11.2% loss=0.29948, acc=0.89062
# [11/100] training 11.4% loss=0.44763, acc=0.75000
# [11/100] training 11.5% loss=0.49363, acc=0.79688
# [11/100] training 11.7% loss=0.20702, acc=0.95312
# [11/100] training 11.8% loss=0.29533, acc=0.87500
# [11/100] training 12.1% loss=0.34008, acc=0.84375
# [11/100] training 12.2% loss=0.30715, acc=0.85938
# [11/100] training 12.4% loss=0.32987, acc=0.85938
# [11/100] training 12.6% loss=0.31596, acc=0.87500
# [11/100] training 12.7% loss=0.40354, acc=0.81250
# [11/100] training 12.9% loss=0.34109, acc=0.87500
# [11/100] training 13.0% loss=0.33375, acc=0.89062
# [11/100] training 13.3% loss=0.26139, acc=0.92188
# [11/100] training 13.4% loss=0.37351, acc=0.81250
# [11/100] training 13.6% loss=0.26715, acc=0.92188
# [11/100] training 13.7% loss=0.54493, acc=0.81250
# [11/100] training 13.9% loss=0.36757, acc=0.85938
# [11/100] training 14.1% loss=0.31222, acc=0.87500
# [11/100] training 14.3% loss=0.32796, acc=0.87500
# [11/100] training 14.5% loss=0.28161, acc=0.84375
# [11/100] training 14.6% loss=0.24594, acc=0.90625
# [11/100] training 14.8% loss=0.30376, acc=0.85938
# [11/100] training 14.9% loss=0.31087, acc=0.82812
# [11/100] training 15.1% loss=0.34037, acc=0.85938
# [11/100] training 15.4% loss=0.41953, acc=0.82812
# [11/100] training 15.5% loss=0.35086, acc=0.81250
# [11/100] training 15.7% loss=0.49017, acc=0.79688
# [11/100] training 15.8% loss=0.25442, acc=0.87500
# [11/100] training 16.0% loss=0.38908, acc=0.81250
# [11/100] training 16.1% loss=0.47517, acc=0.71875
# [11/100] training 16.3% loss=0.42489, acc=0.81250
# [11/100] training 16.4% loss=0.34129, acc=0.85938
# [11/100] training 16.7% loss=0.34362, acc=0.79688
# [11/100] training 16.9% loss=0.41927, acc=0.82812
# [11/100] training 17.0% loss=0.35801, acc=0.82812
# [11/100] training 17.2% loss=0.27671, acc=0.84375
# [11/100] training 17.3% loss=0.29716, acc=0.89062
# [11/100] training 17.5% loss=0.29231, acc=0.87500
# [11/100] training 17.7% loss=0.33902, acc=0.82812
# [11/100] training 17.9% loss=0.40979, acc=0.84375
# [11/100] training 18.1% loss=0.36782, acc=0.87500
# [11/100] training 18.2% loss=0.34554, acc=0.82812
# [11/100] training 18.4% loss=0.41104, acc=0.82812
# [11/100] training 18.5% loss=0.36560, acc=0.85938
# [11/100] training 18.8% loss=0.26115, acc=0.87500
# [11/100] training 18.9% loss=0.23494, acc=0.90625
# [11/100] training 19.1% loss=0.41736, acc=0.84375
# [11/100] training 19.2% loss=0.24984, acc=0.89062
# [11/100] training 19.4% loss=0.17650, acc=0.93750
# [11/100] training 19.6% loss=0.41326, acc=0.81250
# [11/100] training 19.7% loss=0.37962, acc=0.82812
# [11/100] training 20.0% loss=0.24590, acc=0.89062
# [11/100] training 20.1% loss=0.27064, acc=0.89062
# [11/100] training 20.3% loss=0.38074, acc=0.85938
# [11/100] training 20.4% loss=0.44512, acc=0.78125
# [11/100] training 20.6% loss=0.37344, acc=0.82812
# [11/100] training 20.8% loss=0.32170, acc=0.84375
# [11/100] training 20.9% loss=0.29173, acc=0.90625
# [11/100] training 21.2% loss=0.31067, acc=0.85938
# [11/100] training 21.3% loss=0.33965, acc=0.84375
# [11/100] training 21.5% loss=0.31927, acc=0.87500
# [11/100] training 21.6% loss=0.23544, acc=0.93750
# [11/100] training 21.8% loss=0.21788, acc=0.89062
# [11/100] training 21.9% loss=0.38879, acc=0.82812
# [11/100] training 22.2% loss=0.29240, acc=0.85938
# [11/100] training 22.4% loss=0.45918, acc=0.82812
# [11/100] training 22.5% loss=0.24508, acc=0.93750
# [11/100] training 22.7% loss=0.31520, acc=0.87500
# [11/100] training 22.8% loss=0.40815, acc=0.82812
# [11/100] training 23.0% loss=0.18229, acc=0.93750
# [11/100] training 23.1% loss=0.35347, acc=0.82812
# [11/100] training 23.4% loss=0.31344, acc=0.85938
# [11/100] training 23.6% loss=0.31235, acc=0.89062
# [11/100] training 23.7% loss=0.34988, acc=0.85938
# [11/100] training 23.9% loss=0.22588, acc=0.90625
# [11/100] training 24.0% loss=0.40474, acc=0.78125
# [11/100] training 24.2% loss=0.35203, acc=0.85938
# [11/100] training 24.3% loss=0.32677, acc=0.85938
# [11/100] training 24.6% loss=0.31451, acc=0.84375
# [11/100] training 24.7% loss=0.47674, acc=0.75000
# [11/100] training 24.9% loss=0.34981, acc=0.85938
# [11/100] training 25.1% loss=0.36167, acc=0.82812
# [11/100] training 25.2% loss=0.21771, acc=0.96875
# [11/100] training 25.4% loss=0.36139, acc=0.87500
# [11/100] training 25.6% loss=0.35515, acc=0.81250
# [11/100] training 25.8% loss=0.40156, acc=0.81250
# [11/100] training 25.9% loss=0.29864, acc=0.87500
# [11/100] training 26.1% loss=0.39407, acc=0.81250
# [11/100] training 26.3% loss=0.30895, acc=0.90625
# [11/100] training 26.4% loss=0.26492, acc=0.87500
# [11/100] training 26.6% loss=0.26885, acc=0.85938
# [11/100] training 26.8% loss=0.32119, acc=0.85938
# [11/100] training 27.0% loss=0.36868, acc=0.87500
# [11/100] training 27.1% loss=0.24543, acc=0.92188
# [11/100] training 27.3% loss=0.30662, acc=0.79688
# [11/100] training 27.4% loss=0.18335, acc=0.90625
# [11/100] training 27.6% loss=0.34030, acc=0.89062
# [11/100] training 27.9% loss=0.35831, acc=0.84375
# [11/100] training 28.0% loss=0.43707, acc=0.79688
# [11/100] training 28.2% loss=0.27849, acc=0.87500
# [11/100] training 28.3% loss=0.22887, acc=0.93750
# [11/100] training 28.5% loss=0.37277, acc=0.89062
# [11/100] training 28.6% loss=0.35413, acc=0.89062
# [11/100] training 28.8% loss=0.21410, acc=0.92188
# [11/100] training 29.1% loss=0.35618, acc=0.85938
# [11/100] training 29.2% loss=0.24632, acc=0.89062
# [11/100] training 29.4% loss=0.34190, acc=0.78125
# [11/100] training 29.5% loss=0.29450, acc=0.90625
# [11/100] training 29.7% loss=0.37998, acc=0.84375
# [11/100] training 29.8% loss=0.34119, acc=0.85938
# [11/100] training 30.0% loss=0.41477, acc=0.78125
# [11/100] training 30.2% loss=0.22385, acc=0.89062
# [11/100] training 30.4% loss=0.27303, acc=0.90625
# [11/100] training 30.6% loss=0.42199, acc=0.87500
# [11/100] training 30.7% loss=0.27074, acc=0.85938
# [11/100] training 30.9% loss=0.31278, acc=0.89062
# [11/100] training 31.0% loss=0.32301, acc=0.87500
# [11/100] training 31.3% loss=0.34287, acc=0.87500
# [11/100] training 31.4% loss=0.46039, acc=0.73438
# [11/100] training 31.6% loss=0.42072, acc=0.81250
# [11/100] training 31.8% loss=0.37669, acc=0.78125
# [11/100] training 31.9% loss=0.32982, acc=0.90625
# [11/100] training 32.1% loss=0.37631, acc=0.85938
# [11/100] training 32.2% loss=0.34406, acc=0.87500
# [11/100] training 32.5% loss=0.28425, acc=0.85938
# [11/100] training 32.6% loss=0.29277, acc=0.85938
# [11/100] training 32.8% loss=0.22659, acc=0.90625
# [11/100] training 32.9% loss=0.34833, acc=0.84375
# [11/100] training 33.1% loss=0.41430, acc=0.82812
# [11/100] training 33.3% loss=0.30943, acc=0.84375
# [11/100] training 33.4% loss=0.33696, acc=0.89062
# [11/100] training 33.7% loss=0.36902, acc=0.85938
# [11/100] training 33.8% loss=0.43091, acc=0.79688
# [11/100] training 34.0% loss=0.38025, acc=0.82812
# [11/100] training 34.1% loss=0.31525, acc=0.89062
# [11/100] training 34.3% loss=0.30716, acc=0.87500
# [11/100] training 34.5% loss=0.39777, acc=0.81250
# [11/100] training 34.7% loss=0.22409, acc=0.90625
# [11/100] training 34.9% loss=0.28272, acc=0.85938
# [11/100] training 35.0% loss=0.24172, acc=0.90625
# [11/100] training 35.2% loss=0.39751, acc=0.81250
# [11/100] training 35.3% loss=0.30486, acc=0.84375
# [11/100] training 35.5% loss=0.41125, acc=0.79688
# [11/100] training 35.6% loss=0.28859, acc=0.85938
# [11/100] training 35.9% loss=0.34257, acc=0.79688
# [11/100] training 36.1% loss=0.37559, acc=0.84375
# [11/100] training 36.2% loss=0.35923, acc=0.85938
# [11/100] training 36.4% loss=0.39126, acc=0.85938
# [11/100] training 36.5% loss=0.27608, acc=0.90625
# [11/100] training 36.7% loss=0.29113, acc=0.85938
# [11/100] training 36.8% loss=0.25800, acc=0.85938
# [11/100] training 37.1% loss=0.43270, acc=0.84375
# [11/100] training 37.3% loss=0.37782, acc=0.85938
# [11/100] training 37.4% loss=0.36215, acc=0.79688
# [11/100] training 37.6% loss=0.30695, acc=0.82812
# [11/100] training 37.7% loss=0.30257, acc=0.90625
# [11/100] training 37.9% loss=0.34696, acc=0.85938
# [11/100] training 38.1% loss=0.45839, acc=0.79688
# [11/100] training 38.3% loss=0.33713, acc=0.85938
# [11/100] training 38.4% loss=0.27942, acc=0.89062
# [11/100] training 38.6% loss=0.30448, acc=0.82812
# [11/100] training 38.8% loss=0.33419, acc=0.82812
# [11/100] training 38.9% loss=0.28520, acc=0.90625
# [11/100] training 39.1% loss=0.34870, acc=0.84375
# [11/100] training 39.3% loss=0.29794, acc=0.85938
# [11/100] training 39.5% loss=0.38221, acc=0.84375
# [11/100] training 39.6% loss=0.34510, acc=0.89062
# [11/100] training 39.8% loss=0.26588, acc=0.82812
# [11/100] training 40.0% loss=0.26294, acc=0.87500
# [11/100] training 40.1% loss=0.30125, acc=0.90625
# [11/100] training 40.4% loss=0.24129, acc=0.85938
# [11/100] training 40.5% loss=0.25000, acc=0.92188
# [11/100] training 40.7% loss=0.28255, acc=0.85938
# [11/100] training 40.8% loss=0.27619, acc=0.85938
# [11/100] training 41.0% loss=0.19200, acc=0.95312
# [11/100] training 41.1% loss=0.41352, acc=0.82812
# [11/100] training 41.3% loss=0.52990, acc=0.73438
# [11/100] training 41.6% loss=0.37500, acc=0.84375
# [11/100] training 41.7% loss=0.42017, acc=0.76562
# [11/100] training 41.9% loss=0.21244, acc=0.92188
# [11/100] training 42.0% loss=0.28446, acc=0.84375
# [11/100] training 42.2% loss=0.33978, acc=0.85938
# [11/100] training 42.3% loss=0.26955, acc=0.89062
# [11/100] training 42.5% loss=0.20591, acc=0.93750
# [11/100] training 42.8% loss=0.25474, acc=0.90625
# [11/100] training 42.9% loss=0.20446, acc=0.89062
# [11/100] training 43.1% loss=0.27381, acc=0.89062
# [11/100] training 43.2% loss=0.26187, acc=0.90625
# [11/100] training 43.4% loss=0.34198, acc=0.90625
# [11/100] training 43.5% loss=0.29017, acc=0.85938
# [11/100] training 43.8% loss=0.39335, acc=0.84375
# [11/100] training 43.9% loss=0.32165, acc=0.84375
# [11/100] training 44.1% loss=0.22749, acc=0.90625
# [11/100] training 44.3% loss=0.24454, acc=0.90625
# [11/100] training 44.4% loss=0.37133, acc=0.81250
# [11/100] training 44.6% loss=0.33966, acc=0.84375
# [11/100] training 44.7% loss=0.41135, acc=0.79688
# [11/100] training 45.0% loss=0.27122, acc=0.85938
# [11/100] training 45.1% loss=0.45585, acc=0.84375
# [11/100] training 45.3% loss=0.40726, acc=0.81250
# [11/100] training 45.5% loss=0.18201, acc=0.96875
# [11/100] training 45.6% loss=0.24714, acc=0.89062
# [11/100] training 45.8% loss=0.30829, acc=0.85938
# [11/100] training 45.9% loss=0.26996, acc=0.89062
# [11/100] training 46.2% loss=0.24419, acc=0.87500
# [11/100] training 46.3% loss=0.21273, acc=0.92188
# [11/100] training 46.5% loss=0.46751, acc=0.79688
# [11/100] training 46.6% loss=0.23212, acc=0.92188
# [11/100] training 46.8% loss=0.33531, acc=0.81250
# [11/100] training 47.0% loss=0.32622, acc=0.85938
# [11/100] training 47.2% loss=0.32538, acc=0.85938
# [11/100] training 47.4% loss=0.38303, acc=0.85938
# [11/100] training 47.5% loss=0.36908, acc=0.84375
# [11/100] training 47.7% loss=0.25006, acc=0.87500
# [11/100] training 47.8% loss=0.39195, acc=0.82812
# [11/100] training 48.0% loss=0.39900, acc=0.84375
# [11/100] training 48.3% loss=0.14246, acc=0.95312
# [11/100] training 48.4% loss=0.22542, acc=0.90625
# [11/100] training 48.6% loss=0.24457, acc=0.93750
# [11/100] training 48.7% loss=0.44926, acc=0.84375
# [11/100] training 48.9% loss=0.35853, acc=0.85938
# [11/100] training 49.0% loss=0.29487, acc=0.87500
# [11/100] training 49.2% loss=0.35333, acc=0.79688
# [11/100] training 49.3% loss=0.26388, acc=0.90625
# [11/100] training 49.6% loss=0.35925, acc=0.84375
# [11/100] training 49.8% loss=0.32977, acc=0.85938
# [11/100] training 49.9% loss=0.31244, acc=0.85938
# [11/100] training 50.1% loss=0.28314, acc=0.89062
# [11/100] training 50.2% loss=0.38853, acc=0.79688
# [11/100] training 50.4% loss=0.42818, acc=0.84375
# [11/100] training 50.6% loss=0.44993, acc=0.82812
# [11/100] training 50.8% loss=0.44188, acc=0.76562
# [11/100] training 51.0% loss=0.46671, acc=0.71875
# [11/100] training 51.1% loss=0.30323, acc=0.85938
# [11/100] training 51.3% loss=0.30558, acc=0.89062
# [11/100] training 51.4% loss=0.26267, acc=0.90625
# [11/100] training 51.7% loss=0.53034, acc=0.78125
# [11/100] training 51.8% loss=0.35673, acc=0.79688
# [11/100] training 52.0% loss=0.31148, acc=0.85938
# [11/100] training 52.1% loss=0.46287, acc=0.79688
# [11/100] training 52.3% loss=0.44200, acc=0.76562
# [11/100] training 52.5% loss=0.19750, acc=0.95312
# [11/100] training 52.6% loss=0.32956, acc=0.85938
# [11/100] training 52.9% loss=0.43144, acc=0.82812
# [11/100] training 53.0% loss=0.29517, acc=0.84375
# [11/100] training 53.2% loss=0.33706, acc=0.87500
# [11/100] training 53.3% loss=0.19535, acc=0.95312
# [11/100] training 53.5% loss=0.35480, acc=0.84375
# [11/100] training 53.7% loss=0.35378, acc=0.82812
# [11/100] training 53.8% loss=0.44590, acc=0.79688
# [11/100] training 54.1% loss=0.45256, acc=0.81250
# [11/100] training 54.2% loss=0.33422, acc=0.87500
# [11/100] training 54.4% loss=0.29036, acc=0.85938
# [11/100] training 54.5% loss=0.39866, acc=0.85938
# [11/100] training 54.7% loss=0.41361, acc=0.75000
# [11/100] training 54.8% loss=0.19987, acc=0.92188
# [11/100] training 55.1% loss=0.20859, acc=0.90625
# [11/100] training 55.3% loss=0.19194, acc=0.92188
# [11/100] training 55.4% loss=0.29406, acc=0.85938
# [11/100] training 55.6% loss=0.30948, acc=0.84375
# [11/100] training 55.7% loss=0.32618, acc=0.84375
# [11/100] training 55.9% loss=0.27207, acc=0.90625
# [11/100] training 56.0% loss=0.24615, acc=0.85938
# [11/100] training 56.3% loss=0.63378, acc=0.71875
# [11/100] training 56.5% loss=0.32730, acc=0.85938
# [11/100] training 56.6% loss=0.34957, acc=0.84375
# [11/100] training 56.8% loss=0.39343, acc=0.82812
# [11/100] training 56.9% loss=0.39665, acc=0.81250
# [11/100] training 57.1% loss=0.34496, acc=0.85938
# [11/100] training 57.2% loss=0.21786, acc=0.92188
# [11/100] training 57.5% loss=0.29526, acc=0.89062
# [11/100] training 57.6% loss=0.34900, acc=0.79688
# [11/100] training 57.8% loss=0.30164, acc=0.87500
# [11/100] training 58.0% loss=0.20456, acc=0.92188
# [11/100] training 58.1% loss=0.31385, acc=0.87500
# [11/100] training 58.3% loss=0.16962, acc=0.93750
# [11/100] training 58.4% loss=0.30770, acc=0.93750
# [11/100] training 58.7% loss=0.33579, acc=0.87500
# [11/100] training 58.8% loss=0.40303, acc=0.82812
# [11/100] training 59.0% loss=0.26594, acc=0.89062
# [11/100] training 59.2% loss=0.37583, acc=0.84375
# [11/100] training 59.3% loss=0.24569, acc=0.89062
# [11/100] training 59.5% loss=0.33114, acc=0.85938
# [11/100] training 59.7% loss=0.34795, acc=0.84375
# [11/100] training 59.9% loss=0.21546, acc=0.93750
# [11/100] training 60.0% loss=0.21767, acc=0.90625
# [11/100] training 60.2% loss=0.36499, acc=0.76562
# [11/100] training 60.3% loss=0.27354, acc=0.84375
# [11/100] training 60.5% loss=0.31238, acc=0.82812
# [11/100] training 60.8% loss=0.35383, acc=0.81250
# [11/100] training 60.9% loss=0.36699, acc=0.81250
# [11/100] training 61.1% loss=0.47172, acc=0.75000
# [11/100] training 61.2% loss=0.39090, acc=0.79688
# [11/100] training 61.4% loss=0.38766, acc=0.84375
# [11/100] training 61.5% loss=0.46702, acc=0.81250
# [11/100] training 61.7% loss=0.39390, acc=0.81250
# [11/100] training 62.0% loss=0.37502, acc=0.81250
# [11/100] training 62.1% loss=0.37991, acc=0.78125
# [11/100] training 62.3% loss=0.21111, acc=0.93750
# [11/100] training 62.4% loss=0.26242, acc=0.87500
# [11/100] training 62.6% loss=0.52114, acc=0.78125
# [11/100] training 62.7% loss=0.26237, acc=0.87500
# [11/100] training 62.9% loss=0.40518, acc=0.76562
# [11/100] training 63.1% loss=0.21929, acc=0.92188
# [11/100] training 63.3% loss=0.37184, acc=0.84375
# [11/100] training 63.5% loss=0.39329, acc=0.78125
# [11/100] training 63.6% loss=0.34065, acc=0.82812
# [11/100] training 63.8% loss=0.36699, acc=0.82812
# [11/100] training 63.9% loss=0.26134, acc=0.95312
# [11/100] training 64.2% loss=0.23434, acc=0.90625
# [11/100] training 64.3% loss=0.29373, acc=0.82812
# [11/100] training 64.5% loss=0.36896, acc=0.89062
# [11/100] training 64.7% loss=0.27488, acc=0.85938
# [11/100] training 64.8% loss=0.49914, acc=0.81250
# [11/100] training 65.0% loss=0.34884, acc=0.85938
# [11/100] training 65.1% loss=0.40024, acc=0.78125
# [11/100] training 65.4% loss=0.38346, acc=0.82812
# [11/100] training 65.5% loss=0.29529, acc=0.85938
# [11/100] training 65.7% loss=0.19393, acc=0.93750
# [11/100] training 65.8% loss=0.39215, acc=0.82812
# [11/100] training 66.0% loss=0.37857, acc=0.78125
# [11/100] training 66.2% loss=0.22330, acc=0.93750
# [11/100] training 66.3% loss=0.56586, acc=0.75000
# [11/100] training 66.6% loss=0.33680, acc=0.78125
# [11/100] training 66.7% loss=0.15724, acc=0.96875
# [11/100] training 66.9% loss=0.39276, acc=0.85938
# [11/100] training 67.0% loss=0.34408, acc=0.81250
# [11/100] training 67.2% loss=0.23891, acc=0.90625
# [11/100] training 67.4% loss=0.34618, acc=0.87500
# [11/100] training 67.6% loss=0.22717, acc=0.89062
# [11/100] training 67.8% loss=0.32065, acc=0.84375
# [11/100] training 67.9% loss=0.14811, acc=0.96875
# [11/100] training 68.1% loss=0.24412, acc=0.90625
# [11/100] training 68.2% loss=0.21006, acc=0.95312
# [11/100] training 68.4% loss=0.19308, acc=0.93750
# [11/100] training 68.5% loss=0.41948, acc=0.79688
# [11/100] training 68.8% loss=0.41526, acc=0.82812
# [11/100] training 69.0% loss=0.46134, acc=0.81250
# [11/100] training 69.1% loss=0.22986, acc=0.92188
# [11/100] training 69.3% loss=0.26923, acc=0.87500
# [11/100] training 69.4% loss=0.29831, acc=0.85938
# [11/100] training 69.6% loss=0.33404, acc=0.82812
# [11/100] training 69.7% loss=0.41231, acc=0.81250
# [11/100] training 70.0% loss=0.39177, acc=0.85938
# [11/100] training 70.2% loss=0.31687, acc=0.84375
# [11/100] training 70.3% loss=0.36338, acc=0.84375
# [11/100] training 70.5% loss=0.34231, acc=0.89062
# [11/100] training 70.6% loss=0.27381, acc=0.89062
# [11/100] training 70.8% loss=0.36583, acc=0.78125
# [11/100] training 71.0% loss=0.32663, acc=0.87500
# [11/100] training 71.2% loss=0.22470, acc=0.90625
# [11/100] training 71.3% loss=0.28729, acc=0.87500
# [11/100] training 71.5% loss=0.37941, acc=0.84375
# [11/100] training 71.7% loss=0.41608, acc=0.79688
# [11/100] training 71.8% loss=0.39828, acc=0.84375
# [11/100] training 72.0% loss=0.20838, acc=0.90625
# [11/100] training 72.2% loss=0.27249, acc=0.85938
# [11/100] training 72.4% loss=0.40424, acc=0.84375
# [11/100] training 72.5% loss=0.42002, acc=0.85938
# [11/100] training 72.7% loss=0.42221, acc=0.76562
# [11/100] training 72.9% loss=0.26261, acc=0.89062
# [11/100] training 73.0% loss=0.22892, acc=0.92188
# [11/100] training 73.3% loss=0.28918, acc=0.85938
# [11/100] training 73.4% loss=0.20883, acc=0.90625
# [11/100] training 73.6% loss=0.21479, acc=0.89062
# [11/100] training 73.7% loss=0.28896, acc=0.89062
# [11/100] training 73.9% loss=0.30596, acc=0.90625
# [11/100] training 74.0% loss=0.32039, acc=0.85938
# [11/100] training 74.2% loss=0.36603, acc=0.87500
# [11/100] training 74.5% loss=0.28101, acc=0.84375
# [11/100] training 74.6% loss=0.45978, acc=0.79688
# [11/100] training 74.8% loss=0.51695, acc=0.79688
# [11/100] training 74.9% loss=0.42489, acc=0.84375
# [11/100] training 75.1% loss=0.31815, acc=0.89062
# [11/100] training 75.2% loss=0.26549, acc=0.92188
# [11/100] training 75.4% loss=0.37136, acc=0.84375
# [11/100] training 75.7% loss=0.32602, acc=0.85938
# [11/100] training 75.8% loss=0.38325, acc=0.85938
# [11/100] training 76.0% loss=0.15869, acc=0.96875
# [11/100] training 76.1% loss=0.34516, acc=0.84375
# [11/100] training 76.3% loss=0.24602, acc=0.93750
# [11/100] training 76.4% loss=0.34345, acc=0.81250
# [11/100] training 76.7% loss=0.33967, acc=0.87500
# [11/100] training 76.8% loss=0.25749, acc=0.90625
# [11/100] training 77.0% loss=0.24047, acc=0.90625
# [11/100] training 77.2% loss=0.37306, acc=0.85938
# [11/100] training 77.3% loss=0.19634, acc=0.95312
# [11/100] training 77.5% loss=0.37939, acc=0.81250
# [11/100] training 77.6% loss=0.26980, acc=0.90625
# [11/100] training 77.9% loss=0.32000, acc=0.84375
# [11/100] training 78.0% loss=0.33211, acc=0.82812
# [11/100] training 78.2% loss=0.44021, acc=0.84375
# [11/100] training 78.4% loss=0.23474, acc=0.89062
# [11/100] training 78.5% loss=0.37780, acc=0.85938
# [11/100] training 78.7% loss=0.31564, acc=0.82812
# [11/100] training 78.8% loss=0.20876, acc=0.92188
# [11/100] training 79.1% loss=0.21590, acc=0.93750
# [11/100] training 79.2% loss=0.22648, acc=0.93750
# [11/100] training 79.4% loss=0.50822, acc=0.78125
# [11/100] training 79.5% loss=0.33092, acc=0.84375
# [11/100] training 79.7% loss=0.19036, acc=0.93750
# [11/100] training 79.9% loss=0.26307, acc=0.89062
# [11/100] training 80.1% loss=0.25661, acc=0.93750
# [11/100] training 80.3% loss=0.37598, acc=0.81250
# [11/100] training 80.4% loss=0.40607, acc=0.84375
# [11/100] training 80.6% loss=0.32055, acc=0.82812
# [11/100] training 80.7% loss=0.22245, acc=0.95312
# [11/100] training 80.9% loss=0.37779, acc=0.85938
# [11/100] training 81.2% loss=0.32176, acc=0.89062
# [11/100] training 81.3% loss=0.33312, acc=0.85938
# [11/100] training 81.5% loss=0.32307, acc=0.87500
# [11/100] training 81.6% loss=0.31817, acc=0.87500
# [11/100] training 81.8% loss=0.34624, acc=0.82812
# [11/100] training 81.9% loss=0.42434, acc=0.85938
# [11/100] training 82.1% loss=0.23224, acc=0.92188
# [11/100] training 82.2% loss=0.43397, acc=0.81250
# [11/100] training 82.5% loss=0.29768, acc=0.85938
# [11/100] training 82.7% loss=0.27524, acc=0.85938
# [11/100] training 82.8% loss=0.30594, acc=0.90625
# [11/100] training 83.0% loss=0.30888, acc=0.84375
# [11/100] training 83.1% loss=0.25429, acc=0.92188
# [11/100] training 83.3% loss=0.21964, acc=0.90625
# [11/100] training 83.5% loss=0.25325, acc=0.89062
# [11/100] training 83.7% loss=0.40909, acc=0.79688
# [11/100] training 83.9% loss=0.28782, acc=0.85938
# [11/100] training 84.0% loss=0.29434, acc=0.89062
# [11/100] training 84.2% loss=0.28209, acc=0.85938
# [11/100] training 84.3% loss=0.22159, acc=0.92188
# [11/100] training 84.5% loss=0.29959, acc=0.85938
# [11/100] training 84.7% loss=0.37560, acc=0.81250
# [11/100] training 84.9% loss=0.32979, acc=0.82812
# [11/100] training 85.0% loss=0.29821, acc=0.81250
# [11/100] training 85.2% loss=0.31759, acc=0.90625
# [11/100] training 85.4% loss=0.33920, acc=0.92188
# [11/100] training 85.5% loss=0.26015, acc=0.90625
# [11/100] training 85.8% loss=0.29545, acc=0.87500
# [11/100] training 85.9% loss=0.46479, acc=0.79688
# [11/100] training 86.1% loss=0.24239, acc=0.90625
# [11/100] training 86.2% loss=0.20140, acc=0.93750
# [11/100] training 86.4% loss=0.40351, acc=0.84375
# [11/100] training 86.6% loss=0.38409, acc=0.81250
# [11/100] training 86.7% loss=0.30749, acc=0.87500
# [11/100] training 87.0% loss=0.29736, acc=0.89062
# [11/100] training 87.1% loss=0.25633, acc=0.90625
# [11/100] training 87.3% loss=0.34508, acc=0.87500
# [11/100] training 87.4% loss=0.28218, acc=0.87500
# [11/100] training 87.6% loss=0.20457, acc=0.95312
# [11/100] training 87.7% loss=0.36491, acc=0.87500
# [11/100] training 87.9% loss=0.30538, acc=0.89062
# [11/100] training 88.2% loss=0.23375, acc=0.85938
# [11/100] training 88.3% loss=0.33626, acc=0.85938
# [11/100] training 88.5% loss=0.30640, acc=0.85938
# [11/100] training 88.6% loss=0.18058, acc=0.95312
# [11/100] training 88.8% loss=0.30876, acc=0.92188
# [11/100] training 88.9% loss=0.35474, acc=0.85938
# [11/100] training 89.2% loss=0.14891, acc=0.95312
# [11/100] training 89.4% loss=0.26330, acc=0.90625
# [11/100] training 89.5% loss=0.36427, acc=0.81250
# [11/100] training 89.7% loss=0.42372, acc=0.81250
# [11/100] training 89.8% loss=0.32054, acc=0.87500
# [11/100] training 90.0% loss=0.36669, acc=0.76562
# [11/100] training 90.1% loss=0.29255, acc=0.87500
# [11/100] training 90.4% loss=0.33336, acc=0.84375
# [11/100] training 90.5% loss=0.18251, acc=0.95312
# [11/100] training 90.7% loss=0.28656, acc=0.82812
# [11/100] training 90.9% loss=0.25107, acc=0.92188
# [11/100] training 91.0% loss=0.22995, acc=0.92188
# [11/100] training 91.2% loss=0.34528, acc=0.81250
# [11/100] training 91.3% loss=0.48189, acc=0.79688
# [11/100] training 91.6% loss=0.39058, acc=0.84375
# [11/100] training 91.7% loss=0.26983, acc=0.87500
# [11/100] training 91.9% loss=0.36841, acc=0.82812
# [11/100] training 92.1% loss=0.29727, acc=0.85938
# [11/100] training 92.2% loss=0.28157, acc=0.84375
# [11/100] training 92.4% loss=0.21960, acc=0.92188
# [11/100] training 92.6% loss=0.33660, acc=0.87500
# [11/100] training 92.8% loss=0.51757, acc=0.75000
# [11/100] training 92.9% loss=0.36303, acc=0.87500
# [11/100] training 93.1% loss=0.39352, acc=0.84375
# [11/100] training 93.2% loss=0.34657, acc=0.85938
# [11/100] training 93.4% loss=0.21207, acc=0.92188
# [11/100] training 93.7% loss=0.23142, acc=0.87500
# [11/100] training 93.8% loss=0.29213, acc=0.84375
# [11/100] training 94.0% loss=0.25153, acc=0.90625
# [11/100] training 94.1% loss=0.29068, acc=0.89062
# [11/100] training 94.3% loss=0.20922, acc=0.95312
# [11/100] training 94.4% loss=0.34383, acc=0.85938
# [11/100] training 94.6% loss=0.25908, acc=0.90625
# [11/100] training 94.9% loss=0.33547, acc=0.84375
# [11/100] training 95.0% loss=0.39415, acc=0.87500
# [11/100] training 95.2% loss=0.46586, acc=0.81250
# [11/100] training 95.3% loss=0.29808, acc=0.87500
# [11/100] training 95.5% loss=0.26338, acc=0.93750
# [11/100] training 95.6% loss=0.37774, acc=0.84375
# [11/100] training 95.8% loss=0.29489, acc=0.87500
# [11/100] training 96.0% loss=0.29468, acc=0.87500
# [11/100] training 96.2% loss=0.22102, acc=0.92188
# [11/100] training 96.4% loss=0.23289, acc=0.90625
# [11/100] training 96.5% loss=0.35195, acc=0.79688
# [11/100] training 96.7% loss=0.27539, acc=0.85938
# [11/100] training 96.8% loss=0.36277, acc=0.85938
# [11/100] training 97.1% loss=0.35081, acc=0.78125
# [11/100] training 97.2% loss=0.32891, acc=0.87500
# [11/100] training 97.4% loss=0.29033, acc=0.89062
# [11/100] training 97.6% loss=0.39953, acc=0.76562
# [11/100] training 97.7% loss=0.27698, acc=0.85938
# [11/100] training 97.9% loss=0.22660, acc=0.90625
# [11/100] training 98.0% loss=0.34307, acc=0.84375
# [11/100] training 98.3% loss=0.29806, acc=0.87500
# [11/100] training 98.4% loss=0.27333, acc=0.92188
# [11/100] training 98.6% loss=0.47608, acc=0.79688
# [11/100] training 98.7% loss=0.40503, acc=0.82812
# [11/100] training 98.9% loss=0.20260, acc=0.90625
# [11/100] training 99.1% loss=0.19827, acc=0.90625
# [11/100] training 99.2% loss=0.22902, acc=0.89062
# [11/100] training 99.5% loss=0.33247, acc=0.82812
# [11/100] training 99.6% loss=0.30081, acc=0.84375
# [11/100] training 99.8% loss=0.21281, acc=0.90625
# [11/100] training 99.9% loss=0.13914, acc=0.96875
# [11/100] testing 0.9% loss=0.17103, acc=0.93750
# [11/100] testing 1.8% loss=0.64301, acc=0.76562
# [11/100] testing 2.2% loss=0.27113, acc=0.84375
# [11/100] testing 3.1% loss=0.28264, acc=0.87500
# [11/100] testing 3.5% loss=0.27114, acc=0.87500
# [11/100] testing 4.4% loss=0.25828, acc=0.90625
# [11/100] testing 4.8% loss=0.48556, acc=0.82812
# [11/100] testing 5.7% loss=0.42738, acc=0.81250
# [11/100] testing 6.6% loss=0.31528, acc=0.85938
# [11/100] testing 7.0% loss=0.22151, acc=0.92188
# [11/100] testing 7.9% loss=0.43131, acc=0.82812
# [11/100] testing 8.3% loss=0.24822, acc=0.90625
# [11/100] testing 9.2% loss=0.38725, acc=0.81250
# [11/100] testing 9.7% loss=0.21761, acc=0.92188
# [11/100] testing 10.5% loss=0.47263, acc=0.78125
# [11/100] testing 11.0% loss=0.33254, acc=0.78125
# [11/100] testing 11.8% loss=0.47740, acc=0.81250
# [11/100] testing 12.7% loss=0.60906, acc=0.76562
# [11/100] testing 13.2% loss=0.24831, acc=0.90625
# [11/100] testing 14.0% loss=0.42833, acc=0.82812
# [11/100] testing 14.5% loss=0.50887, acc=0.81250
# [11/100] testing 15.4% loss=0.38400, acc=0.84375
# [11/100] testing 15.8% loss=0.26589, acc=0.85938
# [11/100] testing 16.7% loss=0.31387, acc=0.89062
# [11/100] testing 17.5% loss=0.28518, acc=0.85938
# [11/100] testing 18.0% loss=0.25648, acc=0.90625
# [11/100] testing 18.9% loss=0.20034, acc=0.90625
# [11/100] testing 19.3% loss=0.34721, acc=0.89062
# [11/100] testing 20.2% loss=0.56811, acc=0.79688
# [11/100] testing 20.6% loss=0.44340, acc=0.79688
# [11/100] testing 21.5% loss=0.33829, acc=0.85938
# [11/100] testing 21.9% loss=0.52300, acc=0.73438
# [11/100] testing 22.8% loss=0.60339, acc=0.81250
# [11/100] testing 23.7% loss=0.36298, acc=0.84375
# [11/100] testing 24.1% loss=0.30807, acc=0.87500
# [11/100] testing 25.0% loss=0.50270, acc=0.82812
# [11/100] testing 25.4% loss=0.19177, acc=0.95312
# [11/100] testing 26.3% loss=0.32775, acc=0.79688
# [11/100] testing 26.8% loss=0.36602, acc=0.87500
# [11/100] testing 27.6% loss=0.40667, acc=0.85938
# [11/100] testing 28.5% loss=0.36186, acc=0.89062
# [11/100] testing 29.0% loss=0.20736, acc=0.90625
# [11/100] testing 29.8% loss=0.46336, acc=0.84375
# [11/100] testing 30.3% loss=0.35225, acc=0.85938
# [11/100] testing 31.1% loss=0.54059, acc=0.81250
# [11/100] testing 31.6% loss=0.24146, acc=0.90625
# [11/100] testing 32.5% loss=0.28995, acc=0.92188
# [11/100] testing 32.9% loss=0.39837, acc=0.89062
# [11/100] testing 33.8% loss=0.45570, acc=0.81250
# [11/100] testing 34.7% loss=0.44998, acc=0.85938
# [11/100] testing 35.1% loss=0.34024, acc=0.89062
# [11/100] testing 36.0% loss=0.46169, acc=0.81250
# [11/100] testing 36.4% loss=0.30483, acc=0.87500
# [11/100] testing 37.3% loss=0.37302, acc=0.87500
# [11/100] testing 37.7% loss=0.45962, acc=0.79688
# [11/100] testing 38.6% loss=0.28536, acc=0.92188
# [11/100] testing 39.5% loss=0.48488, acc=0.84375
# [11/100] testing 39.9% loss=0.29499, acc=0.90625
# [11/100] testing 40.8% loss=0.41247, acc=0.82812
# [11/100] testing 41.2% loss=0.28792, acc=0.93750
# [11/100] testing 42.1% loss=0.41884, acc=0.85938
# [11/100] testing 42.5% loss=0.32472, acc=0.84375
# [11/100] testing 43.4% loss=0.37983, acc=0.89062
# [11/100] testing 43.9% loss=0.34353, acc=0.89062
# [11/100] testing 44.7% loss=0.48463, acc=0.82812
# [11/100] testing 45.6% loss=0.32361, acc=0.85938
# [11/100] testing 46.1% loss=0.32322, acc=0.84375
# [11/100] testing 46.9% loss=0.31628, acc=0.84375
# [11/100] testing 47.4% loss=0.22178, acc=0.90625
# [11/100] testing 48.3% loss=0.42234, acc=0.84375
# [11/100] testing 48.7% loss=0.57297, acc=0.78125
# [11/100] testing 49.6% loss=0.48620, acc=0.81250
# [11/100] testing 50.4% loss=0.29271, acc=0.89062
# [11/100] testing 50.9% loss=0.38067, acc=0.79688
# [11/100] testing 51.8% loss=0.46080, acc=0.82812
# [11/100] testing 52.2% loss=0.35162, acc=0.89062
# [11/100] testing 53.1% loss=0.31475, acc=0.82812
# [11/100] testing 53.5% loss=0.30063, acc=0.90625
# [11/100] testing 54.4% loss=0.58532, acc=0.75000
# [11/100] testing 54.8% loss=0.44546, acc=0.79688
# [11/100] testing 55.7% loss=0.20488, acc=0.90625
# [11/100] testing 56.6% loss=0.50860, acc=0.81250
# [11/100] testing 57.0% loss=0.57580, acc=0.76562
# [11/100] testing 57.9% loss=0.48860, acc=0.78125
# [11/100] testing 58.3% loss=0.52840, acc=0.78125
# [11/100] testing 59.2% loss=0.32376, acc=0.84375
# [11/100] testing 59.7% loss=0.31574, acc=0.82812
# [11/100] testing 60.5% loss=0.43351, acc=0.79688
# [11/100] testing 61.4% loss=0.24031, acc=0.90625
# [11/100] testing 61.9% loss=0.38376, acc=0.89062
# [11/100] testing 62.7% loss=0.32943, acc=0.85938
# [11/100] testing 63.2% loss=0.50948, acc=0.79688
# [11/100] testing 64.0% loss=0.39731, acc=0.81250
# [11/100] testing 64.5% loss=0.41908, acc=0.76562
# [11/100] testing 65.4% loss=0.28370, acc=0.89062
# [11/100] testing 65.8% loss=0.38187, acc=0.82812
# [11/100] testing 66.7% loss=0.22180, acc=0.90625
# [11/100] testing 67.6% loss=0.50484, acc=0.79688
# [11/100] testing 68.0% loss=0.31860, acc=0.85938
# [11/100] testing 68.9% loss=0.33785, acc=0.87500
# [11/100] testing 69.3% loss=0.40478, acc=0.81250
# [11/100] testing 70.2% loss=0.48332, acc=0.84375
# [11/100] testing 70.6% loss=0.49763, acc=0.84375
# [11/100] testing 71.5% loss=0.51065, acc=0.84375
# [11/100] testing 72.4% loss=0.30220, acc=0.89062
# [11/100] testing 72.8% loss=0.42525, acc=0.76562
# [11/100] testing 73.7% loss=0.30150, acc=0.84375
# [11/100] testing 74.1% loss=0.50484, acc=0.82812
# [11/100] testing 75.0% loss=0.23742, acc=0.87500
# [11/100] testing 75.4% loss=0.37881, acc=0.84375
# [11/100] testing 76.3% loss=0.14506, acc=0.93750
# [11/100] testing 76.8% loss=0.44213, acc=0.84375
# [11/100] testing 77.6% loss=0.32439, acc=0.85938
# [11/100] testing 78.5% loss=0.69374, acc=0.76562
# [11/100] testing 79.0% loss=0.33591, acc=0.84375
# [11/100] testing 79.8% loss=0.39017, acc=0.81250
# [11/100] testing 80.3% loss=0.28315, acc=0.87500
# [11/100] testing 81.2% loss=0.56087, acc=0.82812
# [11/100] testing 81.6% loss=0.32049, acc=0.84375
# [11/100] testing 82.5% loss=0.25788, acc=0.87500
# [11/100] testing 83.3% loss=0.25250, acc=0.95312
# [11/100] testing 83.8% loss=0.23180, acc=0.92188
# [11/100] testing 84.7% loss=0.36331, acc=0.85938
# [11/100] testing 85.1% loss=0.44978, acc=0.79688
# [11/100] testing 86.0% loss=0.36638, acc=0.84375
# [11/100] testing 86.4% loss=0.52733, acc=0.84375
# [11/100] testing 87.3% loss=0.52882, acc=0.81250
# [11/100] testing 87.7% loss=0.30359, acc=0.87500
# [11/100] testing 88.6% loss=0.27265, acc=0.89062
# [11/100] testing 89.5% loss=0.59139, acc=0.70312
# [11/100] testing 89.9% loss=0.33564, acc=0.81250
# [11/100] testing 90.8% loss=0.29560, acc=0.92188
# [11/100] testing 91.2% loss=0.24111, acc=0.92188
# [11/100] testing 92.1% loss=0.46919, acc=0.82812
# [11/100] testing 92.6% loss=0.33335, acc=0.85938
# [11/100] testing 93.4% loss=0.48346, acc=0.81250
# [11/100] testing 94.3% loss=0.26855, acc=0.89062
# [11/100] testing 94.7% loss=0.33866, acc=0.85938
# [11/100] testing 95.6% loss=0.39303, acc=0.84375
# [11/100] testing 96.1% loss=0.25548, acc=0.87500
# [11/100] testing 96.9% loss=0.40290, acc=0.78125
# [11/100] testing 97.4% loss=0.29064, acc=0.90625
# [11/100] testing 98.3% loss=0.33094, acc=0.87500
# [11/100] testing 98.7% loss=0.28403, acc=0.90625
# [11/100] testing 99.6% loss=0.28780, acc=0.89062
# [12/100] training 0.2% loss=0.41122, acc=0.76562
# [12/100] training 0.4% loss=0.46867, acc=0.78125
# [12/100] training 0.5% loss=0.32003, acc=0.81250
# [12/100] training 0.8% loss=0.38986, acc=0.82812
# [12/100] training 0.9% loss=0.35246, acc=0.85938
# [12/100] training 1.1% loss=0.23002, acc=0.95312
# [12/100] training 1.2% loss=0.29530, acc=0.90625
# [12/100] training 1.4% loss=0.37129, acc=0.82812
# [12/100] training 1.6% loss=0.19300, acc=0.89062
# [12/100] training 1.8% loss=0.26752, acc=0.90625
# [12/100] training 2.0% loss=0.33824, acc=0.85938
# [12/100] training 2.1% loss=0.29827, acc=0.87500
# [12/100] training 2.3% loss=0.27681, acc=0.82812
# [12/100] training 2.4% loss=0.33975, acc=0.82812
# [12/100] training 2.6% loss=0.32359, acc=0.85938
# [12/100] training 2.7% loss=0.37187, acc=0.85938
# [12/100] training 3.0% loss=0.34354, acc=0.85938
# [12/100] training 3.2% loss=0.23440, acc=0.90625
# [12/100] training 3.3% loss=0.39613, acc=0.85938
# [12/100] training 3.5% loss=0.27080, acc=0.87500
# [12/100] training 3.6% loss=0.51527, acc=0.76562
# [12/100] training 3.8% loss=0.19274, acc=0.92188
# [12/100] training 3.9% loss=0.31573, acc=0.85938
# [12/100] training 4.2% loss=0.31163, acc=0.90625
# [12/100] training 4.4% loss=0.27098, acc=0.92188
# [12/100] training 4.5% loss=0.30910, acc=0.84375
# [12/100] training 4.7% loss=0.35591, acc=0.85938
# [12/100] training 4.8% loss=0.39893, acc=0.85938
# [12/100] training 5.0% loss=0.30833, acc=0.81250
# [12/100] training 5.2% loss=0.47408, acc=0.81250
# [12/100] training 5.4% loss=0.23361, acc=0.89062
# [12/100] training 5.5% loss=0.36229, acc=0.79688
# [12/100] training 5.7% loss=0.25475, acc=0.90625
# [12/100] training 5.9% loss=0.38326, acc=0.79688
# [12/100] training 6.0% loss=0.35309, acc=0.82812
# [12/100] training 6.3% loss=0.31614, acc=0.89062
# [12/100] training 6.4% loss=0.28793, acc=0.84375
# [12/100] training 6.6% loss=0.31452, acc=0.85938
# [12/100] training 6.7% loss=0.38054, acc=0.78125
# [12/100] training 6.9% loss=0.31041, acc=0.85938
# [12/100] training 7.1% loss=0.38246, acc=0.85938
# [12/100] training 7.2% loss=0.31516, acc=0.84375
# [12/100] training 7.5% loss=0.28699, acc=0.87500
# [12/100] training 7.6% loss=0.30752, acc=0.84375
# [12/100] training 7.8% loss=0.34332, acc=0.82812
# [12/100] training 7.9% loss=0.40628, acc=0.82812
# [12/100] training 8.1% loss=0.27819, acc=0.89062
# [12/100] training 8.2% loss=0.30377, acc=0.89062
# [12/100] training 8.4% loss=0.36372, acc=0.87500
# [12/100] training 8.7% loss=0.32222, acc=0.87500
# [12/100] training 8.8% loss=0.39887, acc=0.76562
# [12/100] training 9.0% loss=0.32729, acc=0.85938
# [12/100] training 9.1% loss=0.38680, acc=0.79688
# [12/100] training 9.3% loss=0.49999, acc=0.84375
# [12/100] training 9.4% loss=0.26848, acc=0.90625
# [12/100] training 9.7% loss=0.32974, acc=0.84375
# [12/100] training 9.9% loss=0.32723, acc=0.85938
# [12/100] training 10.0% loss=0.28410, acc=0.87500
# [12/100] training 10.2% loss=0.23761, acc=0.95312
# [12/100] training 10.3% loss=0.28066, acc=0.84375
# [12/100] training 10.5% loss=0.43924, acc=0.82812
# [12/100] training 10.6% loss=0.31331, acc=0.85938
# [12/100] training 10.9% loss=0.25859, acc=0.92188
# [12/100] training 11.0% loss=0.36250, acc=0.85938
# [12/100] training 11.2% loss=0.20790, acc=0.92188
# [12/100] training 11.4% loss=0.45100, acc=0.84375
# [12/100] training 11.5% loss=0.46331, acc=0.81250
# [12/100] training 11.7% loss=0.27483, acc=0.90625
# [12/100] training 11.8% loss=0.27710, acc=0.82812
# [12/100] training 12.1% loss=0.29821, acc=0.87500
# [12/100] training 12.2% loss=0.30163, acc=0.85938
# [12/100] training 12.4% loss=0.24545, acc=0.87500
# [12/100] training 12.6% loss=0.30166, acc=0.89062
# [12/100] training 12.7% loss=0.44209, acc=0.82812
# [12/100] training 12.9% loss=0.39537, acc=0.85938
# [12/100] training 13.0% loss=0.31435, acc=0.87500
# [12/100] training 13.3% loss=0.33594, acc=0.89062
# [12/100] training 13.4% loss=0.32900, acc=0.85938
# [12/100] training 13.6% loss=0.29094, acc=0.89062
# [12/100] training 13.7% loss=0.41775, acc=0.82812
# [12/100] training 13.9% loss=0.40812, acc=0.82812
# [12/100] training 14.1% loss=0.31932, acc=0.87500
# [12/100] training 14.3% loss=0.28738, acc=0.84375
# [12/100] training 14.5% loss=0.28744, acc=0.89062
# [12/100] training 14.6% loss=0.23996, acc=0.90625
# [12/100] training 14.8% loss=0.32635, acc=0.87500
# [12/100] training 14.9% loss=0.34131, acc=0.79688
# [12/100] training 15.1% loss=0.39486, acc=0.85938
# [12/100] training 15.4% loss=0.29193, acc=0.90625
# [12/100] training 15.5% loss=0.34811, acc=0.81250
# [12/100] training 15.7% loss=0.38735, acc=0.82812
# [12/100] training 15.8% loss=0.22978, acc=0.92188
# [12/100] training 16.0% loss=0.42301, acc=0.82812
# [12/100] training 16.1% loss=0.52374, acc=0.79688
# [12/100] training 16.3% loss=0.34240, acc=0.89062
# [12/100] training 16.4% loss=0.24854, acc=0.87500
# [12/100] training 16.7% loss=0.32529, acc=0.87500
# [12/100] training 16.9% loss=0.44113, acc=0.79688
# [12/100] training 17.0% loss=0.30884, acc=0.82812
# [12/100] training 17.2% loss=0.29775, acc=0.89062
# [12/100] training 17.3% loss=0.27772, acc=0.89062
# [12/100] training 17.5% loss=0.26520, acc=0.89062
# [12/100] training 17.7% loss=0.32824, acc=0.82812
# [12/100] training 17.9% loss=0.37474, acc=0.87500
# [12/100] training 18.1% loss=0.35769, acc=0.89062
# [12/100] training 18.2% loss=0.34480, acc=0.84375
# [12/100] training 18.4% loss=0.37362, acc=0.81250
# [12/100] training 18.5% loss=0.26299, acc=0.89062
# [12/100] training 18.8% loss=0.25367, acc=0.90625
# [12/100] training 18.9% loss=0.25312, acc=0.87500
# [12/100] training 19.1% loss=0.34867, acc=0.85938
# [12/100] training 19.2% loss=0.24155, acc=0.84375
# [12/100] training 19.4% loss=0.16096, acc=0.95312
# [12/100] training 19.6% loss=0.37771, acc=0.85938
# [12/100] training 19.7% loss=0.28954, acc=0.90625
# [12/100] training 20.0% loss=0.21668, acc=0.92188
# [12/100] training 20.1% loss=0.32548, acc=0.89062
# [12/100] training 20.3% loss=0.32622, acc=0.89062
# [12/100] training 20.4% loss=0.43081, acc=0.78125
# [12/100] training 20.6% loss=0.32853, acc=0.84375
# [12/100] training 20.8% loss=0.25868, acc=0.92188
# [12/100] training 20.9% loss=0.31381, acc=0.84375
# [12/100] training 21.2% loss=0.29415, acc=0.85938
# [12/100] training 21.3% loss=0.32388, acc=0.89062
# [12/100] training 21.5% loss=0.28941, acc=0.90625
# [12/100] training 21.6% loss=0.14178, acc=0.95312
# [12/100] training 21.8% loss=0.32998, acc=0.89062
# [12/100] training 21.9% loss=0.31758, acc=0.90625
# [12/100] training 22.2% loss=0.26518, acc=0.90625
# [12/100] training 22.4% loss=0.36200, acc=0.84375
# [12/100] training 22.5% loss=0.27392, acc=0.90625
# [12/100] training 22.7% loss=0.28681, acc=0.87500
# [12/100] training 22.8% loss=0.36629, acc=0.89062
# [12/100] training 23.0% loss=0.18205, acc=0.95312
# [12/100] training 23.1% loss=0.37604, acc=0.82812
# [12/100] training 23.4% loss=0.30313, acc=0.87500
# [12/100] training 23.6% loss=0.35389, acc=0.89062
# [12/100] training 23.7% loss=0.32746, acc=0.85938
# [12/100] training 23.9% loss=0.23160, acc=0.93750
# [12/100] training 24.0% loss=0.51365, acc=0.76562
# [12/100] training 24.2% loss=0.35641, acc=0.87500
# [12/100] training 24.3% loss=0.36427, acc=0.82812
# [12/100] training 24.6% loss=0.32190, acc=0.82812
# [12/100] training 24.7% loss=0.45668, acc=0.75000
# [12/100] training 24.9% loss=0.26202, acc=0.90625
# [12/100] training 25.1% loss=0.33581, acc=0.85938
# [12/100] training 25.2% loss=0.21946, acc=0.93750
# [12/100] training 25.4% loss=0.27638, acc=0.90625
# [12/100] training 25.6% loss=0.34942, acc=0.84375
# [12/100] training 25.8% loss=0.40667, acc=0.84375
# [12/100] training 25.9% loss=0.25021, acc=0.89062
# [12/100] training 26.1% loss=0.35259, acc=0.78125
# [12/100] training 26.3% loss=0.43602, acc=0.84375
# [12/100] training 26.4% loss=0.21188, acc=0.90625
# [12/100] training 26.6% loss=0.29190, acc=0.89062
# [12/100] training 26.8% loss=0.38204, acc=0.82812
# [12/100] training 27.0% loss=0.34908, acc=0.84375
# [12/100] training 27.1% loss=0.22068, acc=0.92188
# [12/100] training 27.3% loss=0.42383, acc=0.79688
# [12/100] training 27.4% loss=0.28949, acc=0.85938
# [12/100] training 27.6% loss=0.40889, acc=0.84375
# [12/100] training 27.9% loss=0.27741, acc=0.85938
# [12/100] training 28.0% loss=0.35444, acc=0.85938
# [12/100] training 28.2% loss=0.30394, acc=0.89062
# [12/100] training 28.3% loss=0.22841, acc=0.92188
# [12/100] training 28.5% loss=0.32890, acc=0.87500
# [12/100] training 28.6% loss=0.35323, acc=0.87500
# [12/100] training 28.8% loss=0.19571, acc=0.95312
# [12/100] training 29.1% loss=0.29131, acc=0.81250
# [12/100] training 29.2% loss=0.30239, acc=0.89062
# [12/100] training 29.4% loss=0.34118, acc=0.81250
# [12/100] training 29.5% loss=0.22636, acc=0.93750
# [12/100] training 29.7% loss=0.30732, acc=0.89062
# [12/100] training 29.8% loss=0.32295, acc=0.85938
# [12/100] training 30.0% loss=0.49951, acc=0.76562
# [12/100] training 30.2% loss=0.27050, acc=0.85938
# [12/100] training 30.4% loss=0.31510, acc=0.87500
# [12/100] training 30.6% loss=0.43417, acc=0.85938
# [12/100] training 30.7% loss=0.33431, acc=0.82812
# [12/100] training 30.9% loss=0.34866, acc=0.89062
# [12/100] training 31.0% loss=0.29419, acc=0.82812
# [12/100] training 31.3% loss=0.31172, acc=0.85938
# [12/100] training 31.4% loss=0.36214, acc=0.79688
# [12/100] training 31.6% loss=0.42311, acc=0.82812
# [12/100] training 31.8% loss=0.29964, acc=0.84375
# [12/100] training 31.9% loss=0.35942, acc=0.84375
# [12/100] training 32.1% loss=0.39699, acc=0.89062
# [12/100] training 32.2% loss=0.25460, acc=0.92188
# [12/100] training 32.5% loss=0.23407, acc=0.92188
# [12/100] training 32.6% loss=0.23561, acc=0.90625
# [12/100] training 32.8% loss=0.26762, acc=0.89062
# [12/100] training 32.9% loss=0.36808, acc=0.85938
# [12/100] training 33.1% loss=0.33268, acc=0.84375
# [12/100] training 33.3% loss=0.34992, acc=0.82812
# [12/100] training 33.4% loss=0.40349, acc=0.89062
# [12/100] training 33.7% loss=0.36173, acc=0.84375
# [12/100] training 33.8% loss=0.40516, acc=0.84375
# [12/100] training 34.0% loss=0.32254, acc=0.89062
# [12/100] training 34.1% loss=0.30788, acc=0.87500
# [12/100] training 34.3% loss=0.33304, acc=0.84375
# [12/100] training 34.5% loss=0.35292, acc=0.89062
# [12/100] training 34.7% loss=0.24351, acc=0.93750
# [12/100] training 34.9% loss=0.31845, acc=0.85938
# [12/100] training 35.0% loss=0.25527, acc=0.90625
# [12/100] training 35.2% loss=0.35369, acc=0.84375
# [12/100] training 35.3% loss=0.30717, acc=0.85938
# [12/100] training 35.5% loss=0.32536, acc=0.84375
# [12/100] training 35.6% loss=0.36020, acc=0.82812
# [12/100] training 35.9% loss=0.39194, acc=0.84375
# [12/100] training 36.1% loss=0.37629, acc=0.84375
# [12/100] training 36.2% loss=0.38681, acc=0.82812
# [12/100] training 36.4% loss=0.42796, acc=0.84375
# [12/100] training 36.5% loss=0.29113, acc=0.87500
# [12/100] training 36.7% loss=0.29516, acc=0.84375
# [12/100] training 36.8% loss=0.26831, acc=0.90625
# [12/100] training 37.1% loss=0.50372, acc=0.78125
# [12/100] training 37.3% loss=0.33705, acc=0.82812
# [12/100] training 37.4% loss=0.31704, acc=0.90625
# [12/100] training 37.6% loss=0.38068, acc=0.79688
# [12/100] training 37.7% loss=0.41004, acc=0.89062
# [12/100] training 37.9% loss=0.29906, acc=0.87500
# [12/100] training 38.1% loss=0.44882, acc=0.73438
# [12/100] training 38.3% loss=0.31928, acc=0.87500
# [12/100] training 38.4% loss=0.24892, acc=0.92188
# [12/100] training 38.6% loss=0.23963, acc=0.92188
# [12/100] training 38.8% loss=0.34692, acc=0.81250
# [12/100] training 38.9% loss=0.34229, acc=0.87500
# [12/100] training 39.1% loss=0.27251, acc=0.87500
# [12/100] training 39.3% loss=0.24099, acc=0.92188
# [12/100] training 39.5% loss=0.35740, acc=0.89062
# [12/100] training 39.6% loss=0.33094, acc=0.89062
# [12/100] training 39.8% loss=0.29299, acc=0.87500
# [12/100] training 40.0% loss=0.36172, acc=0.82812
# [12/100] training 40.1% loss=0.33275, acc=0.87500
# [12/100] training 40.4% loss=0.24812, acc=0.84375
# [12/100] training 40.5% loss=0.27715, acc=0.90625
# [12/100] training 40.7% loss=0.35526, acc=0.84375
# [12/100] training 40.8% loss=0.28665, acc=0.84375
# [12/100] training 41.0% loss=0.17515, acc=0.93750
# [12/100] training 41.1% loss=0.42883, acc=0.82812
# [12/100] training 41.3% loss=0.40985, acc=0.76562
# [12/100] training 41.6% loss=0.32311, acc=0.85938
# [12/100] training 41.7% loss=0.42953, acc=0.78125
# [12/100] training 41.9% loss=0.24202, acc=0.85938
# [12/100] training 42.0% loss=0.35831, acc=0.79688
# [12/100] training 42.2% loss=0.38374, acc=0.84375
# [12/100] training 42.3% loss=0.31105, acc=0.85938
# [12/100] training 42.5% loss=0.27754, acc=0.92188
# [12/100] training 42.8% loss=0.21824, acc=0.93750
# [12/100] training 42.9% loss=0.21698, acc=0.89062
# [12/100] training 43.1% loss=0.40869, acc=0.79688
# [12/100] training 43.2% loss=0.24530, acc=0.90625
# [12/100] training 43.4% loss=0.35004, acc=0.85938
# [12/100] training 43.5% loss=0.32516, acc=0.84375
# [12/100] training 43.8% loss=0.33299, acc=0.84375
# [12/100] training 43.9% loss=0.27511, acc=0.87500
# [12/100] training 44.1% loss=0.21230, acc=0.95312
# [12/100] training 44.3% loss=0.26767, acc=0.90625
# [12/100] training 44.4% loss=0.36970, acc=0.79688
# [12/100] training 44.6% loss=0.47697, acc=0.78125
# [12/100] training 44.7% loss=0.44104, acc=0.82812
# [12/100] training 45.0% loss=0.26711, acc=0.87500
# [12/100] training 45.1% loss=0.44331, acc=0.82812
# [12/100] training 45.3% loss=0.28662, acc=0.87500
# [12/100] training 45.5% loss=0.20637, acc=0.93750
# [12/100] training 45.6% loss=0.27818, acc=0.89062
# [12/100] training 45.8% loss=0.26313, acc=0.85938
# [12/100] training 45.9% loss=0.26767, acc=0.89062
# [12/100] training 46.2% loss=0.28783, acc=0.87500
# [12/100] training 46.3% loss=0.22349, acc=0.87500
# [12/100] training 46.5% loss=0.43896, acc=0.81250
# [12/100] training 46.6% loss=0.27153, acc=0.90625
# [12/100] training 46.8% loss=0.28311, acc=0.84375
# [12/100] training 47.0% loss=0.25606, acc=0.84375
# [12/100] training 47.2% loss=0.29471, acc=0.89062
# [12/100] training 47.4% loss=0.27977, acc=0.87500
# [12/100] training 47.5% loss=0.37272, acc=0.81250
# [12/100] training 47.7% loss=0.25510, acc=0.89062
# [12/100] training 47.8% loss=0.36418, acc=0.85938
# [12/100] training 48.0% loss=0.29839, acc=0.85938
# [12/100] training 48.3% loss=0.14127, acc=0.96875
# [12/100] training 48.4% loss=0.18688, acc=0.93750
# [12/100] training 48.6% loss=0.20998, acc=0.93750
# [12/100] training 48.7% loss=0.46252, acc=0.84375
# [12/100] training 48.9% loss=0.27755, acc=0.90625
# [12/100] training 49.0% loss=0.29083, acc=0.89062
# [12/100] training 49.2% loss=0.36082, acc=0.79688
# [12/100] training 49.3% loss=0.26892, acc=0.92188
# [12/100] training 49.6% loss=0.27430, acc=0.84375
# [12/100] training 49.8% loss=0.32701, acc=0.84375
# [12/100] training 49.9% loss=0.38541, acc=0.79688
# [12/100] training 50.1% loss=0.28375, acc=0.90625
# [12/100] training 50.2% loss=0.39748, acc=0.78125
# [12/100] training 50.4% loss=0.36545, acc=0.85938
# [12/100] training 50.6% loss=0.39629, acc=0.79688
# [12/100] training 50.8% loss=0.37778, acc=0.84375
# [12/100] training 51.0% loss=0.37550, acc=0.84375
# [12/100] training 51.1% loss=0.34790, acc=0.84375
# [12/100] training 51.3% loss=0.28971, acc=0.90625
# [12/100] training 51.4% loss=0.26585, acc=0.90625
# [12/100] training 51.7% loss=0.39180, acc=0.82812
# [12/100] training 51.8% loss=0.27395, acc=0.90625
# [12/100] training 52.0% loss=0.32364, acc=0.85938
# [12/100] training 52.1% loss=0.36822, acc=0.82812
# [12/100] training 52.3% loss=0.35496, acc=0.82812
# [12/100] training 52.5% loss=0.21130, acc=0.92188
# [12/100] training 52.6% loss=0.25621, acc=0.89062
# [12/100] training 52.9% loss=0.48255, acc=0.81250
# [12/100] training 53.0% loss=0.31435, acc=0.84375
# [12/100] training 53.2% loss=0.37303, acc=0.84375
# [12/100] training 53.3% loss=0.21031, acc=0.93750
# [12/100] training 53.5% loss=0.32278, acc=0.85938
# [12/100] training 53.7% loss=0.34285, acc=0.84375
# [12/100] training 53.8% loss=0.44240, acc=0.73438
# [12/100] training 54.1% loss=0.40453, acc=0.84375
# [12/100] training 54.2% loss=0.28343, acc=0.89062
# [12/100] training 54.4% loss=0.26030, acc=0.92188
# [12/100] training 54.5% loss=0.43223, acc=0.76562
# [12/100] training 54.7% loss=0.41484, acc=0.85938
# [12/100] training 54.8% loss=0.21964, acc=0.90625
# [12/100] training 55.1% loss=0.29518, acc=0.82812
# [12/100] training 55.3% loss=0.22577, acc=0.92188
# [12/100] training 55.4% loss=0.30292, acc=0.87500
# [12/100] training 55.6% loss=0.36777, acc=0.82812
# [12/100] training 55.7% loss=0.29009, acc=0.90625
# [12/100] training 55.9% loss=0.27332, acc=0.90625
# [12/100] training 56.0% loss=0.29819, acc=0.84375
# [12/100] training 56.3% loss=0.58129, acc=0.78125
# [12/100] training 56.5% loss=0.26656, acc=0.90625
# [12/100] training 56.6% loss=0.38930, acc=0.81250
# [12/100] training 56.8% loss=0.34808, acc=0.82812
# [12/100] training 56.9% loss=0.37576, acc=0.84375
# [12/100] training 57.1% loss=0.34618, acc=0.90625
# [12/100] training 57.2% loss=0.26642, acc=0.89062
# [12/100] training 57.5% loss=0.25850, acc=0.87500
# [12/100] training 57.6% loss=0.40200, acc=0.78125
# [12/100] training 57.8% loss=0.36415, acc=0.85938
# [12/100] training 58.0% loss=0.18877, acc=0.92188
# [12/100] training 58.1% loss=0.32586, acc=0.87500
# [12/100] training 58.3% loss=0.16394, acc=0.95312
# [12/100] training 58.4% loss=0.39305, acc=0.85938
# [12/100] training 58.7% loss=0.32296, acc=0.85938
# [12/100] training 58.8% loss=0.39250, acc=0.79688
# [12/100] training 59.0% loss=0.30422, acc=0.87500
# [12/100] training 59.2% loss=0.35891, acc=0.85938
# [12/100] training 59.3% loss=0.35796, acc=0.85938
# [12/100] training 59.5% loss=0.26779, acc=0.90625
# [12/100] training 59.7% loss=0.32459, acc=0.87500
# [12/100] training 59.9% loss=0.25273, acc=0.90625
# [12/100] training 60.0% loss=0.25527, acc=0.92188
# [12/100] training 60.2% loss=0.39293, acc=0.76562
# [12/100] training 60.3% loss=0.30654, acc=0.82812
# [12/100] training 60.5% loss=0.21972, acc=0.89062
# [12/100] training 60.8% loss=0.40468, acc=0.81250
# [12/100] training 60.9% loss=0.40022, acc=0.85938
# [12/100] training 61.1% loss=0.33731, acc=0.82812
# [12/100] training 61.2% loss=0.38212, acc=0.81250
# [12/100] training 61.4% loss=0.33039, acc=0.85938
# [12/100] training 61.5% loss=0.47356, acc=0.81250
# [12/100] training 61.7% loss=0.36698, acc=0.84375
# [12/100] training 62.0% loss=0.32056, acc=0.84375
# [12/100] training 62.1% loss=0.37947, acc=0.82812
# [12/100] training 62.3% loss=0.21657, acc=0.93750
# [12/100] training 62.4% loss=0.26259, acc=0.87500
# [12/100] training 62.6% loss=0.48668, acc=0.76562
# [12/100] training 62.7% loss=0.31608, acc=0.84375
# [12/100] training 62.9% loss=0.45578, acc=0.76562
# [12/100] training 63.1% loss=0.28599, acc=0.87500
# [12/100] training 63.3% loss=0.28746, acc=0.87500
# [12/100] training 63.5% loss=0.41480, acc=0.78125
# [12/100] training 63.6% loss=0.35837, acc=0.81250
# [12/100] training 63.8% loss=0.34998, acc=0.92188
# [12/100] training 63.9% loss=0.26512, acc=0.92188
# [12/100] training 64.2% loss=0.26672, acc=0.85938
# [12/100] training 64.3% loss=0.35712, acc=0.82812
# [12/100] training 64.5% loss=0.38904, acc=0.79688
# [12/100] training 64.7% loss=0.30388, acc=0.84375
# [12/100] training 64.8% loss=0.54263, acc=0.78125
# [12/100] training 65.0% loss=0.48112, acc=0.76562
# [12/100] training 65.1% loss=0.45679, acc=0.78125
# [12/100] training 65.4% loss=0.44144, acc=0.82812
# [12/100] training 65.5% loss=0.35319, acc=0.89062
# [12/100] training 65.7% loss=0.19979, acc=0.93750
# [12/100] training 65.8% loss=0.39751, acc=0.87500
# [12/100] training 66.0% loss=0.36067, acc=0.82812
# [12/100] training 66.2% loss=0.18204, acc=0.96875
# [12/100] training 66.3% loss=0.44719, acc=0.82812
# [12/100] training 66.6% loss=0.34092, acc=0.81250
# [12/100] training 66.7% loss=0.15169, acc=0.95312
# [12/100] training 66.9% loss=0.36705, acc=0.87500
# [12/100] training 67.0% loss=0.36868, acc=0.82812
# [12/100] training 67.2% loss=0.25522, acc=0.93750
# [12/100] training 67.4% loss=0.35128, acc=0.84375
# [12/100] training 67.6% loss=0.30908, acc=0.89062
# [12/100] training 67.8% loss=0.30259, acc=0.89062
# [12/100] training 67.9% loss=0.17376, acc=0.96875
# [12/100] training 68.1% loss=0.27526, acc=0.85938
# [12/100] training 68.2% loss=0.20887, acc=0.89062
# [12/100] training 68.4% loss=0.21622, acc=0.89062
# [12/100] training 68.5% loss=0.52407, acc=0.78125
# [12/100] training 68.8% loss=0.27069, acc=0.89062
# [12/100] training 69.0% loss=0.43947, acc=0.81250
# [12/100] training 69.1% loss=0.14237, acc=0.95312
# [12/100] training 69.3% loss=0.33569, acc=0.81250
# [12/100] training 69.4% loss=0.29504, acc=0.84375
# [12/100] training 69.6% loss=0.37294, acc=0.82812
# [12/100] training 69.7% loss=0.31893, acc=0.87500
# [12/100] training 70.0% loss=0.40769, acc=0.79688
# [12/100] training 70.2% loss=0.33497, acc=0.82812
# [12/100] training 70.3% loss=0.35898, acc=0.82812
# [12/100] training 70.5% loss=0.34359, acc=0.81250
# [12/100] training 70.6% loss=0.24754, acc=0.93750
# [12/100] training 70.8% loss=0.31213, acc=0.85938
# [12/100] training 71.0% loss=0.38302, acc=0.85938
# [12/100] training 71.2% loss=0.26056, acc=0.89062
# [12/100] training 71.3% loss=0.27596, acc=0.87500
# [12/100] training 71.5% loss=0.38859, acc=0.84375
# [12/100] training 71.7% loss=0.35788, acc=0.79688
# [12/100] training 71.8% loss=0.37641, acc=0.81250
# [12/100] training 72.0% loss=0.18769, acc=0.92188
# [12/100] training 72.2% loss=0.20753, acc=0.93750
# [12/100] training 72.4% loss=0.42693, acc=0.85938
# [12/100] training 72.5% loss=0.43027, acc=0.81250
# [12/100] training 72.7% loss=0.41803, acc=0.78125
# [12/100] training 72.9% loss=0.30784, acc=0.87500
# [12/100] training 73.0% loss=0.21132, acc=0.92188
# [12/100] training 73.3% loss=0.32527, acc=0.87500
# [12/100] training 73.4% loss=0.26484, acc=0.89062
# [12/100] training 73.6% loss=0.29902, acc=0.89062
# [12/100] training 73.7% loss=0.23717, acc=0.93750
# [12/100] training 73.9% loss=0.36344, acc=0.84375
# [12/100] training 74.0% loss=0.35080, acc=0.84375
# [12/100] training 74.2% loss=0.34066, acc=0.82812
# [12/100] training 74.5% loss=0.28519, acc=0.84375
# [12/100] training 74.6% loss=0.39873, acc=0.79688
# [12/100] training 74.8% loss=0.50651, acc=0.75000
# [12/100] training 74.9% loss=0.43123, acc=0.76562
# [12/100] training 75.1% loss=0.31493, acc=0.85938
# [12/100] training 75.2% loss=0.26290, acc=0.90625
# [12/100] training 75.4% loss=0.32527, acc=0.87500
# [12/100] training 75.7% loss=0.38910, acc=0.78125
# [12/100] training 75.8% loss=0.42083, acc=0.78125
# [12/100] training 76.0% loss=0.16443, acc=0.95312
# [12/100] training 76.1% loss=0.36787, acc=0.79688
# [12/100] training 76.3% loss=0.27882, acc=0.90625
# [12/100] training 76.4% loss=0.29899, acc=0.84375
# [12/100] training 76.7% loss=0.29130, acc=0.89062
# [12/100] training 76.8% loss=0.23953, acc=0.90625
# [12/100] training 77.0% loss=0.18044, acc=0.93750
# [12/100] training 77.2% loss=0.31037, acc=0.85938
# [12/100] training 77.3% loss=0.19010, acc=0.92188
# [12/100] training 77.5% loss=0.37299, acc=0.81250
# [12/100] training 77.6% loss=0.26922, acc=0.90625
# [12/100] training 77.9% loss=0.36689, acc=0.81250
# [12/100] training 78.0% loss=0.35572, acc=0.84375
# [12/100] training 78.2% loss=0.30927, acc=0.90625
# [12/100] training 78.4% loss=0.25514, acc=0.89062
# [12/100] training 78.5% loss=0.48298, acc=0.81250
# [12/100] training 78.7% loss=0.33042, acc=0.87500
# [12/100] training 78.8% loss=0.20211, acc=0.90625
# [12/100] training 79.1% loss=0.24170, acc=0.89062
# [12/100] training 79.2% loss=0.23855, acc=0.90625
# [12/100] training 79.4% loss=0.42069, acc=0.82812
# [12/100] training 79.5% loss=0.32804, acc=0.82812
# [12/100] training 79.7% loss=0.20661, acc=0.93750
# [12/100] training 79.9% loss=0.29880, acc=0.89062
# [12/100] training 80.1% loss=0.32319, acc=0.84375
# [12/100] training 80.3% loss=0.47530, acc=0.78125
# [12/100] training 80.4% loss=0.30336, acc=0.87500
# [12/100] training 80.6% loss=0.38074, acc=0.84375
# [12/100] training 80.7% loss=0.41256, acc=0.87500
# [12/100] training 80.9% loss=0.43059, acc=0.81250
# [12/100] training 81.2% loss=0.36201, acc=0.84375
# [12/100] training 81.3% loss=0.36655, acc=0.85938
# [12/100] training 81.5% loss=0.27981, acc=0.90625
# [12/100] training 81.6% loss=0.31347, acc=0.84375
# [12/100] training 81.8% loss=0.35786, acc=0.85938
# [12/100] training 81.9% loss=0.38676, acc=0.90625
# [12/100] training 82.1% loss=0.18445, acc=0.92188
# [12/100] training 82.2% loss=0.39223, acc=0.84375
# [12/100] training 82.5% loss=0.22267, acc=0.90625
# [12/100] training 82.7% loss=0.29528, acc=0.84375
# [12/100] training 82.8% loss=0.30353, acc=0.87500
# [12/100] training 83.0% loss=0.26059, acc=0.90625
# [12/100] training 83.1% loss=0.31939, acc=0.85938
# [12/100] training 83.3% loss=0.21386, acc=0.90625
# [12/100] training 83.5% loss=0.25060, acc=0.89062
# [12/100] training 83.7% loss=0.38874, acc=0.87500
# [12/100] training 83.9% loss=0.28214, acc=0.84375
# [12/100] training 84.0% loss=0.23447, acc=0.93750
# [12/100] training 84.2% loss=0.20754, acc=0.92188
# [12/100] training 84.3% loss=0.31628, acc=0.84375
# [12/100] training 84.5% loss=0.29880, acc=0.84375
# [12/100] training 84.7% loss=0.33063, acc=0.85938
# [12/100] training 84.9% loss=0.21756, acc=0.89062
# [12/100] training 85.0% loss=0.26025, acc=0.87500
# [12/100] training 85.2% loss=0.30433, acc=0.85938
# [12/100] training 85.4% loss=0.24540, acc=0.93750
# [12/100] training 85.5% loss=0.17606, acc=0.95312
# [12/100] training 85.8% loss=0.29332, acc=0.87500
# [12/100] training 85.9% loss=0.30935, acc=0.87500
# [12/100] training 86.1% loss=0.26832, acc=0.87500
# [12/100] training 86.2% loss=0.23430, acc=0.90625
# [12/100] training 86.4% loss=0.44210, acc=0.76562
# [12/100] training 86.6% loss=0.39519, acc=0.85938
# [12/100] training 86.7% loss=0.25976, acc=0.90625
# [12/100] training 87.0% loss=0.28315, acc=0.90625
# [12/100] training 87.1% loss=0.30687, acc=0.84375
# [12/100] training 87.3% loss=0.31974, acc=0.82812
# [12/100] training 87.4% loss=0.29259, acc=0.87500
# [12/100] training 87.6% loss=0.26878, acc=0.84375
# [12/100] training 87.7% loss=0.36500, acc=0.87500
# [12/100] training 87.9% loss=0.28535, acc=0.87500
# [12/100] training 88.2% loss=0.18688, acc=0.90625
# [12/100] training 88.3% loss=0.28634, acc=0.90625
# [12/100] training 88.5% loss=0.27511, acc=0.89062
# [12/100] training 88.6% loss=0.16094, acc=0.95312
# [12/100] training 88.8% loss=0.28891, acc=0.87500
# [12/100] training 88.9% loss=0.30773, acc=0.87500
# [12/100] training 89.2% loss=0.24417, acc=0.87500
# [12/100] training 89.4% loss=0.30330, acc=0.87500
# [12/100] training 89.5% loss=0.37592, acc=0.81250
# [12/100] training 89.7% loss=0.37585, acc=0.82812
# [12/100] training 89.8% loss=0.26387, acc=0.90625
# [12/100] training 90.0% loss=0.30962, acc=0.85938
# [12/100] training 90.1% loss=0.28926, acc=0.90625
# [12/100] training 90.4% loss=0.25283, acc=0.87500
# [12/100] training 90.5% loss=0.16958, acc=0.96875
# [12/100] training 90.7% loss=0.29734, acc=0.87500
# [12/100] training 90.9% loss=0.18509, acc=0.92188
# [12/100] training 91.0% loss=0.24154, acc=0.87500
# [12/100] training 91.2% loss=0.36938, acc=0.79688
# [12/100] training 91.3% loss=0.46954, acc=0.82812
# [12/100] training 91.6% loss=0.35782, acc=0.82812
# [12/100] training 91.7% loss=0.30259, acc=0.84375
# [12/100] training 91.9% loss=0.36708, acc=0.84375
# [12/100] training 92.1% loss=0.26392, acc=0.90625
# [12/100] training 92.2% loss=0.22329, acc=0.92188
# [12/100] training 92.4% loss=0.27723, acc=0.89062
# [12/100] training 92.6% loss=0.40742, acc=0.81250
# [12/100] training 92.8% loss=0.40367, acc=0.79688
# [12/100] training 92.9% loss=0.42070, acc=0.81250
# [12/100] training 93.1% loss=0.38937, acc=0.84375
# [12/100] training 93.2% loss=0.32243, acc=0.84375
# [12/100] training 93.4% loss=0.21009, acc=0.92188
# [12/100] training 93.7% loss=0.25134, acc=0.89062
# [12/100] training 93.8% loss=0.24715, acc=0.87500
# [12/100] training 94.0% loss=0.29910, acc=0.82812
# [12/100] training 94.1% loss=0.36060, acc=0.81250
# [12/100] training 94.3% loss=0.22995, acc=0.92188
# [12/100] training 94.4% loss=0.22478, acc=0.87500
# [12/100] training 94.6% loss=0.21026, acc=0.92188
# [12/100] training 94.9% loss=0.26408, acc=0.89062
# [12/100] training 95.0% loss=0.44490, acc=0.81250
# [12/100] training 95.2% loss=0.44288, acc=0.79688
# [12/100] training 95.3% loss=0.26030, acc=0.90625
# [12/100] training 95.5% loss=0.26249, acc=0.93750
# [12/100] training 95.6% loss=0.37825, acc=0.81250
# [12/100] training 95.8% loss=0.32615, acc=0.82812
# [12/100] training 96.0% loss=0.28456, acc=0.87500
# [12/100] training 96.2% loss=0.21551, acc=0.93750
# [12/100] training 96.4% loss=0.24581, acc=0.92188
# [12/100] training 96.5% loss=0.33257, acc=0.85938
# [12/100] training 96.7% loss=0.22258, acc=0.89062
# [12/100] training 96.8% loss=0.41886, acc=0.82812
# [12/100] training 97.1% loss=0.27825, acc=0.82812
# [12/100] training 97.2% loss=0.34848, acc=0.84375
# [12/100] training 97.4% loss=0.24846, acc=0.92188
# [12/100] training 97.6% loss=0.33954, acc=0.85938
# [12/100] training 97.7% loss=0.19634, acc=0.92188
# [12/100] training 97.9% loss=0.21043, acc=0.92188
# [12/100] training 98.0% loss=0.42000, acc=0.82812
# [12/100] training 98.3% loss=0.24991, acc=0.90625
# [12/100] training 98.4% loss=0.21489, acc=0.87500
# [12/100] training 98.6% loss=0.44804, acc=0.76562
# [12/100] training 98.7% loss=0.29396, acc=0.85938
# [12/100] training 98.9% loss=0.24101, acc=0.85938
# [12/100] training 99.1% loss=0.34395, acc=0.81250
# [12/100] training 99.2% loss=0.25990, acc=0.87500
# [12/100] training 99.5% loss=0.40993, acc=0.89062
# [12/100] training 99.6% loss=0.34101, acc=0.79688
# [12/100] training 99.8% loss=0.21692, acc=0.89062
# [12/100] training 99.9% loss=0.18981, acc=0.93750
# [12/100] testing 0.9% loss=0.22799, acc=0.90625
# [12/100] testing 1.8% loss=0.62749, acc=0.76562
# [12/100] testing 2.2% loss=0.26448, acc=0.90625
# [12/100] testing 3.1% loss=0.29968, acc=0.84375
# [12/100] testing 3.5% loss=0.25141, acc=0.90625
# [12/100] testing 4.4% loss=0.39948, acc=0.79688
# [12/100] testing 4.8% loss=0.45336, acc=0.82812
# [12/100] testing 5.7% loss=0.41568, acc=0.84375
# [12/100] testing 6.6% loss=0.41806, acc=0.84375
# [12/100] testing 7.0% loss=0.32285, acc=0.87500
# [12/100] testing 7.9% loss=0.49604, acc=0.76562
# [12/100] testing 8.3% loss=0.29618, acc=0.87500
# [12/100] testing 9.2% loss=0.40639, acc=0.84375
# [12/100] testing 9.7% loss=0.25671, acc=0.92188
# [12/100] testing 10.5% loss=0.43566, acc=0.78125
# [12/100] testing 11.0% loss=0.36770, acc=0.82812
# [12/100] testing 11.8% loss=0.40917, acc=0.87500
# [12/100] testing 12.7% loss=0.65458, acc=0.68750
# [12/100] testing 13.2% loss=0.27908, acc=0.85938
# [12/100] testing 14.0% loss=0.52960, acc=0.76562
# [12/100] testing 14.5% loss=0.38432, acc=0.84375
# [12/100] testing 15.4% loss=0.41993, acc=0.87500
# [12/100] testing 15.8% loss=0.32054, acc=0.87500
# [12/100] testing 16.7% loss=0.35693, acc=0.81250
# [12/100] testing 17.5% loss=0.44327, acc=0.81250
# [12/100] testing 18.0% loss=0.29671, acc=0.84375
# [12/100] testing 18.9% loss=0.19831, acc=0.90625
# [12/100] testing 19.3% loss=0.40868, acc=0.84375
# [12/100] testing 20.2% loss=0.62335, acc=0.73438
# [12/100] testing 20.6% loss=0.53003, acc=0.76562
# [12/100] testing 21.5% loss=0.32445, acc=0.90625
# [12/100] testing 21.9% loss=0.61417, acc=0.67188
# [12/100] testing 22.8% loss=0.49457, acc=0.82812
# [12/100] testing 23.7% loss=0.35876, acc=0.82812
# [12/100] testing 24.1% loss=0.38890, acc=0.84375
# [12/100] testing 25.0% loss=0.43048, acc=0.81250
# [12/100] testing 25.4% loss=0.22935, acc=0.85938
# [12/100] testing 26.3% loss=0.49759, acc=0.78125
# [12/100] testing 26.8% loss=0.40074, acc=0.81250
# [12/100] testing 27.6% loss=0.46678, acc=0.76562
# [12/100] testing 28.5% loss=0.41003, acc=0.82812
# [12/100] testing 29.0% loss=0.33966, acc=0.79688
# [12/100] testing 29.8% loss=0.47912, acc=0.81250
# [12/100] testing 30.3% loss=0.51392, acc=0.78125
# [12/100] testing 31.1% loss=0.54069, acc=0.79688
# [12/100] testing 31.6% loss=0.34567, acc=0.82812
# [12/100] testing 32.5% loss=0.32526, acc=0.85938
# [12/100] testing 32.9% loss=0.42478, acc=0.81250
# [12/100] testing 33.8% loss=0.41594, acc=0.81250
# [12/100] testing 34.7% loss=0.41487, acc=0.82812
# [12/100] testing 35.1% loss=0.39782, acc=0.84375
# [12/100] testing 36.0% loss=0.52396, acc=0.76562
# [12/100] testing 36.4% loss=0.29953, acc=0.81250
# [12/100] testing 37.3% loss=0.41176, acc=0.85938
# [12/100] testing 37.7% loss=0.49757, acc=0.76562
# [12/100] testing 38.6% loss=0.31376, acc=0.89062
# [12/100] testing 39.5% loss=0.46468, acc=0.85938
# [12/100] testing 39.9% loss=0.38159, acc=0.84375
# [12/100] testing 40.8% loss=0.40743, acc=0.81250
# [12/100] testing 41.2% loss=0.29466, acc=0.89062
# [12/100] testing 42.1% loss=0.43552, acc=0.82812
# [12/100] testing 42.5% loss=0.30430, acc=0.85938
# [12/100] testing 43.4% loss=0.39254, acc=0.84375
# [12/100] testing 43.9% loss=0.31255, acc=0.90625
# [12/100] testing 44.7% loss=0.50389, acc=0.82812
# [12/100] testing 45.6% loss=0.42354, acc=0.81250
# [12/100] testing 46.1% loss=0.27078, acc=0.89062
# [12/100] testing 46.9% loss=0.28932, acc=0.89062
# [12/100] testing 47.4% loss=0.27918, acc=0.89062
# [12/100] testing 48.3% loss=0.45791, acc=0.81250
# [12/100] testing 48.7% loss=0.62150, acc=0.75000
# [12/100] testing 49.6% loss=0.54421, acc=0.79688
# [12/100] testing 50.4% loss=0.24663, acc=0.85938
# [12/100] testing 50.9% loss=0.41434, acc=0.79688
# [12/100] testing 51.8% loss=0.41895, acc=0.81250
# [12/100] testing 52.2% loss=0.33412, acc=0.89062
# [12/100] testing 53.1% loss=0.31692, acc=0.82812
# [12/100] testing 53.5% loss=0.25647, acc=0.92188
# [12/100] testing 54.4% loss=0.51256, acc=0.76562
# [12/100] testing 54.8% loss=0.46485, acc=0.79688
# [12/100] testing 55.7% loss=0.24230, acc=0.89062
# [12/100] testing 56.6% loss=0.46966, acc=0.82812
# [12/100] testing 57.0% loss=0.51186, acc=0.76562
# [12/100] testing 57.9% loss=0.38116, acc=0.82812
# [12/100] testing 58.3% loss=0.52858, acc=0.75000
# [12/100] testing 59.2% loss=0.47788, acc=0.81250
# [12/100] testing 59.7% loss=0.27989, acc=0.89062
# [12/100] testing 60.5% loss=0.38969, acc=0.84375
# [12/100] testing 61.4% loss=0.26524, acc=0.90625
# [12/100] testing 61.9% loss=0.35469, acc=0.85938
# [12/100] testing 62.7% loss=0.35073, acc=0.84375
# [12/100] testing 63.2% loss=0.48957, acc=0.76562
# [12/100] testing 64.0% loss=0.37657, acc=0.87500
# [12/100] testing 64.5% loss=0.44799, acc=0.78125
# [12/100] testing 65.4% loss=0.31466, acc=0.84375
# [12/100] testing 65.8% loss=0.46917, acc=0.82812
# [12/100] testing 66.7% loss=0.22241, acc=0.90625
# [12/100] testing 67.6% loss=0.48148, acc=0.79688
# [12/100] testing 68.0% loss=0.41286, acc=0.81250
# [12/100] testing 68.9% loss=0.38392, acc=0.84375
# [12/100] testing 69.3% loss=0.44478, acc=0.81250
# [12/100] testing 70.2% loss=0.52891, acc=0.85938
# [12/100] testing 70.6% loss=0.32565, acc=0.82812
# [12/100] testing 71.5% loss=0.51540, acc=0.81250
# [12/100] testing 72.4% loss=0.34711, acc=0.85938
# [12/100] testing 72.8% loss=0.44272, acc=0.78125
# [12/100] testing 73.7% loss=0.35227, acc=0.89062
# [12/100] testing 74.1% loss=0.57226, acc=0.75000
# [12/100] testing 75.0% loss=0.33733, acc=0.82812
# [12/100] testing 75.4% loss=0.33101, acc=0.85938
# [12/100] testing 76.3% loss=0.15644, acc=0.96875
# [12/100] testing 76.8% loss=0.38234, acc=0.82812
# [12/100] testing 77.6% loss=0.33934, acc=0.84375
# [12/100] testing 78.5% loss=0.69071, acc=0.70312
# [12/100] testing 79.0% loss=0.41079, acc=0.81250
# [12/100] testing 79.8% loss=0.32557, acc=0.81250
# [12/100] testing 80.3% loss=0.34622, acc=0.85938
# [12/100] testing 81.2% loss=0.56178, acc=0.81250
# [12/100] testing 81.6% loss=0.39889, acc=0.84375
# [12/100] testing 82.5% loss=0.30825, acc=0.85938
# [12/100] testing 83.3% loss=0.20789, acc=0.90625
# [12/100] testing 83.8% loss=0.25750, acc=0.89062
# [12/100] testing 84.7% loss=0.41671, acc=0.79688
# [12/100] testing 85.1% loss=0.44425, acc=0.81250
# [12/100] testing 86.0% loss=0.46149, acc=0.79688
# [12/100] testing 86.4% loss=0.58937, acc=0.79688
# [12/100] testing 87.3% loss=0.53465, acc=0.75000
# [12/100] testing 87.7% loss=0.37823, acc=0.84375
# [12/100] testing 88.6% loss=0.27383, acc=0.87500
# [12/100] testing 89.5% loss=0.55314, acc=0.75000
# [12/100] testing 89.9% loss=0.38885, acc=0.82812
# [12/100] testing 90.8% loss=0.37164, acc=0.82812
# [12/100] testing 91.2% loss=0.18325, acc=0.92188
# [12/100] testing 92.1% loss=0.40029, acc=0.81250
# [12/100] testing 92.6% loss=0.31920, acc=0.84375
# [12/100] testing 93.4% loss=0.52969, acc=0.75000
# [12/100] testing 94.3% loss=0.26282, acc=0.87500
# [12/100] testing 94.7% loss=0.35691, acc=0.84375
# [12/100] testing 95.6% loss=0.42140, acc=0.84375
# [12/100] testing 96.1% loss=0.36896, acc=0.79688
# [12/100] testing 96.9% loss=0.34613, acc=0.79688
# [12/100] testing 97.4% loss=0.19869, acc=0.90625
# [12/100] testing 98.3% loss=0.29131, acc=0.85938
# [12/100] testing 98.7% loss=0.42109, acc=0.84375
# [12/100] testing 99.6% loss=0.36063, acc=0.81250
# [13/100] training 0.2% loss=0.49431, acc=0.73438
# [13/100] training 0.4% loss=0.48165, acc=0.68750
# [13/100] training 0.5% loss=0.27737, acc=0.90625
# [13/100] training 0.8% loss=0.38385, acc=0.82812
# [13/100] training 0.9% loss=0.30962, acc=0.87500
# [13/100] training 1.1% loss=0.22011, acc=0.95312
# [13/100] training 1.2% loss=0.46126, acc=0.81250
# [13/100] training 1.4% loss=0.34585, acc=0.84375
# [13/100] training 1.6% loss=0.21544, acc=0.93750
# [13/100] training 1.8% loss=0.23336, acc=0.89062
# [13/100] training 2.0% loss=0.34625, acc=0.84375
# [13/100] training 2.1% loss=0.29671, acc=0.85938
# [13/100] training 2.3% loss=0.29154, acc=0.85938
# [13/100] training 2.4% loss=0.41731, acc=0.84375
# [13/100] training 2.6% loss=0.32504, acc=0.84375
# [13/100] training 2.7% loss=0.38979, acc=0.84375
# [13/100] training 3.0% loss=0.38216, acc=0.84375
# [13/100] training 3.2% loss=0.27086, acc=0.84375
# [13/100] training 3.3% loss=0.31252, acc=0.87500
# [13/100] training 3.5% loss=0.29428, acc=0.89062
# [13/100] training 3.6% loss=0.47071, acc=0.78125
# [13/100] training 3.8% loss=0.20135, acc=0.89062
# [13/100] training 3.9% loss=0.24722, acc=0.87500
# [13/100] training 4.2% loss=0.27264, acc=0.93750
# [13/100] training 4.4% loss=0.27726, acc=0.87500
# [13/100] training 4.5% loss=0.25621, acc=0.89062
# [13/100] training 4.7% loss=0.37627, acc=0.84375
# [13/100] training 4.8% loss=0.34952, acc=0.81250
# [13/100] training 5.0% loss=0.43373, acc=0.82812
# [13/100] training 5.2% loss=0.38500, acc=0.85938
# [13/100] training 5.4% loss=0.25935, acc=0.92188
# [13/100] training 5.5% loss=0.32505, acc=0.84375
# [13/100] training 5.7% loss=0.28986, acc=0.85938
# [13/100] training 5.9% loss=0.44303, acc=0.81250
# [13/100] training 6.0% loss=0.39935, acc=0.81250
# [13/100] training 6.3% loss=0.38198, acc=0.84375
# [13/100] training 6.4% loss=0.26920, acc=0.89062
# [13/100] training 6.6% loss=0.36069, acc=0.79688
# [13/100] training 6.7% loss=0.34246, acc=0.81250
# [13/100] training 6.9% loss=0.28667, acc=0.85938
# [13/100] training 7.1% loss=0.35871, acc=0.84375
# [13/100] training 7.2% loss=0.38720, acc=0.81250
# [13/100] training 7.5% loss=0.28955, acc=0.87500
# [13/100] training 7.6% loss=0.25391, acc=0.92188
# [13/100] training 7.8% loss=0.31639, acc=0.85938
# [13/100] training 7.9% loss=0.45411, acc=0.81250
# [13/100] training 8.1% loss=0.27639, acc=0.84375
# [13/100] training 8.2% loss=0.25824, acc=0.89062
# [13/100] training 8.4% loss=0.33060, acc=0.87500
# [13/100] training 8.7% loss=0.26689, acc=0.89062
# [13/100] training 8.8% loss=0.31465, acc=0.85938
# [13/100] training 9.0% loss=0.27045, acc=0.87500
# [13/100] training 9.1% loss=0.33132, acc=0.85938
# [13/100] training 9.3% loss=0.43331, acc=0.85938
# [13/100] training 9.4% loss=0.23925, acc=0.87500
# [13/100] training 9.7% loss=0.28164, acc=0.89062
# [13/100] training 9.9% loss=0.31431, acc=0.90625
# [13/100] training 10.0% loss=0.34022, acc=0.84375
# [13/100] training 10.2% loss=0.27436, acc=0.89062
# [13/100] training 10.3% loss=0.23748, acc=0.87500
# [13/100] training 10.5% loss=0.34425, acc=0.85938
# [13/100] training 10.6% loss=0.40236, acc=0.79688
# [13/100] training 10.9% loss=0.24655, acc=0.92188
# [13/100] training 11.0% loss=0.38767, acc=0.81250
# [13/100] training 11.2% loss=0.25999, acc=0.89062
# [13/100] training 11.4% loss=0.43430, acc=0.81250
# [13/100] training 11.5% loss=0.44130, acc=0.82812
# [13/100] training 11.7% loss=0.22734, acc=0.93750
# [13/100] training 11.8% loss=0.32342, acc=0.82812
# [13/100] training 12.1% loss=0.31124, acc=0.85938
# [13/100] training 12.2% loss=0.24223, acc=0.89062
# [13/100] training 12.4% loss=0.31155, acc=0.85938
# [13/100] training 12.6% loss=0.28942, acc=0.87500
# [13/100] training 12.7% loss=0.28854, acc=0.82812
# [13/100] training 12.9% loss=0.29775, acc=0.89062
# [13/100] training 13.0% loss=0.29375, acc=0.92188
# [13/100] training 13.3% loss=0.30949, acc=0.92188
# [13/100] training 13.4% loss=0.29496, acc=0.84375
# [13/100] training 13.6% loss=0.29506, acc=0.89062
# [13/100] training 13.7% loss=0.55172, acc=0.79688
# [13/100] training 13.9% loss=0.32586, acc=0.85938
# [13/100] training 14.1% loss=0.38857, acc=0.82812
# [13/100] training 14.3% loss=0.38611, acc=0.82812
# [13/100] training 14.5% loss=0.30799, acc=0.82812
# [13/100] training 14.6% loss=0.28434, acc=0.90625
# [13/100] training 14.8% loss=0.30461, acc=0.82812
# [13/100] training 14.9% loss=0.33691, acc=0.78125
# [13/100] training 15.1% loss=0.41383, acc=0.84375
# [13/100] training 15.4% loss=0.32305, acc=0.85938
# [13/100] training 15.5% loss=0.34088, acc=0.84375
# [13/100] training 15.7% loss=0.37096, acc=0.85938
# [13/100] training 15.8% loss=0.21144, acc=0.92188
# [13/100] training 16.0% loss=0.43627, acc=0.87500
# [13/100] training 16.1% loss=0.59527, acc=0.78125
# [13/100] training 16.3% loss=0.32033, acc=0.82812
# [13/100] training 16.4% loss=0.30878, acc=0.89062
# [13/100] training 16.7% loss=0.35117, acc=0.85938
# [13/100] training 16.9% loss=0.35368, acc=0.82812
# [13/100] training 17.0% loss=0.27836, acc=0.84375
# [13/100] training 17.2% loss=0.33790, acc=0.87500
# [13/100] training 17.3% loss=0.28461, acc=0.87500
# [13/100] training 17.5% loss=0.29677, acc=0.84375
# [13/100] training 17.7% loss=0.29009, acc=0.87500
# [13/100] training 17.9% loss=0.47537, acc=0.79688
# [13/100] training 18.1% loss=0.39214, acc=0.84375
# [13/100] training 18.2% loss=0.37170, acc=0.84375
# [13/100] training 18.4% loss=0.40062, acc=0.81250
# [13/100] training 18.5% loss=0.28990, acc=0.90625
# [13/100] training 18.8% loss=0.24820, acc=0.92188
# [13/100] training 18.9% loss=0.22178, acc=0.89062
# [13/100] training 19.1% loss=0.41630, acc=0.79688
# [13/100] training 19.2% loss=0.23795, acc=0.87500
# [13/100] training 19.4% loss=0.19232, acc=0.93750
# [13/100] training 19.6% loss=0.36300, acc=0.85938
# [13/100] training 19.7% loss=0.32945, acc=0.79688
# [13/100] training 20.0% loss=0.20312, acc=0.90625
# [13/100] training 20.1% loss=0.25333, acc=0.90625
# [13/100] training 20.3% loss=0.39525, acc=0.87500
# [13/100] training 20.4% loss=0.34140, acc=0.78125
# [13/100] training 20.6% loss=0.32841, acc=0.85938
# [13/100] training 20.8% loss=0.25118, acc=0.92188
# [13/100] training 20.9% loss=0.32368, acc=0.89062
# [13/100] training 21.2% loss=0.32940, acc=0.87500
# [13/100] training 21.3% loss=0.34125, acc=0.84375
# [13/100] training 21.5% loss=0.32770, acc=0.89062
# [13/100] training 21.6% loss=0.19896, acc=0.92188
# [13/100] training 21.8% loss=0.25052, acc=0.90625
# [13/100] training 21.9% loss=0.34528, acc=0.89062
# [13/100] training 22.2% loss=0.27724, acc=0.85938
# [13/100] training 22.4% loss=0.39778, acc=0.82812
# [13/100] training 22.5% loss=0.29143, acc=0.87500
# [13/100] training 22.7% loss=0.34207, acc=0.87500
# [13/100] training 22.8% loss=0.40289, acc=0.81250
# [13/100] training 23.0% loss=0.18569, acc=0.93750
# [13/100] training 23.1% loss=0.36006, acc=0.82812
# [13/100] training 23.4% loss=0.34072, acc=0.87500
# [13/100] training 23.6% loss=0.30132, acc=0.90625
# [13/100] training 23.7% loss=0.24673, acc=0.89062
# [13/100] training 23.9% loss=0.26117, acc=0.90625
# [13/100] training 24.0% loss=0.34646, acc=0.82812
# [13/100] training 24.2% loss=0.40927, acc=0.82812
# [13/100] training 24.3% loss=0.36581, acc=0.87500
# [13/100] training 24.6% loss=0.30085, acc=0.87500
# [13/100] training 24.7% loss=0.37976, acc=0.82812
# [13/100] training 24.9% loss=0.26747, acc=0.90625
# [13/100] training 25.1% loss=0.42402, acc=0.78125
# [13/100] training 25.2% loss=0.27074, acc=0.90625
# [13/100] training 25.4% loss=0.41684, acc=0.78125
# [13/100] training 25.6% loss=0.26069, acc=0.90625
# [13/100] training 25.8% loss=0.44811, acc=0.78125
# [13/100] training 25.9% loss=0.31731, acc=0.87500
# [13/100] training 26.1% loss=0.35325, acc=0.84375
# [13/100] training 26.3% loss=0.30768, acc=0.85938
# [13/100] training 26.4% loss=0.19519, acc=0.92188
# [13/100] training 26.6% loss=0.27032, acc=0.90625
# [13/100] training 26.8% loss=0.27239, acc=0.87500
# [13/100] training 27.0% loss=0.33399, acc=0.87500
# [13/100] training 27.1% loss=0.27761, acc=0.89062
# [13/100] training 27.3% loss=0.31627, acc=0.81250
# [13/100] training 27.4% loss=0.26819, acc=0.89062
# [13/100] training 27.6% loss=0.30017, acc=0.89062
# [13/100] training 27.9% loss=0.28752, acc=0.84375
# [13/100] training 28.0% loss=0.42580, acc=0.81250
# [13/100] training 28.2% loss=0.29973, acc=0.89062
# [13/100] training 28.3% loss=0.21555, acc=0.89062
# [13/100] training 28.5% loss=0.38348, acc=0.79688
# [13/100] training 28.6% loss=0.35663, acc=0.85938
# [13/100] training 28.8% loss=0.17353, acc=0.96875
# [13/100] training 29.1% loss=0.34600, acc=0.85938
# [13/100] training 29.2% loss=0.29492, acc=0.84375
# [13/100] training 29.4% loss=0.35515, acc=0.81250
# [13/100] training 29.5% loss=0.23396, acc=0.90625
# [13/100] training 29.7% loss=0.43435, acc=0.87500
# [13/100] training 29.8% loss=0.23766, acc=0.87500
# [13/100] training 30.0% loss=0.41110, acc=0.76562
# [13/100] training 30.2% loss=0.29267, acc=0.92188
# [13/100] training 30.4% loss=0.23729, acc=0.92188
# [13/100] training 30.6% loss=0.33359, acc=0.82812
# [13/100] training 30.7% loss=0.23704, acc=0.85938
# [13/100] training 30.9% loss=0.22444, acc=0.92188
# [13/100] training 31.0% loss=0.23925, acc=0.89062
# [13/100] training 31.3% loss=0.32234, acc=0.84375
# [13/100] training 31.4% loss=0.40265, acc=0.84375
# [13/100] training 31.6% loss=0.51928, acc=0.81250
# [13/100] training 31.8% loss=0.31184, acc=0.87500
# [13/100] training 31.9% loss=0.29402, acc=0.85938
# [13/100] training 32.1% loss=0.33040, acc=0.84375
# [13/100] training 32.2% loss=0.30129, acc=0.85938
# [13/100] training 32.5% loss=0.31115, acc=0.85938
# [13/100] training 32.6% loss=0.30198, acc=0.84375
# [13/100] training 32.8% loss=0.21690, acc=0.92188
# [13/100] training 32.9% loss=0.37536, acc=0.82812
# [13/100] training 33.1% loss=0.34773, acc=0.84375
# [13/100] training 33.3% loss=0.30249, acc=0.87500
# [13/100] training 33.4% loss=0.32511, acc=0.90625
# [13/100] training 33.7% loss=0.30221, acc=0.89062
# [13/100] training 33.8% loss=0.43798, acc=0.82812
# [13/100] training 34.0% loss=0.26363, acc=0.87500
# [13/100] training 34.1% loss=0.28258, acc=0.89062
# [13/100] training 34.3% loss=0.31843, acc=0.85938
# [13/100] training 34.5% loss=0.34895, acc=0.81250
# [13/100] training 34.7% loss=0.19283, acc=0.95312
# [13/100] training 34.9% loss=0.26305, acc=0.87500
# [13/100] training 35.0% loss=0.28153, acc=0.85938
# [13/100] training 35.2% loss=0.42451, acc=0.79688
# [13/100] training 35.3% loss=0.33079, acc=0.82812
# [13/100] training 35.5% loss=0.35627, acc=0.78125
# [13/100] training 35.6% loss=0.36895, acc=0.82812
# [13/100] training 35.9% loss=0.42163, acc=0.78125
# [13/100] training 36.1% loss=0.37922, acc=0.85938
# [13/100] training 36.2% loss=0.31737, acc=0.85938
# [13/100] training 36.4% loss=0.39823, acc=0.82812
# [13/100] training 36.5% loss=0.28480, acc=0.90625
# [13/100] training 36.7% loss=0.24417, acc=0.90625
# [13/100] training 36.8% loss=0.23100, acc=0.90625
# [13/100] training 37.1% loss=0.39575, acc=0.79688
# [13/100] training 37.3% loss=0.34206, acc=0.84375
# [13/100] training 37.4% loss=0.35419, acc=0.79688
# [13/100] training 37.6% loss=0.35144, acc=0.82812
# [13/100] training 37.7% loss=0.34288, acc=0.89062
# [13/100] training 37.9% loss=0.28969, acc=0.87500
# [13/100] training 38.1% loss=0.36297, acc=0.85938
# [13/100] training 38.3% loss=0.35280, acc=0.84375
# [13/100] training 38.4% loss=0.23594, acc=0.93750
# [13/100] training 38.6% loss=0.23447, acc=0.87500
# [13/100] training 38.8% loss=0.30105, acc=0.90625
# [13/100] training 38.9% loss=0.27846, acc=0.87500
# [13/100] training 39.1% loss=0.34818, acc=0.84375
# [13/100] training 39.3% loss=0.34145, acc=0.85938
# [13/100] training 39.5% loss=0.39949, acc=0.79688
# [13/100] training 39.6% loss=0.19861, acc=0.92188
# [13/100] training 39.8% loss=0.22588, acc=0.87500
# [13/100] training 40.0% loss=0.42351, acc=0.82812
# [13/100] training 40.1% loss=0.49339, acc=0.82812
# [13/100] training 40.4% loss=0.25584, acc=0.85938
# [13/100] training 40.5% loss=0.26889, acc=0.87500
# [13/100] training 40.7% loss=0.30628, acc=0.89062
# [13/100] training 40.8% loss=0.25887, acc=0.89062
# [13/100] training 41.0% loss=0.22727, acc=0.95312
# [13/100] training 41.1% loss=0.48201, acc=0.78125
# [13/100] training 41.3% loss=0.47295, acc=0.75000
# [13/100] training 41.6% loss=0.33314, acc=0.87500
# [13/100] training 41.7% loss=0.40987, acc=0.78125
# [13/100] training 41.9% loss=0.24989, acc=0.92188
# [13/100] training 42.0% loss=0.35222, acc=0.84375
# [13/100] training 42.2% loss=0.34756, acc=0.85938
# [13/100] training 42.3% loss=0.29019, acc=0.85938
# [13/100] training 42.5% loss=0.24352, acc=0.90625
# [13/100] training 42.8% loss=0.21881, acc=0.92188
# [13/100] training 42.9% loss=0.24825, acc=0.89062
# [13/100] training 43.1% loss=0.30239, acc=0.89062
# [13/100] training 43.2% loss=0.23382, acc=0.89062
# [13/100] training 43.4% loss=0.38725, acc=0.87500
# [13/100] training 43.5% loss=0.25308, acc=0.87500
# [13/100] training 43.8% loss=0.38892, acc=0.84375
# [13/100] training 43.9% loss=0.29009, acc=0.87500
# [13/100] training 44.1% loss=0.23255, acc=0.90625
# [13/100] training 44.3% loss=0.35417, acc=0.89062
# [13/100] training 44.4% loss=0.31276, acc=0.82812
# [13/100] training 44.6% loss=0.43879, acc=0.79688
# [13/100] training 44.7% loss=0.34994, acc=0.89062
# [13/100] training 45.0% loss=0.25873, acc=0.89062
# [13/100] training 45.1% loss=0.38078, acc=0.85938
# [13/100] training 45.3% loss=0.29244, acc=0.85938
# [13/100] training 45.5% loss=0.21604, acc=0.95312
# [13/100] training 45.6% loss=0.24516, acc=0.93750
# [13/100] training 45.8% loss=0.30521, acc=0.82812
# [13/100] training 45.9% loss=0.30215, acc=0.87500
# [13/100] training 46.2% loss=0.32189, acc=0.84375
# [13/100] training 46.3% loss=0.23464, acc=0.87500
# [13/100] training 46.5% loss=0.45983, acc=0.82812
# [13/100] training 46.6% loss=0.29313, acc=0.85938
# [13/100] training 46.8% loss=0.25294, acc=0.92188
# [13/100] training 47.0% loss=0.41559, acc=0.81250
# [13/100] training 47.2% loss=0.40688, acc=0.82812
# [13/100] training 47.4% loss=0.26940, acc=0.87500
# [13/100] training 47.5% loss=0.31099, acc=0.87500
# [13/100] training 47.7% loss=0.38585, acc=0.85938
# [13/100] training 47.8% loss=0.35392, acc=0.85938
# [13/100] training 48.0% loss=0.39677, acc=0.84375
# [13/100] training 48.3% loss=0.15253, acc=0.93750
# [13/100] training 48.4% loss=0.19117, acc=0.90625
# [13/100] training 48.6% loss=0.20909, acc=0.93750
# [13/100] training 48.7% loss=0.42269, acc=0.82812
# [13/100] training 48.9% loss=0.31818, acc=0.87500
# [13/100] training 49.0% loss=0.31406, acc=0.79688
# [13/100] training 49.2% loss=0.34619, acc=0.87500
# [13/100] training 49.3% loss=0.28726, acc=0.87500
# [13/100] training 49.6% loss=0.24151, acc=0.90625
# [13/100] training 49.8% loss=0.39391, acc=0.79688
# [13/100] training 49.9% loss=0.31711, acc=0.82812
# [13/100] training 50.1% loss=0.27845, acc=0.89062
# [13/100] training 50.2% loss=0.33279, acc=0.89062
# [13/100] training 50.4% loss=0.37062, acc=0.85938
# [13/100] training 50.6% loss=0.46081, acc=0.81250
# [13/100] training 50.8% loss=0.46623, acc=0.75000
# [13/100] training 51.0% loss=0.42291, acc=0.84375
# [13/100] training 51.1% loss=0.36497, acc=0.76562
# [13/100] training 51.3% loss=0.38865, acc=0.87500
# [13/100] training 51.4% loss=0.25684, acc=0.92188
# [13/100] training 51.7% loss=0.47872, acc=0.79688
# [13/100] training 51.8% loss=0.36195, acc=0.85938
# [13/100] training 52.0% loss=0.24258, acc=0.90625
# [13/100] training 52.1% loss=0.45848, acc=0.85938
# [13/100] training 52.3% loss=0.35198, acc=0.89062
# [13/100] training 52.5% loss=0.18051, acc=0.95312
# [13/100] training 52.6% loss=0.25383, acc=0.90625
# [13/100] training 52.9% loss=0.53336, acc=0.79688
# [13/100] training 53.0% loss=0.28166, acc=0.84375
# [13/100] training 53.2% loss=0.30858, acc=0.89062
# [13/100] training 53.3% loss=0.23144, acc=0.92188
# [13/100] training 53.5% loss=0.34073, acc=0.87500
# [13/100] training 53.7% loss=0.24070, acc=0.90625
# [13/100] training 53.8% loss=0.36775, acc=0.84375
# [13/100] training 54.1% loss=0.40221, acc=0.79688
# [13/100] training 54.2% loss=0.26947, acc=0.89062
# [13/100] training 54.4% loss=0.28308, acc=0.89062
# [13/100] training 54.5% loss=0.35928, acc=0.84375
# [13/100] training 54.7% loss=0.40785, acc=0.78125
# [13/100] training 54.8% loss=0.18333, acc=0.95312
# [13/100] training 55.1% loss=0.28572, acc=0.89062
# [13/100] training 55.3% loss=0.18744, acc=0.90625
# [13/100] training 55.4% loss=0.29093, acc=0.85938
# [13/100] training 55.6% loss=0.30920, acc=0.87500
# [13/100] training 55.7% loss=0.32487, acc=0.85938
# [13/100] training 55.9% loss=0.27785, acc=0.87500
# [13/100] training 56.0% loss=0.40839, acc=0.82812
# [13/100] training 56.3% loss=0.49268, acc=0.76562
# [13/100] training 56.5% loss=0.31757, acc=0.84375
# [13/100] training 56.6% loss=0.37102, acc=0.84375
# [13/100] training 56.8% loss=0.34983, acc=0.85938
# [13/100] training 56.9% loss=0.28818, acc=0.85938
# [13/100] training 57.1% loss=0.31435, acc=0.87500
# [13/100] training 57.2% loss=0.16145, acc=0.96875
# [13/100] training 57.5% loss=0.28083, acc=0.92188
# [13/100] training 57.6% loss=0.33986, acc=0.81250
# [13/100] training 57.8% loss=0.24989, acc=0.89062
# [13/100] training 58.0% loss=0.21862, acc=0.90625
# [13/100] training 58.1% loss=0.29450, acc=0.89062
# [13/100] training 58.3% loss=0.27193, acc=0.87500
# [13/100] training 58.4% loss=0.26185, acc=0.90625
# [13/100] training 58.7% loss=0.28252, acc=0.85938
# [13/100] training 58.8% loss=0.41008, acc=0.81250
# [13/100] training 59.0% loss=0.25671, acc=0.92188
# [13/100] training 59.2% loss=0.34048, acc=0.82812
# [13/100] training 59.3% loss=0.31746, acc=0.85938
# [13/100] training 59.5% loss=0.32853, acc=0.84375
# [13/100] training 59.7% loss=0.29987, acc=0.87500
# [13/100] training 59.9% loss=0.31574, acc=0.90625
# [13/100] training 60.0% loss=0.27828, acc=0.92188
# [13/100] training 60.2% loss=0.37424, acc=0.82812
# [13/100] training 60.3% loss=0.36196, acc=0.84375
# [13/100] training 60.5% loss=0.27582, acc=0.90625
# [13/100] training 60.8% loss=0.37040, acc=0.82812
# [13/100] training 60.9% loss=0.43431, acc=0.79688
# [13/100] training 61.1% loss=0.38253, acc=0.82812
# [13/100] training 61.2% loss=0.30487, acc=0.87500
# [13/100] training 61.4% loss=0.31079, acc=0.85938
# [13/100] training 61.5% loss=0.47320, acc=0.79688
# [13/100] training 61.7% loss=0.37795, acc=0.87500
# [13/100] training 62.0% loss=0.36890, acc=0.76562
# [13/100] training 62.1% loss=0.37608, acc=0.82812
# [13/100] training 62.3% loss=0.24748, acc=0.92188
# [13/100] training 62.4% loss=0.30079, acc=0.81250
# [13/100] training 62.6% loss=0.46464, acc=0.79688
# [13/100] training 62.7% loss=0.28315, acc=0.85938
# [13/100] training 62.9% loss=0.32061, acc=0.84375
# [13/100] training 63.1% loss=0.19240, acc=0.90625
# [13/100] training 63.3% loss=0.34090, acc=0.87500
# [13/100] training 63.5% loss=0.36652, acc=0.87500
# [13/100] training 63.6% loss=0.26710, acc=0.92188
# [13/100] training 63.8% loss=0.36950, acc=0.85938
# [13/100] training 63.9% loss=0.25064, acc=0.92188
# [13/100] training 64.2% loss=0.23885, acc=0.90625
# [13/100] training 64.3% loss=0.37660, acc=0.81250
# [13/100] training 64.5% loss=0.29861, acc=0.87500
# [13/100] training 64.7% loss=0.26960, acc=0.84375
# [13/100] training 64.8% loss=0.55254, acc=0.79688
# [13/100] training 65.0% loss=0.30735, acc=0.87500
# [13/100] training 65.1% loss=0.32628, acc=0.85938
# [13/100] training 65.4% loss=0.22609, acc=0.90625
# [13/100] training 65.5% loss=0.20827, acc=0.95312
# [13/100] training 65.7% loss=0.20649, acc=0.92188
# [13/100] training 65.8% loss=0.33362, acc=0.90625
# [13/100] training 66.0% loss=0.32698, acc=0.82812
# [13/100] training 66.2% loss=0.14982, acc=0.95312
# [13/100] training 66.3% loss=0.37735, acc=0.84375
# [13/100] training 66.6% loss=0.37773, acc=0.78125
# [13/100] training 66.7% loss=0.24455, acc=0.95312
# [13/100] training 66.9% loss=0.36850, acc=0.82812
# [13/100] training 67.0% loss=0.41158, acc=0.78125
# [13/100] training 67.2% loss=0.30116, acc=0.85938
# [13/100] training 67.4% loss=0.24436, acc=0.92188
# [13/100] training 67.6% loss=0.29376, acc=0.90625
# [13/100] training 67.8% loss=0.33842, acc=0.85938
# [13/100] training 67.9% loss=0.17836, acc=0.96875
# [13/100] training 68.1% loss=0.28388, acc=0.90625
# [13/100] training 68.2% loss=0.19568, acc=0.92188
# [13/100] training 68.4% loss=0.22586, acc=0.92188
# [13/100] training 68.5% loss=0.39548, acc=0.84375
# [13/100] training 68.8% loss=0.36329, acc=0.85938
# [13/100] training 69.0% loss=0.38482, acc=0.82812
# [13/100] training 69.1% loss=0.20746, acc=0.93750
# [13/100] training 69.3% loss=0.32141, acc=0.84375
# [13/100] training 69.4% loss=0.34572, acc=0.85938
# [13/100] training 69.6% loss=0.37212, acc=0.87500
# [13/100] training 69.7% loss=0.32629, acc=0.85938
# [13/100] training 70.0% loss=0.38012, acc=0.79688
# [13/100] training 70.2% loss=0.32848, acc=0.89062
# [13/100] training 70.3% loss=0.33090, acc=0.82812
# [13/100] training 70.5% loss=0.34996, acc=0.78125
# [13/100] training 70.6% loss=0.25716, acc=0.90625
# [13/100] training 70.8% loss=0.31761, acc=0.85938
# [13/100] training 71.0% loss=0.28961, acc=0.87500
# [13/100] training 71.2% loss=0.32399, acc=0.85938
# [13/100] training 71.3% loss=0.23459, acc=0.93750
# [13/100] training 71.5% loss=0.40471, acc=0.85938
# [13/100] training 71.7% loss=0.34767, acc=0.85938
# [13/100] training 71.8% loss=0.33342, acc=0.84375
# [13/100] training 72.0% loss=0.20278, acc=0.93750
# [13/100] training 72.2% loss=0.29053, acc=0.87500
# [13/100] training 72.4% loss=0.37651, acc=0.84375
# [13/100] training 72.5% loss=0.41179, acc=0.81250
# [13/100] training 72.7% loss=0.40117, acc=0.81250
# [13/100] training 72.9% loss=0.26905, acc=0.87500
# [13/100] training 73.0% loss=0.22394, acc=0.95312
# [13/100] training 73.3% loss=0.30484, acc=0.84375
# [13/100] training 73.4% loss=0.19420, acc=0.95312
# [13/100] training 73.6% loss=0.24118, acc=0.89062
# [13/100] training 73.7% loss=0.30224, acc=0.87500
# [13/100] training 73.9% loss=0.27592, acc=0.85938
# [13/100] training 74.0% loss=0.38717, acc=0.85938
# [13/100] training 74.2% loss=0.30276, acc=0.84375
# [13/100] training 74.5% loss=0.27271, acc=0.87500
# [13/100] training 74.6% loss=0.47198, acc=0.76562
# [13/100] training 74.8% loss=0.39819, acc=0.81250
# [13/100] training 74.9% loss=0.46433, acc=0.78125
# [13/100] training 75.1% loss=0.27824, acc=0.85938
# [13/100] training 75.2% loss=0.24271, acc=0.90625
# [13/100] training 75.4% loss=0.29204, acc=0.89062
# [13/100] training 75.7% loss=0.27026, acc=0.85938
# [13/100] training 75.8% loss=0.40669, acc=0.84375
# [13/100] training 76.0% loss=0.17419, acc=0.93750
# [13/100] training 76.1% loss=0.40350, acc=0.81250
# [13/100] training 76.3% loss=0.23821, acc=0.95312
# [13/100] training 76.4% loss=0.31135, acc=0.81250
# [13/100] training 76.7% loss=0.22736, acc=0.92188
# [13/100] training 76.8% loss=0.19816, acc=0.93750
# [13/100] training 77.0% loss=0.22666, acc=0.90625
# [13/100] training 77.2% loss=0.38413, acc=0.85938
# [13/100] training 77.3% loss=0.17415, acc=0.93750
# [13/100] training 77.5% loss=0.25708, acc=0.84375
# [13/100] training 77.6% loss=0.32035, acc=0.85938
# [13/100] training 77.9% loss=0.24982, acc=0.85938
# [13/100] training 78.0% loss=0.29118, acc=0.87500
# [13/100] training 78.2% loss=0.31614, acc=0.87500
# [13/100] training 78.4% loss=0.13618, acc=0.93750
# [13/100] training 78.5% loss=0.37908, acc=0.87500
# [13/100] training 78.7% loss=0.31226, acc=0.87500
# [13/100] training 78.8% loss=0.19242, acc=0.92188
# [13/100] training 79.1% loss=0.22422, acc=0.90625
# [13/100] training 79.2% loss=0.20767, acc=0.95312
# [13/100] training 79.4% loss=0.51497, acc=0.76562
# [13/100] training 79.5% loss=0.29015, acc=0.87500
# [13/100] training 79.7% loss=0.15100, acc=0.93750
# [13/100] training 79.9% loss=0.31042, acc=0.87500
# [13/100] training 80.1% loss=0.30901, acc=0.89062
# [13/100] training 80.3% loss=0.40411, acc=0.78125
# [13/100] training 80.4% loss=0.29318, acc=0.84375
# [13/100] training 80.6% loss=0.35757, acc=0.78125
# [13/100] training 80.7% loss=0.21218, acc=0.95312
# [13/100] training 80.9% loss=0.46155, acc=0.82812
# [13/100] training 81.2% loss=0.34328, acc=0.87500
# [13/100] training 81.3% loss=0.32302, acc=0.87500
# [13/100] training 81.5% loss=0.26881, acc=0.90625
# [13/100] training 81.6% loss=0.32582, acc=0.82812
# [13/100] training 81.8% loss=0.32615, acc=0.85938
# [13/100] training 81.9% loss=0.43750, acc=0.87500
# [13/100] training 82.1% loss=0.24469, acc=0.85938
# [13/100] training 82.2% loss=0.37674, acc=0.85938
# [13/100] training 82.5% loss=0.25072, acc=0.92188
# [13/100] training 82.7% loss=0.32035, acc=0.87500
# [13/100] training 82.8% loss=0.33266, acc=0.87500
# [13/100] training 83.0% loss=0.28752, acc=0.87500
# [13/100] training 83.1% loss=0.27459, acc=0.90625
# [13/100] training 83.3% loss=0.22000, acc=0.90625
# [13/100] training 83.5% loss=0.23312, acc=0.89062
# [13/100] training 83.7% loss=0.45328, acc=0.85938
# [13/100] training 83.9% loss=0.29947, acc=0.84375
# [13/100] training 84.0% loss=0.25543, acc=0.90625
# [13/100] training 84.2% loss=0.20424, acc=0.90625
# [13/100] training 84.3% loss=0.26179, acc=0.87500
# [13/100] training 84.5% loss=0.21582, acc=0.92188
# [13/100] training 84.7% loss=0.25099, acc=0.85938
# [13/100] training 84.9% loss=0.29483, acc=0.89062
# [13/100] training 85.0% loss=0.32787, acc=0.81250
# [13/100] training 85.2% loss=0.28064, acc=0.89062
# [13/100] training 85.4% loss=0.31785, acc=0.92188
# [13/100] training 85.5% loss=0.31919, acc=0.87500
# [13/100] training 85.8% loss=0.26597, acc=0.89062
# [13/100] training 85.9% loss=0.31978, acc=0.92188
# [13/100] training 86.1% loss=0.33407, acc=0.81250
# [13/100] training 86.2% loss=0.27684, acc=0.85938
# [13/100] training 86.4% loss=0.38081, acc=0.82812
# [13/100] training 86.6% loss=0.40834, acc=0.84375
# [13/100] training 86.7% loss=0.25037, acc=0.90625
# [13/100] training 87.0% loss=0.29872, acc=0.89062
# [13/100] training 87.1% loss=0.32484, acc=0.85938
# [13/100] training 87.3% loss=0.33042, acc=0.84375
# [13/100] training 87.4% loss=0.30908, acc=0.89062
# [13/100] training 87.6% loss=0.30709, acc=0.89062
# [13/100] training 87.7% loss=0.41910, acc=0.89062
# [13/100] training 87.9% loss=0.31516, acc=0.87500
# [13/100] training 88.2% loss=0.18892, acc=0.93750
# [13/100] training 88.3% loss=0.32956, acc=0.89062
# [13/100] training 88.5% loss=0.32942, acc=0.81250
# [13/100] training 88.6% loss=0.15650, acc=0.92188
# [13/100] training 88.8% loss=0.26485, acc=0.87500
# [13/100] training 88.9% loss=0.32099, acc=0.89062
# [13/100] training 89.2% loss=0.25562, acc=0.92188
# [13/100] training 89.4% loss=0.25240, acc=0.87500
# [13/100] training 89.5% loss=0.29980, acc=0.89062
# [13/100] training 89.7% loss=0.32736, acc=0.85938
# [13/100] training 89.8% loss=0.27485, acc=0.92188
# [13/100] training 90.0% loss=0.23682, acc=0.89062
# [13/100] training 90.1% loss=0.27778, acc=0.89062
# [13/100] training 90.4% loss=0.22843, acc=0.92188
# [13/100] training 90.5% loss=0.18480, acc=0.92188
# [13/100] training 90.7% loss=0.33057, acc=0.85938
# [13/100] training 90.9% loss=0.12856, acc=0.93750
# [13/100] training 91.0% loss=0.29409, acc=0.87500
# [13/100] training 91.2% loss=0.31405, acc=0.82812
# [13/100] training 91.3% loss=0.43458, acc=0.81250
# [13/100] training 91.6% loss=0.37122, acc=0.81250
# [13/100] training 91.7% loss=0.24452, acc=0.87500
# [13/100] training 91.9% loss=0.33008, acc=0.84375
# [13/100] training 92.1% loss=0.31761, acc=0.84375
# [13/100] training 92.2% loss=0.17911, acc=0.96875
# [13/100] training 92.4% loss=0.25608, acc=0.90625
# [13/100] training 92.6% loss=0.41936, acc=0.76562
# [13/100] training 92.8% loss=0.40367, acc=0.85938
# [13/100] training 92.9% loss=0.27660, acc=0.92188
# [13/100] training 93.1% loss=0.33425, acc=0.84375
# [13/100] training 93.2% loss=0.28342, acc=0.89062
# [13/100] training 93.4% loss=0.25711, acc=0.93750
# [13/100] training 93.7% loss=0.22856, acc=0.87500
# [13/100] training 93.8% loss=0.26423, acc=0.87500
# [13/100] training 94.0% loss=0.20656, acc=0.89062
# [13/100] training 94.1% loss=0.32154, acc=0.84375
# [13/100] training 94.3% loss=0.18034, acc=0.93750
# [13/100] training 94.4% loss=0.29457, acc=0.84375
# [13/100] training 94.6% loss=0.19250, acc=0.92188
# [13/100] training 94.9% loss=0.31780, acc=0.81250
# [13/100] training 95.0% loss=0.51228, acc=0.81250
# [13/100] training 95.2% loss=0.42020, acc=0.82812
# [13/100] training 95.3% loss=0.29343, acc=0.92188
# [13/100] training 95.5% loss=0.25714, acc=0.92188
# [13/100] training 95.6% loss=0.37484, acc=0.87500
# [13/100] training 95.8% loss=0.39489, acc=0.78125
# [13/100] training 96.0% loss=0.26347, acc=0.92188
# [13/100] training 96.2% loss=0.24409, acc=0.89062
# [13/100] training 96.4% loss=0.27310, acc=0.90625
# [13/100] training 96.5% loss=0.30472, acc=0.85938
# [13/100] training 96.7% loss=0.22828, acc=0.90625
# [13/100] training 96.8% loss=0.36258, acc=0.82812
# [13/100] training 97.1% loss=0.24284, acc=0.85938
# [13/100] training 97.2% loss=0.36835, acc=0.82812
# [13/100] training 97.4% loss=0.25707, acc=0.87500
# [13/100] training 97.6% loss=0.35941, acc=0.84375
# [13/100] training 97.7% loss=0.21727, acc=0.90625
# [13/100] training 97.9% loss=0.23799, acc=0.90625
# [13/100] training 98.0% loss=0.28103, acc=0.87500
# [13/100] training 98.3% loss=0.26582, acc=0.90625
# [13/100] training 98.4% loss=0.23578, acc=0.89062
# [13/100] training 98.6% loss=0.40222, acc=0.84375
# [13/100] training 98.7% loss=0.41131, acc=0.85938
# [13/100] training 98.9% loss=0.27495, acc=0.85938
# [13/100] training 99.1% loss=0.24808, acc=0.90625
# [13/100] training 99.2% loss=0.22587, acc=0.89062
# [13/100] training 99.5% loss=0.42810, acc=0.81250
# [13/100] training 99.6% loss=0.28819, acc=0.87500
# [13/100] training 99.8% loss=0.30067, acc=0.89062
# [13/100] training 99.9% loss=0.12683, acc=1.00000
# [13/100] testing 0.9% loss=0.20137, acc=0.92188
# [13/100] testing 1.8% loss=0.50782, acc=0.76562
# [13/100] testing 2.2% loss=0.29486, acc=0.87500
# [13/100] testing 3.1% loss=0.32629, acc=0.84375
# [13/100] testing 3.5% loss=0.21124, acc=0.90625
# [13/100] testing 4.4% loss=0.36320, acc=0.81250
# [13/100] testing 4.8% loss=0.35908, acc=0.81250
# [13/100] testing 5.7% loss=0.36566, acc=0.87500
# [13/100] testing 6.6% loss=0.23280, acc=0.90625
# [13/100] testing 7.0% loss=0.25480, acc=0.89062
# [13/100] testing 7.9% loss=0.37546, acc=0.82812
# [13/100] testing 8.3% loss=0.25464, acc=0.89062
# [13/100] testing 9.2% loss=0.31844, acc=0.87500
# [13/100] testing 9.7% loss=0.19216, acc=0.93750
# [13/100] testing 10.5% loss=0.38576, acc=0.84375
# [13/100] testing 11.0% loss=0.27789, acc=0.85938
# [13/100] testing 11.8% loss=0.44585, acc=0.84375
# [13/100] testing 12.7% loss=0.40541, acc=0.82812
# [13/100] testing 13.2% loss=0.26005, acc=0.87500
# [13/100] testing 14.0% loss=0.42097, acc=0.85938
# [13/100] testing 14.5% loss=0.43074, acc=0.81250
# [13/100] testing 15.4% loss=0.35536, acc=0.85938
# [13/100] testing 15.8% loss=0.24278, acc=0.89062
# [13/100] testing 16.7% loss=0.27214, acc=0.89062
# [13/100] testing 17.5% loss=0.36749, acc=0.81250
# [13/100] testing 18.0% loss=0.28874, acc=0.90625
# [13/100] testing 18.9% loss=0.16594, acc=0.92188
# [13/100] testing 19.3% loss=0.32183, acc=0.92188
# [13/100] testing 20.2% loss=0.41210, acc=0.84375
# [13/100] testing 20.6% loss=0.39670, acc=0.87500
# [13/100] testing 21.5% loss=0.34009, acc=0.89062
# [13/100] testing 21.9% loss=0.45254, acc=0.78125
# [13/100] testing 22.8% loss=0.43791, acc=0.82812
# [13/100] testing 23.7% loss=0.29850, acc=0.90625
# [13/100] testing 24.1% loss=0.26587, acc=0.89062
# [13/100] testing 25.0% loss=0.42699, acc=0.85938
# [13/100] testing 25.4% loss=0.19509, acc=0.90625
# [13/100] testing 26.3% loss=0.34163, acc=0.87500
# [13/100] testing 26.8% loss=0.25441, acc=0.87500
# [13/100] testing 27.6% loss=0.38690, acc=0.85938
# [13/100] testing 28.5% loss=0.33872, acc=0.90625
# [13/100] testing 29.0% loss=0.23701, acc=0.89062
# [13/100] testing 29.8% loss=0.39946, acc=0.87500
# [13/100] testing 30.3% loss=0.32299, acc=0.90625
# [13/100] testing 31.1% loss=0.42409, acc=0.79688
# [13/100] testing 31.6% loss=0.26835, acc=0.85938
# [13/100] testing 32.5% loss=0.28994, acc=0.89062
# [13/100] testing 32.9% loss=0.37051, acc=0.87500
# [13/100] testing 33.8% loss=0.37264, acc=0.84375
# [13/100] testing 34.7% loss=0.35015, acc=0.82812
# [13/100] testing 35.1% loss=0.28548, acc=0.79688
# [13/100] testing 36.0% loss=0.32487, acc=0.82812
# [13/100] testing 36.4% loss=0.28244, acc=0.85938
# [13/100] testing 37.3% loss=0.41444, acc=0.84375
# [13/100] testing 37.7% loss=0.45446, acc=0.81250
# [13/100] testing 38.6% loss=0.23669, acc=0.92188
# [13/100] testing 39.5% loss=0.40653, acc=0.82812
# [13/100] testing 39.9% loss=0.42187, acc=0.84375
# [13/100] testing 40.8% loss=0.35782, acc=0.82812
# [13/100] testing 41.2% loss=0.27036, acc=0.92188
# [13/100] testing 42.1% loss=0.33156, acc=0.85938
# [13/100] testing 42.5% loss=0.32861, acc=0.82812
# [13/100] testing 43.4% loss=0.29423, acc=0.89062
# [13/100] testing 43.9% loss=0.25806, acc=0.90625
# [13/100] testing 44.7% loss=0.43624, acc=0.84375
# [13/100] testing 45.6% loss=0.24768, acc=0.87500
# [13/100] testing 46.1% loss=0.30544, acc=0.87500
# [13/100] testing 46.9% loss=0.31543, acc=0.87500
# [13/100] testing 47.4% loss=0.22814, acc=0.87500
# [13/100] testing 48.3% loss=0.32512, acc=0.84375
# [13/100] testing 48.7% loss=0.52017, acc=0.78125
# [13/100] testing 49.6% loss=0.39330, acc=0.81250
# [13/100] testing 50.4% loss=0.25360, acc=0.89062
# [13/100] testing 50.9% loss=0.30486, acc=0.82812
# [13/100] testing 51.8% loss=0.28834, acc=0.90625
# [13/100] testing 52.2% loss=0.29275, acc=0.89062
# [13/100] testing 53.1% loss=0.26524, acc=0.89062
# [13/100] testing 53.5% loss=0.29483, acc=0.90625
# [13/100] testing 54.4% loss=0.51940, acc=0.78125
# [13/100] testing 54.8% loss=0.29965, acc=0.92188
# [13/100] testing 55.7% loss=0.25424, acc=0.90625
# [13/100] testing 56.6% loss=0.27545, acc=0.85938
# [13/100] testing 57.0% loss=0.45008, acc=0.78125
# [13/100] testing 57.9% loss=0.43112, acc=0.79688
# [13/100] testing 58.3% loss=0.44164, acc=0.79688
# [13/100] testing 59.2% loss=0.39999, acc=0.84375
# [13/100] testing 59.7% loss=0.26093, acc=0.89062
# [13/100] testing 60.5% loss=0.39024, acc=0.85938
# [13/100] testing 61.4% loss=0.19283, acc=0.90625
# [13/100] testing 61.9% loss=0.36457, acc=0.85938
# [13/100] testing 62.7% loss=0.25670, acc=0.85938
# [13/100] testing 63.2% loss=0.40302, acc=0.79688
# [13/100] testing 64.0% loss=0.48138, acc=0.79688
# [13/100] testing 64.5% loss=0.33714, acc=0.84375
# [13/100] testing 65.4% loss=0.19316, acc=0.96875
# [13/100] testing 65.8% loss=0.45938, acc=0.81250
# [13/100] testing 66.7% loss=0.26275, acc=0.89062
# [13/100] testing 67.6% loss=0.33447, acc=0.87500
# [13/100] testing 68.0% loss=0.23072, acc=0.90625
# [13/100] testing 68.9% loss=0.34557, acc=0.85938
# [13/100] testing 69.3% loss=0.44677, acc=0.79688
# [13/100] testing 70.2% loss=0.40886, acc=0.79688
# [13/100] testing 70.6% loss=0.43965, acc=0.81250
# [13/100] testing 71.5% loss=0.37134, acc=0.85938
# [13/100] testing 72.4% loss=0.24596, acc=0.90625
# [13/100] testing 72.8% loss=0.33480, acc=0.84375
# [13/100] testing 73.7% loss=0.23470, acc=0.89062
# [13/100] testing 74.1% loss=0.44088, acc=0.82812
# [13/100] testing 75.0% loss=0.30874, acc=0.85938
# [13/100] testing 75.4% loss=0.35964, acc=0.87500
# [13/100] testing 76.3% loss=0.17999, acc=0.92188
# [13/100] testing 76.8% loss=0.35976, acc=0.84375
# [13/100] testing 77.6% loss=0.24961, acc=0.89062
# [13/100] testing 78.5% loss=0.53718, acc=0.79688
# [13/100] testing 79.0% loss=0.38639, acc=0.82812
# [13/100] testing 79.8% loss=0.35359, acc=0.81250
# [13/100] testing 80.3% loss=0.28594, acc=0.85938
# [13/100] testing 81.2% loss=0.49300, acc=0.81250
# [13/100] testing 81.6% loss=0.26673, acc=0.90625
# [13/100] testing 82.5% loss=0.31579, acc=0.85938
# [13/100] testing 83.3% loss=0.28041, acc=0.89062
# [13/100] testing 83.8% loss=0.21748, acc=0.89062
# [13/100] testing 84.7% loss=0.32674, acc=0.84375
# [13/100] testing 85.1% loss=0.44689, acc=0.79688
# [13/100] testing 86.0% loss=0.35627, acc=0.84375
# [13/100] testing 86.4% loss=0.46524, acc=0.81250
# [13/100] testing 87.3% loss=0.38991, acc=0.85938
# [13/100] testing 87.7% loss=0.33626, acc=0.85938
# [13/100] testing 88.6% loss=0.27585, acc=0.89062
# [13/100] testing 89.5% loss=0.42730, acc=0.79688
# [13/100] testing 89.9% loss=0.32266, acc=0.82812
# [13/100] testing 90.8% loss=0.27718, acc=0.92188
# [13/100] testing 91.2% loss=0.17585, acc=0.92188
# [13/100] testing 92.1% loss=0.42160, acc=0.82812
# [13/100] testing 92.6% loss=0.27607, acc=0.84375
# [13/100] testing 93.4% loss=0.33619, acc=0.85938
# [13/100] testing 94.3% loss=0.19662, acc=0.90625
# [13/100] testing 94.7% loss=0.34663, acc=0.89062
# [13/100] testing 95.6% loss=0.35951, acc=0.81250
# [13/100] testing 96.1% loss=0.25243, acc=0.90625
# [13/100] testing 96.9% loss=0.30890, acc=0.85938
# [13/100] testing 97.4% loss=0.24948, acc=0.90625
# [13/100] testing 98.3% loss=0.33708, acc=0.81250
# [13/100] testing 98.7% loss=0.30371, acc=0.85938
# [13/100] testing 99.6% loss=0.23029, acc=0.90625
# [14/100] training 0.2% loss=0.40834, acc=0.81250
# [14/100] training 0.4% loss=0.41400, acc=0.84375
# [14/100] training 0.5% loss=0.27207, acc=0.93750
# [14/100] training 0.8% loss=0.44193, acc=0.82812
# [14/100] training 0.9% loss=0.26057, acc=0.89062
# [14/100] training 1.1% loss=0.22793, acc=0.93750
# [14/100] training 1.2% loss=0.34338, acc=0.84375
# [14/100] training 1.4% loss=0.35734, acc=0.85938
# [14/100] training 1.6% loss=0.17697, acc=0.93750
# [14/100] training 1.8% loss=0.31183, acc=0.90625
# [14/100] training 2.0% loss=0.27074, acc=0.89062
# [14/100] training 2.1% loss=0.28945, acc=0.89062
# [14/100] training 2.3% loss=0.23449, acc=0.89062
# [14/100] training 2.4% loss=0.35430, acc=0.84375
# [14/100] training 2.6% loss=0.26030, acc=0.85938
# [14/100] training 2.7% loss=0.32026, acc=0.89062
# [14/100] training 3.0% loss=0.30251, acc=0.87500
# [14/100] training 3.2% loss=0.27912, acc=0.92188
# [14/100] training 3.3% loss=0.32968, acc=0.87500
# [14/100] training 3.5% loss=0.30064, acc=0.92188
# [14/100] training 3.6% loss=0.41589, acc=0.81250
# [14/100] training 3.8% loss=0.21971, acc=0.89062
# [14/100] training 3.9% loss=0.22094, acc=0.90625
# [14/100] training 4.2% loss=0.26720, acc=0.89062
# [14/100] training 4.4% loss=0.21712, acc=0.92188
# [14/100] training 4.5% loss=0.29897, acc=0.85938
# [14/100] training 4.7% loss=0.32267, acc=0.85938
# [14/100] training 4.8% loss=0.32502, acc=0.85938
# [14/100] training 5.0% loss=0.28063, acc=0.90625
# [14/100] training 5.2% loss=0.26140, acc=0.87500
# [14/100] training 5.4% loss=0.21279, acc=0.90625
# [14/100] training 5.5% loss=0.26161, acc=0.85938
# [14/100] training 5.7% loss=0.25782, acc=0.85938
# [14/100] training 5.9% loss=0.41733, acc=0.82812
# [14/100] training 6.0% loss=0.38385, acc=0.81250
# [14/100] training 6.3% loss=0.35028, acc=0.87500
# [14/100] training 6.4% loss=0.23656, acc=0.90625
# [14/100] training 6.6% loss=0.31325, acc=0.82812
# [14/100] training 6.7% loss=0.35944, acc=0.78125
# [14/100] training 6.9% loss=0.25723, acc=0.90625
# [14/100] training 7.1% loss=0.30979, acc=0.84375
# [14/100] training 7.2% loss=0.29471, acc=0.89062
# [14/100] training 7.5% loss=0.27010, acc=0.89062
# [14/100] training 7.6% loss=0.25532, acc=0.87500
# [14/100] training 7.8% loss=0.35279, acc=0.85938
# [14/100] training 7.9% loss=0.34502, acc=0.84375
# [14/100] training 8.1% loss=0.23938, acc=0.92188
# [14/100] training 8.2% loss=0.25889, acc=0.95312
# [14/100] training 8.4% loss=0.34543, acc=0.85938
# [14/100] training 8.7% loss=0.29025, acc=0.89062
# [14/100] training 8.8% loss=0.35320, acc=0.78125
# [14/100] training 9.0% loss=0.27546, acc=0.87500
# [14/100] training 9.1% loss=0.34237, acc=0.84375
# [14/100] training 9.3% loss=0.55719, acc=0.78125
# [14/100] training 9.4% loss=0.25027, acc=0.95312
# [14/100] training 9.7% loss=0.23993, acc=0.90625
# [14/100] training 9.9% loss=0.37241, acc=0.82812
# [14/100] training 10.0% loss=0.32548, acc=0.82812
# [14/100] training 10.2% loss=0.22131, acc=0.90625
# [14/100] training 10.3% loss=0.22365, acc=0.90625
# [14/100] training 10.5% loss=0.51202, acc=0.82812
# [14/100] training 10.6% loss=0.27212, acc=0.85938
# [14/100] training 10.9% loss=0.25527, acc=0.90625
# [14/100] training 11.0% loss=0.30494, acc=0.87500
# [14/100] training 11.2% loss=0.23607, acc=0.90625
# [14/100] training 11.4% loss=0.37624, acc=0.81250
# [14/100] training 11.5% loss=0.40796, acc=0.87500
# [14/100] training 11.7% loss=0.14959, acc=0.96875
# [14/100] training 11.8% loss=0.28480, acc=0.85938
# [14/100] training 12.1% loss=0.30457, acc=0.87500
# [14/100] training 12.2% loss=0.23138, acc=0.90625
# [14/100] training 12.4% loss=0.35441, acc=0.81250
# [14/100] training 12.6% loss=0.23275, acc=0.92188
# [14/100] training 12.7% loss=0.28716, acc=0.85938
# [14/100] training 12.9% loss=0.31536, acc=0.90625
# [14/100] training 13.0% loss=0.28781, acc=0.85938
# [14/100] training 13.3% loss=0.27663, acc=0.90625
# [14/100] training 13.4% loss=0.32265, acc=0.85938
# [14/100] training 13.6% loss=0.29026, acc=0.90625
# [14/100] training 13.7% loss=0.47357, acc=0.78125
# [14/100] training 13.9% loss=0.32074, acc=0.82812
# [14/100] training 14.1% loss=0.35064, acc=0.87500
# [14/100] training 14.3% loss=0.33443, acc=0.84375
# [14/100] training 14.5% loss=0.31836, acc=0.82812
# [14/100] training 14.6% loss=0.26146, acc=0.89062
# [14/100] training 14.8% loss=0.29421, acc=0.90625
# [14/100] training 14.9% loss=0.29977, acc=0.81250
# [14/100] training 15.1% loss=0.34815, acc=0.84375
# [14/100] training 15.4% loss=0.29532, acc=0.84375
# [14/100] training 15.5% loss=0.36239, acc=0.84375
# [14/100] training 15.7% loss=0.38775, acc=0.85938
# [14/100] training 15.8% loss=0.21372, acc=0.93750
# [14/100] training 16.0% loss=0.37424, acc=0.89062
# [14/100] training 16.1% loss=0.51556, acc=0.78125
# [14/100] training 16.3% loss=0.28990, acc=0.87500
# [14/100] training 16.4% loss=0.28173, acc=0.89062
# [14/100] training 16.7% loss=0.40055, acc=0.81250
# [14/100] training 16.9% loss=0.45860, acc=0.78125
# [14/100] training 17.0% loss=0.29557, acc=0.89062
# [14/100] training 17.2% loss=0.25732, acc=0.89062
# [14/100] training 17.3% loss=0.24758, acc=0.87500
# [14/100] training 17.5% loss=0.30312, acc=0.82812
# [14/100] training 17.7% loss=0.32493, acc=0.84375
# [14/100] training 17.9% loss=0.36710, acc=0.85938
# [14/100] training 18.1% loss=0.44379, acc=0.82812
# [14/100] training 18.2% loss=0.33992, acc=0.87500
# [14/100] training 18.4% loss=0.49219, acc=0.71875
# [14/100] training 18.5% loss=0.26921, acc=0.92188
# [14/100] training 18.8% loss=0.25872, acc=0.92188
# [14/100] training 18.9% loss=0.19668, acc=0.90625
# [14/100] training 19.1% loss=0.36522, acc=0.82812
# [14/100] training 19.2% loss=0.25994, acc=0.87500
# [14/100] training 19.4% loss=0.15820, acc=0.92188
# [14/100] training 19.6% loss=0.34360, acc=0.84375
# [14/100] training 19.7% loss=0.40711, acc=0.79688
# [14/100] training 20.0% loss=0.18236, acc=0.92188
# [14/100] training 20.1% loss=0.28235, acc=0.89062
# [14/100] training 20.3% loss=0.34566, acc=0.89062
# [14/100] training 20.4% loss=0.31789, acc=0.82812
# [14/100] training 20.6% loss=0.36639, acc=0.87500
# [14/100] training 20.8% loss=0.30710, acc=0.85938
# [14/100] training 20.9% loss=0.35525, acc=0.82812
# [14/100] training 21.2% loss=0.33317, acc=0.81250
# [14/100] training 21.3% loss=0.39395, acc=0.81250
# [14/100] training 21.5% loss=0.27080, acc=0.85938
# [14/100] training 21.6% loss=0.21344, acc=0.92188
# [14/100] training 21.8% loss=0.24413, acc=0.92188
# [14/100] training 21.9% loss=0.36921, acc=0.84375
# [14/100] training 22.2% loss=0.29792, acc=0.84375
# [14/100] training 22.4% loss=0.42270, acc=0.84375
# [14/100] training 22.5% loss=0.26078, acc=0.93750
# [14/100] training 22.7% loss=0.30040, acc=0.90625
# [14/100] training 22.8% loss=0.41226, acc=0.87500
# [14/100] training 23.0% loss=0.20991, acc=0.92188
# [14/100] training 23.1% loss=0.35140, acc=0.85938
# [14/100] training 23.4% loss=0.32490, acc=0.87500
# [14/100] training 23.6% loss=0.32273, acc=0.90625
# [14/100] training 23.7% loss=0.30036, acc=0.87500
# [14/100] training 23.9% loss=0.29228, acc=0.89062
# [14/100] training 24.0% loss=0.26580, acc=0.87500
# [14/100] training 24.2% loss=0.29413, acc=0.84375
# [14/100] training 24.3% loss=0.37940, acc=0.84375
# [14/100] training 24.6% loss=0.24000, acc=0.90625
# [14/100] training 24.7% loss=0.43597, acc=0.79688
# [14/100] training 24.9% loss=0.28126, acc=0.90625
# [14/100] training 25.1% loss=0.35114, acc=0.85938
# [14/100] training 25.2% loss=0.17897, acc=0.92188
# [14/100] training 25.4% loss=0.29347, acc=0.89062
# [14/100] training 25.6% loss=0.22635, acc=0.89062
# [14/100] training 25.8% loss=0.32269, acc=0.82812
# [14/100] training 25.9% loss=0.23530, acc=0.90625
# [14/100] training 26.1% loss=0.31343, acc=0.87500
# [14/100] training 26.3% loss=0.34564, acc=0.89062
# [14/100] training 26.4% loss=0.27004, acc=0.89062
# [14/100] training 26.6% loss=0.21334, acc=0.92188
# [14/100] training 26.8% loss=0.35043, acc=0.82812
# [14/100] training 27.0% loss=0.33191, acc=0.87500
# [14/100] training 27.1% loss=0.22261, acc=0.92188
# [14/100] training 27.3% loss=0.26949, acc=0.90625
# [14/100] training 27.4% loss=0.16912, acc=0.95312
# [14/100] training 27.6% loss=0.30426, acc=0.90625
# [14/100] training 27.9% loss=0.32219, acc=0.85938
# [14/100] training 28.0% loss=0.44387, acc=0.84375
# [14/100] training 28.2% loss=0.28074, acc=0.85938
# [14/100] training 28.3% loss=0.19381, acc=0.92188
# [14/100] training 28.5% loss=0.29327, acc=0.85938
# [14/100] training 28.6% loss=0.30575, acc=0.90625
# [14/100] training 28.8% loss=0.20270, acc=0.92188
# [14/100] training 29.1% loss=0.37229, acc=0.81250
# [14/100] training 29.2% loss=0.27408, acc=0.90625
# [14/100] training 29.4% loss=0.34479, acc=0.85938
# [14/100] training 29.5% loss=0.23081, acc=0.87500
# [14/100] training 29.7% loss=0.40785, acc=0.79688
# [14/100] training 29.8% loss=0.21419, acc=0.89062
# [14/100] training 30.0% loss=0.34830, acc=0.84375
# [14/100] training 30.2% loss=0.25006, acc=0.89062
# [14/100] training 30.4% loss=0.22645, acc=0.92188
# [14/100] training 30.6% loss=0.39271, acc=0.87500
# [14/100] training 30.7% loss=0.28283, acc=0.84375
# [14/100] training 30.9% loss=0.33974, acc=0.85938
# [14/100] training 31.0% loss=0.28894, acc=0.89062
# [14/100] training 31.3% loss=0.28629, acc=0.85938
# [14/100] training 31.4% loss=0.36044, acc=0.79688
# [14/100] training 31.6% loss=0.36974, acc=0.82812
# [14/100] training 31.8% loss=0.22558, acc=0.89062
# [14/100] training 31.9% loss=0.24496, acc=0.93750
# [14/100] training 32.1% loss=0.31159, acc=0.90625
# [14/100] training 32.2% loss=0.28951, acc=0.89062
# [14/100] training 32.5% loss=0.20955, acc=0.90625
# [14/100] training 32.6% loss=0.32285, acc=0.85938
# [14/100] training 32.8% loss=0.20786, acc=0.92188
# [14/100] training 32.9% loss=0.30421, acc=0.89062
# [14/100] training 33.1% loss=0.36314, acc=0.82812
# [14/100] training 33.3% loss=0.30658, acc=0.85938
# [14/100] training 33.4% loss=0.25500, acc=0.92188
# [14/100] training 33.7% loss=0.41519, acc=0.84375
# [14/100] training 33.8% loss=0.33567, acc=0.81250
# [14/100] training 34.0% loss=0.31140, acc=0.87500
# [14/100] training 34.1% loss=0.30393, acc=0.89062
# [14/100] training 34.3% loss=0.24270, acc=0.90625
# [14/100] training 34.5% loss=0.44074, acc=0.81250
# [14/100] training 34.7% loss=0.15144, acc=0.93750
# [14/100] training 34.9% loss=0.22158, acc=0.92188
# [14/100] training 35.0% loss=0.24180, acc=0.92188
# [14/100] training 35.2% loss=0.37668, acc=0.85938
# [14/100] training 35.3% loss=0.23030, acc=0.89062
# [14/100] training 35.5% loss=0.29035, acc=0.85938
# [14/100] training 35.6% loss=0.35608, acc=0.82812
# [14/100] training 35.9% loss=0.28353, acc=0.87500
# [14/100] training 36.1% loss=0.42784, acc=0.73438
# [14/100] training 36.2% loss=0.30257, acc=0.85938
# [14/100] training 36.4% loss=0.34178, acc=0.85938
# [14/100] training 36.5% loss=0.27201, acc=0.92188
# [14/100] training 36.7% loss=0.21672, acc=0.92188
# [14/100] training 36.8% loss=0.23077, acc=0.92188
# [14/100] training 37.1% loss=0.36796, acc=0.85938
# [14/100] training 37.3% loss=0.37454, acc=0.90625
# [14/100] training 37.4% loss=0.33868, acc=0.85938
# [14/100] training 37.6% loss=0.26799, acc=0.84375
# [14/100] training 37.7% loss=0.24408, acc=0.96875
# [14/100] training 37.9% loss=0.30632, acc=0.87500
# [14/100] training 38.1% loss=0.38990, acc=0.82812
# [14/100] training 38.3% loss=0.30944, acc=0.85938
# [14/100] training 38.4% loss=0.22698, acc=0.92188
# [14/100] training 38.6% loss=0.24915, acc=0.92188
# [14/100] training 38.8% loss=0.35608, acc=0.82812
# [14/100] training 38.9% loss=0.28572, acc=0.89062
# [14/100] training 39.1% loss=0.25937, acc=0.89062
# [14/100] training 39.3% loss=0.27628, acc=0.87500
# [14/100] training 39.5% loss=0.34942, acc=0.84375
# [14/100] training 39.6% loss=0.30584, acc=0.89062
# [14/100] training 39.8% loss=0.19224, acc=0.90625
# [14/100] training 40.0% loss=0.30194, acc=0.87500
# [14/100] training 40.1% loss=0.29982, acc=0.85938
# [14/100] training 40.4% loss=0.17151, acc=0.93750
# [14/100] training 40.5% loss=0.29522, acc=0.90625
# [14/100] training 40.7% loss=0.30053, acc=0.85938
# [14/100] training 40.8% loss=0.22682, acc=0.92188
# [14/100] training 41.0% loss=0.20275, acc=0.89062
# [14/100] training 41.1% loss=0.41464, acc=0.85938
# [14/100] training 41.3% loss=0.35140, acc=0.81250
# [14/100] training 41.6% loss=0.36679, acc=0.85938
# [14/100] training 41.7% loss=0.36182, acc=0.79688
# [14/100] training 41.9% loss=0.24734, acc=0.90625
# [14/100] training 42.0% loss=0.27312, acc=0.90625
# [14/100] training 42.2% loss=0.37365, acc=0.84375
# [14/100] training 42.3% loss=0.28674, acc=0.89062
# [14/100] training 42.5% loss=0.23656, acc=0.92188
# [14/100] training 42.8% loss=0.18037, acc=0.92188
# [14/100] training 42.9% loss=0.25972, acc=0.89062
# [14/100] training 43.1% loss=0.25832, acc=0.89062
# [14/100] training 43.2% loss=0.24834, acc=0.92188
# [14/100] training 43.4% loss=0.30128, acc=0.90625
# [14/100] training 43.5% loss=0.28456, acc=0.87500
# [14/100] training 43.8% loss=0.38891, acc=0.81250
# [14/100] training 43.9% loss=0.26154, acc=0.87500
# [14/100] training 44.1% loss=0.21294, acc=0.93750
# [14/100] training 44.3% loss=0.25359, acc=0.95312
# [14/100] training 44.4% loss=0.29634, acc=0.87500
# [14/100] training 44.6% loss=0.32729, acc=0.85938
# [14/100] training 44.7% loss=0.33900, acc=0.90625
# [14/100] training 45.0% loss=0.27469, acc=0.89062
# [14/100] training 45.1% loss=0.40011, acc=0.85938
# [14/100] training 45.3% loss=0.26445, acc=0.89062
# [14/100] training 45.5% loss=0.18931, acc=0.95312
# [14/100] training 45.6% loss=0.26937, acc=0.89062
# [14/100] training 45.8% loss=0.30095, acc=0.84375
# [14/100] training 45.9% loss=0.22243, acc=0.85938
# [14/100] training 46.2% loss=0.28854, acc=0.92188
# [14/100] training 46.3% loss=0.18376, acc=0.90625
# [14/100] training 46.5% loss=0.42005, acc=0.81250
# [14/100] training 46.6% loss=0.31755, acc=0.89062
# [14/100] training 46.8% loss=0.28319, acc=0.85938
# [14/100] training 47.0% loss=0.29202, acc=0.87500
# [14/100] training 47.2% loss=0.34283, acc=0.87500
# [14/100] training 47.4% loss=0.28268, acc=0.89062
# [14/100] training 47.5% loss=0.28336, acc=0.84375
# [14/100] training 47.7% loss=0.19521, acc=0.90625
# [14/100] training 47.8% loss=0.29025, acc=0.87500
# [14/100] training 48.0% loss=0.43040, acc=0.79688
# [14/100] training 48.3% loss=0.16844, acc=0.92188
# [14/100] training 48.4% loss=0.19514, acc=0.92188
# [14/100] training 48.6% loss=0.20381, acc=0.92188
# [14/100] training 48.7% loss=0.32022, acc=0.89062
# [14/100] training 48.9% loss=0.33256, acc=0.89062
# [14/100] training 49.0% loss=0.29621, acc=0.87500
# [14/100] training 49.2% loss=0.33842, acc=0.81250
# [14/100] training 49.3% loss=0.24030, acc=0.93750
# [14/100] training 49.6% loss=0.24653, acc=0.89062
# [14/100] training 49.8% loss=0.26454, acc=0.85938
# [14/100] training 49.9% loss=0.33362, acc=0.84375
# [14/100] training 50.1% loss=0.25518, acc=0.85938
# [14/100] training 50.2% loss=0.47506, acc=0.76562
# [14/100] training 50.4% loss=0.43294, acc=0.89062
# [14/100] training 50.6% loss=0.40320, acc=0.81250
# [14/100] training 50.8% loss=0.34280, acc=0.84375
# [14/100] training 51.0% loss=0.34355, acc=0.85938
# [14/100] training 51.1% loss=0.29926, acc=0.89062
# [14/100] training 51.3% loss=0.29817, acc=0.87500
# [14/100] training 51.4% loss=0.25076, acc=0.92188
# [14/100] training 51.7% loss=0.36566, acc=0.82812
# [14/100] training 51.8% loss=0.32157, acc=0.81250
# [14/100] training 52.0% loss=0.22761, acc=0.92188
# [14/100] training 52.1% loss=0.33383, acc=0.85938
# [14/100] training 52.3% loss=0.34800, acc=0.87500
# [14/100] training 52.5% loss=0.19333, acc=0.93750
# [14/100] training 52.6% loss=0.26042, acc=0.89062
# [14/100] training 52.9% loss=0.53398, acc=0.84375
# [14/100] training 53.0% loss=0.28797, acc=0.84375
# [14/100] training 53.2% loss=0.31300, acc=0.87500
# [14/100] training 53.3% loss=0.21959, acc=0.92188
# [14/100] training 53.5% loss=0.28493, acc=0.85938
# [14/100] training 53.7% loss=0.24068, acc=0.85938
# [14/100] training 53.8% loss=0.36399, acc=0.81250
# [14/100] training 54.1% loss=0.39125, acc=0.82812
# [14/100] training 54.2% loss=0.20382, acc=0.90625
# [14/100] training 54.4% loss=0.29932, acc=0.89062
# [14/100] training 54.5% loss=0.34935, acc=0.85938
# [14/100] training 54.7% loss=0.45349, acc=0.78125
# [14/100] training 54.8% loss=0.17070, acc=0.93750
# [14/100] training 55.1% loss=0.20432, acc=0.90625
# [14/100] training 55.3% loss=0.21605, acc=0.90625
# [14/100] training 55.4% loss=0.33797, acc=0.85938
# [14/100] training 55.6% loss=0.34676, acc=0.82812
# [14/100] training 55.7% loss=0.36786, acc=0.81250
# [14/100] training 55.9% loss=0.24938, acc=0.89062
# [14/100] training 56.0% loss=0.32633, acc=0.82812
# [14/100] training 56.3% loss=0.51356, acc=0.82812
# [14/100] training 56.5% loss=0.30294, acc=0.89062
# [14/100] training 56.6% loss=0.31990, acc=0.87500
# [14/100] training 56.8% loss=0.37882, acc=0.85938
# [14/100] training 56.9% loss=0.36044, acc=0.87500
# [14/100] training 57.1% loss=0.33036, acc=0.89062
# [14/100] training 57.2% loss=0.21645, acc=0.89062
# [14/100] training 57.5% loss=0.33325, acc=0.85938
# [14/100] training 57.6% loss=0.32893, acc=0.85938
# [14/100] training 57.8% loss=0.23331, acc=0.95312
# [14/100] training 58.0% loss=0.23856, acc=0.89062
# [14/100] training 58.1% loss=0.28773, acc=0.89062
# [14/100] training 58.3% loss=0.19578, acc=0.89062
# [14/100] training 58.4% loss=0.28258, acc=0.87500
# [14/100] training 58.7% loss=0.36599, acc=0.90625
# [14/100] training 58.8% loss=0.39280, acc=0.82812
# [14/100] training 59.0% loss=0.23149, acc=0.95312
# [14/100] training 59.2% loss=0.29943, acc=0.89062
# [14/100] training 59.3% loss=0.21020, acc=0.89062
# [14/100] training 59.5% loss=0.28107, acc=0.87500
# [14/100] training 59.7% loss=0.28414, acc=0.90625
# [14/100] training 59.9% loss=0.27483, acc=0.92188
# [14/100] training 60.0% loss=0.25461, acc=0.90625
# [14/100] training 60.2% loss=0.37726, acc=0.84375
# [14/100] training 60.3% loss=0.33366, acc=0.85938
# [14/100] training 60.5% loss=0.28469, acc=0.87500
# [14/100] training 60.8% loss=0.38254, acc=0.81250
# [14/100] training 60.9% loss=0.28567, acc=0.87500
# [14/100] training 61.1% loss=0.34373, acc=0.89062
# [14/100] training 61.2% loss=0.30805, acc=0.84375
# [14/100] training 61.4% loss=0.33425, acc=0.89062
# [14/100] training 61.5% loss=0.43613, acc=0.76562
# [14/100] training 61.7% loss=0.37428, acc=0.89062
# [14/100] training 62.0% loss=0.34518, acc=0.85938
# [14/100] training 62.1% loss=0.36860, acc=0.84375
# [14/100] training 62.3% loss=0.20798, acc=0.93750
# [14/100] training 62.4% loss=0.20739, acc=0.87500
# [14/100] training 62.6% loss=0.42517, acc=0.79688
# [14/100] training 62.7% loss=0.29496, acc=0.85938
# [14/100] training 62.9% loss=0.29984, acc=0.82812
# [14/100] training 63.1% loss=0.24390, acc=0.89062
# [14/100] training 63.3% loss=0.35707, acc=0.82812
# [14/100] training 63.5% loss=0.38664, acc=0.84375
# [14/100] training 63.6% loss=0.30913, acc=0.85938
# [14/100] training 63.8% loss=0.34522, acc=0.85938
# [14/100] training 63.9% loss=0.27726, acc=0.87500
# [14/100] training 64.2% loss=0.23582, acc=0.89062
# [14/100] training 64.3% loss=0.36708, acc=0.81250
# [14/100] training 64.5% loss=0.32369, acc=0.82812
# [14/100] training 64.7% loss=0.28030, acc=0.89062
# [14/100] training 64.8% loss=0.56422, acc=0.79688
# [14/100] training 65.0% loss=0.36300, acc=0.84375
# [14/100] training 65.1% loss=0.35558, acc=0.82812
# [14/100] training 65.4% loss=0.27619, acc=0.87500
# [14/100] training 65.5% loss=0.21822, acc=0.95312
# [14/100] training 65.7% loss=0.22105, acc=0.92188
# [14/100] training 65.8% loss=0.34887, acc=0.87500
# [14/100] training 66.0% loss=0.24921, acc=0.87500
# [14/100] training 66.2% loss=0.12049, acc=0.95312
# [14/100] training 66.3% loss=0.40218, acc=0.82812
# [14/100] training 66.6% loss=0.31170, acc=0.82812
# [14/100] training 66.7% loss=0.21889, acc=0.96875
# [14/100] training 66.9% loss=0.30927, acc=0.89062
# [14/100] training 67.0% loss=0.37849, acc=0.81250
# [14/100] training 67.2% loss=0.20711, acc=0.90625
# [14/100] training 67.4% loss=0.29730, acc=0.85938
# [14/100] training 67.6% loss=0.32132, acc=0.89062
# [14/100] training 67.8% loss=0.28859, acc=0.85938
# [14/100] training 67.9% loss=0.20516, acc=0.93750
# [14/100] training 68.1% loss=0.24933, acc=0.90625
# [14/100] training 68.2% loss=0.16969, acc=0.90625
# [14/100] training 68.4% loss=0.13348, acc=0.95312
# [14/100] training 68.5% loss=0.30994, acc=0.85938
# [14/100] training 68.8% loss=0.30160, acc=0.85938
# [14/100] training 69.0% loss=0.34162, acc=0.89062
# [14/100] training 69.1% loss=0.21922, acc=0.92188
# [14/100] training 69.3% loss=0.24498, acc=0.84375
# [14/100] training 69.4% loss=0.21841, acc=0.89062
# [14/100] training 69.6% loss=0.38872, acc=0.82812
# [14/100] training 69.7% loss=0.31970, acc=0.87500
# [14/100] training 70.0% loss=0.29383, acc=0.89062
# [14/100] training 70.2% loss=0.30599, acc=0.85938
# [14/100] training 70.3% loss=0.29499, acc=0.89062
# [14/100] training 70.5% loss=0.29590, acc=0.87500
# [14/100] training 70.6% loss=0.20307, acc=0.93750
# [14/100] training 70.8% loss=0.34316, acc=0.82812
# [14/100] training 71.0% loss=0.29762, acc=0.87500
# [14/100] training 71.2% loss=0.25032, acc=0.89062
# [14/100] training 71.3% loss=0.22309, acc=0.90625
# [14/100] training 71.5% loss=0.40707, acc=0.85938
# [14/100] training 71.7% loss=0.34535, acc=0.84375
# [14/100] training 71.8% loss=0.34433, acc=0.85938
# [14/100] training 72.0% loss=0.31111, acc=0.84375
# [14/100] training 72.2% loss=0.33421, acc=0.84375
# [14/100] training 72.4% loss=0.40917, acc=0.84375
# [14/100] training 72.5% loss=0.53209, acc=0.87500
# [14/100] training 72.7% loss=0.42250, acc=0.75000
# [14/100] training 72.9% loss=0.30796, acc=0.84375
# [14/100] training 73.0% loss=0.28129, acc=0.90625
# [14/100] training 73.3% loss=0.34374, acc=0.85938
# [14/100] training 73.4% loss=0.23020, acc=0.92188
# [14/100] training 73.6% loss=0.27259, acc=0.87500
# [14/100] training 73.7% loss=0.24453, acc=0.92188
# [14/100] training 73.9% loss=0.25499, acc=0.92188
# [14/100] training 74.0% loss=0.37090, acc=0.85938
# [14/100] training 74.2% loss=0.26509, acc=0.87500
# [14/100] training 74.5% loss=0.26561, acc=0.85938
# [14/100] training 74.6% loss=0.46968, acc=0.79688
# [14/100] training 74.8% loss=0.35810, acc=0.84375
# [14/100] training 74.9% loss=0.44084, acc=0.78125
# [14/100] training 75.1% loss=0.27644, acc=0.87500
# [14/100] training 75.2% loss=0.23592, acc=0.93750
# [14/100] training 75.4% loss=0.25632, acc=0.90625
# [14/100] training 75.7% loss=0.34890, acc=0.82812
# [14/100] training 75.8% loss=0.32771, acc=0.87500
# [14/100] training 76.0% loss=0.13455, acc=0.93750
# [14/100] training 76.1% loss=0.37075, acc=0.85938
# [14/100] training 76.3% loss=0.22010, acc=0.96875
# [14/100] training 76.4% loss=0.29545, acc=0.87500
# [14/100] training 76.7% loss=0.23062, acc=0.92188
# [14/100] training 76.8% loss=0.18794, acc=0.95312
# [14/100] training 77.0% loss=0.32405, acc=0.82812
# [14/100] training 77.2% loss=0.30497, acc=0.89062
# [14/100] training 77.3% loss=0.18725, acc=0.92188
# [14/100] training 77.5% loss=0.32124, acc=0.87500
# [14/100] training 77.6% loss=0.25500, acc=0.90625
# [14/100] training 77.9% loss=0.25998, acc=0.85938
# [14/100] training 78.0% loss=0.31389, acc=0.85938
# [14/100] training 78.2% loss=0.26398, acc=0.90625
# [14/100] training 78.4% loss=0.16037, acc=0.92188
# [14/100] training 78.5% loss=0.34925, acc=0.89062
# [14/100] training 78.7% loss=0.34198, acc=0.85938
# [14/100] training 78.8% loss=0.20372, acc=0.92188
# [14/100] training 79.1% loss=0.18686, acc=0.95312
# [14/100] training 79.2% loss=0.19703, acc=0.93750
# [14/100] training 79.4% loss=0.53577, acc=0.82812
# [14/100] training 79.5% loss=0.27023, acc=0.89062
# [14/100] training 79.7% loss=0.17276, acc=0.93750
# [14/100] training 79.9% loss=0.27468, acc=0.87500
# [14/100] training 80.1% loss=0.29711, acc=0.90625
# [14/100] training 80.3% loss=0.35428, acc=0.79688
# [14/100] training 80.4% loss=0.31695, acc=0.85938
# [14/100] training 80.6% loss=0.36117, acc=0.84375
# [14/100] training 80.7% loss=0.29957, acc=0.89062
# [14/100] training 80.9% loss=0.33448, acc=0.84375
# [14/100] training 81.2% loss=0.33491, acc=0.87500
# [14/100] training 81.3% loss=0.30358, acc=0.89062
# [14/100] training 81.5% loss=0.34756, acc=0.84375
# [14/100] training 81.6% loss=0.32164, acc=0.85938
# [14/100] training 81.8% loss=0.24614, acc=0.89062
# [14/100] training 81.9% loss=0.49816, acc=0.84375
# [14/100] training 82.1% loss=0.22512, acc=0.87500
# [14/100] training 82.2% loss=0.48884, acc=0.78125
# [14/100] training 82.5% loss=0.21012, acc=0.92188
# [14/100] training 82.7% loss=0.28369, acc=0.87500
# [14/100] training 82.8% loss=0.25192, acc=0.92188
# [14/100] training 83.0% loss=0.26991, acc=0.87500
# [14/100] training 83.1% loss=0.30564, acc=0.89062
# [14/100] training 83.3% loss=0.15980, acc=0.96875
# [14/100] training 83.5% loss=0.24218, acc=0.90625
# [14/100] training 83.7% loss=0.38129, acc=0.84375
# [14/100] training 83.9% loss=0.28789, acc=0.87500
# [14/100] training 84.0% loss=0.19238, acc=0.89062
# [14/100] training 84.2% loss=0.21826, acc=0.90625
# [14/100] training 84.3% loss=0.26025, acc=0.85938
# [14/100] training 84.5% loss=0.21616, acc=0.93750
# [14/100] training 84.7% loss=0.20870, acc=0.90625
# [14/100] training 84.9% loss=0.28589, acc=0.87500
# [14/100] training 85.0% loss=0.30549, acc=0.85938
# [14/100] training 85.2% loss=0.33256, acc=0.84375
# [14/100] training 85.4% loss=0.26757, acc=0.92188
# [14/100] training 85.5% loss=0.20090, acc=0.90625
# [14/100] training 85.8% loss=0.29381, acc=0.85938
# [14/100] training 85.9% loss=0.31300, acc=0.85938
# [14/100] training 86.1% loss=0.27499, acc=0.89062
# [14/100] training 86.2% loss=0.17531, acc=0.92188
# [14/100] training 86.4% loss=0.39645, acc=0.87500
# [14/100] training 86.6% loss=0.31275, acc=0.89062
# [14/100] training 86.7% loss=0.19456, acc=0.92188
# [14/100] training 87.0% loss=0.34286, acc=0.84375
# [14/100] training 87.1% loss=0.30798, acc=0.84375
# [14/100] training 87.3% loss=0.29364, acc=0.85938
# [14/100] training 87.4% loss=0.32607, acc=0.87500
# [14/100] training 87.6% loss=0.22508, acc=0.92188
# [14/100] training 87.7% loss=0.31898, acc=0.93750
# [14/100] training 87.9% loss=0.28030, acc=0.89062
# [14/100] training 88.2% loss=0.18138, acc=0.93750
# [14/100] training 88.3% loss=0.29353, acc=0.92188
# [14/100] training 88.5% loss=0.31704, acc=0.87500
# [14/100] training 88.6% loss=0.12881, acc=0.95312
# [14/100] training 88.8% loss=0.34997, acc=0.89062
# [14/100] training 88.9% loss=0.29292, acc=0.89062
# [14/100] training 89.2% loss=0.19754, acc=0.92188
# [14/100] training 89.4% loss=0.24996, acc=0.87500
# [14/100] training 89.5% loss=0.33553, acc=0.82812
# [14/100] training 89.7% loss=0.29261, acc=0.84375
# [14/100] training 89.8% loss=0.30028, acc=0.87500
# [14/100] training 90.0% loss=0.24412, acc=0.90625
# [14/100] training 90.1% loss=0.25197, acc=0.90625
# [14/100] training 90.4% loss=0.26265, acc=0.87500
# [14/100] training 90.5% loss=0.22855, acc=0.92188
# [14/100] training 90.7% loss=0.36472, acc=0.81250
# [14/100] training 90.9% loss=0.15740, acc=0.95312
# [14/100] training 91.0% loss=0.20098, acc=0.93750
# [14/100] training 91.2% loss=0.23356, acc=0.90625
# [14/100] training 91.3% loss=0.45271, acc=0.79688
# [14/100] training 91.6% loss=0.33254, acc=0.85938
# [14/100] training 91.7% loss=0.25236, acc=0.90625
# [14/100] training 91.9% loss=0.30767, acc=0.85938
# [14/100] training 92.1% loss=0.28199, acc=0.89062
# [14/100] training 92.2% loss=0.21084, acc=0.93750
# [14/100] training 92.4% loss=0.20672, acc=0.90625
# [14/100] training 92.6% loss=0.39122, acc=0.79688
# [14/100] training 92.8% loss=0.48716, acc=0.84375
# [14/100] training 92.9% loss=0.30568, acc=0.87500
# [14/100] training 93.1% loss=0.37092, acc=0.82812
# [14/100] training 93.2% loss=0.27621, acc=0.89062
# [14/100] training 93.4% loss=0.18709, acc=0.93750
# [14/100] training 93.7% loss=0.16732, acc=0.96875
# [14/100] training 93.8% loss=0.20345, acc=0.90625
# [14/100] training 94.0% loss=0.26180, acc=0.89062
# [14/100] training 94.1% loss=0.29982, acc=0.90625
# [14/100] training 94.3% loss=0.18875, acc=0.90625
# [14/100] training 94.4% loss=0.19016, acc=0.92188
# [14/100] training 94.6% loss=0.19480, acc=0.92188
# [14/100] training 94.9% loss=0.21997, acc=0.92188
# [14/100] training 95.0% loss=0.47311, acc=0.82812
# [14/100] training 95.2% loss=0.45673, acc=0.84375
# [14/100] training 95.3% loss=0.34934, acc=0.90625
# [14/100] training 95.5% loss=0.22837, acc=0.92188
# [14/100] training 95.6% loss=0.38629, acc=0.79688
# [14/100] training 95.8% loss=0.33539, acc=0.82812
# [14/100] training 96.0% loss=0.29346, acc=0.85938
# [14/100] training 96.2% loss=0.21974, acc=0.90625
# [14/100] training 96.4% loss=0.22319, acc=0.95312
# [14/100] training 96.5% loss=0.29811, acc=0.87500
# [14/100] training 96.7% loss=0.26445, acc=0.89062
# [14/100] training 96.8% loss=0.37529, acc=0.85938
# [14/100] training 97.1% loss=0.26600, acc=0.95312
# [14/100] training 97.2% loss=0.36889, acc=0.87500
# [14/100] training 97.4% loss=0.28465, acc=0.87500
# [14/100] training 97.6% loss=0.33501, acc=0.85938
# [14/100] training 97.7% loss=0.19117, acc=0.93750
# [14/100] training 97.9% loss=0.19300, acc=0.89062
# [14/100] training 98.0% loss=0.27874, acc=0.89062
# [14/100] training 98.3% loss=0.26960, acc=0.89062
# [14/100] training 98.4% loss=0.36534, acc=0.84375
# [14/100] training 98.6% loss=0.47451, acc=0.75000
# [14/100] training 98.7% loss=0.38757, acc=0.82812
# [14/100] training 98.9% loss=0.24311, acc=0.87500
# [14/100] training 99.1% loss=0.24041, acc=0.87500
# [14/100] training 99.2% loss=0.22151, acc=0.87500
# [14/100] training 99.5% loss=0.32391, acc=0.82812
# [14/100] training 99.6% loss=0.32830, acc=0.89062
# [14/100] training 99.8% loss=0.24838, acc=0.89062
# [14/100] training 99.9% loss=0.10855, acc=0.96875
# [14/100] testing 0.9% loss=0.24071, acc=0.87500
# [14/100] testing 1.8% loss=0.60444, acc=0.76562
# [14/100] testing 2.2% loss=0.23329, acc=0.90625
# [14/100] testing 3.1% loss=0.39774, acc=0.79688
# [14/100] testing 3.5% loss=0.15780, acc=0.93750
# [14/100] testing 4.4% loss=0.32552, acc=0.87500
# [14/100] testing 4.8% loss=0.38647, acc=0.81250
# [14/100] testing 5.7% loss=0.39397, acc=0.79688
# [14/100] testing 6.6% loss=0.28624, acc=0.89062
# [14/100] testing 7.0% loss=0.23848, acc=0.92188
# [14/100] testing 7.9% loss=0.43658, acc=0.82812
# [14/100] testing 8.3% loss=0.23423, acc=0.92188
# [14/100] testing 9.2% loss=0.33692, acc=0.89062
# [14/100] testing 9.7% loss=0.19402, acc=0.90625
# [14/100] testing 10.5% loss=0.44810, acc=0.79688
# [14/100] testing 11.0% loss=0.35300, acc=0.84375
# [14/100] testing 11.8% loss=0.42398, acc=0.85938
# [14/100] testing 12.7% loss=0.58066, acc=0.78125
# [14/100] testing 13.2% loss=0.29698, acc=0.87500
# [14/100] testing 14.0% loss=0.56484, acc=0.81250
# [14/100] testing 14.5% loss=0.49691, acc=0.82812
# [14/100] testing 15.4% loss=0.38887, acc=0.85938
# [14/100] testing 15.8% loss=0.27533, acc=0.89062
# [14/100] testing 16.7% loss=0.35080, acc=0.85938
# [14/100] testing 17.5% loss=0.34549, acc=0.85938
# [14/100] testing 18.0% loss=0.34813, acc=0.89062
# [14/100] testing 18.9% loss=0.19195, acc=0.87500
# [14/100] testing 19.3% loss=0.31140, acc=0.90625
# [14/100] testing 20.2% loss=0.51113, acc=0.78125
# [14/100] testing 20.6% loss=0.46461, acc=0.82812
# [14/100] testing 21.5% loss=0.30401, acc=0.82812
# [14/100] testing 21.9% loss=0.47349, acc=0.79688
# [14/100] testing 22.8% loss=0.46227, acc=0.85938
# [14/100] testing 23.7% loss=0.37323, acc=0.87500
# [14/100] testing 24.1% loss=0.29528, acc=0.87500
# [14/100] testing 25.0% loss=0.44632, acc=0.90625
# [14/100] testing 25.4% loss=0.19350, acc=0.93750
# [14/100] testing 26.3% loss=0.42045, acc=0.81250
# [14/100] testing 26.8% loss=0.37219, acc=0.84375
# [14/100] testing 27.6% loss=0.43275, acc=0.81250
# [14/100] testing 28.5% loss=0.44468, acc=0.81250
# [14/100] testing 29.0% loss=0.24024, acc=0.92188
# [14/100] testing 29.8% loss=0.46198, acc=0.84375
# [14/100] testing 30.3% loss=0.32338, acc=0.93750
# [14/100] testing 31.1% loss=0.45083, acc=0.81250
# [14/100] testing 31.6% loss=0.27009, acc=0.89062
# [14/100] testing 32.5% loss=0.26934, acc=0.87500
# [14/100] testing 32.9% loss=0.47461, acc=0.84375
# [14/100] testing 33.8% loss=0.33583, acc=0.82812
# [14/100] testing 34.7% loss=0.33890, acc=0.85938
# [14/100] testing 35.1% loss=0.36953, acc=0.82812
# [14/100] testing 36.0% loss=0.32505, acc=0.89062
# [14/100] testing 36.4% loss=0.35565, acc=0.87500
# [14/100] testing 37.3% loss=0.35203, acc=0.85938
# [14/100] testing 37.7% loss=0.51930, acc=0.81250
# [14/100] testing 38.6% loss=0.28104, acc=0.89062
# [14/100] testing 39.5% loss=0.41205, acc=0.87500
# [14/100] testing 39.9% loss=0.43615, acc=0.85938
# [14/100] testing 40.8% loss=0.44837, acc=0.87500
# [14/100] testing 41.2% loss=0.26867, acc=0.93750
# [14/100] testing 42.1% loss=0.35887, acc=0.84375
# [14/100] testing 42.5% loss=0.34781, acc=0.84375
# [14/100] testing 43.4% loss=0.39064, acc=0.84375
# [14/100] testing 43.9% loss=0.24824, acc=0.93750
# [14/100] testing 44.7% loss=0.45512, acc=0.85938
# [14/100] testing 45.6% loss=0.37190, acc=0.84375
# [14/100] testing 46.1% loss=0.27945, acc=0.90625
# [14/100] testing 46.9% loss=0.28106, acc=0.85938
# [14/100] testing 47.4% loss=0.16815, acc=0.93750
# [14/100] testing 48.3% loss=0.51729, acc=0.82812
# [14/100] testing 48.7% loss=0.59030, acc=0.78125
# [14/100] testing 49.6% loss=0.51753, acc=0.78125
# [14/100] testing 50.4% loss=0.22219, acc=0.90625
# [14/100] testing 50.9% loss=0.27572, acc=0.89062
# [14/100] testing 51.8% loss=0.39809, acc=0.81250
# [14/100] testing 52.2% loss=0.23876, acc=0.90625
# [14/100] testing 53.1% loss=0.30786, acc=0.84375
# [14/100] testing 53.5% loss=0.28664, acc=0.93750
# [14/100] testing 54.4% loss=0.55359, acc=0.75000
# [14/100] testing 54.8% loss=0.45478, acc=0.76562
# [14/100] testing 55.7% loss=0.22601, acc=0.90625
# [14/100] testing 56.6% loss=0.47178, acc=0.84375
# [14/100] testing 57.0% loss=0.44789, acc=0.81250
# [14/100] testing 57.9% loss=0.40504, acc=0.85938
# [14/100] testing 58.3% loss=0.48677, acc=0.73438
# [14/100] testing 59.2% loss=0.38693, acc=0.79688
# [14/100] testing 59.7% loss=0.32630, acc=0.85938
# [14/100] testing 60.5% loss=0.41683, acc=0.84375
# [14/100] testing 61.4% loss=0.27286, acc=0.87500
# [14/100] testing 61.9% loss=0.37053, acc=0.84375
# [14/100] testing 62.7% loss=0.26189, acc=0.87500
# [14/100] testing 63.2% loss=0.48323, acc=0.81250
# [14/100] testing 64.0% loss=0.38225, acc=0.82812
# [14/100] testing 64.5% loss=0.32293, acc=0.84375
# [14/100] testing 65.4% loss=0.40473, acc=0.84375
# [14/100] testing 65.8% loss=0.42393, acc=0.81250
# [14/100] testing 66.7% loss=0.29800, acc=0.87500
# [14/100] testing 67.6% loss=0.41869, acc=0.82812
# [14/100] testing 68.0% loss=0.26166, acc=0.89062
# [14/100] testing 68.9% loss=0.37670, acc=0.89062
# [14/100] testing 69.3% loss=0.42289, acc=0.79688
# [14/100] testing 70.2% loss=0.46686, acc=0.81250
# [14/100] testing 70.6% loss=0.34975, acc=0.84375
# [14/100] testing 71.5% loss=0.44089, acc=0.87500
# [14/100] testing 72.4% loss=0.28507, acc=0.89062
# [14/100] testing 72.8% loss=0.35571, acc=0.85938
# [14/100] testing 73.7% loss=0.35969, acc=0.87500
# [14/100] testing 74.1% loss=0.46964, acc=0.78125
# [14/100] testing 75.0% loss=0.34410, acc=0.81250
# [14/100] testing 75.4% loss=0.36321, acc=0.85938
# [14/100] testing 76.3% loss=0.10005, acc=0.98438
# [14/100] testing 76.8% loss=0.41952, acc=0.84375
# [14/100] testing 77.6% loss=0.42404, acc=0.82812
# [14/100] testing 78.5% loss=0.64478, acc=0.73438
# [14/100] testing 79.0% loss=0.36229, acc=0.84375
# [14/100] testing 79.8% loss=0.47236, acc=0.79688
# [14/100] testing 80.3% loss=0.37915, acc=0.87500
# [14/100] testing 81.2% loss=0.52466, acc=0.82812
# [14/100] testing 81.6% loss=0.26443, acc=0.87500
# [14/100] testing 82.5% loss=0.22572, acc=0.90625
# [14/100] testing 83.3% loss=0.21914, acc=0.90625
# [14/100] testing 83.8% loss=0.25749, acc=0.89062
# [14/100] testing 84.7% loss=0.39244, acc=0.82812
# [14/100] testing 85.1% loss=0.36445, acc=0.87500
# [14/100] testing 86.0% loss=0.38580, acc=0.84375
# [14/100] testing 86.4% loss=0.46400, acc=0.82812
# [14/100] testing 87.3% loss=0.49313, acc=0.79688
# [14/100] testing 87.7% loss=0.28367, acc=0.85938
# [14/100] testing 88.6% loss=0.29091, acc=0.87500
# [14/100] testing 89.5% loss=0.45348, acc=0.81250
# [14/100] testing 89.9% loss=0.35909, acc=0.81250
# [14/100] testing 90.8% loss=0.38719, acc=0.85938
# [14/100] testing 91.2% loss=0.14905, acc=0.92188
# [14/100] testing 92.1% loss=0.42128, acc=0.85938
# [14/100] testing 92.6% loss=0.28992, acc=0.84375
# [14/100] testing 93.4% loss=0.40815, acc=0.84375
# [14/100] testing 94.3% loss=0.18637, acc=0.90625
# [14/100] testing 94.7% loss=0.27419, acc=0.89062
# [14/100] testing 95.6% loss=0.38190, acc=0.84375
# [14/100] testing 96.1% loss=0.25151, acc=0.89062
# [14/100] testing 96.9% loss=0.30984, acc=0.89062
# [14/100] testing 97.4% loss=0.19578, acc=0.93750
# [14/100] testing 98.3% loss=0.43108, acc=0.79688
# [14/100] testing 98.7% loss=0.27399, acc=0.87500
# [14/100] testing 99.6% loss=0.24675, acc=0.87500
# [15/100] training 0.2% loss=0.50793, acc=0.78125
# [15/100] training 0.4% loss=0.47025, acc=0.75000
# [15/100] training 0.5% loss=0.32030, acc=0.84375
# [15/100] training 0.8% loss=0.34938, acc=0.84375
# [15/100] training 0.9% loss=0.29742, acc=0.85938
# [15/100] training 1.1% loss=0.29309, acc=0.90625
# [15/100] training 1.2% loss=0.38843, acc=0.82812
# [15/100] training 1.4% loss=0.40659, acc=0.82812
# [15/100] training 1.6% loss=0.22653, acc=0.90625
# [15/100] training 1.8% loss=0.31574, acc=0.90625
# [15/100] training 2.0% loss=0.34981, acc=0.82812
# [15/100] training 2.1% loss=0.33708, acc=0.84375
# [15/100] training 2.3% loss=0.26554, acc=0.89062
# [15/100] training 2.4% loss=0.43585, acc=0.76562
# [15/100] training 2.6% loss=0.28466, acc=0.87500
# [15/100] training 2.7% loss=0.34457, acc=0.84375
# [15/100] training 3.0% loss=0.30669, acc=0.85938
# [15/100] training 3.2% loss=0.20250, acc=0.93750
# [15/100] training 3.3% loss=0.37057, acc=0.84375
# [15/100] training 3.5% loss=0.32577, acc=0.85938
# [15/100] training 3.6% loss=0.47761, acc=0.79688
# [15/100] training 3.8% loss=0.25745, acc=0.84375
# [15/100] training 3.9% loss=0.24771, acc=0.89062
# [15/100] training 4.2% loss=0.25483, acc=0.90625
# [15/100] training 4.4% loss=0.25543, acc=0.84375
# [15/100] training 4.5% loss=0.21729, acc=0.92188
# [15/100] training 4.7% loss=0.26675, acc=0.89062
# [15/100] training 4.8% loss=0.30454, acc=0.89062
# [15/100] training 5.0% loss=0.35239, acc=0.85938
# [15/100] training 5.2% loss=0.35874, acc=0.89062
# [15/100] training 5.4% loss=0.26335, acc=0.89062
# [15/100] training 5.5% loss=0.34389, acc=0.82812
# [15/100] training 5.7% loss=0.33562, acc=0.81250
# [15/100] training 5.9% loss=0.40678, acc=0.79688
# [15/100] training 6.0% loss=0.33695, acc=0.84375
# [15/100] training 6.3% loss=0.38862, acc=0.85938
# [15/100] training 6.4% loss=0.24288, acc=0.92188
# [15/100] training 6.6% loss=0.33193, acc=0.89062
# [15/100] training 6.7% loss=0.35538, acc=0.79688
# [15/100] training 6.9% loss=0.24616, acc=0.89062
# [15/100] training 7.1% loss=0.41019, acc=0.84375
# [15/100] training 7.2% loss=0.29573, acc=0.84375
# [15/100] training 7.5% loss=0.34307, acc=0.84375
# [15/100] training 7.6% loss=0.29582, acc=0.87500
# [15/100] training 7.8% loss=0.39255, acc=0.82812
# [15/100] training 7.9% loss=0.35405, acc=0.82812
# [15/100] training 8.1% loss=0.23103, acc=0.93750
# [15/100] training 8.2% loss=0.27998, acc=0.89062
# [15/100] training 8.4% loss=0.38584, acc=0.85938
# [15/100] training 8.7% loss=0.35147, acc=0.89062
# [15/100] training 8.8% loss=0.36682, acc=0.82812
# [15/100] training 9.0% loss=0.35283, acc=0.89062
# [15/100] training 9.1% loss=0.42950, acc=0.73438
# [15/100] training 9.3% loss=0.39946, acc=0.84375
# [15/100] training 9.4% loss=0.29761, acc=0.85938
# [15/100] training 9.7% loss=0.34533, acc=0.89062
# [15/100] training 9.9% loss=0.34146, acc=0.85938
# [15/100] training 10.0% loss=0.32434, acc=0.81250
# [15/100] training 10.2% loss=0.30906, acc=0.90625
# [15/100] training 10.3% loss=0.22709, acc=0.89062
# [15/100] training 10.5% loss=0.34550, acc=0.84375
# [15/100] training 10.6% loss=0.39748, acc=0.79688
# [15/100] training 10.9% loss=0.29177, acc=0.84375
# [15/100] training 11.0% loss=0.31540, acc=0.87500
# [15/100] training 11.2% loss=0.22440, acc=0.89062
# [15/100] training 11.4% loss=0.44485, acc=0.85938
# [15/100] training 11.5% loss=0.47027, acc=0.82812
# [15/100] training 11.7% loss=0.23639, acc=0.93750
# [15/100] training 11.8% loss=0.27400, acc=0.89062
# [15/100] training 12.1% loss=0.30416, acc=0.90625
# [15/100] training 12.2% loss=0.35986, acc=0.84375
# [15/100] training 12.4% loss=0.29045, acc=0.85938
# [15/100] training 12.6% loss=0.26237, acc=0.85938
# [15/100] training 12.7% loss=0.32986, acc=0.84375
# [15/100] training 12.9% loss=0.41015, acc=0.85938
# [15/100] training 13.0% loss=0.25869, acc=0.90625
# [15/100] training 13.3% loss=0.26184, acc=0.89062
# [15/100] training 13.4% loss=0.25545, acc=0.90625
# [15/100] training 13.6% loss=0.30966, acc=0.90625
# [15/100] training 13.7% loss=0.48735, acc=0.81250
# [15/100] training 13.9% loss=0.29456, acc=0.85938
# [15/100] training 14.1% loss=0.32652, acc=0.89062
# [15/100] training 14.3% loss=0.30865, acc=0.89062
# [15/100] training 14.5% loss=0.24726, acc=0.90625
# [15/100] training 14.6% loss=0.22953, acc=0.93750
# [15/100] training 14.8% loss=0.27073, acc=0.90625
# [15/100] training 14.9% loss=0.23863, acc=0.89062
# [15/100] training 15.1% loss=0.40438, acc=0.84375
# [15/100] training 15.4% loss=0.27555, acc=0.87500
# [15/100] training 15.5% loss=0.29706, acc=0.87500
# [15/100] training 15.7% loss=0.41608, acc=0.89062
# [15/100] training 15.8% loss=0.21834, acc=0.87500
# [15/100] training 16.0% loss=0.30884, acc=0.89062
# [15/100] training 16.1% loss=0.49091, acc=0.81250
# [15/100] training 16.3% loss=0.31326, acc=0.87500
# [15/100] training 16.4% loss=0.24822, acc=0.90625
# [15/100] training 16.7% loss=0.36882, acc=0.82812
# [15/100] training 16.9% loss=0.34007, acc=0.87500
# [15/100] training 17.0% loss=0.29889, acc=0.87500
# [15/100] training 17.2% loss=0.25244, acc=0.85938
# [15/100] training 17.3% loss=0.23707, acc=0.89062
# [15/100] training 17.5% loss=0.27375, acc=0.85938
# [15/100] training 17.7% loss=0.27679, acc=0.84375
# [15/100] training 17.9% loss=0.43040, acc=0.85938
# [15/100] training 18.1% loss=0.33781, acc=0.89062
# [15/100] training 18.2% loss=0.40495, acc=0.84375
# [15/100] training 18.4% loss=0.40477, acc=0.79688
# [15/100] training 18.5% loss=0.25835, acc=0.90625
# [15/100] training 18.8% loss=0.25297, acc=0.89062
# [15/100] training 18.9% loss=0.21000, acc=0.90625
# [15/100] training 19.1% loss=0.38375, acc=0.81250
# [15/100] training 19.2% loss=0.24954, acc=0.87500
# [15/100] training 19.4% loss=0.16571, acc=0.93750
# [15/100] training 19.6% loss=0.38831, acc=0.79688
# [15/100] training 19.7% loss=0.32391, acc=0.85938
# [15/100] training 20.0% loss=0.21351, acc=0.90625
# [15/100] training 20.1% loss=0.26406, acc=0.90625
# [15/100] training 20.3% loss=0.30049, acc=0.90625
# [15/100] training 20.4% loss=0.39516, acc=0.82812
# [15/100] training 20.6% loss=0.40504, acc=0.81250
# [15/100] training 20.8% loss=0.26388, acc=0.87500
# [15/100] training 20.9% loss=0.32997, acc=0.89062
# [15/100] training 21.2% loss=0.29460, acc=0.87500
# [15/100] training 21.3% loss=0.40168, acc=0.82812
# [15/100] training 21.5% loss=0.30121, acc=0.87500
# [15/100] training 21.6% loss=0.17793, acc=0.93750
# [15/100] training 21.8% loss=0.26478, acc=0.87500
# [15/100] training 21.9% loss=0.35676, acc=0.89062
# [15/100] training 22.2% loss=0.24218, acc=0.89062
# [15/100] training 22.4% loss=0.37881, acc=0.84375
# [15/100] training 22.5% loss=0.23358, acc=0.89062
# [15/100] training 22.7% loss=0.25907, acc=0.90625
# [15/100] training 22.8% loss=0.42992, acc=0.82812
# [15/100] training 23.0% loss=0.16012, acc=0.93750
# [15/100] training 23.1% loss=0.34766, acc=0.85938
# [15/100] training 23.4% loss=0.38591, acc=0.84375
# [15/100] training 23.6% loss=0.32263, acc=0.87500
# [15/100] training 23.7% loss=0.28973, acc=0.92188
# [15/100] training 23.9% loss=0.30630, acc=0.84375
# [15/100] training 24.0% loss=0.29643, acc=0.85938
# [15/100] training 24.2% loss=0.38381, acc=0.85938
# [15/100] training 24.3% loss=0.38377, acc=0.87500
# [15/100] training 24.6% loss=0.24672, acc=0.89062
# [15/100] training 24.7% loss=0.37190, acc=0.85938
# [15/100] training 24.9% loss=0.21927, acc=0.92188
# [15/100] training 25.1% loss=0.27076, acc=0.89062
# [15/100] training 25.2% loss=0.19542, acc=0.90625
# [15/100] training 25.4% loss=0.30428, acc=0.84375
# [15/100] training 25.6% loss=0.24453, acc=0.92188
# [15/100] training 25.8% loss=0.34721, acc=0.84375
# [15/100] training 25.9% loss=0.22654, acc=0.90625
# [15/100] training 26.1% loss=0.24046, acc=0.90625
# [15/100] training 26.3% loss=0.32502, acc=0.89062
# [15/100] training 26.4% loss=0.20745, acc=0.90625
# [15/100] training 26.6% loss=0.20987, acc=0.92188
# [15/100] training 26.8% loss=0.31650, acc=0.89062
# [15/100] training 27.0% loss=0.27059, acc=0.92188
# [15/100] training 27.1% loss=0.19853, acc=0.92188
# [15/100] training 27.3% loss=0.31019, acc=0.84375
# [15/100] training 27.4% loss=0.21360, acc=0.90625
# [15/100] training 27.6% loss=0.31058, acc=0.87500
# [15/100] training 27.9% loss=0.23008, acc=0.92188
# [15/100] training 28.0% loss=0.42478, acc=0.78125
# [15/100] training 28.2% loss=0.20209, acc=0.92188
# [15/100] training 28.3% loss=0.13111, acc=0.95312
# [15/100] training 28.5% loss=0.32866, acc=0.87500
# [15/100] training 28.6% loss=0.27676, acc=0.87500
# [15/100] training 28.8% loss=0.16380, acc=0.90625
# [15/100] training 29.1% loss=0.27175, acc=0.90625
# [15/100] training 29.2% loss=0.20006, acc=0.95312
# [15/100] training 29.4% loss=0.38350, acc=0.79688
# [15/100] training 29.5% loss=0.21678, acc=0.89062
# [15/100] training 29.7% loss=0.39017, acc=0.84375
# [15/100] training 29.8% loss=0.23749, acc=0.87500
# [15/100] training 30.0% loss=0.37308, acc=0.82812
# [15/100] training 30.2% loss=0.24442, acc=0.87500
# [15/100] training 30.4% loss=0.25499, acc=0.90625
# [15/100] training 30.6% loss=0.32217, acc=0.84375
# [15/100] training 30.7% loss=0.25813, acc=0.89062
# [15/100] training 30.9% loss=0.24876, acc=0.92188
# [15/100] training 31.0% loss=0.18293, acc=0.95312
# [15/100] training 31.3% loss=0.25446, acc=0.87500
# [15/100] training 31.4% loss=0.41276, acc=0.82812
# [15/100] training 31.6% loss=0.44705, acc=0.84375
# [15/100] training 31.8% loss=0.24874, acc=0.87500
# [15/100] training 31.9% loss=0.28972, acc=0.85938
# [15/100] training 32.1% loss=0.32443, acc=0.90625
# [15/100] training 32.2% loss=0.34065, acc=0.87500
# [15/100] training 32.5% loss=0.24203, acc=0.93750
# [15/100] training 32.6% loss=0.33211, acc=0.89062
# [15/100] training 32.8% loss=0.21236, acc=0.90625
# [15/100] training 32.9% loss=0.32159, acc=0.87500
# [15/100] training 33.1% loss=0.40483, acc=0.82812
# [15/100] training 33.3% loss=0.26038, acc=0.85938
# [15/100] training 33.4% loss=0.29412, acc=0.90625
# [15/100] training 33.7% loss=0.31528, acc=0.85938
# [15/100] training 33.8% loss=0.27909, acc=0.85938
# [15/100] training 34.0% loss=0.27191, acc=0.85938
# [15/100] training 34.1% loss=0.28655, acc=0.89062
# [15/100] training 34.3% loss=0.25453, acc=0.89062
# [15/100] training 34.5% loss=0.34039, acc=0.84375
# [15/100] training 34.7% loss=0.21038, acc=0.93750
# [15/100] training 34.9% loss=0.25338, acc=0.90625
# [15/100] training 35.0% loss=0.31681, acc=0.84375
# [15/100] training 35.2% loss=0.41406, acc=0.79688
# [15/100] training 35.3% loss=0.25327, acc=0.90625
# [15/100] training 35.5% loss=0.28569, acc=0.85938
# [15/100] training 35.6% loss=0.42621, acc=0.81250
# [15/100] training 35.9% loss=0.33729, acc=0.85938
# [15/100] training 36.1% loss=0.35719, acc=0.85938
# [15/100] training 36.2% loss=0.32800, acc=0.87500
# [15/100] training 36.4% loss=0.36257, acc=0.85938
# [15/100] training 36.5% loss=0.29425, acc=0.89062
# [15/100] training 36.7% loss=0.22711, acc=0.92188
# [15/100] training 36.8% loss=0.22586, acc=0.87500
# [15/100] training 37.1% loss=0.42152, acc=0.84375
# [15/100] training 37.3% loss=0.33318, acc=0.87500
# [15/100] training 37.4% loss=0.35589, acc=0.85938
# [15/100] training 37.6% loss=0.27698, acc=0.85938
# [15/100] training 37.7% loss=0.28088, acc=0.89062
# [15/100] training 37.9% loss=0.25918, acc=0.89062
# [15/100] training 38.1% loss=0.36084, acc=0.85938
# [15/100] training 38.3% loss=0.27484, acc=0.89062
# [15/100] training 38.4% loss=0.23423, acc=0.90625
# [15/100] training 38.6% loss=0.25797, acc=0.90625
# [15/100] training 38.8% loss=0.34202, acc=0.87500
# [15/100] training 38.9% loss=0.30321, acc=0.87500
# [15/100] training 39.1% loss=0.26955, acc=0.90625
# [15/100] training 39.3% loss=0.25917, acc=0.90625
# [15/100] training 39.5% loss=0.39539, acc=0.85938
# [15/100] training 39.6% loss=0.32012, acc=0.92188
# [15/100] training 39.8% loss=0.21224, acc=0.92188
# [15/100] training 40.0% loss=0.25845, acc=0.89062
# [15/100] training 40.1% loss=0.29542, acc=0.89062
# [15/100] training 40.4% loss=0.22900, acc=0.87500
# [15/100] training 40.5% loss=0.27011, acc=0.87500
# [15/100] training 40.7% loss=0.26909, acc=0.87500
# [15/100] training 40.8% loss=0.22155, acc=0.89062
# [15/100] training 41.0% loss=0.18693, acc=0.93750
# [15/100] training 41.1% loss=0.37262, acc=0.82812
# [15/100] training 41.3% loss=0.37798, acc=0.81250
# [15/100] training 41.6% loss=0.30914, acc=0.89062
# [15/100] training 41.7% loss=0.36688, acc=0.85938
# [15/100] training 41.9% loss=0.24434, acc=0.89062
# [15/100] training 42.0% loss=0.25822, acc=0.87500
# [15/100] training 42.2% loss=0.34363, acc=0.85938
# [15/100] training 42.3% loss=0.32252, acc=0.87500
# [15/100] training 42.5% loss=0.22714, acc=0.90625
# [15/100] training 42.8% loss=0.19437, acc=0.92188
# [15/100] training 42.9% loss=0.25828, acc=0.85938
# [15/100] training 43.1% loss=0.22185, acc=0.92188
# [15/100] training 43.2% loss=0.20995, acc=0.90625
# [15/100] training 43.4% loss=0.29594, acc=0.89062
# [15/100] training 43.5% loss=0.19938, acc=0.90625
# [15/100] training 43.8% loss=0.38232, acc=0.82812
# [15/100] training 43.9% loss=0.25819, acc=0.92188
# [15/100] training 44.1% loss=0.23389, acc=0.95312
# [15/100] training 44.3% loss=0.31000, acc=0.87500
# [15/100] training 44.4% loss=0.34300, acc=0.78125
# [15/100] training 44.6% loss=0.35599, acc=0.84375
# [15/100] training 44.7% loss=0.34658, acc=0.90625
# [15/100] training 45.0% loss=0.30245, acc=0.85938
# [15/100] training 45.1% loss=0.36106, acc=0.85938
# [15/100] training 45.3% loss=0.21006, acc=0.92188
# [15/100] training 45.5% loss=0.15039, acc=0.96875
# [15/100] training 45.6% loss=0.28463, acc=0.89062
# [15/100] training 45.8% loss=0.22024, acc=0.92188
# [15/100] training 45.9% loss=0.19235, acc=0.93750
# [15/100] training 46.2% loss=0.22737, acc=0.92188
# [15/100] training 46.3% loss=0.16550, acc=0.95312
# [15/100] training 46.5% loss=0.42827, acc=0.81250
# [15/100] training 46.6% loss=0.24603, acc=0.92188
# [15/100] training 46.8% loss=0.20413, acc=0.92188
# [15/100] training 47.0% loss=0.34159, acc=0.85938
# [15/100] training 47.2% loss=0.32015, acc=0.82812
# [15/100] training 47.4% loss=0.26934, acc=0.90625
# [15/100] training 47.5% loss=0.37773, acc=0.82812
# [15/100] training 47.7% loss=0.32790, acc=0.87500
# [15/100] training 47.8% loss=0.34278, acc=0.90625
# [15/100] training 48.0% loss=0.33061, acc=0.85938
# [15/100] training 48.3% loss=0.13103, acc=0.93750
# [15/100] training 48.4% loss=0.17608, acc=0.93750
# [15/100] training 48.6% loss=0.20028, acc=0.92188
# [15/100] training 48.7% loss=0.42014, acc=0.82812
# [15/100] training 48.9% loss=0.32668, acc=0.85938
# [15/100] training 49.0% loss=0.33185, acc=0.82812
# [15/100] training 49.2% loss=0.35428, acc=0.82812
# [15/100] training 49.3% loss=0.24253, acc=0.92188
# [15/100] training 49.6% loss=0.25728, acc=0.90625
# [15/100] training 49.8% loss=0.32291, acc=0.84375
# [15/100] training 49.9% loss=0.28442, acc=0.89062
# [15/100] training 50.1% loss=0.30597, acc=0.89062
# [15/100] training 50.2% loss=0.34227, acc=0.87500
# [15/100] training 50.4% loss=0.35838, acc=0.89062
# [15/100] training 50.6% loss=0.31753, acc=0.87500
# [15/100] training 50.8% loss=0.39381, acc=0.82812
# [15/100] training 51.0% loss=0.33324, acc=0.84375
# [15/100] training 51.1% loss=0.31796, acc=0.84375
# [15/100] training 51.3% loss=0.26636, acc=0.89062
# [15/100] training 51.4% loss=0.19811, acc=0.96875
# [15/100] training 51.7% loss=0.38666, acc=0.81250
# [15/100] training 51.8% loss=0.24524, acc=0.92188
# [15/100] training 52.0% loss=0.25824, acc=0.89062
# [15/100] training 52.1% loss=0.31756, acc=0.89062
# [15/100] training 52.3% loss=0.35885, acc=0.82812
# [15/100] training 52.5% loss=0.14426, acc=0.95312
# [15/100] training 52.6% loss=0.21117, acc=0.95312
# [15/100] training 52.9% loss=0.58419, acc=0.76562
# [15/100] training 53.0% loss=0.18305, acc=0.92188
# [15/100] training 53.2% loss=0.38488, acc=0.87500
# [15/100] training 53.3% loss=0.25635, acc=0.90625
# [15/100] training 53.5% loss=0.30982, acc=0.84375
# [15/100] training 53.7% loss=0.27363, acc=0.87500
# [15/100] training 53.8% loss=0.42473, acc=0.79688
# [15/100] training 54.1% loss=0.36684, acc=0.84375
# [15/100] training 54.2% loss=0.26867, acc=0.89062
# [15/100] training 54.4% loss=0.27224, acc=0.90625
# [15/100] training 54.5% loss=0.39900, acc=0.75000
# [15/100] training 54.7% loss=0.38434, acc=0.85938
# [15/100] training 54.8% loss=0.22229, acc=0.92188
# [15/100] training 55.1% loss=0.20606, acc=0.90625
# [15/100] training 55.3% loss=0.21193, acc=0.89062
# [15/100] training 55.4% loss=0.24637, acc=0.87500
# [15/100] training 55.6% loss=0.31845, acc=0.82812
# [15/100] training 55.7% loss=0.22592, acc=0.90625
# [15/100] training 55.9% loss=0.21410, acc=0.92188
# [15/100] training 56.0% loss=0.32704, acc=0.84375
# [15/100] training 56.3% loss=0.49669, acc=0.82812
# [15/100] training 56.5% loss=0.32578, acc=0.87500
# [15/100] training 56.6% loss=0.31469, acc=0.84375
# [15/100] training 56.8% loss=0.28552, acc=0.89062
# [15/100] training 56.9% loss=0.34720, acc=0.82812
# [15/100] training 57.1% loss=0.37453, acc=0.87500
# [15/100] training 57.2% loss=0.20210, acc=0.93750
# [15/100] training 57.5% loss=0.24007, acc=0.90625
# [15/100] training 57.6% loss=0.35251, acc=0.84375
# [15/100] training 57.8% loss=0.24085, acc=0.87500
# [15/100] training 58.0% loss=0.19334, acc=0.92188
# [15/100] training 58.1% loss=0.29451, acc=0.90625
# [15/100] training 58.3% loss=0.20053, acc=0.89062
# [15/100] training 58.4% loss=0.29251, acc=0.90625
# [15/100] training 58.7% loss=0.25109, acc=0.89062
# [15/100] training 58.8% loss=0.32837, acc=0.85938
# [15/100] training 59.0% loss=0.30895, acc=0.84375
# [15/100] training 59.2% loss=0.38021, acc=0.84375
# [15/100] training 59.3% loss=0.32016, acc=0.85938
# [15/100] training 59.5% loss=0.23972, acc=0.92188
# [15/100] training 59.7% loss=0.36514, acc=0.84375
# [15/100] training 59.9% loss=0.22087, acc=0.90625
# [15/100] training 60.0% loss=0.23715, acc=0.89062
# [15/100] training 60.2% loss=0.35259, acc=0.78125
# [15/100] training 60.3% loss=0.22533, acc=0.90625
# [15/100] training 60.5% loss=0.27778, acc=0.89062
# [15/100] training 60.8% loss=0.31113, acc=0.84375
# [15/100] training 60.9% loss=0.35213, acc=0.85938
# [15/100] training 61.1% loss=0.42543, acc=0.79688
# [15/100] training 61.2% loss=0.31840, acc=0.84375
# [15/100] training 61.4% loss=0.32066, acc=0.84375
# [15/100] training 61.5% loss=0.49395, acc=0.76562
# [15/100] training 61.7% loss=0.31623, acc=0.85938
# [15/100] training 62.0% loss=0.30411, acc=0.82812
# [15/100] training 62.1% loss=0.37888, acc=0.78125
# [15/100] training 62.3% loss=0.22337, acc=0.93750
# [15/100] training 62.4% loss=0.29756, acc=0.87500
# [15/100] training 62.6% loss=0.49091, acc=0.76562
# [15/100] training 62.7% loss=0.27039, acc=0.90625
# [15/100] training 62.9% loss=0.34721, acc=0.82812
# [15/100] training 63.1% loss=0.19350, acc=0.92188
# [15/100] training 63.3% loss=0.29076, acc=0.82812
# [15/100] training 63.5% loss=0.35912, acc=0.85938
# [15/100] training 63.6% loss=0.27276, acc=0.85938
# [15/100] training 63.8% loss=0.31940, acc=0.81250
# [15/100] training 63.9% loss=0.26006, acc=0.89062
# [15/100] training 64.2% loss=0.22235, acc=0.89062
# [15/100] training 64.3% loss=0.36859, acc=0.89062
# [15/100] training 64.5% loss=0.25515, acc=0.87500
# [15/100] training 64.7% loss=0.28297, acc=0.85938
# [15/100] training 64.8% loss=0.58533, acc=0.75000
# [15/100] training 65.0% loss=0.31270, acc=0.87500
# [15/100] training 65.1% loss=0.29138, acc=0.87500
# [15/100] training 65.4% loss=0.24292, acc=0.89062
# [15/100] training 65.5% loss=0.21951, acc=0.92188
# [15/100] training 65.7% loss=0.21939, acc=0.89062
# [15/100] training 65.8% loss=0.39494, acc=0.82812
# [15/100] training 66.0% loss=0.32110, acc=0.82812
# [15/100] training 66.2% loss=0.16108, acc=0.92188
# [15/100] training 66.3% loss=0.34482, acc=0.87500
# [15/100] training 66.6% loss=0.32173, acc=0.84375
# [15/100] training 66.7% loss=0.22888, acc=0.93750
# [15/100] training 66.9% loss=0.25724, acc=0.89062
# [15/100] training 67.0% loss=0.42573, acc=0.79688
# [15/100] training 67.2% loss=0.20889, acc=0.92188
# [15/100] training 67.4% loss=0.27739, acc=0.87500
# [15/100] training 67.6% loss=0.33555, acc=0.84375
# [15/100] training 67.8% loss=0.24350, acc=0.90625
# [15/100] training 67.9% loss=0.17561, acc=0.95312
# [15/100] training 68.1% loss=0.22568, acc=0.87500
# [15/100] training 68.2% loss=0.17659, acc=0.93750
# [15/100] training 68.4% loss=0.20579, acc=0.90625
# [15/100] training 68.5% loss=0.36320, acc=0.84375
# [15/100] training 68.8% loss=0.33641, acc=0.84375
# [15/100] training 69.0% loss=0.35059, acc=0.84375
# [15/100] training 69.1% loss=0.16495, acc=0.90625
# [15/100] training 69.3% loss=0.27819, acc=0.84375
# [15/100] training 69.4% loss=0.21830, acc=0.89062
# [15/100] training 69.6% loss=0.40210, acc=0.87500
# [15/100] training 69.7% loss=0.26961, acc=0.87500
# [15/100] training 70.0% loss=0.35560, acc=0.85938
# [15/100] training 70.2% loss=0.37233, acc=0.85938
# [15/100] training 70.3% loss=0.28324, acc=0.84375
# [15/100] training 70.5% loss=0.29983, acc=0.92188
# [15/100] training 70.6% loss=0.22740, acc=0.90625
# [15/100] training 70.8% loss=0.33900, acc=0.82812
# [15/100] training 71.0% loss=0.27694, acc=0.90625
# [15/100] training 71.2% loss=0.24532, acc=0.85938
# [15/100] training 71.3% loss=0.18908, acc=0.95312
# [15/100] training 71.5% loss=0.47496, acc=0.84375
# [15/100] training 71.7% loss=0.32062, acc=0.85938
# [15/100] training 71.8% loss=0.33169, acc=0.89062
# [15/100] training 72.0% loss=0.18375, acc=0.93750
# [15/100] training 72.2% loss=0.26883, acc=0.87500
# [15/100] training 72.4% loss=0.37779, acc=0.82812
# [15/100] training 72.5% loss=0.32146, acc=0.85938
# [15/100] training 72.7% loss=0.45657, acc=0.76562
# [15/100] training 72.9% loss=0.26007, acc=0.87500
# [15/100] training 73.0% loss=0.20992, acc=0.93750
# [15/100] training 73.3% loss=0.30998, acc=0.87500
# [15/100] training 73.4% loss=0.17619, acc=0.95312
# [15/100] training 73.6% loss=0.18174, acc=0.93750
# [15/100] training 73.7% loss=0.23719, acc=0.87500
# [15/100] training 73.9% loss=0.31329, acc=0.85938
# [15/100] training 74.0% loss=0.41691, acc=0.84375
# [15/100] training 74.2% loss=0.25623, acc=0.89062
# [15/100] training 74.5% loss=0.25311, acc=0.87500
# [15/100] training 74.6% loss=0.46448, acc=0.79688
# [15/100] training 74.8% loss=0.42902, acc=0.79688
# [15/100] training 74.9% loss=0.41730, acc=0.76562
# [15/100] training 75.1% loss=0.30491, acc=0.89062
# [15/100] training 75.2% loss=0.27290, acc=0.87500
# [15/100] training 75.4% loss=0.33429, acc=0.85938
# [15/100] training 75.7% loss=0.30576, acc=0.84375
# [15/100] training 75.8% loss=0.33585, acc=0.82812
# [15/100] training 76.0% loss=0.17678, acc=0.93750
# [15/100] training 76.1% loss=0.26954, acc=0.82812
# [15/100] training 76.3% loss=0.33310, acc=0.92188
# [15/100] training 76.4% loss=0.31259, acc=0.85938
# [15/100] training 76.7% loss=0.27903, acc=0.90625
# [15/100] training 76.8% loss=0.30713, acc=0.87500
# [15/100] training 77.0% loss=0.20637, acc=0.92188
# [15/100] training 77.2% loss=0.35068, acc=0.89062
# [15/100] training 77.3% loss=0.26766, acc=0.89062
# [15/100] training 77.5% loss=0.32731, acc=0.87500
# [15/100] training 77.6% loss=0.32258, acc=0.82812
# [15/100] training 77.9% loss=0.26343, acc=0.87500
# [15/100] training 78.0% loss=0.32503, acc=0.85938
# [15/100] training 78.2% loss=0.30181, acc=0.85938
# [15/100] training 78.4% loss=0.14573, acc=0.89062
# [15/100] training 78.5% loss=0.45191, acc=0.85938
# [15/100] training 78.7% loss=0.23211, acc=0.92188
# [15/100] training 78.8% loss=0.15083, acc=0.98438
# [15/100] training 79.1% loss=0.18238, acc=0.93750
# [15/100] training 79.2% loss=0.21934, acc=0.89062
# [15/100] training 79.4% loss=0.42369, acc=0.82812
# [15/100] training 79.5% loss=0.31725, acc=0.87500
# [15/100] training 79.7% loss=0.17382, acc=0.96875
# [15/100] training 79.9% loss=0.23891, acc=0.92188
# [15/100] training 80.1% loss=0.22425, acc=0.90625
# [15/100] training 80.3% loss=0.41900, acc=0.84375
# [15/100] training 80.4% loss=0.28314, acc=0.87500
# [15/100] training 80.6% loss=0.31014, acc=0.85938
# [15/100] training 80.7% loss=0.22623, acc=0.90625
# [15/100] training 80.9% loss=0.33116, acc=0.87500
# [15/100] training 81.2% loss=0.31829, acc=0.85938
# [15/100] training 81.3% loss=0.36823, acc=0.79688
# [15/100] training 81.5% loss=0.33178, acc=0.87500
# [15/100] training 81.6% loss=0.37187, acc=0.78125
# [15/100] training 81.8% loss=0.27597, acc=0.89062
# [15/100] training 81.9% loss=0.43970, acc=0.89062
# [15/100] training 82.1% loss=0.17506, acc=0.96875
# [15/100] training 82.2% loss=0.32947, acc=0.85938
# [15/100] training 82.5% loss=0.25461, acc=0.92188
# [15/100] training 82.7% loss=0.34016, acc=0.89062
# [15/100] training 82.8% loss=0.28863, acc=0.90625
# [15/100] training 83.0% loss=0.25291, acc=0.92188
# [15/100] training 83.1% loss=0.31640, acc=0.85938
# [15/100] training 83.3% loss=0.27787, acc=0.85938
# [15/100] training 83.5% loss=0.28923, acc=0.87500
# [15/100] training 83.7% loss=0.42307, acc=0.81250
# [15/100] training 83.9% loss=0.25752, acc=0.90625
# [15/100] training 84.0% loss=0.29520, acc=0.87500
# [15/100] training 84.2% loss=0.24659, acc=0.89062
# [15/100] training 84.3% loss=0.23624, acc=0.90625
# [15/100] training 84.5% loss=0.21552, acc=0.92188
# [15/100] training 84.7% loss=0.32700, acc=0.89062
# [15/100] training 84.9% loss=0.21168, acc=0.89062
# [15/100] training 85.0% loss=0.24847, acc=0.90625
# [15/100] training 85.2% loss=0.32256, acc=0.90625
# [15/100] training 85.4% loss=0.36418, acc=0.93750
# [15/100] training 85.5% loss=0.32333, acc=0.87500
# [15/100] training 85.8% loss=0.28328, acc=0.85938
# [15/100] training 85.9% loss=0.34497, acc=0.87500
# [15/100] training 86.1% loss=0.26474, acc=0.90625
# [15/100] training 86.2% loss=0.16849, acc=0.95312
# [15/100] training 86.4% loss=0.36310, acc=0.84375
# [15/100] training 86.6% loss=0.32801, acc=0.85938
# [15/100] training 86.7% loss=0.20612, acc=0.92188
# [15/100] training 87.0% loss=0.32031, acc=0.84375
# [15/100] training 87.1% loss=0.29263, acc=0.82812
# [15/100] training 87.3% loss=0.30764, acc=0.84375
# [15/100] training 87.4% loss=0.26352, acc=0.90625
# [15/100] training 87.6% loss=0.26547, acc=0.90625
# [15/100] training 87.7% loss=0.34355, acc=0.92188
# [15/100] training 87.9% loss=0.32689, acc=0.87500
# [15/100] training 88.2% loss=0.21771, acc=0.92188
# [15/100] training 88.3% loss=0.29309, acc=0.87500
# [15/100] training 88.5% loss=0.26995, acc=0.89062
# [15/100] training 88.6% loss=0.14767, acc=0.95312
# [15/100] training 88.8% loss=0.24101, acc=0.92188
# [15/100] training 88.9% loss=0.28607, acc=0.92188
# [15/100] training 89.2% loss=0.23056, acc=0.89062
# [15/100] training 89.4% loss=0.25168, acc=0.92188
# [15/100] training 89.5% loss=0.28721, acc=0.85938
# [15/100] training 89.7% loss=0.30769, acc=0.85938
# [15/100] training 89.8% loss=0.27941, acc=0.90625
# [15/100] training 90.0% loss=0.25465, acc=0.87500
# [15/100] training 90.1% loss=0.24591, acc=0.92188
# [15/100] training 90.4% loss=0.25109, acc=0.90625
# [15/100] training 90.5% loss=0.24734, acc=0.87500
# [15/100] training 90.7% loss=0.25714, acc=0.87500
# [15/100] training 90.9% loss=0.24494, acc=0.90625
# [15/100] training 91.0% loss=0.28889, acc=0.85938
# [15/100] training 91.2% loss=0.32465, acc=0.85938
# [15/100] training 91.3% loss=0.40817, acc=0.81250
# [15/100] training 91.6% loss=0.37517, acc=0.85938
# [15/100] training 91.7% loss=0.23234, acc=0.89062
# [15/100] training 91.9% loss=0.29562, acc=0.89062
# [15/100] training 92.1% loss=0.29608, acc=0.85938
# [15/100] training 92.2% loss=0.18672, acc=0.95312
# [15/100] training 92.4% loss=0.25306, acc=0.87500
# [15/100] training 92.6% loss=0.33434, acc=0.85938
# [15/100] training 92.8% loss=0.41652, acc=0.81250
# [15/100] training 92.9% loss=0.32998, acc=0.89062
# [15/100] training 93.1% loss=0.33495, acc=0.85938
# [15/100] training 93.2% loss=0.27642, acc=0.87500
# [15/100] training 93.4% loss=0.25150, acc=0.93750
# [15/100] training 93.7% loss=0.20829, acc=0.90625
# [15/100] training 93.8% loss=0.24193, acc=0.87500
# [15/100] training 94.0% loss=0.17901, acc=0.93750
# [15/100] training 94.1% loss=0.38213, acc=0.82812
# [15/100] training 94.3% loss=0.19688, acc=0.93750
# [15/100] training 94.4% loss=0.27349, acc=0.89062
# [15/100] training 94.6% loss=0.19600, acc=0.87500
# [15/100] training 94.9% loss=0.26845, acc=0.87500
# [15/100] training 95.0% loss=0.40395, acc=0.87500
# [15/100] training 95.2% loss=0.33577, acc=0.85938
# [15/100] training 95.3% loss=0.32755, acc=0.89062
# [15/100] training 95.5% loss=0.23441, acc=0.92188
# [15/100] training 95.6% loss=0.41953, acc=0.79688
# [15/100] training 95.8% loss=0.33902, acc=0.81250
# [15/100] training 96.0% loss=0.30259, acc=0.84375
# [15/100] training 96.2% loss=0.19978, acc=0.93750
# [15/100] training 96.4% loss=0.26315, acc=0.89062
# [15/100] training 96.5% loss=0.26881, acc=0.87500
# [15/100] training 96.7% loss=0.22399, acc=0.90625
# [15/100] training 96.8% loss=0.27331, acc=0.90625
# [15/100] training 97.1% loss=0.26469, acc=0.84375
# [15/100] training 97.2% loss=0.29305, acc=0.89062
# [15/100] training 97.4% loss=0.27633, acc=0.90625
# [15/100] training 97.6% loss=0.36342, acc=0.89062
# [15/100] training 97.7% loss=0.19284, acc=0.93750
# [15/100] training 97.9% loss=0.18242, acc=0.92188
# [15/100] training 98.0% loss=0.29202, acc=0.81250
# [15/100] training 98.3% loss=0.28451, acc=0.85938
# [15/100] training 98.4% loss=0.25740, acc=0.92188
# [15/100] training 98.6% loss=0.42362, acc=0.81250
# [15/100] training 98.7% loss=0.32247, acc=0.87500
# [15/100] training 98.9% loss=0.19762, acc=0.90625
# [15/100] training 99.1% loss=0.22612, acc=0.92188
# [15/100] training 99.2% loss=0.23669, acc=0.90625
# [15/100] training 99.5% loss=0.32562, acc=0.82812
# [15/100] training 99.6% loss=0.30755, acc=0.85938
# [15/100] training 99.8% loss=0.33480, acc=0.89062
# [15/100] training 99.9% loss=0.10882, acc=0.96875
# [15/100] testing 0.9% loss=0.12838, acc=0.96875
# [15/100] testing 1.8% loss=0.47966, acc=0.81250
# [15/100] testing 2.2% loss=0.27247, acc=0.87500
# [15/100] testing 3.1% loss=0.34225, acc=0.81250
# [15/100] testing 3.5% loss=0.23446, acc=0.89062
# [15/100] testing 4.4% loss=0.40908, acc=0.81250
# [15/100] testing 4.8% loss=0.39779, acc=0.82812
# [15/100] testing 5.7% loss=0.22705, acc=0.92188
# [15/100] testing 6.6% loss=0.21086, acc=0.93750
# [15/100] testing 7.0% loss=0.23658, acc=0.89062
# [15/100] testing 7.9% loss=0.47859, acc=0.78125
# [15/100] testing 8.3% loss=0.26525, acc=0.92188
# [15/100] testing 9.2% loss=0.35365, acc=0.85938
# [15/100] testing 9.7% loss=0.11345, acc=0.96875
# [15/100] testing 10.5% loss=0.38954, acc=0.81250
# [15/100] testing 11.0% loss=0.35921, acc=0.82812
# [15/100] testing 11.8% loss=0.41747, acc=0.87500
# [15/100] testing 12.7% loss=0.51448, acc=0.78125
# [15/100] testing 13.2% loss=0.25387, acc=0.89062
# [15/100] testing 14.0% loss=0.56152, acc=0.82812
# [15/100] testing 14.5% loss=0.42278, acc=0.85938
# [15/100] testing 15.4% loss=0.46309, acc=0.84375
# [15/100] testing 15.8% loss=0.18599, acc=0.90625
# [15/100] testing 16.7% loss=0.27753, acc=0.85938
# [15/100] testing 17.5% loss=0.28611, acc=0.87500
# [15/100] testing 18.0% loss=0.17094, acc=0.90625
# [15/100] testing 18.9% loss=0.20626, acc=0.92188
# [15/100] testing 19.3% loss=0.33850, acc=0.89062
# [15/100] testing 20.2% loss=0.54905, acc=0.81250
# [15/100] testing 20.6% loss=0.48439, acc=0.81250
# [15/100] testing 21.5% loss=0.29105, acc=0.89062
# [15/100] testing 21.9% loss=0.58655, acc=0.75000
# [15/100] testing 22.8% loss=0.50759, acc=0.81250
# [15/100] testing 23.7% loss=0.39861, acc=0.85938
# [15/100] testing 24.1% loss=0.25437, acc=0.89062
# [15/100] testing 25.0% loss=0.45182, acc=0.87500
# [15/100] testing 25.4% loss=0.16503, acc=0.93750
# [15/100] testing 26.3% loss=0.39147, acc=0.81250
# [15/100] testing 26.8% loss=0.46122, acc=0.89062
# [15/100] testing 27.6% loss=0.40783, acc=0.84375
# [15/100] testing 28.5% loss=0.35025, acc=0.89062
# [15/100] testing 29.0% loss=0.17208, acc=0.90625
# [15/100] testing 29.8% loss=0.48287, acc=0.89062
# [15/100] testing 30.3% loss=0.40606, acc=0.82812
# [15/100] testing 31.1% loss=0.46945, acc=0.81250
# [15/100] testing 31.6% loss=0.23773, acc=0.85938
# [15/100] testing 32.5% loss=0.26862, acc=0.92188
# [15/100] testing 32.9% loss=0.46523, acc=0.82812
# [15/100] testing 33.8% loss=0.33086, acc=0.89062
# [15/100] testing 34.7% loss=0.45504, acc=0.84375
# [15/100] testing 35.1% loss=0.35834, acc=0.87500
# [15/100] testing 36.0% loss=0.30230, acc=0.90625
# [15/100] testing 36.4% loss=0.31466, acc=0.90625
# [15/100] testing 37.3% loss=0.41773, acc=0.84375
# [15/100] testing 37.7% loss=0.40532, acc=0.82812
# [15/100] testing 38.6% loss=0.23113, acc=0.90625
# [15/100] testing 39.5% loss=0.41158, acc=0.87500
# [15/100] testing 39.9% loss=0.39027, acc=0.89062
# [15/100] testing 40.8% loss=0.38561, acc=0.87500
# [15/100] testing 41.2% loss=0.24431, acc=0.92188
# [15/100] testing 42.1% loss=0.35572, acc=0.85938
# [15/100] testing 42.5% loss=0.28792, acc=0.84375
# [15/100] testing 43.4% loss=0.44521, acc=0.82812
# [15/100] testing 43.9% loss=0.31496, acc=0.92188
# [15/100] testing 44.7% loss=0.43953, acc=0.87500
# [15/100] testing 45.6% loss=0.28967, acc=0.89062
# [15/100] testing 46.1% loss=0.22943, acc=0.92188
# [15/100] testing 46.9% loss=0.27176, acc=0.89062
# [15/100] testing 47.4% loss=0.12221, acc=0.92188
# [15/100] testing 48.3% loss=0.45180, acc=0.84375
# [15/100] testing 48.7% loss=0.54200, acc=0.79688
# [15/100] testing 49.6% loss=0.58421, acc=0.81250
# [15/100] testing 50.4% loss=0.25753, acc=0.90625
# [15/100] testing 50.9% loss=0.36725, acc=0.82812
# [15/100] testing 51.8% loss=0.40149, acc=0.82812
# [15/100] testing 52.2% loss=0.33160, acc=0.89062
# [15/100] testing 53.1% loss=0.25026, acc=0.87500
# [15/100] testing 53.5% loss=0.21100, acc=0.92188
# [15/100] testing 54.4% loss=0.55339, acc=0.75000
# [15/100] testing 54.8% loss=0.46704, acc=0.82812
# [15/100] testing 55.7% loss=0.17408, acc=0.93750
# [15/100] testing 56.6% loss=0.41854, acc=0.85938
# [15/100] testing 57.0% loss=0.49569, acc=0.82812
# [15/100] testing 57.9% loss=0.49624, acc=0.79688
# [15/100] testing 58.3% loss=0.64434, acc=0.75000
# [15/100] testing 59.2% loss=0.36954, acc=0.82812
# [15/100] testing 59.7% loss=0.41498, acc=0.84375
# [15/100] testing 60.5% loss=0.36313, acc=0.85938
# [15/100] testing 61.4% loss=0.18703, acc=0.92188
# [15/100] testing 61.9% loss=0.37173, acc=0.87500
# [15/100] testing 62.7% loss=0.35949, acc=0.85938
# [15/100] testing 63.2% loss=0.47738, acc=0.85938
# [15/100] testing 64.0% loss=0.42181, acc=0.81250
# [15/100] testing 64.5% loss=0.22445, acc=0.87500
# [15/100] testing 65.4% loss=0.25722, acc=0.90625
# [15/100] testing 65.8% loss=0.45627, acc=0.84375
# [15/100] testing 66.7% loss=0.29706, acc=0.90625
# [15/100] testing 67.6% loss=0.34800, acc=0.84375
# [15/100] testing 68.0% loss=0.27158, acc=0.84375
# [15/100] testing 68.9% loss=0.37738, acc=0.90625
# [15/100] testing 69.3% loss=0.42611, acc=0.82812
# [15/100] testing 70.2% loss=0.47517, acc=0.84375
# [15/100] testing 70.6% loss=0.45682, acc=0.82812
# [15/100] testing 71.5% loss=0.46095, acc=0.84375
# [15/100] testing 72.4% loss=0.24585, acc=0.85938
# [15/100] testing 72.8% loss=0.40234, acc=0.82812
# [15/100] testing 73.7% loss=0.40285, acc=0.89062
# [15/100] testing 74.1% loss=0.50536, acc=0.82812
# [15/100] testing 75.0% loss=0.30722, acc=0.84375
# [15/100] testing 75.4% loss=0.32106, acc=0.89062
# [15/100] testing 76.3% loss=0.09996, acc=0.98438
# [15/100] testing 76.8% loss=0.41184, acc=0.85938
# [15/100] testing 77.6% loss=0.29926, acc=0.85938
# [15/100] testing 78.5% loss=0.60140, acc=0.76562
# [15/100] testing 79.0% loss=0.41312, acc=0.84375
# [15/100] testing 79.8% loss=0.30151, acc=0.84375
# [15/100] testing 80.3% loss=0.28980, acc=0.89062
# [15/100] testing 81.2% loss=0.55458, acc=0.84375
# [15/100] testing 81.6% loss=0.32842, acc=0.90625
# [15/100] testing 82.5% loss=0.25518, acc=0.92188
# [15/100] testing 83.3% loss=0.23249, acc=0.93750
# [15/100] testing 83.8% loss=0.19331, acc=0.92188
# [15/100] testing 84.7% loss=0.31516, acc=0.87500
# [15/100] testing 85.1% loss=0.35064, acc=0.92188
# [15/100] testing 86.0% loss=0.36382, acc=0.87500
# [15/100] testing 86.4% loss=0.60877, acc=0.81250
# [15/100] testing 87.3% loss=0.61647, acc=0.79688
# [15/100] testing 87.7% loss=0.30512, acc=0.89062
# [15/100] testing 88.6% loss=0.21888, acc=0.90625
# [15/100] testing 89.5% loss=0.57810, acc=0.75000
# [15/100] testing 89.9% loss=0.33163, acc=0.82812
# [15/100] testing 90.8% loss=0.35852, acc=0.82812
# [15/100] testing 91.2% loss=0.12108, acc=0.93750
# [15/100] testing 92.1% loss=0.36215, acc=0.87500
# [15/100] testing 92.6% loss=0.22691, acc=0.93750
# [15/100] testing 93.4% loss=0.44206, acc=0.84375
# [15/100] testing 94.3% loss=0.27329, acc=0.87500
# [15/100] testing 94.7% loss=0.32717, acc=0.87500
# [15/100] testing 95.6% loss=0.41802, acc=0.85938
# [15/100] testing 96.1% loss=0.30145, acc=0.82812
# [15/100] testing 96.9% loss=0.37046, acc=0.85938
# [15/100] testing 97.4% loss=0.25700, acc=0.92188
# [15/100] testing 98.3% loss=0.26795, acc=0.85938
# [15/100] testing 98.7% loss=0.32181, acc=0.89062
# [15/100] testing 99.6% loss=0.30134, acc=0.90625
# [16/100] training 0.2% loss=0.52103, acc=0.75000
# [16/100] training 0.4% loss=0.48634, acc=0.78125
# [16/100] training 0.5% loss=0.19953, acc=0.93750
# [16/100] training 0.8% loss=0.38039, acc=0.82812
# [16/100] training 0.9% loss=0.33339, acc=0.82812
# [16/100] training 1.1% loss=0.23913, acc=0.92188
# [16/100] training 1.2% loss=0.31993, acc=0.84375
# [16/100] training 1.4% loss=0.28379, acc=0.89062
# [16/100] training 1.6% loss=0.13149, acc=0.96875
# [16/100] training 1.8% loss=0.33049, acc=0.89062
# [16/100] training 2.0% loss=0.23341, acc=0.92188
# [16/100] training 2.1% loss=0.28588, acc=0.87500
# [16/100] training 2.3% loss=0.17940, acc=0.92188
# [16/100] training 2.4% loss=0.32769, acc=0.90625
# [16/100] training 2.6% loss=0.22861, acc=0.90625
# [16/100] training 2.7% loss=0.22528, acc=0.89062
# [16/100] training 3.0% loss=0.36831, acc=0.82812
# [16/100] training 3.2% loss=0.22293, acc=0.92188
# [16/100] training 3.3% loss=0.28232, acc=0.87500
# [16/100] training 3.5% loss=0.28277, acc=0.85938
# [16/100] training 3.6% loss=0.51383, acc=0.76562
# [16/100] training 3.8% loss=0.22867, acc=0.85938
# [16/100] training 3.9% loss=0.22916, acc=0.92188
# [16/100] training 4.2% loss=0.27207, acc=0.85938
# [16/100] training 4.4% loss=0.24117, acc=0.87500
# [16/100] training 4.5% loss=0.22851, acc=0.87500
# [16/100] training 4.7% loss=0.33251, acc=0.85938
# [16/100] training 4.8% loss=0.32116, acc=0.87500
# [16/100] training 5.0% loss=0.17894, acc=0.93750
# [16/100] training 5.2% loss=0.26744, acc=0.93750
# [16/100] training 5.4% loss=0.28887, acc=0.87500
# [16/100] training 5.5% loss=0.27040, acc=0.87500
# [16/100] training 5.7% loss=0.17168, acc=0.95312
# [16/100] training 5.9% loss=0.35486, acc=0.84375
# [16/100] training 6.0% loss=0.29677, acc=0.84375
# [16/100] training 6.3% loss=0.25895, acc=0.84375
# [16/100] training 6.4% loss=0.28381, acc=0.84375
# [16/100] training 6.6% loss=0.32060, acc=0.89062
# [16/100] training 6.7% loss=0.34495, acc=0.85938
# [16/100] training 6.9% loss=0.28869, acc=0.89062
# [16/100] training 7.1% loss=0.33897, acc=0.84375
# [16/100] training 7.2% loss=0.32631, acc=0.87500
# [16/100] training 7.5% loss=0.25182, acc=0.90625
# [16/100] training 7.6% loss=0.30268, acc=0.82812
# [16/100] training 7.8% loss=0.27250, acc=0.85938
# [16/100] training 7.9% loss=0.30204, acc=0.87500
# [16/100] training 8.1% loss=0.20695, acc=0.92188
# [16/100] training 8.2% loss=0.25920, acc=0.87500
# [16/100] training 8.4% loss=0.30749, acc=0.87500
# [16/100] training 8.7% loss=0.28362, acc=0.92188
# [16/100] training 8.8% loss=0.32997, acc=0.84375
# [16/100] training 9.0% loss=0.29661, acc=0.85938
# [16/100] training 9.1% loss=0.30880, acc=0.84375
# [16/100] training 9.3% loss=0.46206, acc=0.85938
# [16/100] training 9.4% loss=0.28775, acc=0.90625
# [16/100] training 9.7% loss=0.33785, acc=0.87500
# [16/100] training 9.9% loss=0.29686, acc=0.87500
# [16/100] training 10.0% loss=0.34553, acc=0.85938
# [16/100] training 10.2% loss=0.22949, acc=0.90625
# [16/100] training 10.3% loss=0.22796, acc=0.93750
# [16/100] training 10.5% loss=0.38768, acc=0.87500
# [16/100] training 10.6% loss=0.27221, acc=0.87500
# [16/100] training 10.9% loss=0.16130, acc=0.95312
# [16/100] training 11.0% loss=0.30311, acc=0.87500
# [16/100] training 11.2% loss=0.21714, acc=0.92188
# [16/100] training 11.4% loss=0.30660, acc=0.87500
# [16/100] training 11.5% loss=0.47323, acc=0.82812
# [16/100] training 11.7% loss=0.17344, acc=0.95312
# [16/100] training 11.8% loss=0.24853, acc=0.87500
# [16/100] training 12.1% loss=0.26329, acc=0.89062
# [16/100] training 12.2% loss=0.30217, acc=0.82812
# [16/100] training 12.4% loss=0.28243, acc=0.89062
# [16/100] training 12.6% loss=0.24240, acc=0.95312
# [16/100] training 12.7% loss=0.28138, acc=0.85938
# [16/100] training 12.9% loss=0.27957, acc=0.87500
# [16/100] training 13.0% loss=0.27382, acc=0.90625
# [16/100] training 13.3% loss=0.25199, acc=0.90625
# [16/100] training 13.4% loss=0.28234, acc=0.85938
# [16/100] training 13.6% loss=0.26015, acc=0.89062
# [16/100] training 13.7% loss=0.43584, acc=0.81250
# [16/100] training 13.9% loss=0.29062, acc=0.89062
# [16/100] training 14.1% loss=0.35441, acc=0.84375
# [16/100] training 14.3% loss=0.19564, acc=0.92188
# [16/100] training 14.5% loss=0.30274, acc=0.85938
# [16/100] training 14.6% loss=0.19342, acc=0.95312
# [16/100] training 14.8% loss=0.29372, acc=0.87500
# [16/100] training 14.9% loss=0.29930, acc=0.78125
# [16/100] training 15.1% loss=0.38728, acc=0.84375
# [16/100] training 15.4% loss=0.30297, acc=0.81250
# [16/100] training 15.5% loss=0.34153, acc=0.84375
# [16/100] training 15.7% loss=0.44714, acc=0.82812
# [16/100] training 15.8% loss=0.19895, acc=0.90625
# [16/100] training 16.0% loss=0.43045, acc=0.81250
# [16/100] training 16.1% loss=0.47468, acc=0.82812
# [16/100] training 16.3% loss=0.26792, acc=0.90625
# [16/100] training 16.4% loss=0.25970, acc=0.89062
# [16/100] training 16.7% loss=0.40981, acc=0.79688
# [16/100] training 16.9% loss=0.39992, acc=0.82812
# [16/100] training 17.0% loss=0.23796, acc=0.87500
# [16/100] training 17.2% loss=0.22408, acc=0.90625
# [16/100] training 17.3% loss=0.22162, acc=0.93750
# [16/100] training 17.5% loss=0.29672, acc=0.89062
# [16/100] training 17.7% loss=0.26371, acc=0.87500
# [16/100] training 17.9% loss=0.40068, acc=0.85938
# [16/100] training 18.1% loss=0.33118, acc=0.90625
# [16/100] training 18.2% loss=0.37395, acc=0.87500
# [16/100] training 18.4% loss=0.38583, acc=0.84375
# [16/100] training 18.5% loss=0.22316, acc=0.89062
# [16/100] training 18.8% loss=0.26779, acc=0.89062
# [16/100] training 18.9% loss=0.20040, acc=0.93750
# [16/100] training 19.1% loss=0.35310, acc=0.85938
# [16/100] training 19.2% loss=0.23144, acc=0.90625
# [16/100] training 19.4% loss=0.19439, acc=0.90625
# [16/100] training 19.6% loss=0.33330, acc=0.89062
# [16/100] training 19.7% loss=0.29587, acc=0.85938
# [16/100] training 20.0% loss=0.17559, acc=0.92188
# [16/100] training 20.1% loss=0.31728, acc=0.85938
# [16/100] training 20.3% loss=0.30666, acc=0.87500
# [16/100] training 20.4% loss=0.40132, acc=0.79688
# [16/100] training 20.6% loss=0.34604, acc=0.87500
# [16/100] training 20.8% loss=0.25133, acc=0.92188
# [16/100] training 20.9% loss=0.38197, acc=0.87500
# [16/100] training 21.2% loss=0.39103, acc=0.84375
# [16/100] training 21.3% loss=0.42507, acc=0.81250
# [16/100] training 21.5% loss=0.32967, acc=0.89062
# [16/100] training 21.6% loss=0.15220, acc=0.96875
# [16/100] training 21.8% loss=0.22188, acc=0.93750
# [16/100] training 21.9% loss=0.33907, acc=0.79688
# [16/100] training 22.2% loss=0.26492, acc=0.90625
# [16/100] training 22.4% loss=0.33258, acc=0.85938
# [16/100] training 22.5% loss=0.23092, acc=0.92188
# [16/100] training 22.7% loss=0.25106, acc=0.90625
# [16/100] training 22.8% loss=0.42327, acc=0.85938
# [16/100] training 23.0% loss=0.24097, acc=0.89062
# [16/100] training 23.1% loss=0.29680, acc=0.87500
# [16/100] training 23.4% loss=0.30915, acc=0.87500
# [16/100] training 23.6% loss=0.39152, acc=0.89062
# [16/100] training 23.7% loss=0.28061, acc=0.89062
# [16/100] training 23.9% loss=0.25063, acc=0.89062
# [16/100] training 24.0% loss=0.35548, acc=0.84375
# [16/100] training 24.2% loss=0.38225, acc=0.87500
# [16/100] training 24.3% loss=0.31089, acc=0.87500
# [16/100] training 24.6% loss=0.25659, acc=0.85938
# [16/100] training 24.7% loss=0.35796, acc=0.84375
# [16/100] training 24.9% loss=0.21339, acc=0.90625
# [16/100] training 25.1% loss=0.30761, acc=0.84375
# [16/100] training 25.2% loss=0.18142, acc=0.93750
# [16/100] training 25.4% loss=0.22810, acc=0.89062
# [16/100] training 25.6% loss=0.28624, acc=0.85938
# [16/100] training 25.8% loss=0.39997, acc=0.87500
# [16/100] training 25.9% loss=0.17270, acc=0.92188
# [16/100] training 26.1% loss=0.29038, acc=0.89062
# [16/100] training 26.3% loss=0.31369, acc=0.90625
# [16/100] training 26.4% loss=0.23266, acc=0.89062
# [16/100] training 26.6% loss=0.18291, acc=0.90625
# [16/100] training 26.8% loss=0.30948, acc=0.90625
# [16/100] training 27.0% loss=0.25577, acc=0.89062
# [16/100] training 27.1% loss=0.24058, acc=0.85938
# [16/100] training 27.3% loss=0.25253, acc=0.90625
# [16/100] training 27.4% loss=0.19532, acc=0.93750
# [16/100] training 27.6% loss=0.31196, acc=0.90625
# [16/100] training 27.9% loss=0.20323, acc=0.92188
# [16/100] training 28.0% loss=0.41396, acc=0.84375
# [16/100] training 28.2% loss=0.29704, acc=0.90625
# [16/100] training 28.3% loss=0.10738, acc=0.95312
# [16/100] training 28.5% loss=0.29575, acc=0.90625
# [16/100] training 28.6% loss=0.23145, acc=0.92188
# [16/100] training 28.8% loss=0.15599, acc=0.93750
# [16/100] training 29.1% loss=0.32949, acc=0.85938
# [16/100] training 29.2% loss=0.27640, acc=0.87500
# [16/100] training 29.4% loss=0.40797, acc=0.76562
# [16/100] training 29.5% loss=0.19338, acc=0.93750
# [16/100] training 29.7% loss=0.29972, acc=0.87500
# [16/100] training 29.8% loss=0.17884, acc=0.90625
# [16/100] training 30.0% loss=0.39204, acc=0.82812
# [16/100] training 30.2% loss=0.20704, acc=0.92188
# [16/100] training 30.4% loss=0.21743, acc=0.90625
# [16/100] training 30.6% loss=0.38464, acc=0.87500
# [16/100] training 30.7% loss=0.23699, acc=0.85938
# [16/100] training 30.9% loss=0.34811, acc=0.84375
# [16/100] training 31.0% loss=0.22142, acc=0.90625
# [16/100] training 31.3% loss=0.24258, acc=0.92188
# [16/100] training 31.4% loss=0.37776, acc=0.81250
# [16/100] training 31.6% loss=0.30715, acc=0.84375
# [16/100] training 31.8% loss=0.20402, acc=0.90625
# [16/100] training 31.9% loss=0.30572, acc=0.89062
# [16/100] training 32.1% loss=0.36898, acc=0.82812
# [16/100] training 32.2% loss=0.27241, acc=0.90625
# [16/100] training 32.5% loss=0.22890, acc=0.89062
# [16/100] training 32.6% loss=0.33509, acc=0.82812
# [16/100] training 32.8% loss=0.20531, acc=0.90625
# [16/100] training 32.9% loss=0.32834, acc=0.89062
# [16/100] training 33.1% loss=0.35097, acc=0.84375
# [16/100] training 33.3% loss=0.25388, acc=0.89062
# [16/100] training 33.4% loss=0.28084, acc=0.84375
# [16/100] training 33.7% loss=0.35000, acc=0.82812
# [16/100] training 33.8% loss=0.28194, acc=0.89062
# [16/100] training 34.0% loss=0.27250, acc=0.85938
# [16/100] training 34.1% loss=0.26907, acc=0.89062
# [16/100] training 34.3% loss=0.23048, acc=0.89062
# [16/100] training 34.5% loss=0.35949, acc=0.82812
# [16/100] training 34.7% loss=0.21089, acc=0.92188
# [16/100] training 34.9% loss=0.23051, acc=0.90625
# [16/100] training 35.0% loss=0.24287, acc=0.89062
# [16/100] training 35.2% loss=0.36092, acc=0.85938
# [16/100] training 35.3% loss=0.26824, acc=0.87500
# [16/100] training 35.5% loss=0.29656, acc=0.89062
# [16/100] training 35.6% loss=0.36323, acc=0.89062
# [16/100] training 35.9% loss=0.22428, acc=0.89062
# [16/100] training 36.1% loss=0.32200, acc=0.89062
# [16/100] training 36.2% loss=0.29293, acc=0.87500
# [16/100] training 36.4% loss=0.47413, acc=0.84375
# [16/100] training 36.5% loss=0.32515, acc=0.87500
# [16/100] training 36.7% loss=0.22929, acc=0.90625
# [16/100] training 36.8% loss=0.24415, acc=0.87500
# [16/100] training 37.1% loss=0.39041, acc=0.84375
# [16/100] training 37.3% loss=0.34576, acc=0.84375
# [16/100] training 37.4% loss=0.29792, acc=0.87500
# [16/100] training 37.6% loss=0.23456, acc=0.92188
# [16/100] training 37.7% loss=0.32266, acc=0.85938
# [16/100] training 37.9% loss=0.24710, acc=0.90625
# [16/100] training 38.1% loss=0.35010, acc=0.84375
# [16/100] training 38.3% loss=0.26264, acc=0.87500
# [16/100] training 38.4% loss=0.20534, acc=0.93750
# [16/100] training 38.6% loss=0.21566, acc=0.92188
# [16/100] training 38.8% loss=0.35848, acc=0.84375
# [16/100] training 38.9% loss=0.30354, acc=0.90625
# [16/100] training 39.1% loss=0.31424, acc=0.85938
# [16/100] training 39.3% loss=0.23364, acc=0.90625
# [16/100] training 39.5% loss=0.36981, acc=0.81250
# [16/100] training 39.6% loss=0.27162, acc=0.92188
# [16/100] training 39.8% loss=0.27207, acc=0.89062
# [16/100] training 40.0% loss=0.31486, acc=0.81250
# [16/100] training 40.1% loss=0.37511, acc=0.85938
# [16/100] training 40.4% loss=0.27140, acc=0.85938
# [16/100] training 40.5% loss=0.23950, acc=0.92188
# [16/100] training 40.7% loss=0.28465, acc=0.87500
# [16/100] training 40.8% loss=0.21799, acc=0.92188
# [16/100] training 41.0% loss=0.17585, acc=0.93750
# [16/100] training 41.1% loss=0.40663, acc=0.82812
# [16/100] training 41.3% loss=0.48574, acc=0.79688
# [16/100] training 41.6% loss=0.32974, acc=0.89062
# [16/100] training 41.7% loss=0.34696, acc=0.89062
# [16/100] training 41.9% loss=0.21551, acc=0.90625
# [16/100] training 42.0% loss=0.31753, acc=0.82812
# [16/100] training 42.2% loss=0.27122, acc=0.89062
# [16/100] training 42.3% loss=0.25993, acc=0.89062
# [16/100] training 42.5% loss=0.23061, acc=0.92188
# [16/100] training 42.8% loss=0.26089, acc=0.90625
# [16/100] training 42.9% loss=0.21021, acc=0.89062
# [16/100] training 43.1% loss=0.24975, acc=0.90625
# [16/100] training 43.2% loss=0.20252, acc=0.92188
# [16/100] training 43.4% loss=0.24068, acc=0.90625
# [16/100] training 43.5% loss=0.20577, acc=0.92188
# [16/100] training 43.8% loss=0.39846, acc=0.82812
# [16/100] training 43.9% loss=0.24470, acc=0.90625
# [16/100] training 44.1% loss=0.25648, acc=0.89062
# [16/100] training 44.3% loss=0.25118, acc=0.95312
# [16/100] training 44.4% loss=0.31961, acc=0.79688
# [16/100] training 44.6% loss=0.26666, acc=0.87500
# [16/100] training 44.7% loss=0.37632, acc=0.85938
# [16/100] training 45.0% loss=0.18790, acc=0.92188
# [16/100] training 45.1% loss=0.35721, acc=0.87500
# [16/100] training 45.3% loss=0.20590, acc=0.89062
# [16/100] training 45.5% loss=0.20466, acc=0.92188
# [16/100] training 45.6% loss=0.31931, acc=0.87500
# [16/100] training 45.8% loss=0.19033, acc=0.90625
# [16/100] training 45.9% loss=0.19623, acc=0.90625
# [16/100] training 46.2% loss=0.28328, acc=0.90625
# [16/100] training 46.3% loss=0.15357, acc=0.90625
# [16/100] training 46.5% loss=0.50401, acc=0.78125
# [16/100] training 46.6% loss=0.23269, acc=0.90625
# [16/100] training 46.8% loss=0.23917, acc=0.90625
# [16/100] training 47.0% loss=0.33506, acc=0.89062
# [16/100] training 47.2% loss=0.29060, acc=0.87500
# [16/100] training 47.4% loss=0.25145, acc=0.90625
# [16/100] training 47.5% loss=0.32695, acc=0.87500
# [16/100] training 47.7% loss=0.26039, acc=0.89062
# [16/100] training 47.8% loss=0.34705, acc=0.87500
# [16/100] training 48.0% loss=0.36250, acc=0.85938
# [16/100] training 48.3% loss=0.15426, acc=0.95312
# [16/100] training 48.4% loss=0.19353, acc=0.95312
# [16/100] training 48.6% loss=0.15941, acc=0.92188
# [16/100] training 48.7% loss=0.35822, acc=0.85938
# [16/100] training 48.9% loss=0.27761, acc=0.90625
# [16/100] training 49.0% loss=0.23115, acc=0.87500
# [16/100] training 49.2% loss=0.34838, acc=0.84375
# [16/100] training 49.3% loss=0.22742, acc=0.92188
# [16/100] training 49.6% loss=0.36633, acc=0.84375
# [16/100] training 49.8% loss=0.30857, acc=0.89062
# [16/100] training 49.9% loss=0.26145, acc=0.90625
# [16/100] training 50.1% loss=0.19573, acc=0.90625
# [16/100] training 50.2% loss=0.33547, acc=0.85938
# [16/100] training 50.4% loss=0.43927, acc=0.89062
# [16/100] training 50.6% loss=0.38395, acc=0.82812
# [16/100] training 50.8% loss=0.38325, acc=0.84375
# [16/100] training 51.0% loss=0.37853, acc=0.82812
# [16/100] training 51.1% loss=0.34425, acc=0.82812
# [16/100] training 51.3% loss=0.31021, acc=0.84375
# [16/100] training 51.4% loss=0.23681, acc=0.90625
# [16/100] training 51.7% loss=0.36584, acc=0.82812
# [16/100] training 51.8% loss=0.31270, acc=0.87500
# [16/100] training 52.0% loss=0.30719, acc=0.87500
# [16/100] training 52.1% loss=0.28649, acc=0.90625
# [16/100] training 52.3% loss=0.30644, acc=0.85938
# [16/100] training 52.5% loss=0.22037, acc=0.87500
# [16/100] training 52.6% loss=0.22764, acc=0.89062
# [16/100] training 52.9% loss=0.55555, acc=0.78125
# [16/100] training 53.0% loss=0.25898, acc=0.87500
# [16/100] training 53.2% loss=0.36862, acc=0.87500
# [16/100] training 53.3% loss=0.18282, acc=0.93750
# [16/100] training 53.5% loss=0.21461, acc=0.92188
# [16/100] training 53.7% loss=0.15997, acc=0.95312
# [16/100] training 53.8% loss=0.30097, acc=0.82812
# [16/100] training 54.1% loss=0.41058, acc=0.79688
# [16/100] training 54.2% loss=0.26562, acc=0.87500
# [16/100] training 54.4% loss=0.34666, acc=0.84375
# [16/100] training 54.5% loss=0.22755, acc=0.95312
# [16/100] training 54.7% loss=0.41172, acc=0.81250
# [16/100] training 54.8% loss=0.19222, acc=0.90625
# [16/100] training 55.1% loss=0.24580, acc=0.89062
# [16/100] training 55.3% loss=0.23670, acc=0.92188
# [16/100] training 55.4% loss=0.25715, acc=0.87500
# [16/100] training 55.6% loss=0.37290, acc=0.84375
# [16/100] training 55.7% loss=0.31607, acc=0.84375
# [16/100] training 55.9% loss=0.25610, acc=0.87500
# [16/100] training 56.0% loss=0.32049, acc=0.79688
# [16/100] training 56.3% loss=0.49251, acc=0.81250
# [16/100] training 56.5% loss=0.27463, acc=0.89062
# [16/100] training 56.6% loss=0.32213, acc=0.84375
# [16/100] training 56.8% loss=0.41452, acc=0.84375
# [16/100] training 56.9% loss=0.33619, acc=0.82812
# [16/100] training 57.1% loss=0.34273, acc=0.87500
# [16/100] training 57.2% loss=0.15425, acc=0.93750
# [16/100] training 57.5% loss=0.24024, acc=0.90625
# [16/100] training 57.6% loss=0.39830, acc=0.79688
# [16/100] training 57.8% loss=0.26533, acc=0.85938
# [16/100] training 58.0% loss=0.17736, acc=0.93750
# [16/100] training 58.1% loss=0.26999, acc=0.87500
# [16/100] training 58.3% loss=0.16513, acc=0.93750
# [16/100] training 58.4% loss=0.27555, acc=0.90625
# [16/100] training 58.7% loss=0.26867, acc=0.89062
# [16/100] training 58.8% loss=0.38991, acc=0.84375
# [16/100] training 59.0% loss=0.27405, acc=0.90625
# [16/100] training 59.2% loss=0.32020, acc=0.87500
# [16/100] training 59.3% loss=0.24240, acc=0.90625
# [16/100] training 59.5% loss=0.29634, acc=0.87500
# [16/100] training 59.7% loss=0.28495, acc=0.89062
# [16/100] training 59.9% loss=0.23566, acc=0.93750
# [16/100] training 60.0% loss=0.21639, acc=0.90625
# [16/100] training 60.2% loss=0.26692, acc=0.85938
# [16/100] training 60.3% loss=0.20746, acc=0.92188
# [16/100] training 60.5% loss=0.19051, acc=0.89062
# [16/100] training 60.8% loss=0.36418, acc=0.82812
# [16/100] training 60.9% loss=0.34359, acc=0.85938
# [16/100] training 61.1% loss=0.29612, acc=0.84375
# [16/100] training 61.2% loss=0.25409, acc=0.90625
# [16/100] training 61.4% loss=0.30219, acc=0.87500
# [16/100] training 61.5% loss=0.40892, acc=0.78125
# [16/100] training 61.7% loss=0.39941, acc=0.84375
# [16/100] training 62.0% loss=0.33030, acc=0.84375
# [16/100] training 62.1% loss=0.36688, acc=0.79688
# [16/100] training 62.3% loss=0.16440, acc=0.93750
# [16/100] training 62.4% loss=0.20555, acc=0.89062
# [16/100] training 62.6% loss=0.39466, acc=0.78125
# [16/100] training 62.7% loss=0.26065, acc=0.85938
# [16/100] training 62.9% loss=0.35225, acc=0.82812
# [16/100] training 63.1% loss=0.19544, acc=0.95312
# [16/100] training 63.3% loss=0.25499, acc=0.89062
# [16/100] training 63.5% loss=0.34598, acc=0.87500
# [16/100] training 63.6% loss=0.27348, acc=0.87500
# [16/100] training 63.8% loss=0.33960, acc=0.87500
# [16/100] training 63.9% loss=0.25365, acc=0.89062
# [16/100] training 64.2% loss=0.26083, acc=0.85938
# [16/100] training 64.3% loss=0.24345, acc=0.89062
# [16/100] training 64.5% loss=0.34519, acc=0.85938
# [16/100] training 64.7% loss=0.26918, acc=0.90625
# [16/100] training 64.8% loss=0.48693, acc=0.81250
# [16/100] training 65.0% loss=0.25661, acc=0.89062
# [16/100] training 65.1% loss=0.35607, acc=0.89062
# [16/100] training 65.4% loss=0.22194, acc=0.93750
# [16/100] training 65.5% loss=0.21802, acc=0.93750
# [16/100] training 65.7% loss=0.20907, acc=0.90625
# [16/100] training 65.8% loss=0.35233, acc=0.85938
# [16/100] training 66.0% loss=0.31327, acc=0.84375
# [16/100] training 66.2% loss=0.15837, acc=0.90625
# [16/100] training 66.3% loss=0.31692, acc=0.87500
# [16/100] training 66.6% loss=0.28571, acc=0.85938
# [16/100] training 66.7% loss=0.18298, acc=0.92188
# [16/100] training 66.9% loss=0.32320, acc=0.82812
# [16/100] training 67.0% loss=0.40294, acc=0.84375
# [16/100] training 67.2% loss=0.21454, acc=0.90625
# [16/100] training 67.4% loss=0.22059, acc=0.90625
# [16/100] training 67.6% loss=0.22565, acc=0.90625
# [16/100] training 67.8% loss=0.25464, acc=0.87500
# [16/100] training 67.9% loss=0.17554, acc=0.93750
# [16/100] training 68.1% loss=0.19753, acc=0.93750
# [16/100] training 68.2% loss=0.21358, acc=0.90625
# [16/100] training 68.4% loss=0.17625, acc=0.95312
# [16/100] training 68.5% loss=0.32638, acc=0.81250
# [16/100] training 68.8% loss=0.28608, acc=0.87500
# [16/100] training 69.0% loss=0.37547, acc=0.89062
# [16/100] training 69.1% loss=0.17871, acc=0.92188
# [16/100] training 69.3% loss=0.26203, acc=0.89062
# [16/100] training 69.4% loss=0.21021, acc=0.87500
# [16/100] training 69.6% loss=0.35687, acc=0.82812
# [16/100] training 69.7% loss=0.31179, acc=0.85938
# [16/100] training 70.0% loss=0.34883, acc=0.85938
# [16/100] training 70.2% loss=0.34163, acc=0.84375
# [16/100] training 70.3% loss=0.28288, acc=0.89062
# [16/100] training 70.5% loss=0.30010, acc=0.90625
# [16/100] training 70.6% loss=0.20904, acc=0.92188
# [16/100] training 70.8% loss=0.28919, acc=0.85938
# [16/100] training 71.0% loss=0.30669, acc=0.90625
# [16/100] training 71.2% loss=0.27587, acc=0.85938
# [16/100] training 71.3% loss=0.15655, acc=0.92188
# [16/100] training 71.5% loss=0.43462, acc=0.82812
# [16/100] training 71.7% loss=0.32978, acc=0.84375
# [16/100] training 71.8% loss=0.39401, acc=0.82812
# [16/100] training 72.0% loss=0.20246, acc=0.87500
# [16/100] training 72.2% loss=0.26978, acc=0.87500
# [16/100] training 72.4% loss=0.36564, acc=0.90625
# [16/100] training 72.5% loss=0.44632, acc=0.84375
# [16/100] training 72.7% loss=0.36362, acc=0.85938
# [16/100] training 72.9% loss=0.27599, acc=0.89062
# [16/100] training 73.0% loss=0.21430, acc=0.93750
# [16/100] training 73.3% loss=0.31583, acc=0.89062
# [16/100] training 73.4% loss=0.25241, acc=0.84375
# [16/100] training 73.6% loss=0.22258, acc=0.87500
# [16/100] training 73.7% loss=0.22410, acc=0.92188
# [16/100] training 73.9% loss=0.26107, acc=0.89062
# [16/100] training 74.0% loss=0.40510, acc=0.82812
# [16/100] training 74.2% loss=0.28234, acc=0.85938
# [16/100] training 74.5% loss=0.25192, acc=0.90625
# [16/100] training 74.6% loss=0.57561, acc=0.78125
# [16/100] training 74.8% loss=0.49967, acc=0.81250
# [16/100] training 74.9% loss=0.51694, acc=0.73438
# [16/100] training 75.1% loss=0.26986, acc=0.87500
# [16/100] training 75.2% loss=0.23848, acc=0.92188
# [16/100] training 75.4% loss=0.29857, acc=0.90625
# [16/100] training 75.7% loss=0.29550, acc=0.87500
# [16/100] training 75.8% loss=0.40379, acc=0.84375
# [16/100] training 76.0% loss=0.18889, acc=0.90625
# [16/100] training 76.1% loss=0.51268, acc=0.79688
# [16/100] training 76.3% loss=0.26131, acc=0.89062
# [16/100] training 76.4% loss=0.26312, acc=0.84375
# [16/100] training 76.7% loss=0.20949, acc=0.89062
# [16/100] training 76.8% loss=0.24595, acc=0.89062
# [16/100] training 77.0% loss=0.29377, acc=0.89062
# [16/100] training 77.2% loss=0.32411, acc=0.85938
# [16/100] training 77.3% loss=0.19507, acc=0.92188
# [16/100] training 77.5% loss=0.23053, acc=0.89062
# [16/100] training 77.6% loss=0.29214, acc=0.87500
# [16/100] training 77.9% loss=0.25488, acc=0.85938
# [16/100] training 78.0% loss=0.25889, acc=0.87500
# [16/100] training 78.2% loss=0.28079, acc=0.90625
# [16/100] training 78.4% loss=0.19012, acc=0.89062
# [16/100] training 78.5% loss=0.41097, acc=0.90625
# [16/100] training 78.7% loss=0.24429, acc=0.90625
# [16/100] training 78.8% loss=0.18310, acc=0.95312
# [16/100] training 79.1% loss=0.17180, acc=0.93750
# [16/100] training 79.2% loss=0.18568, acc=0.95312
# [16/100] training 79.4% loss=0.45695, acc=0.76562
# [16/100] training 79.5% loss=0.29574, acc=0.85938
# [16/100] training 79.7% loss=0.14995, acc=0.95312
# [16/100] training 79.9% loss=0.28469, acc=0.90625
# [16/100] training 80.1% loss=0.27856, acc=0.89062
# [16/100] training 80.3% loss=0.33629, acc=0.89062
# [16/100] training 80.4% loss=0.26639, acc=0.85938
# [16/100] training 80.6% loss=0.28096, acc=0.89062
# [16/100] training 80.7% loss=0.17857, acc=0.98438
# [16/100] training 80.9% loss=0.28696, acc=0.85938
# [16/100] training 81.2% loss=0.29732, acc=0.87500
# [16/100] training 81.3% loss=0.28951, acc=0.84375
# [16/100] training 81.5% loss=0.31856, acc=0.84375
# [16/100] training 81.6% loss=0.31377, acc=0.85938
# [16/100] training 81.8% loss=0.27259, acc=0.87500
# [16/100] training 81.9% loss=0.47658, acc=0.84375
# [16/100] training 82.1% loss=0.19019, acc=0.93750
# [16/100] training 82.2% loss=0.26729, acc=0.89062
# [16/100] training 82.5% loss=0.19110, acc=0.95312
# [16/100] training 82.7% loss=0.24604, acc=0.90625
# [16/100] training 82.8% loss=0.32159, acc=0.90625
# [16/100] training 83.0% loss=0.24694, acc=0.89062
# [16/100] training 83.1% loss=0.29733, acc=0.89062
# [16/100] training 83.3% loss=0.16479, acc=0.95312
# [16/100] training 83.5% loss=0.22637, acc=0.89062
# [16/100] training 83.7% loss=0.37645, acc=0.89062
# [16/100] training 83.9% loss=0.28831, acc=0.90625
# [16/100] training 84.0% loss=0.20179, acc=0.96875
# [16/100] training 84.2% loss=0.19688, acc=0.89062
# [16/100] training 84.3% loss=0.22718, acc=0.89062
# [16/100] training 84.5% loss=0.25170, acc=0.90625
# [16/100] training 84.7% loss=0.27006, acc=0.85938
# [16/100] training 84.9% loss=0.27773, acc=0.87500
# [16/100] training 85.0% loss=0.27648, acc=0.89062
# [16/100] training 85.2% loss=0.41705, acc=0.81250
# [16/100] training 85.4% loss=0.41387, acc=0.89062
# [16/100] training 85.5% loss=0.25360, acc=0.87500
# [16/100] training 85.8% loss=0.26621, acc=0.89062
# [16/100] training 85.9% loss=0.30697, acc=0.90625
# [16/100] training 86.1% loss=0.32851, acc=0.84375
# [16/100] training 86.2% loss=0.22539, acc=0.90625
# [16/100] training 86.4% loss=0.30553, acc=0.87500
# [16/100] training 86.6% loss=0.33212, acc=0.87500
# [16/100] training 86.7% loss=0.24660, acc=0.90625
# [16/100] training 87.0% loss=0.21949, acc=0.92188
# [16/100] training 87.1% loss=0.26142, acc=0.92188
# [16/100] training 87.3% loss=0.29406, acc=0.84375
# [16/100] training 87.4% loss=0.26174, acc=0.89062
# [16/100] training 87.6% loss=0.25112, acc=0.90625
# [16/100] training 87.7% loss=0.31184, acc=0.93750
# [16/100] training 87.9% loss=0.41252, acc=0.82812
# [16/100] training 88.2% loss=0.22066, acc=0.85938
# [16/100] training 88.3% loss=0.36538, acc=0.82812
# [16/100] training 88.5% loss=0.25351, acc=0.90625
# [16/100] training 88.6% loss=0.15073, acc=0.95312
# [16/100] training 88.8% loss=0.30344, acc=0.89062
# [16/100] training 88.9% loss=0.26535, acc=0.93750
# [16/100] training 89.2% loss=0.23389, acc=0.90625
# [16/100] training 89.4% loss=0.21637, acc=0.92188
# [16/100] training 89.5% loss=0.38863, acc=0.82812
# [16/100] training 89.7% loss=0.28591, acc=0.85938
# [16/100] training 89.8% loss=0.26254, acc=0.89062
# [16/100] training 90.0% loss=0.20425, acc=0.92188
# [16/100] training 90.1% loss=0.27687, acc=0.90625
# [16/100] training 90.4% loss=0.23442, acc=0.95312
# [16/100] training 90.5% loss=0.18218, acc=0.92188
# [16/100] training 90.7% loss=0.27076, acc=0.89062
# [16/100] training 90.9% loss=0.13617, acc=0.93750
# [16/100] training 91.0% loss=0.24215, acc=0.89062
# [16/100] training 91.2% loss=0.50757, acc=0.81250
# [16/100] training 91.3% loss=0.38278, acc=0.84375
# [16/100] training 91.6% loss=0.51873, acc=0.79688
# [16/100] training 91.7% loss=0.18667, acc=0.93750
# [16/100] training 91.9% loss=0.32638, acc=0.85938
# [16/100] training 92.1% loss=0.27860, acc=0.84375
# [16/100] training 92.2% loss=0.20463, acc=0.87500
# [16/100] training 92.4% loss=0.22168, acc=0.92188
# [16/100] training 92.6% loss=0.28708, acc=0.89062
# [16/100] training 92.8% loss=0.38955, acc=0.87500
# [16/100] training 92.9% loss=0.27129, acc=0.90625
# [16/100] training 93.1% loss=0.38501, acc=0.82812
# [16/100] training 93.2% loss=0.20560, acc=0.92188
# [16/100] training 93.4% loss=0.23162, acc=0.93750
# [16/100] training 93.7% loss=0.18268, acc=0.92188
# [16/100] training 93.8% loss=0.28010, acc=0.85938
# [16/100] training 94.0% loss=0.24652, acc=0.89062
# [16/100] training 94.1% loss=0.28604, acc=0.85938
# [16/100] training 94.3% loss=0.28761, acc=0.84375
# [16/100] training 94.4% loss=0.26315, acc=0.90625
# [16/100] training 94.6% loss=0.17551, acc=0.95312
# [16/100] training 94.9% loss=0.25125, acc=0.89062
# [16/100] training 95.0% loss=0.35562, acc=0.84375
# [16/100] training 95.2% loss=0.34037, acc=0.85938
# [16/100] training 95.3% loss=0.30382, acc=0.90625
# [16/100] training 95.5% loss=0.18817, acc=0.95312
# [16/100] training 95.6% loss=0.31278, acc=0.84375
# [16/100] training 95.8% loss=0.27433, acc=0.87500
# [16/100] training 96.0% loss=0.28556, acc=0.89062
# [16/100] training 96.2% loss=0.19047, acc=0.92188
# [16/100] training 96.4% loss=0.27533, acc=0.89062
# [16/100] training 96.5% loss=0.30328, acc=0.85938
# [16/100] training 96.7% loss=0.18751, acc=0.93750
# [16/100] training 96.8% loss=0.24692, acc=0.90625
# [16/100] training 97.1% loss=0.15356, acc=0.95312
# [16/100] training 97.2% loss=0.30428, acc=0.85938
# [16/100] training 97.4% loss=0.23410, acc=0.92188
# [16/100] training 97.6% loss=0.39026, acc=0.82812
# [16/100] training 97.7% loss=0.21912, acc=0.93750
# [16/100] training 97.9% loss=0.25851, acc=0.85938
# [16/100] training 98.0% loss=0.24939, acc=0.89062
# [16/100] training 98.3% loss=0.27997, acc=0.85938
# [16/100] training 98.4% loss=0.24196, acc=0.92188
# [16/100] training 98.6% loss=0.40071, acc=0.79688
# [16/100] training 98.7% loss=0.33598, acc=0.82812
# [16/100] training 98.9% loss=0.16185, acc=0.95312
# [16/100] training 99.1% loss=0.26783, acc=0.92188
# [16/100] training 99.2% loss=0.19691, acc=0.90625
# [16/100] training 99.5% loss=0.28247, acc=0.87500
# [16/100] training 99.6% loss=0.25133, acc=0.87500
# [16/100] training 99.8% loss=0.24573, acc=0.87500
# [16/100] training 99.9% loss=0.10934, acc=0.98438
# [16/100] testing 0.9% loss=0.20968, acc=0.92188
# [16/100] testing 1.8% loss=0.51661, acc=0.78125
# [16/100] testing 2.2% loss=0.19751, acc=0.93750
# [16/100] testing 3.1% loss=0.27169, acc=0.87500
# [16/100] testing 3.5% loss=0.22585, acc=0.90625
# [16/100] testing 4.4% loss=0.25942, acc=0.89062
# [16/100] testing 4.8% loss=0.35364, acc=0.84375
# [16/100] testing 5.7% loss=0.31182, acc=0.82812
# [16/100] testing 6.6% loss=0.20224, acc=0.93750
# [16/100] testing 7.0% loss=0.20709, acc=0.93750
# [16/100] testing 7.9% loss=0.43087, acc=0.84375
# [16/100] testing 8.3% loss=0.23930, acc=0.90625
# [16/100] testing 9.2% loss=0.32376, acc=0.85938
# [16/100] testing 9.7% loss=0.14716, acc=0.95312
# [16/100] testing 10.5% loss=0.31723, acc=0.85938
# [16/100] testing 11.0% loss=0.29241, acc=0.92188
# [16/100] testing 11.8% loss=0.37009, acc=0.89062
# [16/100] testing 12.7% loss=0.55484, acc=0.79688
# [16/100] testing 13.2% loss=0.29400, acc=0.89062
# [16/100] testing 14.0% loss=0.43145, acc=0.84375
# [16/100] testing 14.5% loss=0.34536, acc=0.85938
# [16/100] testing 15.4% loss=0.35850, acc=0.87500
# [16/100] testing 15.8% loss=0.20977, acc=0.87500
# [16/100] testing 16.7% loss=0.29910, acc=0.90625
# [16/100] testing 17.5% loss=0.33795, acc=0.84375
# [16/100] testing 18.0% loss=0.25686, acc=0.87500
# [16/100] testing 18.9% loss=0.16597, acc=0.92188
# [16/100] testing 19.3% loss=0.32954, acc=0.89062
# [16/100] testing 20.2% loss=0.37815, acc=0.85938
# [16/100] testing 20.6% loss=0.38096, acc=0.81250
# [16/100] testing 21.5% loss=0.26325, acc=0.90625
# [16/100] testing 21.9% loss=0.46048, acc=0.78125
# [16/100] testing 22.8% loss=0.45278, acc=0.85938
# [16/100] testing 23.7% loss=0.43781, acc=0.84375
# [16/100] testing 24.1% loss=0.28398, acc=0.87500
# [16/100] testing 25.0% loss=0.37756, acc=0.85938
# [16/100] testing 25.4% loss=0.15420, acc=0.92188
# [16/100] testing 26.3% loss=0.37381, acc=0.81250
# [16/100] testing 26.8% loss=0.32064, acc=0.92188
# [16/100] testing 27.6% loss=0.28921, acc=0.87500
# [16/100] testing 28.5% loss=0.43355, acc=0.82812
# [16/100] testing 29.0% loss=0.20790, acc=0.89062
# [16/100] testing 29.8% loss=0.44736, acc=0.87500
# [16/100] testing 30.3% loss=0.32047, acc=0.85938
# [16/100] testing 31.1% loss=0.39200, acc=0.81250
# [16/100] testing 31.6% loss=0.23026, acc=0.89062
# [16/100] testing 32.5% loss=0.27392, acc=0.85938
# [16/100] testing 32.9% loss=0.36463, acc=0.85938
# [16/100] testing 33.8% loss=0.29955, acc=0.84375
# [16/100] testing 34.7% loss=0.40468, acc=0.82812
# [16/100] testing 35.1% loss=0.34933, acc=0.87500
# [16/100] testing 36.0% loss=0.32159, acc=0.87500
# [16/100] testing 36.4% loss=0.34236, acc=0.84375
# [16/100] testing 37.3% loss=0.40980, acc=0.84375
# [16/100] testing 37.7% loss=0.49966, acc=0.79688
# [16/100] testing 38.6% loss=0.22923, acc=0.92188
# [16/100] testing 39.5% loss=0.39371, acc=0.84375
# [16/100] testing 39.9% loss=0.29624, acc=0.90625
# [16/100] testing 40.8% loss=0.34864, acc=0.84375
# [16/100] testing 41.2% loss=0.26561, acc=0.90625
# [16/100] testing 42.1% loss=0.32619, acc=0.84375
# [16/100] testing 42.5% loss=0.28607, acc=0.89062
# [16/100] testing 43.4% loss=0.37664, acc=0.87500
# [16/100] testing 43.9% loss=0.24398, acc=0.92188
# [16/100] testing 44.7% loss=0.39597, acc=0.84375
# [16/100] testing 45.6% loss=0.29248, acc=0.92188
# [16/100] testing 46.1% loss=0.28952, acc=0.85938
# [16/100] testing 46.9% loss=0.26019, acc=0.87500
# [16/100] testing 47.4% loss=0.16255, acc=0.92188
# [16/100] testing 48.3% loss=0.45551, acc=0.82812
# [16/100] testing 48.7% loss=0.54484, acc=0.79688
# [16/100] testing 49.6% loss=0.40489, acc=0.84375
# [16/100] testing 50.4% loss=0.20368, acc=0.92188
# [16/100] testing 50.9% loss=0.33459, acc=0.90625
# [16/100] testing 51.8% loss=0.30491, acc=0.85938
# [16/100] testing 52.2% loss=0.31074, acc=0.89062
# [16/100] testing 53.1% loss=0.22025, acc=0.93750
# [16/100] testing 53.5% loss=0.17047, acc=0.92188
# [16/100] testing 54.4% loss=0.44694, acc=0.76562
# [16/100] testing 54.8% loss=0.38392, acc=0.82812
# [16/100] testing 55.7% loss=0.20399, acc=0.92188
# [16/100] testing 56.6% loss=0.38235, acc=0.85938
# [16/100] testing 57.0% loss=0.49455, acc=0.79688
# [16/100] testing 57.9% loss=0.35772, acc=0.84375
# [16/100] testing 58.3% loss=0.50600, acc=0.79688
# [16/100] testing 59.2% loss=0.25778, acc=0.85938
# [16/100] testing 59.7% loss=0.34184, acc=0.84375
# [16/100] testing 60.5% loss=0.33506, acc=0.85938
# [16/100] testing 61.4% loss=0.17890, acc=0.89062
# [16/100] testing 61.9% loss=0.24677, acc=0.92188
# [16/100] testing 62.7% loss=0.27844, acc=0.90625
# [16/100] testing 63.2% loss=0.48317, acc=0.79688
# [16/100] testing 64.0% loss=0.38199, acc=0.79688
# [16/100] testing 64.5% loss=0.31477, acc=0.85938
# [16/100] testing 65.4% loss=0.33457, acc=0.85938
# [16/100] testing 65.8% loss=0.36724, acc=0.82812
# [16/100] testing 66.7% loss=0.22731, acc=0.90625
# [16/100] testing 67.6% loss=0.28966, acc=0.87500
# [16/100] testing 68.0% loss=0.20246, acc=0.90625
# [16/100] testing 68.9% loss=0.31542, acc=0.90625
# [16/100] testing 69.3% loss=0.32158, acc=0.82812
# [16/100] testing 70.2% loss=0.44273, acc=0.81250
# [16/100] testing 70.6% loss=0.35236, acc=0.85938
# [16/100] testing 71.5% loss=0.40872, acc=0.87500
# [16/100] testing 72.4% loss=0.23966, acc=0.90625
# [16/100] testing 72.8% loss=0.30748, acc=0.84375
# [16/100] testing 73.7% loss=0.28814, acc=0.85938
# [16/100] testing 74.1% loss=0.50189, acc=0.82812
# [16/100] testing 75.0% loss=0.27169, acc=0.89062
# [16/100] testing 75.4% loss=0.33950, acc=0.87500
# [16/100] testing 76.3% loss=0.08970, acc=0.98438
# [16/100] testing 76.8% loss=0.35258, acc=0.89062
# [16/100] testing 77.6% loss=0.24294, acc=0.92188
# [16/100] testing 78.5% loss=0.55997, acc=0.81250
# [16/100] testing 79.0% loss=0.35244, acc=0.85938
# [16/100] testing 79.8% loss=0.28598, acc=0.84375
# [16/100] testing 80.3% loss=0.31262, acc=0.87500
# [16/100] testing 81.2% loss=0.47590, acc=0.85938
# [16/100] testing 81.6% loss=0.26990, acc=0.89062
# [16/100] testing 82.5% loss=0.27985, acc=0.92188
# [16/100] testing 83.3% loss=0.17068, acc=0.93750
# [16/100] testing 83.8% loss=0.19171, acc=0.93750
# [16/100] testing 84.7% loss=0.32213, acc=0.84375
# [16/100] testing 85.1% loss=0.37454, acc=0.82812
# [16/100] testing 86.0% loss=0.43272, acc=0.82812
# [16/100] testing 86.4% loss=0.57456, acc=0.82812
# [16/100] testing 87.3% loss=0.41246, acc=0.81250
# [16/100] testing 87.7% loss=0.31625, acc=0.87500
# [16/100] testing 88.6% loss=0.31095, acc=0.87500
# [16/100] testing 89.5% loss=0.55767, acc=0.71875
# [16/100] testing 89.9% loss=0.31088, acc=0.85938
# [16/100] testing 90.8% loss=0.16967, acc=0.95312
# [16/100] testing 91.2% loss=0.20790, acc=0.92188
# [16/100] testing 92.1% loss=0.39523, acc=0.84375
# [16/100] testing 92.6% loss=0.34759, acc=0.84375
# [16/100] testing 93.4% loss=0.39556, acc=0.81250
# [16/100] testing 94.3% loss=0.23309, acc=0.87500
# [16/100] testing 94.7% loss=0.28246, acc=0.89062
# [16/100] testing 95.6% loss=0.36887, acc=0.81250
# [16/100] testing 96.1% loss=0.34531, acc=0.84375
# [16/100] testing 96.9% loss=0.33624, acc=0.85938
# [16/100] testing 97.4% loss=0.19247, acc=0.93750
# [16/100] testing 98.3% loss=0.19787, acc=0.93750
# [16/100] testing 98.7% loss=0.31275, acc=0.90625
# [16/100] testing 99.6% loss=0.31382, acc=0.89062
# [17/100] training 0.2% loss=0.46575, acc=0.79688
# [17/100] training 0.4% loss=0.36830, acc=0.81250
# [17/100] training 0.5% loss=0.16301, acc=0.95312
# [17/100] training 0.8% loss=0.37251, acc=0.85938
# [17/100] training 0.9% loss=0.35657, acc=0.84375
# [17/100] training 1.1% loss=0.24044, acc=0.92188
# [17/100] training 1.2% loss=0.33443, acc=0.85938
# [17/100] training 1.4% loss=0.33369, acc=0.90625
# [17/100] training 1.6% loss=0.20284, acc=0.90625
# [17/100] training 1.8% loss=0.25945, acc=0.92188
# [17/100] training 2.0% loss=0.26753, acc=0.85938
# [17/100] training 2.1% loss=0.28193, acc=0.85938
# [17/100] training 2.3% loss=0.22578, acc=0.87500
# [17/100] training 2.4% loss=0.28911, acc=0.90625
# [17/100] training 2.6% loss=0.26381, acc=0.87500
# [17/100] training 2.7% loss=0.35375, acc=0.87500
# [17/100] training 3.0% loss=0.26500, acc=0.87500
# [17/100] training 3.2% loss=0.28158, acc=0.89062
# [17/100] training 3.3% loss=0.40563, acc=0.82812
# [17/100] training 3.5% loss=0.29478, acc=0.89062
# [17/100] training 3.6% loss=0.35819, acc=0.85938
# [17/100] training 3.8% loss=0.25857, acc=0.90625
# [17/100] training 3.9% loss=0.26891, acc=0.85938
# [17/100] training 4.2% loss=0.23831, acc=0.89062
# [17/100] training 4.4% loss=0.18771, acc=0.95312
# [17/100] training 4.5% loss=0.21024, acc=0.90625
# [17/100] training 4.7% loss=0.27171, acc=0.89062
# [17/100] training 4.8% loss=0.27674, acc=0.87500
# [17/100] training 5.0% loss=0.22970, acc=0.92188
# [17/100] training 5.2% loss=0.30856, acc=0.85938
# [17/100] training 5.4% loss=0.21233, acc=0.92188
# [17/100] training 5.5% loss=0.26222, acc=0.89062
# [17/100] training 5.7% loss=0.21626, acc=0.90625
# [17/100] training 5.9% loss=0.32163, acc=0.82812
# [17/100] training 6.0% loss=0.34363, acc=0.87500
# [17/100] training 6.3% loss=0.27161, acc=0.87500
# [17/100] training 6.4% loss=0.25591, acc=0.87500
# [17/100] training 6.6% loss=0.32015, acc=0.85938
# [17/100] training 6.7% loss=0.37183, acc=0.84375
# [17/100] training 6.9% loss=0.21600, acc=0.89062
# [17/100] training 7.1% loss=0.25583, acc=0.87500
# [17/100] training 7.2% loss=0.37198, acc=0.81250
# [17/100] training 7.5% loss=0.22462, acc=0.90625
# [17/100] training 7.6% loss=0.27468, acc=0.87500
# [17/100] training 7.8% loss=0.35247, acc=0.84375
# [17/100] training 7.9% loss=0.30809, acc=0.85938
# [17/100] training 8.1% loss=0.24664, acc=0.90625
# [17/100] training 8.2% loss=0.22997, acc=0.90625
# [17/100] training 8.4% loss=0.31262, acc=0.85938
# [17/100] training 8.7% loss=0.26390, acc=0.92188
# [17/100] training 8.8% loss=0.27498, acc=0.87500
# [17/100] training 9.0% loss=0.30817, acc=0.87500
# [17/100] training 9.1% loss=0.23426, acc=0.92188
# [17/100] training 9.3% loss=0.46260, acc=0.81250
# [17/100] training 9.4% loss=0.17557, acc=0.96875
# [17/100] training 9.7% loss=0.26869, acc=0.90625
# [17/100] training 9.9% loss=0.33884, acc=0.84375
# [17/100] training 10.0% loss=0.32000, acc=0.85938
# [17/100] training 10.2% loss=0.24693, acc=0.89062
# [17/100] training 10.3% loss=0.26235, acc=0.90625
# [17/100] training 10.5% loss=0.32418, acc=0.92188
# [17/100] training 10.6% loss=0.25085, acc=0.85938
# [17/100] training 10.9% loss=0.18072, acc=0.93750
# [17/100] training 11.0% loss=0.33977, acc=0.87500
# [17/100] training 11.2% loss=0.19482, acc=0.93750
# [17/100] training 11.4% loss=0.33909, acc=0.85938
# [17/100] training 11.5% loss=0.54129, acc=0.81250
# [17/100] training 11.7% loss=0.15859, acc=0.95312
# [17/100] training 11.8% loss=0.32673, acc=0.84375
# [17/100] training 12.1% loss=0.30126, acc=0.87500
# [17/100] training 12.2% loss=0.27299, acc=0.84375
# [17/100] training 12.4% loss=0.26953, acc=0.90625
# [17/100] training 12.6% loss=0.21174, acc=0.95312
# [17/100] training 12.7% loss=0.29920, acc=0.85938
# [17/100] training 12.9% loss=0.29007, acc=0.90625
# [17/100] training 13.0% loss=0.25540, acc=0.89062
# [17/100] training 13.3% loss=0.24581, acc=0.93750
# [17/100] training 13.4% loss=0.22425, acc=0.90625
# [17/100] training 13.6% loss=0.26365, acc=0.89062
# [17/100] training 13.7% loss=0.47249, acc=0.84375
# [17/100] training 13.9% loss=0.26250, acc=0.87500
# [17/100] training 14.1% loss=0.33195, acc=0.89062
# [17/100] training 14.3% loss=0.27656, acc=0.87500
# [17/100] training 14.5% loss=0.25519, acc=0.89062
# [17/100] training 14.6% loss=0.18360, acc=0.93750
# [17/100] training 14.8% loss=0.22330, acc=0.89062
# [17/100] training 14.9% loss=0.19401, acc=0.92188
# [17/100] training 15.1% loss=0.45038, acc=0.81250
# [17/100] training 15.4% loss=0.25246, acc=0.85938
# [17/100] training 15.5% loss=0.33730, acc=0.87500
# [17/100] training 15.7% loss=0.33600, acc=0.92188
# [17/100] training 15.8% loss=0.24319, acc=0.87500
# [17/100] training 16.0% loss=0.43528, acc=0.82812
# [17/100] training 16.1% loss=0.43995, acc=0.79688
# [17/100] training 16.3% loss=0.28545, acc=0.87500
# [17/100] training 16.4% loss=0.25914, acc=0.90625
# [17/100] training 16.7% loss=0.38397, acc=0.79688
# [17/100] training 16.9% loss=0.36533, acc=0.84375
# [17/100] training 17.0% loss=0.25967, acc=0.93750
# [17/100] training 17.2% loss=0.25907, acc=0.90625
# [17/100] training 17.3% loss=0.23318, acc=0.89062
# [17/100] training 17.5% loss=0.36237, acc=0.87500
# [17/100] training 17.7% loss=0.27393, acc=0.89062
# [17/100] training 17.9% loss=0.45240, acc=0.84375
# [17/100] training 18.1% loss=0.31911, acc=0.90625
# [17/100] training 18.2% loss=0.41744, acc=0.84375
# [17/100] training 18.4% loss=0.37214, acc=0.81250
# [17/100] training 18.5% loss=0.26217, acc=0.92188
# [17/100] training 18.8% loss=0.23838, acc=0.89062
# [17/100] training 18.9% loss=0.19687, acc=0.96875
# [17/100] training 19.1% loss=0.34239, acc=0.87500
# [17/100] training 19.2% loss=0.20127, acc=0.90625
# [17/100] training 19.4% loss=0.20315, acc=0.92188
# [17/100] training 19.6% loss=0.32070, acc=0.85938
# [17/100] training 19.7% loss=0.32793, acc=0.84375
# [17/100] training 20.0% loss=0.21078, acc=0.92188
# [17/100] training 20.1% loss=0.27369, acc=0.90625
# [17/100] training 20.3% loss=0.35319, acc=0.89062
# [17/100] training 20.4% loss=0.47237, acc=0.78125
# [17/100] training 20.6% loss=0.37007, acc=0.85938
# [17/100] training 20.8% loss=0.24036, acc=0.87500
# [17/100] training 20.9% loss=0.32756, acc=0.87500
# [17/100] training 21.2% loss=0.32770, acc=0.89062
# [17/100] training 21.3% loss=0.36746, acc=0.84375
# [17/100] training 21.5% loss=0.31189, acc=0.90625
# [17/100] training 21.6% loss=0.19923, acc=0.92188
# [17/100] training 21.8% loss=0.24803, acc=0.87500
# [17/100] training 21.9% loss=0.34665, acc=0.87500
# [17/100] training 22.2% loss=0.26601, acc=0.84375
# [17/100] training 22.4% loss=0.35819, acc=0.82812
# [17/100] training 22.5% loss=0.21568, acc=0.90625
# [17/100] training 22.7% loss=0.26856, acc=0.87500
# [17/100] training 22.8% loss=0.40099, acc=0.82812
# [17/100] training 23.0% loss=0.17062, acc=0.93750
# [17/100] training 23.1% loss=0.42877, acc=0.82812
# [17/100] training 23.4% loss=0.34166, acc=0.84375
# [17/100] training 23.6% loss=0.33663, acc=0.90625
# [17/100] training 23.7% loss=0.23951, acc=0.92188
# [17/100] training 23.9% loss=0.25301, acc=0.87500
# [17/100] training 24.0% loss=0.31301, acc=0.89062
# [17/100] training 24.2% loss=0.28769, acc=0.84375
# [17/100] training 24.3% loss=0.32183, acc=0.85938
# [17/100] training 24.6% loss=0.26655, acc=0.87500
# [17/100] training 24.7% loss=0.28914, acc=0.92188
# [17/100] training 24.9% loss=0.25380, acc=0.92188
# [17/100] training 25.1% loss=0.38641, acc=0.84375
# [17/100] training 25.2% loss=0.19271, acc=0.90625
# [17/100] training 25.4% loss=0.23926, acc=0.90625
# [17/100] training 25.6% loss=0.24096, acc=0.90625
# [17/100] training 25.8% loss=0.30664, acc=0.85938
# [17/100] training 25.9% loss=0.25974, acc=0.89062
# [17/100] training 26.1% loss=0.33057, acc=0.87500
# [17/100] training 26.3% loss=0.26079, acc=0.92188
# [17/100] training 26.4% loss=0.17549, acc=0.93750
# [17/100] training 26.6% loss=0.23057, acc=0.90625
# [17/100] training 26.8% loss=0.31031, acc=0.84375
# [17/100] training 27.0% loss=0.32402, acc=0.92188
# [17/100] training 27.1% loss=0.20814, acc=0.92188
# [17/100] training 27.3% loss=0.31365, acc=0.82812
# [17/100] training 27.4% loss=0.21297, acc=0.93750
# [17/100] training 27.6% loss=0.31023, acc=0.89062
# [17/100] training 27.9% loss=0.21362, acc=0.92188
# [17/100] training 28.0% loss=0.35148, acc=0.87500
# [17/100] training 28.2% loss=0.25739, acc=0.90625
# [17/100] training 28.3% loss=0.19296, acc=0.93750
# [17/100] training 28.5% loss=0.31458, acc=0.85938
# [17/100] training 28.6% loss=0.22064, acc=0.92188
# [17/100] training 28.8% loss=0.19856, acc=0.92188
# [17/100] training 29.1% loss=0.23440, acc=0.90625
# [17/100] training 29.2% loss=0.23526, acc=0.92188
# [17/100] training 29.4% loss=0.38227, acc=0.78125
# [17/100] training 29.5% loss=0.16954, acc=0.92188
# [17/100] training 29.7% loss=0.38000, acc=0.87500
# [17/100] training 29.8% loss=0.20636, acc=0.90625
# [17/100] training 30.0% loss=0.34696, acc=0.81250
# [17/100] training 30.2% loss=0.15959, acc=0.95312
# [17/100] training 30.4% loss=0.16301, acc=0.93750
# [17/100] training 30.6% loss=0.34461, acc=0.87500
# [17/100] training 30.7% loss=0.28340, acc=0.82812
# [17/100] training 30.9% loss=0.35745, acc=0.85938
# [17/100] training 31.0% loss=0.27784, acc=0.90625
# [17/100] training 31.3% loss=0.26803, acc=0.90625
# [17/100] training 31.4% loss=0.32415, acc=0.79688
# [17/100] training 31.6% loss=0.26899, acc=0.87500
# [17/100] training 31.8% loss=0.20591, acc=0.89062
# [17/100] training 31.9% loss=0.28357, acc=0.89062
# [17/100] training 32.1% loss=0.30945, acc=0.90625
# [17/100] training 32.2% loss=0.30380, acc=0.89062
# [17/100] training 32.5% loss=0.14982, acc=0.96875
# [17/100] training 32.6% loss=0.32668, acc=0.87500
# [17/100] training 32.8% loss=0.21590, acc=0.92188
# [17/100] training 32.9% loss=0.29211, acc=0.90625
# [17/100] training 33.1% loss=0.31707, acc=0.87500
# [17/100] training 33.3% loss=0.31416, acc=0.87500
# [17/100] training 33.4% loss=0.33991, acc=0.82812
# [17/100] training 33.7% loss=0.31504, acc=0.89062
# [17/100] training 33.8% loss=0.24839, acc=0.90625
# [17/100] training 34.0% loss=0.26904, acc=0.89062
# [17/100] training 34.1% loss=0.25378, acc=0.90625
# [17/100] training 34.3% loss=0.24069, acc=0.90625
# [17/100] training 34.5% loss=0.31177, acc=0.87500
# [17/100] training 34.7% loss=0.23478, acc=0.93750
# [17/100] training 34.9% loss=0.25245, acc=0.89062
# [17/100] training 35.0% loss=0.24561, acc=0.92188
# [17/100] training 35.2% loss=0.30124, acc=0.84375
# [17/100] training 35.3% loss=0.27246, acc=0.87500
# [17/100] training 35.5% loss=0.28720, acc=0.90625
# [17/100] training 35.6% loss=0.37717, acc=0.84375
# [17/100] training 35.9% loss=0.30479, acc=0.84375
# [17/100] training 36.1% loss=0.32433, acc=0.85938
# [17/100] training 36.2% loss=0.33635, acc=0.84375
# [17/100] training 36.4% loss=0.37708, acc=0.84375
# [17/100] training 36.5% loss=0.24084, acc=0.93750
# [17/100] training 36.7% loss=0.25948, acc=0.87500
# [17/100] training 36.8% loss=0.20877, acc=0.92188
# [17/100] training 37.1% loss=0.28063, acc=0.90625
# [17/100] training 37.3% loss=0.35672, acc=0.85938
# [17/100] training 37.4% loss=0.29661, acc=0.87500
# [17/100] training 37.6% loss=0.21220, acc=0.89062
# [17/100] training 37.7% loss=0.28798, acc=0.89062
# [17/100] training 37.9% loss=0.27654, acc=0.90625
# [17/100] training 38.1% loss=0.38025, acc=0.90625
# [17/100] training 38.3% loss=0.26853, acc=0.89062
# [17/100] training 38.4% loss=0.22939, acc=0.95312
# [17/100] training 38.6% loss=0.20554, acc=0.93750
# [17/100] training 38.8% loss=0.31453, acc=0.84375
# [17/100] training 38.9% loss=0.25432, acc=0.90625
# [17/100] training 39.1% loss=0.22438, acc=0.93750
# [17/100] training 39.3% loss=0.21432, acc=0.89062
# [17/100] training 39.5% loss=0.23063, acc=0.93750
# [17/100] training 39.6% loss=0.21845, acc=0.95312
# [17/100] training 39.8% loss=0.22963, acc=0.90625
# [17/100] training 40.0% loss=0.37233, acc=0.82812
# [17/100] training 40.1% loss=0.33343, acc=0.85938
# [17/100] training 40.4% loss=0.23981, acc=0.90625
# [17/100] training 40.5% loss=0.20132, acc=0.93750
# [17/100] training 40.7% loss=0.26702, acc=0.90625
# [17/100] training 40.8% loss=0.25727, acc=0.90625
# [17/100] training 41.0% loss=0.24422, acc=0.87500
# [17/100] training 41.1% loss=0.42855, acc=0.81250
# [17/100] training 41.3% loss=0.38780, acc=0.79688
# [17/100] training 41.6% loss=0.30167, acc=0.90625
# [17/100] training 41.7% loss=0.42103, acc=0.82812
# [17/100] training 41.9% loss=0.23431, acc=0.89062
# [17/100] training 42.0% loss=0.30765, acc=0.85938
# [17/100] training 42.2% loss=0.30300, acc=0.84375
# [17/100] training 42.3% loss=0.26460, acc=0.93750
# [17/100] training 42.5% loss=0.22036, acc=0.95312
# [17/100] training 42.8% loss=0.23279, acc=0.92188
# [17/100] training 42.9% loss=0.23125, acc=0.92188
# [17/100] training 43.1% loss=0.27088, acc=0.89062
# [17/100] training 43.2% loss=0.24545, acc=0.87500
# [17/100] training 43.4% loss=0.27215, acc=0.89062
# [17/100] training 43.5% loss=0.20625, acc=0.90625
# [17/100] training 43.8% loss=0.30071, acc=0.85938
# [17/100] training 43.9% loss=0.22825, acc=0.90625
# [17/100] training 44.1% loss=0.20050, acc=0.93750
# [17/100] training 44.3% loss=0.27989, acc=0.87500
# [17/100] training 44.4% loss=0.25543, acc=0.90625
# [17/100] training 44.6% loss=0.26753, acc=0.87500
# [17/100] training 44.7% loss=0.31832, acc=0.92188
# [17/100] training 45.0% loss=0.25540, acc=0.89062
# [17/100] training 45.1% loss=0.45259, acc=0.81250
# [17/100] training 45.3% loss=0.25633, acc=0.87500
# [17/100] training 45.5% loss=0.18820, acc=0.95312
# [17/100] training 45.6% loss=0.25009, acc=0.90625
# [17/100] training 45.8% loss=0.15505, acc=0.93750
# [17/100] training 45.9% loss=0.26639, acc=0.84375
# [17/100] training 46.2% loss=0.20557, acc=0.90625
# [17/100] training 46.3% loss=0.16851, acc=0.90625
# [17/100] training 46.5% loss=0.39696, acc=0.84375
# [17/100] training 46.6% loss=0.19710, acc=0.92188
# [17/100] training 46.8% loss=0.21951, acc=0.93750
# [17/100] training 47.0% loss=0.29581, acc=0.90625
# [17/100] training 47.2% loss=0.29327, acc=0.92188
# [17/100] training 47.4% loss=0.28389, acc=0.89062
# [17/100] training 47.5% loss=0.29282, acc=0.90625
# [17/100] training 47.7% loss=0.17689, acc=0.90625
# [17/100] training 47.8% loss=0.34117, acc=0.89062
# [17/100] training 48.0% loss=0.33045, acc=0.81250
# [17/100] training 48.3% loss=0.15297, acc=0.93750
# [17/100] training 48.4% loss=0.22587, acc=0.89062
# [17/100] training 48.6% loss=0.16782, acc=0.95312
# [17/100] training 48.7% loss=0.33212, acc=0.89062
# [17/100] training 48.9% loss=0.29513, acc=0.92188
# [17/100] training 49.0% loss=0.23871, acc=0.85938
# [17/100] training 49.2% loss=0.40784, acc=0.81250
# [17/100] training 49.3% loss=0.25909, acc=0.90625
# [17/100] training 49.6% loss=0.24936, acc=0.89062
# [17/100] training 49.8% loss=0.30342, acc=0.82812
# [17/100] training 49.9% loss=0.23253, acc=0.89062
# [17/100] training 50.1% loss=0.16773, acc=0.92188
# [17/100] training 50.2% loss=0.34487, acc=0.85938
# [17/100] training 50.4% loss=0.34879, acc=0.84375
# [17/100] training 50.6% loss=0.29434, acc=0.87500
# [17/100] training 50.8% loss=0.35794, acc=0.81250
# [17/100] training 51.0% loss=0.41345, acc=0.79688
# [17/100] training 51.1% loss=0.30588, acc=0.81250
# [17/100] training 51.3% loss=0.25253, acc=0.92188
# [17/100] training 51.4% loss=0.30762, acc=0.85938
# [17/100] training 51.7% loss=0.34578, acc=0.84375
# [17/100] training 51.8% loss=0.29872, acc=0.87500
# [17/100] training 52.0% loss=0.23917, acc=0.90625
# [17/100] training 52.1% loss=0.29798, acc=0.89062
# [17/100] training 52.3% loss=0.32057, acc=0.79688
# [17/100] training 52.5% loss=0.13398, acc=0.95312
# [17/100] training 52.6% loss=0.19816, acc=0.93750
# [17/100] training 52.9% loss=0.31499, acc=0.87500
# [17/100] training 53.0% loss=0.26962, acc=0.87500
# [17/100] training 53.2% loss=0.25182, acc=0.90625
# [17/100] training 53.3% loss=0.16035, acc=0.92188
# [17/100] training 53.5% loss=0.24788, acc=0.89062
# [17/100] training 53.7% loss=0.22346, acc=0.90625
# [17/100] training 53.8% loss=0.34549, acc=0.82812
# [17/100] training 54.1% loss=0.36437, acc=0.84375
# [17/100] training 54.2% loss=0.24569, acc=0.87500
# [17/100] training 54.4% loss=0.31802, acc=0.87500
# [17/100] training 54.5% loss=0.32605, acc=0.89062
# [17/100] training 54.7% loss=0.39394, acc=0.79688
# [17/100] training 54.8% loss=0.17806, acc=0.92188
# [17/100] training 55.1% loss=0.28783, acc=0.90625
# [17/100] training 55.3% loss=0.16793, acc=0.92188
# [17/100] training 55.4% loss=0.26250, acc=0.87500
# [17/100] training 55.6% loss=0.34719, acc=0.82812
# [17/100] training 55.7% loss=0.28882, acc=0.89062
# [17/100] training 55.9% loss=0.24069, acc=0.92188
# [17/100] training 56.0% loss=0.31433, acc=0.81250
# [17/100] training 56.3% loss=0.56229, acc=0.76562
# [17/100] training 56.5% loss=0.28578, acc=0.89062
# [17/100] training 56.6% loss=0.31755, acc=0.81250
# [17/100] training 56.8% loss=0.37347, acc=0.84375
# [17/100] training 56.9% loss=0.33639, acc=0.85938
# [17/100] training 57.1% loss=0.38524, acc=0.87500
# [17/100] training 57.2% loss=0.16895, acc=0.95312
# [17/100] training 57.5% loss=0.22177, acc=0.89062
# [17/100] training 57.6% loss=0.31265, acc=0.81250
# [17/100] training 57.8% loss=0.17582, acc=0.98438
# [17/100] training 58.0% loss=0.16537, acc=0.93750
# [17/100] training 58.1% loss=0.22484, acc=0.90625
# [17/100] training 58.3% loss=0.18510, acc=0.92188
# [17/100] training 58.4% loss=0.30162, acc=0.90625
# [17/100] training 58.7% loss=0.25616, acc=0.90625
# [17/100] training 58.8% loss=0.25681, acc=0.90625
# [17/100] training 59.0% loss=0.16760, acc=0.93750
# [17/100] training 59.2% loss=0.32052, acc=0.84375
# [17/100] training 59.3% loss=0.27279, acc=0.85938
# [17/100] training 59.5% loss=0.32258, acc=0.85938
# [17/100] training 59.7% loss=0.29601, acc=0.85938
# [17/100] training 59.9% loss=0.23623, acc=0.93750
# [17/100] training 60.0% loss=0.29590, acc=0.89062
# [17/100] training 60.2% loss=0.30775, acc=0.84375
# [17/100] training 60.3% loss=0.17934, acc=0.93750
# [17/100] training 60.5% loss=0.21595, acc=0.89062
# [17/100] training 60.8% loss=0.33948, acc=0.85938
# [17/100] training 60.9% loss=0.29650, acc=0.85938
# [17/100] training 61.1% loss=0.30833, acc=0.84375
# [17/100] training 61.2% loss=0.24953, acc=0.90625
# [17/100] training 61.4% loss=0.31870, acc=0.87500
# [17/100] training 61.5% loss=0.40819, acc=0.82812
# [17/100] training 61.7% loss=0.34168, acc=0.90625
# [17/100] training 62.0% loss=0.34570, acc=0.82812
# [17/100] training 62.1% loss=0.33159, acc=0.82812
# [17/100] training 62.3% loss=0.16743, acc=0.95312
# [17/100] training 62.4% loss=0.20981, acc=0.90625
# [17/100] training 62.6% loss=0.34279, acc=0.84375
# [17/100] training 62.7% loss=0.27684, acc=0.89062
# [17/100] training 62.9% loss=0.27509, acc=0.90625
# [17/100] training 63.1% loss=0.24687, acc=0.89062
# [17/100] training 63.3% loss=0.29475, acc=0.87500
# [17/100] training 63.5% loss=0.31253, acc=0.87500
# [17/100] training 63.6% loss=0.27751, acc=0.89062
# [17/100] training 63.8% loss=0.27482, acc=0.90625
# [17/100] training 63.9% loss=0.24951, acc=0.89062
# [17/100] training 64.2% loss=0.22678, acc=0.89062
# [17/100] training 64.3% loss=0.27975, acc=0.87500
# [17/100] training 64.5% loss=0.38026, acc=0.81250
# [17/100] training 64.7% loss=0.26342, acc=0.85938
# [17/100] training 64.8% loss=0.46327, acc=0.81250
# [17/100] training 65.0% loss=0.28435, acc=0.84375
# [17/100] training 65.1% loss=0.32323, acc=0.84375
# [17/100] training 65.4% loss=0.27375, acc=0.89062
# [17/100] training 65.5% loss=0.23661, acc=0.89062
# [17/100] training 65.7% loss=0.20167, acc=0.92188
# [17/100] training 65.8% loss=0.25389, acc=0.89062
# [17/100] training 66.0% loss=0.35380, acc=0.85938
# [17/100] training 66.2% loss=0.15604, acc=0.93750
# [17/100] training 66.3% loss=0.44205, acc=0.79688
# [17/100] training 66.6% loss=0.27012, acc=0.89062
# [17/100] training 66.7% loss=0.23714, acc=0.89062
# [17/100] training 66.9% loss=0.31167, acc=0.89062
# [17/100] training 67.0% loss=0.37364, acc=0.82812
# [17/100] training 67.2% loss=0.25239, acc=0.89062
# [17/100] training 67.4% loss=0.33407, acc=0.85938
# [17/100] training 67.6% loss=0.30322, acc=0.89062
# [17/100] training 67.8% loss=0.37318, acc=0.85938
# [17/100] training 67.9% loss=0.17523, acc=0.93750
# [17/100] training 68.1% loss=0.18147, acc=0.92188
# [17/100] training 68.2% loss=0.23734, acc=0.90625
# [17/100] training 68.4% loss=0.17186, acc=0.95312
# [17/100] training 68.5% loss=0.38058, acc=0.81250
# [17/100] training 68.8% loss=0.24658, acc=0.87500
# [17/100] training 69.0% loss=0.41166, acc=0.85938
# [17/100] training 69.1% loss=0.21439, acc=0.93750
# [17/100] training 69.3% loss=0.31481, acc=0.82812
# [17/100] training 69.4% loss=0.18530, acc=0.96875
# [17/100] training 69.6% loss=0.35979, acc=0.82812
# [17/100] training 69.7% loss=0.24822, acc=0.90625
# [17/100] training 70.0% loss=0.25304, acc=0.87500
# [17/100] training 70.2% loss=0.35015, acc=0.82812
# [17/100] training 70.3% loss=0.22819, acc=0.90625
# [17/100] training 70.5% loss=0.27083, acc=0.89062
# [17/100] training 70.6% loss=0.16481, acc=0.90625
# [17/100] training 70.8% loss=0.36064, acc=0.84375
# [17/100] training 71.0% loss=0.40257, acc=0.85938
# [17/100] training 71.2% loss=0.28743, acc=0.85938
# [17/100] training 71.3% loss=0.20012, acc=0.95312
# [17/100] training 71.5% loss=0.25338, acc=0.90625
# [17/100] training 71.7% loss=0.32680, acc=0.89062
# [17/100] training 71.8% loss=0.26949, acc=0.93750
# [17/100] training 72.0% loss=0.20334, acc=0.93750
# [17/100] training 72.2% loss=0.22119, acc=0.93750
# [17/100] training 72.4% loss=0.41586, acc=0.84375
# [17/100] training 72.5% loss=0.39657, acc=0.85938
# [17/100] training 72.7% loss=0.41799, acc=0.79688
# [17/100] training 72.9% loss=0.30185, acc=0.87500
# [17/100] training 73.0% loss=0.14522, acc=0.98438
# [17/100] training 73.3% loss=0.30767, acc=0.87500
# [17/100] training 73.4% loss=0.21669, acc=0.89062
# [17/100] training 73.6% loss=0.20359, acc=0.92188
# [17/100] training 73.7% loss=0.31417, acc=0.84375
# [17/100] training 73.9% loss=0.24475, acc=0.92188
# [17/100] training 74.0% loss=0.26372, acc=0.84375
# [17/100] training 74.2% loss=0.19944, acc=0.92188
# [17/100] training 74.5% loss=0.28971, acc=0.84375
# [17/100] training 74.6% loss=0.40541, acc=0.84375
# [17/100] training 74.8% loss=0.37835, acc=0.84375
# [17/100] training 74.9% loss=0.36076, acc=0.81250
# [17/100] training 75.1% loss=0.33211, acc=0.81250
# [17/100] training 75.2% loss=0.21964, acc=0.92188
# [17/100] training 75.4% loss=0.27673, acc=0.89062
# [17/100] training 75.7% loss=0.29097, acc=0.87500
# [17/100] training 75.8% loss=0.40101, acc=0.79688
# [17/100] training 76.0% loss=0.12997, acc=0.96875
# [17/100] training 76.1% loss=0.39596, acc=0.85938
# [17/100] training 76.3% loss=0.19682, acc=0.95312
# [17/100] training 76.4% loss=0.25095, acc=0.90625
# [17/100] training 76.7% loss=0.19968, acc=0.90625
# [17/100] training 76.8% loss=0.16869, acc=0.93750
# [17/100] training 77.0% loss=0.26621, acc=0.85938
# [17/100] training 77.2% loss=0.26992, acc=0.90625
# [17/100] training 77.3% loss=0.21520, acc=0.93750
# [17/100] training 77.5% loss=0.22237, acc=0.92188
# [17/100] training 77.6% loss=0.22430, acc=0.89062
# [17/100] training 77.9% loss=0.26193, acc=0.89062
# [17/100] training 78.0% loss=0.33846, acc=0.82812
# [17/100] training 78.2% loss=0.28220, acc=0.90625
# [17/100] training 78.4% loss=0.14460, acc=0.95312
# [17/100] training 78.5% loss=0.38258, acc=0.90625
# [17/100] training 78.7% loss=0.29100, acc=0.87500
# [17/100] training 78.8% loss=0.18280, acc=0.95312
# [17/100] training 79.1% loss=0.20574, acc=0.89062
# [17/100] training 79.2% loss=0.15750, acc=0.93750
# [17/100] training 79.4% loss=0.52438, acc=0.81250
# [17/100] training 79.5% loss=0.27480, acc=0.87500
# [17/100] training 79.7% loss=0.16740, acc=0.93750
# [17/100] training 79.9% loss=0.27639, acc=0.85938
# [17/100] training 80.1% loss=0.19361, acc=0.95312
# [17/100] training 80.3% loss=0.32697, acc=0.85938
# [17/100] training 80.4% loss=0.29780, acc=0.87500
# [17/100] training 80.6% loss=0.28515, acc=0.90625
# [17/100] training 80.7% loss=0.21448, acc=0.92188
# [17/100] training 80.9% loss=0.31549, acc=0.87500
# [17/100] training 81.2% loss=0.38828, acc=0.87500
# [17/100] training 81.3% loss=0.33897, acc=0.87500
# [17/100] training 81.5% loss=0.25396, acc=0.89062
# [17/100] training 81.6% loss=0.29949, acc=0.87500
# [17/100] training 81.8% loss=0.20194, acc=0.92188
# [17/100] training 81.9% loss=0.48065, acc=0.78125
# [17/100] training 82.1% loss=0.20892, acc=0.93750
# [17/100] training 82.2% loss=0.26788, acc=0.90625
# [17/100] training 82.5% loss=0.12937, acc=0.95312
# [17/100] training 82.7% loss=0.35061, acc=0.89062
# [17/100] training 82.8% loss=0.29573, acc=0.89062
# [17/100] training 83.0% loss=0.31096, acc=0.82812
# [17/100] training 83.1% loss=0.24458, acc=0.92188
# [17/100] training 83.3% loss=0.20796, acc=0.93750
# [17/100] training 83.5% loss=0.19645, acc=0.93750
# [17/100] training 83.7% loss=0.34509, acc=0.87500
# [17/100] training 83.9% loss=0.26210, acc=0.90625
# [17/100] training 84.0% loss=0.19624, acc=0.92188
# [17/100] training 84.2% loss=0.20137, acc=0.89062
# [17/100] training 84.3% loss=0.28445, acc=0.89062
# [17/100] training 84.5% loss=0.22695, acc=0.89062
# [17/100] training 84.7% loss=0.27576, acc=0.85938
# [17/100] training 84.9% loss=0.25518, acc=0.89062
# [17/100] training 85.0% loss=0.28844, acc=0.87500
# [17/100] training 85.2% loss=0.35735, acc=0.82812
# [17/100] training 85.4% loss=0.30264, acc=0.92188
# [17/100] training 85.5% loss=0.27713, acc=0.85938
# [17/100] training 85.8% loss=0.25586, acc=0.85938
# [17/100] training 85.9% loss=0.36685, acc=0.87500
# [17/100] training 86.1% loss=0.29848, acc=0.89062
# [17/100] training 86.2% loss=0.24056, acc=0.93750
# [17/100] training 86.4% loss=0.34137, acc=0.84375
# [17/100] training 86.6% loss=0.31575, acc=0.90625
# [17/100] training 86.7% loss=0.19425, acc=0.93750
# [17/100] training 87.0% loss=0.34058, acc=0.85938
# [17/100] training 87.1% loss=0.32192, acc=0.82812
# [17/100] training 87.3% loss=0.31233, acc=0.89062
# [17/100] training 87.4% loss=0.25687, acc=0.89062
# [17/100] training 87.6% loss=0.27182, acc=0.85938
# [17/100] training 87.7% loss=0.37969, acc=0.89062
# [17/100] training 87.9% loss=0.34264, acc=0.90625
# [17/100] training 88.2% loss=0.25779, acc=0.85938
# [17/100] training 88.3% loss=0.34254, acc=0.84375
# [17/100] training 88.5% loss=0.25487, acc=0.90625
# [17/100] training 88.6% loss=0.15203, acc=0.95312
# [17/100] training 88.8% loss=0.23447, acc=0.92188
# [17/100] training 88.9% loss=0.28862, acc=0.89062
# [17/100] training 89.2% loss=0.18412, acc=0.93750
# [17/100] training 89.4% loss=0.24030, acc=0.90625
# [17/100] training 89.5% loss=0.30743, acc=0.84375
# [17/100] training 89.7% loss=0.29796, acc=0.84375
# [17/100] training 89.8% loss=0.26638, acc=0.90625
# [17/100] training 90.0% loss=0.22807, acc=0.90625
# [17/100] training 90.1% loss=0.25469, acc=0.92188
# [17/100] training 90.4% loss=0.22299, acc=0.92188
# [17/100] training 90.5% loss=0.19458, acc=0.90625
# [17/100] training 90.7% loss=0.29424, acc=0.90625
# [17/100] training 90.9% loss=0.12611, acc=0.96875
# [17/100] training 91.0% loss=0.22507, acc=0.90625
# [17/100] training 91.2% loss=0.30966, acc=0.87500
# [17/100] training 91.3% loss=0.40925, acc=0.84375
# [17/100] training 91.6% loss=0.46858, acc=0.78125
# [17/100] training 91.7% loss=0.21723, acc=0.89062
# [17/100] training 91.9% loss=0.33558, acc=0.89062
# [17/100] training 92.1% loss=0.36915, acc=0.85938
# [17/100] training 92.2% loss=0.18037, acc=0.93750
# [17/100] training 92.4% loss=0.27265, acc=0.87500
# [17/100] training 92.6% loss=0.35841, acc=0.85938
# [17/100] training 92.8% loss=0.37304, acc=0.85938
# [17/100] training 92.9% loss=0.35922, acc=0.87500
# [17/100] training 93.1% loss=0.31361, acc=0.85938
# [17/100] training 93.2% loss=0.28481, acc=0.89062
# [17/100] training 93.4% loss=0.26311, acc=0.92188
# [17/100] training 93.7% loss=0.27554, acc=0.89062
# [17/100] training 93.8% loss=0.27534, acc=0.87500
# [17/100] training 94.0% loss=0.22730, acc=0.89062
# [17/100] training 94.1% loss=0.26596, acc=0.89062
# [17/100] training 94.3% loss=0.16445, acc=0.93750
# [17/100] training 94.4% loss=0.25882, acc=0.87500
# [17/100] training 94.6% loss=0.16700, acc=0.92188
# [17/100] training 94.9% loss=0.29628, acc=0.85938
# [17/100] training 95.0% loss=0.44409, acc=0.87500
# [17/100] training 95.2% loss=0.44007, acc=0.82812
# [17/100] training 95.3% loss=0.36237, acc=0.85938
# [17/100] training 95.5% loss=0.18124, acc=0.92188
# [17/100] training 95.6% loss=0.27476, acc=0.90625
# [17/100] training 95.8% loss=0.26569, acc=0.87500
# [17/100] training 96.0% loss=0.28329, acc=0.89062
# [17/100] training 96.2% loss=0.18006, acc=0.92188
# [17/100] training 96.4% loss=0.29555, acc=0.90625
# [17/100] training 96.5% loss=0.34260, acc=0.85938
# [17/100] training 96.7% loss=0.23648, acc=0.90625
# [17/100] training 96.8% loss=0.24139, acc=0.89062
# [17/100] training 97.1% loss=0.22763, acc=0.87500
# [17/100] training 97.2% loss=0.25922, acc=0.89062
# [17/100] training 97.4% loss=0.24960, acc=0.92188
# [17/100] training 97.6% loss=0.38189, acc=0.85938
# [17/100] training 97.7% loss=0.22455, acc=0.92188
# [17/100] training 97.9% loss=0.27520, acc=0.84375
# [17/100] training 98.0% loss=0.28110, acc=0.87500
# [17/100] training 98.3% loss=0.23916, acc=0.89062
# [17/100] training 98.4% loss=0.21100, acc=0.92188
# [17/100] training 98.6% loss=0.39941, acc=0.79688
# [17/100] training 98.7% loss=0.28978, acc=0.87500
# [17/100] training 98.9% loss=0.16600, acc=0.90625
# [17/100] training 99.1% loss=0.26623, acc=0.89062
# [17/100] training 99.2% loss=0.21444, acc=0.90625
# [17/100] training 99.5% loss=0.30679, acc=0.79688
# [17/100] training 99.6% loss=0.27458, acc=0.85938
# [17/100] training 99.8% loss=0.27226, acc=0.87500
# [17/100] training 99.9% loss=0.14050, acc=0.95312
# [17/100] testing 0.9% loss=0.18590, acc=0.90625
# [17/100] testing 1.8% loss=0.48653, acc=0.82812
# [17/100] testing 2.2% loss=0.27410, acc=0.89062
# [17/100] testing 3.1% loss=0.24027, acc=0.92188
# [17/100] testing 3.5% loss=0.16743, acc=0.93750
# [17/100] testing 4.4% loss=0.29379, acc=0.84375
# [17/100] testing 4.8% loss=0.44096, acc=0.81250
# [17/100] testing 5.7% loss=0.24714, acc=0.90625
# [17/100] testing 6.6% loss=0.35132, acc=0.87500
# [17/100] testing 7.0% loss=0.26914, acc=0.87500
# [17/100] testing 7.9% loss=0.43174, acc=0.84375
# [17/100] testing 8.3% loss=0.25364, acc=0.89062
# [17/100] testing 9.2% loss=0.35396, acc=0.84375
# [17/100] testing 9.7% loss=0.18300, acc=0.93750
# [17/100] testing 10.5% loss=0.38171, acc=0.81250
# [17/100] testing 11.0% loss=0.36685, acc=0.85938
# [17/100] testing 11.8% loss=0.29410, acc=0.92188
# [17/100] testing 12.7% loss=0.50239, acc=0.84375
# [17/100] testing 13.2% loss=0.25994, acc=0.92188
# [17/100] testing 14.0% loss=0.41171, acc=0.85938
# [17/100] testing 14.5% loss=0.41829, acc=0.89062
# [17/100] testing 15.4% loss=0.33806, acc=0.87500
# [17/100] testing 15.8% loss=0.21907, acc=0.90625
# [17/100] testing 16.7% loss=0.28274, acc=0.92188
# [17/100] testing 17.5% loss=0.22575, acc=0.89062
# [17/100] testing 18.0% loss=0.21130, acc=0.90625
# [17/100] testing 18.9% loss=0.20242, acc=0.89062
# [17/100] testing 19.3% loss=0.37705, acc=0.87500
# [17/100] testing 20.2% loss=0.42877, acc=0.79688
# [17/100] testing 20.6% loss=0.47274, acc=0.81250
# [17/100] testing 21.5% loss=0.33020, acc=0.87500
# [17/100] testing 21.9% loss=0.57066, acc=0.75000
# [17/100] testing 22.8% loss=0.42902, acc=0.84375
# [17/100] testing 23.7% loss=0.44350, acc=0.82812
# [17/100] testing 24.1% loss=0.25385, acc=0.87500
# [17/100] testing 25.0% loss=0.47320, acc=0.84375
# [17/100] testing 25.4% loss=0.15150, acc=0.92188
# [17/100] testing 26.3% loss=0.34075, acc=0.81250
# [17/100] testing 26.8% loss=0.33248, acc=0.85938
# [17/100] testing 27.6% loss=0.38622, acc=0.85938
# [17/100] testing 28.5% loss=0.40730, acc=0.84375
# [17/100] testing 29.0% loss=0.24051, acc=0.89062
# [17/100] testing 29.8% loss=0.38741, acc=0.85938
# [17/100] testing 30.3% loss=0.37255, acc=0.79688
# [17/100] testing 31.1% loss=0.41194, acc=0.76562
# [17/100] testing 31.6% loss=0.34476, acc=0.84375
# [17/100] testing 32.5% loss=0.24518, acc=0.90625
# [17/100] testing 32.9% loss=0.42128, acc=0.82812
# [17/100] testing 33.8% loss=0.37325, acc=0.85938
# [17/100] testing 34.7% loss=0.41003, acc=0.84375
# [17/100] testing 35.1% loss=0.24645, acc=0.85938
# [17/100] testing 36.0% loss=0.38026, acc=0.87500
# [17/100] testing 36.4% loss=0.25027, acc=0.93750
# [17/100] testing 37.3% loss=0.34221, acc=0.85938
# [17/100] testing 37.7% loss=0.47726, acc=0.78125
# [17/100] testing 38.6% loss=0.21615, acc=0.92188
# [17/100] testing 39.5% loss=0.41644, acc=0.85938
# [17/100] testing 39.9% loss=0.31781, acc=0.89062
# [17/100] testing 40.8% loss=0.32870, acc=0.85938
# [17/100] testing 41.2% loss=0.24758, acc=0.92188
# [17/100] testing 42.1% loss=0.25063, acc=0.90625
# [17/100] testing 42.5% loss=0.20330, acc=0.89062
# [17/100] testing 43.4% loss=0.37548, acc=0.84375
# [17/100] testing 43.9% loss=0.29787, acc=0.89062
# [17/100] testing 44.7% loss=0.41920, acc=0.87500
# [17/100] testing 45.6% loss=0.31019, acc=0.82812
# [17/100] testing 46.1% loss=0.29465, acc=0.84375
# [17/100] testing 46.9% loss=0.28314, acc=0.87500
# [17/100] testing 47.4% loss=0.14038, acc=0.93750
# [17/100] testing 48.3% loss=0.46330, acc=0.81250
# [17/100] testing 48.7% loss=0.49686, acc=0.81250
# [17/100] testing 49.6% loss=0.40744, acc=0.82812
# [17/100] testing 50.4% loss=0.21236, acc=0.92188
# [17/100] testing 50.9% loss=0.34650, acc=0.85938
# [17/100] testing 51.8% loss=0.32600, acc=0.85938
# [17/100] testing 52.2% loss=0.28125, acc=0.87500
# [17/100] testing 53.1% loss=0.19472, acc=0.90625
# [17/100] testing 53.5% loss=0.21558, acc=0.95312
# [17/100] testing 54.4% loss=0.45021, acc=0.81250
# [17/100] testing 54.8% loss=0.38731, acc=0.85938
# [17/100] testing 55.7% loss=0.20699, acc=0.92188
# [17/100] testing 56.6% loss=0.35898, acc=0.90625
# [17/100] testing 57.0% loss=0.50285, acc=0.82812
# [17/100] testing 57.9% loss=0.35720, acc=0.82812
# [17/100] testing 58.3% loss=0.53807, acc=0.78125
# [17/100] testing 59.2% loss=0.30302, acc=0.87500
# [17/100] testing 59.7% loss=0.37505, acc=0.85938
# [17/100] testing 60.5% loss=0.29186, acc=0.85938
# [17/100] testing 61.4% loss=0.13670, acc=0.92188
# [17/100] testing 61.9% loss=0.33969, acc=0.85938
# [17/100] testing 62.7% loss=0.27065, acc=0.90625
# [17/100] testing 63.2% loss=0.45685, acc=0.81250
# [17/100] testing 64.0% loss=0.34292, acc=0.82812
# [17/100] testing 64.5% loss=0.30369, acc=0.87500
# [17/100] testing 65.4% loss=0.29750, acc=0.82812
# [17/100] testing 65.8% loss=0.39018, acc=0.84375
# [17/100] testing 66.7% loss=0.25764, acc=0.85938
# [17/100] testing 67.6% loss=0.39898, acc=0.84375
# [17/100] testing 68.0% loss=0.21743, acc=0.89062
# [17/100] testing 68.9% loss=0.32840, acc=0.87500
# [17/100] testing 69.3% loss=0.28988, acc=0.85938
# [17/100] testing 70.2% loss=0.46718, acc=0.79688
# [17/100] testing 70.6% loss=0.39223, acc=0.81250
# [17/100] testing 71.5% loss=0.51774, acc=0.82812
# [17/100] testing 72.4% loss=0.28432, acc=0.85938
# [17/100] testing 72.8% loss=0.21650, acc=0.92188
# [17/100] testing 73.7% loss=0.23532, acc=0.90625
# [17/100] testing 74.1% loss=0.48695, acc=0.82812
# [17/100] testing 75.0% loss=0.22562, acc=0.89062
# [17/100] testing 75.4% loss=0.33912, acc=0.87500
# [17/100] testing 76.3% loss=0.11370, acc=0.98438
# [17/100] testing 76.8% loss=0.33307, acc=0.82812
# [17/100] testing 77.6% loss=0.23527, acc=0.90625
# [17/100] testing 78.5% loss=0.54145, acc=0.79688
# [17/100] testing 79.0% loss=0.27245, acc=0.90625
# [17/100] testing 79.8% loss=0.34295, acc=0.85938
# [17/100] testing 80.3% loss=0.31828, acc=0.89062
# [17/100] testing 81.2% loss=0.59447, acc=0.82812
# [17/100] testing 81.6% loss=0.24590, acc=0.92188
# [17/100] testing 82.5% loss=0.35226, acc=0.85938
# [17/100] testing 83.3% loss=0.18740, acc=0.93750
# [17/100] testing 83.8% loss=0.13825, acc=0.96875
# [17/100] testing 84.7% loss=0.33068, acc=0.84375
# [17/100] testing 85.1% loss=0.32910, acc=0.85938
# [17/100] testing 86.0% loss=0.39691, acc=0.84375
# [17/100] testing 86.4% loss=0.54576, acc=0.81250
# [17/100] testing 87.3% loss=0.46068, acc=0.81250
# [17/100] testing 87.7% loss=0.36468, acc=0.85938
# [17/100] testing 88.6% loss=0.28796, acc=0.89062
# [17/100] testing 89.5% loss=0.43056, acc=0.82812
# [17/100] testing 89.9% loss=0.29670, acc=0.85938
# [17/100] testing 90.8% loss=0.27487, acc=0.90625
# [17/100] testing 91.2% loss=0.15841, acc=0.93750
# [17/100] testing 92.1% loss=0.31524, acc=0.84375
# [17/100] testing 92.6% loss=0.28481, acc=0.89062
# [17/100] testing 93.4% loss=0.44687, acc=0.82812
# [17/100] testing 94.3% loss=0.25220, acc=0.89062
# [17/100] testing 94.7% loss=0.27982, acc=0.89062
# [17/100] testing 95.6% loss=0.41428, acc=0.84375
# [17/100] testing 96.1% loss=0.34986, acc=0.82812
# [17/100] testing 96.9% loss=0.27055, acc=0.89062
# [17/100] testing 97.4% loss=0.17806, acc=0.95312
# [17/100] testing 98.3% loss=0.25083, acc=0.92188
# [17/100] testing 98.7% loss=0.24142, acc=0.89062
# [17/100] testing 99.6% loss=0.28825, acc=0.90625
# [18/100] training 0.2% loss=0.43247, acc=0.79688
# [18/100] training 0.4% loss=0.35808, acc=0.84375
# [18/100] training 0.5% loss=0.25208, acc=0.89062
# [18/100] training 0.8% loss=0.39004, acc=0.82812
# [18/100] training 0.9% loss=0.25163, acc=0.92188
# [18/100] training 1.1% loss=0.28618, acc=0.93750
# [18/100] training 1.2% loss=0.29817, acc=0.85938
# [18/100] training 1.4% loss=0.26610, acc=0.89062
# [18/100] training 1.6% loss=0.14022, acc=0.96875
# [18/100] training 1.8% loss=0.26502, acc=0.92188
# [18/100] training 2.0% loss=0.27014, acc=0.87500
# [18/100] training 2.1% loss=0.33356, acc=0.78125
# [18/100] training 2.3% loss=0.21063, acc=0.92188
# [18/100] training 2.4% loss=0.34968, acc=0.84375
# [18/100] training 2.6% loss=0.18246, acc=0.90625
# [18/100] training 2.7% loss=0.31179, acc=0.90625
# [18/100] training 3.0% loss=0.29426, acc=0.89062
# [18/100] training 3.2% loss=0.27744, acc=0.89062
# [18/100] training 3.3% loss=0.38393, acc=0.84375
# [18/100] training 3.5% loss=0.27002, acc=0.84375
# [18/100] training 3.6% loss=0.35764, acc=0.87500
# [18/100] training 3.8% loss=0.25449, acc=0.85938
# [18/100] training 3.9% loss=0.22699, acc=0.90625
# [18/100] training 4.2% loss=0.25204, acc=0.89062
# [18/100] training 4.4% loss=0.21625, acc=0.92188
# [18/100] training 4.5% loss=0.22526, acc=0.85938
# [18/100] training 4.7% loss=0.23310, acc=0.92188
# [18/100] training 4.8% loss=0.30341, acc=0.87500
# [18/100] training 5.0% loss=0.19931, acc=0.89062
# [18/100] training 5.2% loss=0.35131, acc=0.85938
# [18/100] training 5.4% loss=0.18421, acc=0.93750
# [18/100] training 5.5% loss=0.25771, acc=0.89062
# [18/100] training 5.7% loss=0.19816, acc=0.95312
# [18/100] training 5.9% loss=0.33433, acc=0.87500
# [18/100] training 6.0% loss=0.32951, acc=0.87500
# [18/100] training 6.3% loss=0.33930, acc=0.82812
# [18/100] training 6.4% loss=0.20357, acc=0.95312
# [18/100] training 6.6% loss=0.31085, acc=0.84375
# [18/100] training 6.7% loss=0.32761, acc=0.85938
# [18/100] training 6.9% loss=0.19752, acc=0.92188
# [18/100] training 7.1% loss=0.27704, acc=0.85938
# [18/100] training 7.2% loss=0.35246, acc=0.87500
# [18/100] training 7.5% loss=0.20403, acc=0.92188
# [18/100] training 7.6% loss=0.28525, acc=0.89062
# [18/100] training 7.8% loss=0.31370, acc=0.87500
# [18/100] training 7.9% loss=0.30484, acc=0.87500
# [18/100] training 8.1% loss=0.24656, acc=0.90625
# [18/100] training 8.2% loss=0.19793, acc=0.92188
# [18/100] training 8.4% loss=0.27769, acc=0.87500
# [18/100] training 8.7% loss=0.30551, acc=0.89062
# [18/100] training 8.8% loss=0.26451, acc=0.85938
# [18/100] training 9.0% loss=0.30406, acc=0.85938
# [18/100] training 9.1% loss=0.32652, acc=0.87500
# [18/100] training 9.3% loss=0.47160, acc=0.84375
# [18/100] training 9.4% loss=0.26750, acc=0.90625
# [18/100] training 9.7% loss=0.31215, acc=0.89062
# [18/100] training 9.9% loss=0.32168, acc=0.87500
# [18/100] training 10.0% loss=0.33772, acc=0.85938
# [18/100] training 10.2% loss=0.25037, acc=0.90625
# [18/100] training 10.3% loss=0.20639, acc=0.89062
# [18/100] training 10.5% loss=0.42147, acc=0.85938
# [18/100] training 10.6% loss=0.29715, acc=0.84375
# [18/100] training 10.9% loss=0.18681, acc=0.92188
# [18/100] training 11.0% loss=0.27687, acc=0.85938
# [18/100] training 11.2% loss=0.22339, acc=0.95312
# [18/100] training 11.4% loss=0.31125, acc=0.87500
# [18/100] training 11.5% loss=0.47286, acc=0.84375
# [18/100] training 11.7% loss=0.18823, acc=0.92188
# [18/100] training 11.8% loss=0.22341, acc=0.93750
# [18/100] training 12.1% loss=0.27708, acc=0.84375
# [18/100] training 12.2% loss=0.25696, acc=0.92188
# [18/100] training 12.4% loss=0.22312, acc=0.87500
# [18/100] training 12.6% loss=0.26835, acc=0.89062
# [18/100] training 12.7% loss=0.25750, acc=0.85938
# [18/100] training 12.9% loss=0.30853, acc=0.85938
# [18/100] training 13.0% loss=0.23466, acc=0.89062
# [18/100] training 13.3% loss=0.25958, acc=0.89062
# [18/100] training 13.4% loss=0.26380, acc=0.90625
# [18/100] training 13.6% loss=0.23932, acc=0.92188
# [18/100] training 13.7% loss=0.42123, acc=0.87500
# [18/100] training 13.9% loss=0.28731, acc=0.90625
# [18/100] training 14.1% loss=0.32249, acc=0.87500
# [18/100] training 14.3% loss=0.21095, acc=0.87500
# [18/100] training 14.5% loss=0.31217, acc=0.85938
# [18/100] training 14.6% loss=0.20206, acc=0.95312
# [18/100] training 14.8% loss=0.34033, acc=0.87500
# [18/100] training 14.9% loss=0.22069, acc=0.90625
# [18/100] training 15.1% loss=0.37442, acc=0.85938
# [18/100] training 15.4% loss=0.34136, acc=0.84375
# [18/100] training 15.5% loss=0.28498, acc=0.87500
# [18/100] training 15.7% loss=0.39879, acc=0.82812
# [18/100] training 15.8% loss=0.21165, acc=0.92188
# [18/100] training 16.0% loss=0.39221, acc=0.85938
# [18/100] training 16.1% loss=0.39221, acc=0.82812
# [18/100] training 16.3% loss=0.32221, acc=0.85938
# [18/100] training 16.4% loss=0.20281, acc=0.95312
# [18/100] training 16.7% loss=0.36745, acc=0.85938
# [18/100] training 16.9% loss=0.34875, acc=0.85938
# [18/100] training 17.0% loss=0.25971, acc=0.89062
# [18/100] training 17.2% loss=0.24750, acc=0.89062
# [18/100] training 17.3% loss=0.26514, acc=0.85938
# [18/100] training 17.5% loss=0.33319, acc=0.85938
# [18/100] training 17.7% loss=0.23691, acc=0.90625
# [18/100] training 17.9% loss=0.30905, acc=0.89062
# [18/100] training 18.1% loss=0.32524, acc=0.87500
# [18/100] training 18.2% loss=0.39258, acc=0.78125
# [18/100] training 18.4% loss=0.32443, acc=0.87500
# [18/100] training 18.5% loss=0.19983, acc=0.93750
# [18/100] training 18.8% loss=0.25102, acc=0.87500
# [18/100] training 18.9% loss=0.19307, acc=0.93750
# [18/100] training 19.1% loss=0.37120, acc=0.82812
# [18/100] training 19.2% loss=0.18713, acc=0.90625
# [18/100] training 19.4% loss=0.26327, acc=0.92188
# [18/100] training 19.6% loss=0.29260, acc=0.89062
# [18/100] training 19.7% loss=0.27033, acc=0.90625
# [18/100] training 20.0% loss=0.18923, acc=0.89062
# [18/100] training 20.1% loss=0.25192, acc=0.90625
# [18/100] training 20.3% loss=0.36160, acc=0.90625
# [18/100] training 20.4% loss=0.35708, acc=0.81250
# [18/100] training 20.6% loss=0.31500, acc=0.87500
# [18/100] training 20.8% loss=0.22657, acc=0.90625
# [18/100] training 20.9% loss=0.35753, acc=0.85938
# [18/100] training 21.2% loss=0.30540, acc=0.89062
# [18/100] training 21.3% loss=0.35260, acc=0.84375
# [18/100] training 21.5% loss=0.33463, acc=0.87500
# [18/100] training 21.6% loss=0.13491, acc=0.95312
# [18/100] training 21.8% loss=0.24902, acc=0.87500
# [18/100] training 21.9% loss=0.29985, acc=0.85938
# [18/100] training 22.2% loss=0.21044, acc=0.90625
# [18/100] training 22.4% loss=0.36002, acc=0.85938
# [18/100] training 22.5% loss=0.28643, acc=0.89062
# [18/100] training 22.7% loss=0.28269, acc=0.87500
# [18/100] training 22.8% loss=0.48613, acc=0.84375
# [18/100] training 23.0% loss=0.22100, acc=0.90625
# [18/100] training 23.1% loss=0.38029, acc=0.85938
# [18/100] training 23.4% loss=0.32120, acc=0.84375
# [18/100] training 23.6% loss=0.35743, acc=0.90625
# [18/100] training 23.7% loss=0.30370, acc=0.85938
# [18/100] training 23.9% loss=0.26197, acc=0.89062
# [18/100] training 24.0% loss=0.28972, acc=0.87500
# [18/100] training 24.2% loss=0.34713, acc=0.87500
# [18/100] training 24.3% loss=0.41944, acc=0.82812
# [18/100] training 24.6% loss=0.22878, acc=0.87500
# [18/100] training 24.7% loss=0.26616, acc=0.90625
# [18/100] training 24.9% loss=0.22602, acc=0.92188
# [18/100] training 25.1% loss=0.40130, acc=0.82812
# [18/100] training 25.2% loss=0.16496, acc=0.92188
# [18/100] training 25.4% loss=0.19395, acc=0.92188
# [18/100] training 25.6% loss=0.25386, acc=0.89062
# [18/100] training 25.8% loss=0.47336, acc=0.81250
# [18/100] training 25.9% loss=0.24794, acc=0.87500
# [18/100] training 26.1% loss=0.23401, acc=0.92188
# [18/100] training 26.3% loss=0.33631, acc=0.87500
# [18/100] training 26.4% loss=0.23179, acc=0.89062
# [18/100] training 26.6% loss=0.23084, acc=0.92188
# [18/100] training 26.8% loss=0.37563, acc=0.84375
# [18/100] training 27.0% loss=0.28054, acc=0.92188
# [18/100] training 27.1% loss=0.21228, acc=0.90625
# [18/100] training 27.3% loss=0.25980, acc=0.84375
# [18/100] training 27.4% loss=0.18540, acc=0.92188
# [18/100] training 27.6% loss=0.38091, acc=0.85938
# [18/100] training 27.9% loss=0.26470, acc=0.85938
# [18/100] training 28.0% loss=0.30125, acc=0.84375
# [18/100] training 28.2% loss=0.26160, acc=0.90625
# [18/100] training 28.3% loss=0.12335, acc=0.95312
# [18/100] training 28.5% loss=0.27473, acc=0.89062
# [18/100] training 28.6% loss=0.27094, acc=0.92188
# [18/100] training 28.8% loss=0.18083, acc=0.95312
# [18/100] training 29.1% loss=0.22455, acc=0.89062
# [18/100] training 29.2% loss=0.20846, acc=0.93750
# [18/100] training 29.4% loss=0.29860, acc=0.82812
# [18/100] training 29.5% loss=0.17220, acc=0.90625
# [18/100] training 29.7% loss=0.28164, acc=0.87500
# [18/100] training 29.8% loss=0.20267, acc=0.87500
# [18/100] training 30.0% loss=0.27376, acc=0.89062
# [18/100] training 30.2% loss=0.19140, acc=0.90625
# [18/100] training 30.4% loss=0.21307, acc=0.90625
# [18/100] training 30.6% loss=0.36309, acc=0.89062
# [18/100] training 30.7% loss=0.21927, acc=0.89062
# [18/100] training 30.9% loss=0.31865, acc=0.89062
# [18/100] training 31.0% loss=0.18616, acc=0.90625
# [18/100] training 31.3% loss=0.26062, acc=0.90625
# [18/100] training 31.4% loss=0.43474, acc=0.81250
# [18/100] training 31.6% loss=0.33769, acc=0.85938
# [18/100] training 31.8% loss=0.21510, acc=0.89062
# [18/100] training 31.9% loss=0.31119, acc=0.90625
# [18/100] training 32.1% loss=0.26885, acc=0.92188
# [18/100] training 32.2% loss=0.32768, acc=0.85938
# [18/100] training 32.5% loss=0.19149, acc=0.93750
# [18/100] training 32.6% loss=0.33514, acc=0.87500
# [18/100] training 32.8% loss=0.21055, acc=0.92188
# [18/100] training 32.9% loss=0.36423, acc=0.84375
# [18/100] training 33.1% loss=0.30985, acc=0.85938
# [18/100] training 33.3% loss=0.27139, acc=0.82812
# [18/100] training 33.4% loss=0.30522, acc=0.89062
# [18/100] training 33.7% loss=0.30362, acc=0.84375
# [18/100] training 33.8% loss=0.31484, acc=0.84375
# [18/100] training 34.0% loss=0.24216, acc=0.90625
# [18/100] training 34.1% loss=0.30549, acc=0.89062
# [18/100] training 34.3% loss=0.22863, acc=0.90625
# [18/100] training 34.5% loss=0.29787, acc=0.84375
# [18/100] training 34.7% loss=0.22085, acc=0.90625
# [18/100] training 34.9% loss=0.22223, acc=0.89062
# [18/100] training 35.0% loss=0.18307, acc=0.90625
# [18/100] training 35.2% loss=0.36617, acc=0.81250
# [18/100] training 35.3% loss=0.27230, acc=0.89062
# [18/100] training 35.5% loss=0.24577, acc=0.89062
# [18/100] training 35.6% loss=0.32674, acc=0.84375
# [18/100] training 35.9% loss=0.27973, acc=0.85938
# [18/100] training 36.1% loss=0.33309, acc=0.87500
# [18/100] training 36.2% loss=0.25659, acc=0.89062
# [18/100] training 36.4% loss=0.32543, acc=0.84375
# [18/100] training 36.5% loss=0.28216, acc=0.92188
# [18/100] training 36.7% loss=0.23610, acc=0.93750
# [18/100] training 36.8% loss=0.20013, acc=0.93750
# [18/100] training 37.1% loss=0.37310, acc=0.85938
# [18/100] training 37.3% loss=0.24204, acc=0.92188
# [18/100] training 37.4% loss=0.23578, acc=0.85938
# [18/100] training 37.6% loss=0.26065, acc=0.84375
# [18/100] training 37.7% loss=0.32131, acc=0.87500
# [18/100] training 37.9% loss=0.23206, acc=0.90625
# [18/100] training 38.1% loss=0.37021, acc=0.87500
# [18/100] training 38.3% loss=0.22454, acc=0.92188
# [18/100] training 38.4% loss=0.20001, acc=0.95312
# [18/100] training 38.6% loss=0.23603, acc=0.89062
# [18/100] training 38.8% loss=0.41349, acc=0.76562
# [18/100] training 38.9% loss=0.24104, acc=0.92188
# [18/100] training 39.1% loss=0.28495, acc=0.90625
# [18/100] training 39.3% loss=0.26526, acc=0.89062
# [18/100] training 39.5% loss=0.28472, acc=0.87500
# [18/100] training 39.6% loss=0.28282, acc=0.92188
# [18/100] training 39.8% loss=0.17229, acc=0.90625
# [18/100] training 40.0% loss=0.24124, acc=0.85938
# [18/100] training 40.1% loss=0.40098, acc=0.82812
# [18/100] training 40.4% loss=0.18580, acc=0.92188
# [18/100] training 40.5% loss=0.24587, acc=0.89062
# [18/100] training 40.7% loss=0.20856, acc=0.93750
# [18/100] training 40.8% loss=0.22892, acc=0.89062
# [18/100] training 41.0% loss=0.25587, acc=0.93750
# [18/100] training 41.1% loss=0.36056, acc=0.89062
# [18/100] training 41.3% loss=0.33015, acc=0.87500
# [18/100] training 41.6% loss=0.28985, acc=0.87500
# [18/100] training 41.7% loss=0.32960, acc=0.89062
# [18/100] training 41.9% loss=0.17561, acc=0.93750
# [18/100] training 42.0% loss=0.23813, acc=0.92188
# [18/100] training 42.2% loss=0.24938, acc=0.92188
# [18/100] training 42.3% loss=0.25207, acc=0.89062
# [18/100] training 42.5% loss=0.19956, acc=0.93750
# [18/100] training 42.8% loss=0.18325, acc=0.95312
# [18/100] training 42.9% loss=0.23379, acc=0.85938
# [18/100] training 43.1% loss=0.28859, acc=0.89062
# [18/100] training 43.2% loss=0.24522, acc=0.92188
# [18/100] training 43.4% loss=0.28814, acc=0.92188
# [18/100] training 43.5% loss=0.18727, acc=0.93750
# [18/100] training 43.8% loss=0.34991, acc=0.81250
# [18/100] training 43.9% loss=0.21298, acc=0.89062
# [18/100] training 44.1% loss=0.22583, acc=0.90625
# [18/100] training 44.3% loss=0.32891, acc=0.90625
# [18/100] training 44.4% loss=0.38446, acc=0.84375
# [18/100] training 44.6% loss=0.31879, acc=0.87500
# [18/100] training 44.7% loss=0.26293, acc=0.89062
# [18/100] training 45.0% loss=0.22670, acc=0.90625
# [18/100] training 45.1% loss=0.33161, acc=0.87500
# [18/100] training 45.3% loss=0.30377, acc=0.84375
# [18/100] training 45.5% loss=0.19514, acc=0.95312
# [18/100] training 45.6% loss=0.33010, acc=0.85938
# [18/100] training 45.8% loss=0.29416, acc=0.84375
# [18/100] training 45.9% loss=0.21965, acc=0.89062
# [18/100] training 46.2% loss=0.19145, acc=0.96875
# [18/100] training 46.3% loss=0.16492, acc=0.90625
# [18/100] training 46.5% loss=0.43547, acc=0.79688
# [18/100] training 46.6% loss=0.24155, acc=0.87500
# [18/100] training 46.8% loss=0.16432, acc=0.93750
# [18/100] training 47.0% loss=0.30671, acc=0.84375
# [18/100] training 47.2% loss=0.37940, acc=0.81250
# [18/100] training 47.4% loss=0.23909, acc=0.92188
# [18/100] training 47.5% loss=0.25875, acc=0.89062
# [18/100] training 47.7% loss=0.21189, acc=0.89062
# [18/100] training 47.8% loss=0.32265, acc=0.89062
# [18/100] training 48.0% loss=0.33009, acc=0.89062
# [18/100] training 48.3% loss=0.09693, acc=0.98438
# [18/100] training 48.4% loss=0.12375, acc=0.96875
# [18/100] training 48.6% loss=0.12477, acc=0.93750
# [18/100] training 48.7% loss=0.35213, acc=0.85938
# [18/100] training 48.9% loss=0.31320, acc=0.90625
# [18/100] training 49.0% loss=0.20774, acc=0.89062
# [18/100] training 49.2% loss=0.37398, acc=0.79688
# [18/100] training 49.3% loss=0.20534, acc=0.93750
# [18/100] training 49.6% loss=0.29370, acc=0.82812
# [18/100] training 49.8% loss=0.36007, acc=0.85938
# [18/100] training 49.9% loss=0.27658, acc=0.85938
# [18/100] training 50.1% loss=0.21905, acc=0.90625
# [18/100] training 50.2% loss=0.32808, acc=0.85938
# [18/100] training 50.4% loss=0.38730, acc=0.84375
# [18/100] training 50.6% loss=0.34017, acc=0.84375
# [18/100] training 50.8% loss=0.40872, acc=0.78125
# [18/100] training 51.0% loss=0.34583, acc=0.89062
# [18/100] training 51.1% loss=0.27039, acc=0.84375
# [18/100] training 51.3% loss=0.30229, acc=0.90625
# [18/100] training 51.4% loss=0.31898, acc=0.85938
# [18/100] training 51.7% loss=0.37149, acc=0.85938
# [18/100] training 51.8% loss=0.25180, acc=0.89062
# [18/100] training 52.0% loss=0.21882, acc=0.93750
# [18/100] training 52.1% loss=0.31874, acc=0.87500
# [18/100] training 52.3% loss=0.33837, acc=0.84375
# [18/100] training 52.5% loss=0.14064, acc=0.93750
# [18/100] training 52.6% loss=0.19387, acc=0.92188
# [18/100] training 52.9% loss=0.44646, acc=0.82812
# [18/100] training 53.0% loss=0.18195, acc=0.92188
# [18/100] training 53.2% loss=0.27408, acc=0.90625
# [18/100] training 53.3% loss=0.15579, acc=0.93750
# [18/100] training 53.5% loss=0.25610, acc=0.85938
# [18/100] training 53.7% loss=0.23210, acc=0.90625
# [18/100] training 53.8% loss=0.42993, acc=0.79688
# [18/100] training 54.1% loss=0.42363, acc=0.81250
# [18/100] training 54.2% loss=0.22360, acc=0.90625
# [18/100] training 54.4% loss=0.28550, acc=0.90625
# [18/100] training 54.5% loss=0.28738, acc=0.90625
# [18/100] training 54.7% loss=0.38908, acc=0.81250
# [18/100] training 54.8% loss=0.18930, acc=0.93750
# [18/100] training 55.1% loss=0.20814, acc=0.90625
# [18/100] training 55.3% loss=0.16585, acc=0.92188
# [18/100] training 55.4% loss=0.27532, acc=0.89062
# [18/100] training 55.6% loss=0.32916, acc=0.84375
# [18/100] training 55.7% loss=0.30129, acc=0.87500
# [18/100] training 55.9% loss=0.24284, acc=0.89062
# [18/100] training 56.0% loss=0.29983, acc=0.89062
# [18/100] training 56.3% loss=0.52713, acc=0.73438
# [18/100] training 56.5% loss=0.26999, acc=0.93750
# [18/100] training 56.6% loss=0.26823, acc=0.82812
# [18/100] training 56.8% loss=0.32809, acc=0.84375
# [18/100] training 56.9% loss=0.30172, acc=0.87500
# [18/100] training 57.1% loss=0.35267, acc=0.89062
# [18/100] training 57.2% loss=0.20241, acc=0.90625
# [18/100] training 57.5% loss=0.22483, acc=0.93750
# [18/100] training 57.6% loss=0.29810, acc=0.85938
# [18/100] training 57.8% loss=0.19200, acc=0.92188
# [18/100] training 58.0% loss=0.15123, acc=0.93750
# [18/100] training 58.1% loss=0.21150, acc=0.92188
# [18/100] training 58.3% loss=0.16296, acc=0.93750
# [18/100] training 58.4% loss=0.31869, acc=0.89062
# [18/100] training 58.7% loss=0.34289, acc=0.89062
# [18/100] training 58.8% loss=0.35018, acc=0.85938
# [18/100] training 59.0% loss=0.21836, acc=0.89062
# [18/100] training 59.2% loss=0.24311, acc=0.90625
# [18/100] training 59.3% loss=0.24805, acc=0.92188
# [18/100] training 59.5% loss=0.25192, acc=0.90625
# [18/100] training 59.7% loss=0.23741, acc=0.90625
# [18/100] training 59.9% loss=0.22683, acc=0.93750
# [18/100] training 60.0% loss=0.16292, acc=0.93750
# [18/100] training 60.2% loss=0.22118, acc=0.89062
# [18/100] training 60.3% loss=0.19218, acc=0.93750
# [18/100] training 60.5% loss=0.22803, acc=0.89062
# [18/100] training 60.8% loss=0.25106, acc=0.92188
# [18/100] training 60.9% loss=0.31932, acc=0.82812
# [18/100] training 61.1% loss=0.32353, acc=0.84375
# [18/100] training 61.2% loss=0.24173, acc=0.92188
# [18/100] training 61.4% loss=0.41506, acc=0.79688
# [18/100] training 61.5% loss=0.46670, acc=0.81250
# [18/100] training 61.7% loss=0.32353, acc=0.89062
# [18/100] training 62.0% loss=0.31314, acc=0.85938
# [18/100] training 62.1% loss=0.34207, acc=0.82812
# [18/100] training 62.3% loss=0.17866, acc=0.95312
# [18/100] training 62.4% loss=0.23098, acc=0.87500
# [18/100] training 62.6% loss=0.33904, acc=0.90625
# [18/100] training 62.7% loss=0.24887, acc=0.89062
# [18/100] training 62.9% loss=0.27464, acc=0.87500
# [18/100] training 63.1% loss=0.19975, acc=0.93750
# [18/100] training 63.3% loss=0.22233, acc=0.90625
# [18/100] training 63.5% loss=0.29870, acc=0.85938
# [18/100] training 63.6% loss=0.30993, acc=0.85938
# [18/100] training 63.8% loss=0.31563, acc=0.87500
# [18/100] training 63.9% loss=0.18358, acc=0.95312
# [18/100] training 64.2% loss=0.24383, acc=0.85938
# [18/100] training 64.3% loss=0.30556, acc=0.84375
# [18/100] training 64.5% loss=0.38960, acc=0.84375
# [18/100] training 64.7% loss=0.32097, acc=0.84375
# [18/100] training 64.8% loss=0.40954, acc=0.82812
# [18/100] training 65.0% loss=0.28790, acc=0.87500
# [18/100] training 65.1% loss=0.32555, acc=0.87500
# [18/100] training 65.4% loss=0.19682, acc=0.95312
# [18/100] training 65.5% loss=0.21835, acc=0.92188
# [18/100] training 65.7% loss=0.16022, acc=0.98438
# [18/100] training 65.8% loss=0.32719, acc=0.84375
# [18/100] training 66.0% loss=0.28340, acc=0.89062
# [18/100] training 66.2% loss=0.12103, acc=0.93750
# [18/100] training 66.3% loss=0.32267, acc=0.84375
# [18/100] training 66.6% loss=0.27714, acc=0.85938
# [18/100] training 66.7% loss=0.16787, acc=0.92188
# [18/100] training 66.9% loss=0.40678, acc=0.85938
# [18/100] training 67.0% loss=0.35472, acc=0.84375
# [18/100] training 67.2% loss=0.20072, acc=0.92188
# [18/100] training 67.4% loss=0.28415, acc=0.89062
# [18/100] training 67.6% loss=0.29425, acc=0.87500
# [18/100] training 67.8% loss=0.20399, acc=0.92188
# [18/100] training 67.9% loss=0.12924, acc=0.98438
# [18/100] training 68.1% loss=0.16212, acc=0.93750
# [18/100] training 68.2% loss=0.18239, acc=0.95312
# [18/100] training 68.4% loss=0.14860, acc=0.98438
# [18/100] training 68.5% loss=0.35901, acc=0.87500
# [18/100] training 68.8% loss=0.26699, acc=0.92188
# [18/100] training 69.0% loss=0.34919, acc=0.84375
# [18/100] training 69.1% loss=0.12744, acc=0.95312
# [18/100] training 69.3% loss=0.23196, acc=0.92188
# [18/100] training 69.4% loss=0.28723, acc=0.90625
# [18/100] training 69.6% loss=0.30380, acc=0.90625
# [18/100] training 69.7% loss=0.24550, acc=0.92188
# [18/100] training 70.0% loss=0.41476, acc=0.85938
# [18/100] training 70.2% loss=0.41297, acc=0.82812
# [18/100] training 70.3% loss=0.32184, acc=0.82812
# [18/100] training 70.5% loss=0.34627, acc=0.84375
# [18/100] training 70.6% loss=0.24386, acc=0.87500
# [18/100] training 70.8% loss=0.36312, acc=0.81250
# [18/100] training 71.0% loss=0.28206, acc=0.89062
# [18/100] training 71.2% loss=0.19908, acc=0.92188
# [18/100] training 71.3% loss=0.20653, acc=0.90625
# [18/100] training 71.5% loss=0.40013, acc=0.90625
# [18/100] training 71.7% loss=0.33258, acc=0.82812
# [18/100] training 71.8% loss=0.30298, acc=0.92188
# [18/100] training 72.0% loss=0.19278, acc=0.95312
# [18/100] training 72.2% loss=0.25846, acc=0.89062
# [18/100] training 72.4% loss=0.36347, acc=0.84375
# [18/100] training 72.5% loss=0.39445, acc=0.84375
# [18/100] training 72.7% loss=0.42937, acc=0.84375
# [18/100] training 72.9% loss=0.22649, acc=0.95312
# [18/100] training 73.0% loss=0.16380, acc=0.93750
# [18/100] training 73.3% loss=0.27935, acc=0.89062
# [18/100] training 73.4% loss=0.19018, acc=0.92188
# [18/100] training 73.6% loss=0.16388, acc=0.92188
# [18/100] training 73.7% loss=0.28624, acc=0.92188
# [18/100] training 73.9% loss=0.30189, acc=0.89062
# [18/100] training 74.0% loss=0.28160, acc=0.87500
# [18/100] training 74.2% loss=0.19490, acc=0.90625
# [18/100] training 74.5% loss=0.24195, acc=0.90625
# [18/100] training 74.6% loss=0.46663, acc=0.78125
# [18/100] training 74.8% loss=0.43355, acc=0.81250
# [18/100] training 74.9% loss=0.37051, acc=0.85938
# [18/100] training 75.1% loss=0.23746, acc=0.87500
# [18/100] training 75.2% loss=0.17822, acc=0.93750
# [18/100] training 75.4% loss=0.29361, acc=0.89062
# [18/100] training 75.7% loss=0.33695, acc=0.84375
# [18/100] training 75.8% loss=0.37925, acc=0.84375
# [18/100] training 76.0% loss=0.14265, acc=0.96875
# [18/100] training 76.1% loss=0.41822, acc=0.84375
# [18/100] training 76.3% loss=0.15094, acc=0.98438
# [18/100] training 76.4% loss=0.29305, acc=0.89062
# [18/100] training 76.7% loss=0.22845, acc=0.87500
# [18/100] training 76.8% loss=0.16213, acc=0.95312
# [18/100] training 77.0% loss=0.26082, acc=0.87500
# [18/100] training 77.2% loss=0.36307, acc=0.85938
# [18/100] training 77.3% loss=0.18956, acc=0.93750
# [18/100] training 77.5% loss=0.29857, acc=0.87500
# [18/100] training 77.6% loss=0.21628, acc=0.89062
# [18/100] training 77.9% loss=0.23559, acc=0.90625
# [18/100] training 78.0% loss=0.27760, acc=0.89062
# [18/100] training 78.2% loss=0.35275, acc=0.89062
# [18/100] training 78.4% loss=0.10671, acc=0.98438
# [18/100] training 78.5% loss=0.36761, acc=0.85938
# [18/100] training 78.7% loss=0.27350, acc=0.85938
# [18/100] training 78.8% loss=0.17459, acc=0.93750
# [18/100] training 79.1% loss=0.19279, acc=0.92188
# [18/100] training 79.2% loss=0.19323, acc=0.95312
# [18/100] training 79.4% loss=0.36069, acc=0.76562
# [18/100] training 79.5% loss=0.22166, acc=0.90625
# [18/100] training 79.7% loss=0.14914, acc=0.93750
# [18/100] training 79.9% loss=0.27823, acc=0.92188
# [18/100] training 80.1% loss=0.27294, acc=0.87500
# [18/100] training 80.3% loss=0.33547, acc=0.85938
# [18/100] training 80.4% loss=0.26089, acc=0.85938
# [18/100] training 80.6% loss=0.29354, acc=0.89062
# [18/100] training 80.7% loss=0.21055, acc=0.92188
# [18/100] training 80.9% loss=0.38351, acc=0.89062
# [18/100] training 81.2% loss=0.30438, acc=0.85938
# [18/100] training 81.3% loss=0.25134, acc=0.87500
# [18/100] training 81.5% loss=0.23237, acc=0.93750
# [18/100] training 81.6% loss=0.29629, acc=0.89062
# [18/100] training 81.8% loss=0.22644, acc=0.92188
# [18/100] training 81.9% loss=0.33153, acc=0.92188
# [18/100] training 82.1% loss=0.18921, acc=0.92188
# [18/100] training 82.2% loss=0.28480, acc=0.85938
# [18/100] training 82.5% loss=0.15387, acc=0.93750
# [18/100] training 82.7% loss=0.24352, acc=0.90625
# [18/100] training 82.8% loss=0.24099, acc=0.89062
# [18/100] training 83.0% loss=0.31524, acc=0.84375
# [18/100] training 83.1% loss=0.25511, acc=0.92188
# [18/100] training 83.3% loss=0.24016, acc=0.90625
# [18/100] training 83.5% loss=0.26211, acc=0.87500
# [18/100] training 83.7% loss=0.38323, acc=0.82812
# [18/100] training 83.9% loss=0.31059, acc=0.85938
# [18/100] training 84.0% loss=0.21361, acc=0.90625
# [18/100] training 84.2% loss=0.17149, acc=0.93750
# [18/100] training 84.3% loss=0.23441, acc=0.89062
# [18/100] training 84.5% loss=0.22242, acc=0.92188
# [18/100] training 84.7% loss=0.31546, acc=0.89062
# [18/100] training 84.9% loss=0.22755, acc=0.89062
# [18/100] training 85.0% loss=0.26603, acc=0.87500
# [18/100] training 85.2% loss=0.23661, acc=0.87500
# [18/100] training 85.4% loss=0.32595, acc=0.93750
# [18/100] training 85.5% loss=0.26287, acc=0.89062
# [18/100] training 85.8% loss=0.26803, acc=0.89062
# [18/100] training 85.9% loss=0.23036, acc=0.92188
# [18/100] training 86.1% loss=0.23389, acc=0.90625
# [18/100] training 86.2% loss=0.18508, acc=0.93750
# [18/100] training 86.4% loss=0.37261, acc=0.85938
# [18/100] training 86.6% loss=0.34136, acc=0.90625
# [18/100] training 86.7% loss=0.23769, acc=0.90625
# [18/100] training 87.0% loss=0.30007, acc=0.87500
# [18/100] training 87.1% loss=0.27499, acc=0.87500
# [18/100] training 87.3% loss=0.29073, acc=0.87500
# [18/100] training 87.4% loss=0.26448, acc=0.87500
# [18/100] training 87.6% loss=0.29809, acc=0.82812
# [18/100] training 87.7% loss=0.47254, acc=0.87500
# [18/100] training 87.9% loss=0.28871, acc=0.87500
# [18/100] training 88.2% loss=0.18767, acc=0.92188
# [18/100] training 88.3% loss=0.33189, acc=0.90625
# [18/100] training 88.5% loss=0.27365, acc=0.90625
# [18/100] training 88.6% loss=0.11588, acc=0.98438
# [18/100] training 88.8% loss=0.22687, acc=0.93750
# [18/100] training 88.9% loss=0.34114, acc=0.87500
# [18/100] training 89.2% loss=0.18955, acc=0.93750
# [18/100] training 89.4% loss=0.22906, acc=0.87500
# [18/100] training 89.5% loss=0.26491, acc=0.87500
# [18/100] training 89.7% loss=0.23467, acc=0.90625
# [18/100] training 89.8% loss=0.23741, acc=0.92188
# [18/100] training 90.0% loss=0.18302, acc=0.92188
# [18/100] training 90.1% loss=0.25484, acc=0.90625
# [18/100] training 90.4% loss=0.24504, acc=0.92188
# [18/100] training 90.5% loss=0.18220, acc=0.92188
# [18/100] training 90.7% loss=0.27997, acc=0.85938
# [18/100] training 90.9% loss=0.12694, acc=0.96875
# [18/100] training 91.0% loss=0.23578, acc=0.89062
# [18/100] training 91.2% loss=0.31429, acc=0.85938
# [18/100] training 91.3% loss=0.35521, acc=0.84375
# [18/100] training 91.6% loss=0.36496, acc=0.85938
# [18/100] training 91.7% loss=0.18801, acc=0.93750
# [18/100] training 91.9% loss=0.30700, acc=0.87500
# [18/100] training 92.1% loss=0.25748, acc=0.89062
# [18/100] training 92.2% loss=0.14440, acc=0.93750
# [18/100] training 92.4% loss=0.19649, acc=0.90625
# [18/100] training 92.6% loss=0.29943, acc=0.85938
# [18/100] training 92.8% loss=0.31038, acc=0.85938
# [18/100] training 92.9% loss=0.19701, acc=0.93750
# [18/100] training 93.1% loss=0.31844, acc=0.89062
# [18/100] training 93.2% loss=0.24422, acc=0.89062
# [18/100] training 93.4% loss=0.21218, acc=0.93750
# [18/100] training 93.7% loss=0.15941, acc=0.95312
# [18/100] training 93.8% loss=0.25344, acc=0.87500
# [18/100] training 94.0% loss=0.16762, acc=0.92188
# [18/100] training 94.1% loss=0.37968, acc=0.82812
# [18/100] training 94.3% loss=0.18364, acc=0.93750
# [18/100] training 94.4% loss=0.29612, acc=0.93750
# [18/100] training 94.6% loss=0.19214, acc=0.90625
# [18/100] training 94.9% loss=0.27579, acc=0.90625
# [18/100] training 95.0% loss=0.46593, acc=0.81250
# [18/100] training 95.2% loss=0.31594, acc=0.84375
# [18/100] training 95.3% loss=0.27114, acc=0.90625
# [18/100] training 95.5% loss=0.25971, acc=0.93750
# [18/100] training 95.6% loss=0.35599, acc=0.82812
# [18/100] training 95.8% loss=0.30595, acc=0.82812
# [18/100] training 96.0% loss=0.24415, acc=0.92188
# [18/100] training 96.2% loss=0.14619, acc=0.95312
# [18/100] training 96.4% loss=0.21995, acc=0.95312
# [18/100] training 96.5% loss=0.31447, acc=0.84375
# [18/100] training 96.7% loss=0.16208, acc=0.87500
# [18/100] training 96.8% loss=0.22062, acc=0.92188
# [18/100] training 97.1% loss=0.20332, acc=0.90625
# [18/100] training 97.2% loss=0.36706, acc=0.90625
# [18/100] training 97.4% loss=0.20557, acc=0.95312
# [18/100] training 97.6% loss=0.34577, acc=0.85938
# [18/100] training 97.7% loss=0.18433, acc=0.90625
# [18/100] training 97.9% loss=0.19371, acc=0.87500
# [18/100] training 98.0% loss=0.33678, acc=0.85938
# [18/100] training 98.3% loss=0.25105, acc=0.89062
# [18/100] training 98.4% loss=0.30828, acc=0.90625
# [18/100] training 98.6% loss=0.42571, acc=0.79688
# [18/100] training 98.7% loss=0.32223, acc=0.87500
# [18/100] training 98.9% loss=0.12759, acc=1.00000
# [18/100] training 99.1% loss=0.23201, acc=0.90625
# [18/100] training 99.2% loss=0.20052, acc=0.92188
# [18/100] training 99.5% loss=0.39032, acc=0.79688
# [18/100] training 99.6% loss=0.29398, acc=0.84375
# [18/100] training 99.8% loss=0.21032, acc=0.87500
# [18/100] training 99.9% loss=0.15171, acc=0.96875
# [18/100] testing 0.9% loss=0.17829, acc=0.90625
# [18/100] testing 1.8% loss=0.33714, acc=0.82812
# [18/100] testing 2.2% loss=0.21390, acc=0.93750
# [18/100] testing 3.1% loss=0.32088, acc=0.85938
# [18/100] testing 3.5% loss=0.23822, acc=0.90625
# [18/100] testing 4.4% loss=0.25848, acc=0.85938
# [18/100] testing 4.8% loss=0.32715, acc=0.87500
# [18/100] testing 5.7% loss=0.23520, acc=0.89062
# [18/100] testing 6.6% loss=0.24754, acc=0.87500
# [18/100] testing 7.0% loss=0.20929, acc=0.90625
# [18/100] testing 7.9% loss=0.39769, acc=0.81250
# [18/100] testing 8.3% loss=0.19832, acc=0.92188
# [18/100] testing 9.2% loss=0.32741, acc=0.82812
# [18/100] testing 9.7% loss=0.13919, acc=0.93750
# [18/100] testing 10.5% loss=0.32691, acc=0.89062
# [18/100] testing 11.0% loss=0.29370, acc=0.87500
# [18/100] testing 11.8% loss=0.33175, acc=0.87500
# [18/100] testing 12.7% loss=0.40381, acc=0.81250
# [18/100] testing 13.2% loss=0.24950, acc=0.87500
# [18/100] testing 14.0% loss=0.42271, acc=0.84375
# [18/100] testing 14.5% loss=0.29217, acc=0.85938
# [18/100] testing 15.4% loss=0.35325, acc=0.85938
# [18/100] testing 15.8% loss=0.22698, acc=0.87500
# [18/100] testing 16.7% loss=0.27038, acc=0.90625
# [18/100] testing 17.5% loss=0.30840, acc=0.84375
# [18/100] testing 18.0% loss=0.22650, acc=0.87500
# [18/100] testing 18.9% loss=0.17405, acc=0.93750
# [18/100] testing 19.3% loss=0.31475, acc=0.87500
# [18/100] testing 20.2% loss=0.36014, acc=0.84375
# [18/100] testing 20.6% loss=0.37802, acc=0.82812
# [18/100] testing 21.5% loss=0.26197, acc=0.92188
# [18/100] testing 21.9% loss=0.44560, acc=0.81250
# [18/100] testing 22.8% loss=0.32905, acc=0.89062
# [18/100] testing 23.7% loss=0.26665, acc=0.89062
# [18/100] testing 24.1% loss=0.24811, acc=0.90625
# [18/100] testing 25.0% loss=0.38524, acc=0.90625
# [18/100] testing 25.4% loss=0.15047, acc=0.96875
# [18/100] testing 26.3% loss=0.39010, acc=0.78125
# [18/100] testing 26.8% loss=0.27322, acc=0.90625
# [18/100] testing 27.6% loss=0.32826, acc=0.84375
# [18/100] testing 28.5% loss=0.32593, acc=0.92188
# [18/100] testing 29.0% loss=0.25924, acc=0.87500
# [18/100] testing 29.8% loss=0.33363, acc=0.90625
# [18/100] testing 30.3% loss=0.35134, acc=0.89062
# [18/100] testing 31.1% loss=0.43806, acc=0.76562
# [18/100] testing 31.6% loss=0.22437, acc=0.89062
# [18/100] testing 32.5% loss=0.27102, acc=0.89062
# [18/100] testing 32.9% loss=0.34515, acc=0.90625
# [18/100] testing 33.8% loss=0.35757, acc=0.84375
# [18/100] testing 34.7% loss=0.34590, acc=0.87500
# [18/100] testing 35.1% loss=0.24735, acc=0.92188
# [18/100] testing 36.0% loss=0.32081, acc=0.87500
# [18/100] testing 36.4% loss=0.29063, acc=0.84375
# [18/100] testing 37.3% loss=0.33902, acc=0.89062
# [18/100] testing 37.7% loss=0.49119, acc=0.81250
# [18/100] testing 38.6% loss=0.26966, acc=0.92188
# [18/100] testing 39.5% loss=0.31189, acc=0.89062
# [18/100] testing 39.9% loss=0.30939, acc=0.87500
# [18/100] testing 40.8% loss=0.30619, acc=0.90625
# [18/100] testing 41.2% loss=0.25028, acc=0.93750
# [18/100] testing 42.1% loss=0.28227, acc=0.89062
# [18/100] testing 42.5% loss=0.22473, acc=0.87500
# [18/100] testing 43.4% loss=0.32948, acc=0.87500
# [18/100] testing 43.9% loss=0.31210, acc=0.84375
# [18/100] testing 44.7% loss=0.36347, acc=0.89062
# [18/100] testing 45.6% loss=0.27548, acc=0.89062
# [18/100] testing 46.1% loss=0.26236, acc=0.90625
# [18/100] testing 46.9% loss=0.24990, acc=0.87500
# [18/100] testing 47.4% loss=0.17171, acc=0.93750
# [18/100] testing 48.3% loss=0.36765, acc=0.85938
# [18/100] testing 48.7% loss=0.45074, acc=0.84375
# [18/100] testing 49.6% loss=0.40388, acc=0.85938
# [18/100] testing 50.4% loss=0.22108, acc=0.89062
# [18/100] testing 50.9% loss=0.25242, acc=0.89062
# [18/100] testing 51.8% loss=0.26341, acc=0.85938
# [18/100] testing 52.2% loss=0.29045, acc=0.90625
# [18/100] testing 53.1% loss=0.21579, acc=0.89062
# [18/100] testing 53.5% loss=0.24257, acc=0.87500
# [18/100] testing 54.4% loss=0.37077, acc=0.81250
# [18/100] testing 54.8% loss=0.36027, acc=0.82812
# [18/100] testing 55.7% loss=0.17732, acc=0.93750
# [18/100] testing 56.6% loss=0.35459, acc=0.85938
# [18/100] testing 57.0% loss=0.41242, acc=0.82812
# [18/100] testing 57.9% loss=0.41225, acc=0.82812
# [18/100] testing 58.3% loss=0.45519, acc=0.78125
# [18/100] testing 59.2% loss=0.25787, acc=0.90625
# [18/100] testing 59.7% loss=0.24912, acc=0.84375
# [18/100] testing 60.5% loss=0.37339, acc=0.84375
# [18/100] testing 61.4% loss=0.14784, acc=0.90625
# [18/100] testing 61.9% loss=0.27338, acc=0.89062
# [18/100] testing 62.7% loss=0.23237, acc=0.89062
# [18/100] testing 63.2% loss=0.40271, acc=0.82812
# [18/100] testing 64.0% loss=0.37551, acc=0.81250
# [18/100] testing 64.5% loss=0.23910, acc=0.87500
# [18/100] testing 65.4% loss=0.23705, acc=0.89062
# [18/100] testing 65.8% loss=0.33743, acc=0.85938
# [18/100] testing 66.7% loss=0.21452, acc=0.92188
# [18/100] testing 67.6% loss=0.32073, acc=0.85938
# [18/100] testing 68.0% loss=0.22014, acc=0.90625
# [18/100] testing 68.9% loss=0.31288, acc=0.89062
# [18/100] testing 69.3% loss=0.32958, acc=0.82812
# [18/100] testing 70.2% loss=0.45033, acc=0.79688
# [18/100] testing 70.6% loss=0.28505, acc=0.85938
# [18/100] testing 71.5% loss=0.32862, acc=0.87500
# [18/100] testing 72.4% loss=0.26669, acc=0.89062
# [18/100] testing 72.8% loss=0.24894, acc=0.85938
# [18/100] testing 73.7% loss=0.21458, acc=0.93750
# [18/100] testing 74.1% loss=0.42630, acc=0.81250
# [18/100] testing 75.0% loss=0.22162, acc=0.93750
# [18/100] testing 75.4% loss=0.34495, acc=0.89062
# [18/100] testing 76.3% loss=0.15271, acc=0.92188
# [18/100] testing 76.8% loss=0.29672, acc=0.89062
# [18/100] testing 77.6% loss=0.34738, acc=0.82812
# [18/100] testing 78.5% loss=0.55147, acc=0.76562
# [18/100] testing 79.0% loss=0.27617, acc=0.85938
# [18/100] testing 79.8% loss=0.25641, acc=0.90625
# [18/100] testing 80.3% loss=0.25887, acc=0.89062
# [18/100] testing 81.2% loss=0.42125, acc=0.84375
# [18/100] testing 81.6% loss=0.26671, acc=0.89062
# [18/100] testing 82.5% loss=0.24699, acc=0.92188
# [18/100] testing 83.3% loss=0.22503, acc=0.93750
# [18/100] testing 83.8% loss=0.16428, acc=0.95312
# [18/100] testing 84.7% loss=0.31078, acc=0.87500
# [18/100] testing 85.1% loss=0.30820, acc=0.84375
# [18/100] testing 86.0% loss=0.29547, acc=0.89062
# [18/100] testing 86.4% loss=0.52726, acc=0.79688
# [18/100] testing 87.3% loss=0.35785, acc=0.81250
# [18/100] testing 87.7% loss=0.25595, acc=0.87500
# [18/100] testing 88.6% loss=0.24131, acc=0.89062
# [18/100] testing 89.5% loss=0.43719, acc=0.81250
# [18/100] testing 89.9% loss=0.23173, acc=0.90625
# [18/100] testing 90.8% loss=0.30399, acc=0.87500
# [18/100] testing 91.2% loss=0.12088, acc=0.96875
# [18/100] testing 92.1% loss=0.41332, acc=0.78125
# [18/100] testing 92.6% loss=0.30760, acc=0.85938
# [18/100] testing 93.4% loss=0.40561, acc=0.84375
# [18/100] testing 94.3% loss=0.17467, acc=0.89062
# [18/100] testing 94.7% loss=0.23881, acc=0.87500
# [18/100] testing 95.6% loss=0.33387, acc=0.82812
# [18/100] testing 96.1% loss=0.26758, acc=0.89062
# [18/100] testing 96.9% loss=0.26786, acc=0.92188
# [18/100] testing 97.4% loss=0.17490, acc=0.93750
# [18/100] testing 98.3% loss=0.29650, acc=0.87500
# [18/100] testing 98.7% loss=0.23822, acc=0.90625
# [18/100] testing 99.6% loss=0.24851, acc=0.89062
# [19/100] training 0.2% loss=0.42810, acc=0.84375
# [19/100] training 0.4% loss=0.42191, acc=0.79688
# [19/100] training 0.5% loss=0.21912, acc=0.95312
# [19/100] training 0.8% loss=0.31072, acc=0.87500
# [19/100] training 0.9% loss=0.28343, acc=0.85938
# [19/100] training 1.1% loss=0.24770, acc=0.92188
# [19/100] training 1.2% loss=0.33534, acc=0.81250
# [19/100] training 1.4% loss=0.29593, acc=0.90625
# [19/100] training 1.6% loss=0.17765, acc=0.90625
# [19/100] training 1.8% loss=0.28591, acc=0.89062
# [19/100] training 2.0% loss=0.33673, acc=0.87500
# [19/100] training 2.1% loss=0.31739, acc=0.82812
# [19/100] training 2.3% loss=0.25180, acc=0.90625
# [19/100] training 2.4% loss=0.33122, acc=0.90625
# [19/100] training 2.6% loss=0.22511, acc=0.90625
# [19/100] training 2.7% loss=0.25834, acc=0.92188
# [19/100] training 3.0% loss=0.26896, acc=0.92188
# [19/100] training 3.2% loss=0.26244, acc=0.89062
# [19/100] training 3.3% loss=0.39263, acc=0.82812
# [19/100] training 3.5% loss=0.27542, acc=0.89062
# [19/100] training 3.6% loss=0.39417, acc=0.84375
# [19/100] training 3.8% loss=0.22189, acc=0.84375
# [19/100] training 3.9% loss=0.24584, acc=0.89062
# [19/100] training 4.2% loss=0.21271, acc=0.90625
# [19/100] training 4.4% loss=0.24559, acc=0.92188
# [19/100] training 4.5% loss=0.21792, acc=0.90625
# [19/100] training 4.7% loss=0.30708, acc=0.84375
# [19/100] training 4.8% loss=0.24844, acc=0.92188
# [19/100] training 5.0% loss=0.19549, acc=0.92188
# [19/100] training 5.2% loss=0.25666, acc=0.89062
# [19/100] training 5.4% loss=0.22976, acc=0.90625
# [19/100] training 5.5% loss=0.24846, acc=0.89062
# [19/100] training 5.7% loss=0.21235, acc=0.89062
# [19/100] training 5.9% loss=0.43470, acc=0.84375
# [19/100] training 6.0% loss=0.33209, acc=0.85938
# [19/100] training 6.3% loss=0.34425, acc=0.84375
# [19/100] training 6.4% loss=0.17305, acc=0.92188
# [19/100] training 6.6% loss=0.26524, acc=0.92188
# [19/100] training 6.7% loss=0.36057, acc=0.85938
# [19/100] training 6.9% loss=0.23377, acc=0.93750
# [19/100] training 7.1% loss=0.30484, acc=0.84375
# [19/100] training 7.2% loss=0.30945, acc=0.90625
# [19/100] training 7.5% loss=0.22144, acc=0.93750
# [19/100] training 7.6% loss=0.29780, acc=0.87500
# [19/100] training 7.8% loss=0.30158, acc=0.90625
# [19/100] training 7.9% loss=0.30678, acc=0.84375
# [19/100] training 8.1% loss=0.17380, acc=0.95312
# [19/100] training 8.2% loss=0.25474, acc=0.89062
# [19/100] training 8.4% loss=0.35217, acc=0.85938
# [19/100] training 8.7% loss=0.27788, acc=0.87500
# [19/100] training 8.8% loss=0.31178, acc=0.84375
# [19/100] training 9.0% loss=0.28961, acc=0.84375
# [19/100] training 9.1% loss=0.23096, acc=0.95312
# [19/100] training 9.3% loss=0.48414, acc=0.87500
# [19/100] training 9.4% loss=0.18383, acc=0.92188
# [19/100] training 9.7% loss=0.27677, acc=0.92188
# [19/100] training 9.9% loss=0.31746, acc=0.89062
# [19/100] training 10.0% loss=0.31113, acc=0.84375
# [19/100] training 10.2% loss=0.29121, acc=0.87500
# [19/100] training 10.3% loss=0.28993, acc=0.85938
# [19/100] training 10.5% loss=0.42467, acc=0.89062
# [19/100] training 10.6% loss=0.22792, acc=0.87500
# [19/100] training 10.9% loss=0.20954, acc=0.92188
# [19/100] training 11.0% loss=0.34445, acc=0.82812
# [19/100] training 11.2% loss=0.18706, acc=0.95312
# [19/100] training 11.4% loss=0.31970, acc=0.87500
# [19/100] training 11.5% loss=0.43613, acc=0.85938
# [19/100] training 11.7% loss=0.18213, acc=0.96875
# [19/100] training 11.8% loss=0.26747, acc=0.87500
# [19/100] training 12.1% loss=0.29460, acc=0.89062
# [19/100] training 12.2% loss=0.24733, acc=0.84375
# [19/100] training 12.4% loss=0.26746, acc=0.92188
# [19/100] training 12.6% loss=0.22846, acc=0.93750
# [19/100] training 12.7% loss=0.24401, acc=0.90625
# [19/100] training 12.9% loss=0.28585, acc=0.92188
# [19/100] training 13.0% loss=0.22723, acc=0.90625
# [19/100] training 13.3% loss=0.25930, acc=0.89062
# [19/100] training 13.4% loss=0.22494, acc=0.89062
# [19/100] training 13.6% loss=0.21905, acc=0.92188
# [19/100] training 13.7% loss=0.35586, acc=0.85938
# [19/100] training 13.9% loss=0.27971, acc=0.89062
# [19/100] training 14.1% loss=0.35399, acc=0.90625
# [19/100] training 14.3% loss=0.18993, acc=0.92188
# [19/100] training 14.5% loss=0.30160, acc=0.89062
# [19/100] training 14.6% loss=0.18636, acc=0.95312
# [19/100] training 14.8% loss=0.26153, acc=0.89062
# [19/100] training 14.9% loss=0.31024, acc=0.82812
# [19/100] training 15.1% loss=0.43840, acc=0.82812
# [19/100] training 15.4% loss=0.29630, acc=0.82812
# [19/100] training 15.5% loss=0.27079, acc=0.90625
# [19/100] training 15.7% loss=0.43499, acc=0.81250
# [19/100] training 15.8% loss=0.24643, acc=0.90625
# [19/100] training 16.0% loss=0.36003, acc=0.89062
# [19/100] training 16.1% loss=0.31115, acc=0.90625
# [19/100] training 16.3% loss=0.30787, acc=0.82812
# [19/100] training 16.4% loss=0.26768, acc=0.89062
# [19/100] training 16.7% loss=0.36345, acc=0.82812
# [19/100] training 16.9% loss=0.40818, acc=0.84375
# [19/100] training 17.0% loss=0.27879, acc=0.92188
# [19/100] training 17.2% loss=0.27867, acc=0.90625
# [19/100] training 17.3% loss=0.22060, acc=0.93750
# [19/100] training 17.5% loss=0.29328, acc=0.90625
# [19/100] training 17.7% loss=0.23104, acc=0.92188
# [19/100] training 17.9% loss=0.31826, acc=0.87500
# [19/100] training 18.1% loss=0.35698, acc=0.84375
# [19/100] training 18.2% loss=0.31803, acc=0.87500
# [19/100] training 18.4% loss=0.39087, acc=0.84375
# [19/100] training 18.5% loss=0.25637, acc=0.90625
# [19/100] training 18.8% loss=0.20127, acc=0.92188
# [19/100] training 18.9% loss=0.15128, acc=0.96875
# [19/100] training 19.1% loss=0.31773, acc=0.85938
# [19/100] training 19.2% loss=0.24670, acc=0.90625
# [19/100] training 19.4% loss=0.19275, acc=0.90625
# [19/100] training 19.6% loss=0.31282, acc=0.85938
# [19/100] training 19.7% loss=0.34695, acc=0.84375
# [19/100] training 20.0% loss=0.18089, acc=0.93750
# [19/100] training 20.1% loss=0.28335, acc=0.87500
# [19/100] training 20.3% loss=0.30434, acc=0.89062
# [19/100] training 20.4% loss=0.35860, acc=0.82812
# [19/100] training 20.6% loss=0.38127, acc=0.85938
# [19/100] training 20.8% loss=0.21421, acc=0.93750
# [19/100] training 20.9% loss=0.28227, acc=0.85938
# [19/100] training 21.2% loss=0.35773, acc=0.84375
# [19/100] training 21.3% loss=0.31520, acc=0.85938
# [19/100] training 21.5% loss=0.29974, acc=0.87500
# [19/100] training 21.6% loss=0.14333, acc=0.96875
# [19/100] training 21.8% loss=0.25814, acc=0.87500
# [19/100] training 21.9% loss=0.26966, acc=0.87500
# [19/100] training 22.2% loss=0.24570, acc=0.87500
# [19/100] training 22.4% loss=0.32298, acc=0.89062
# [19/100] training 22.5% loss=0.26901, acc=0.93750
# [19/100] training 22.7% loss=0.25833, acc=0.92188
# [19/100] training 22.8% loss=0.41373, acc=0.89062
# [19/100] training 23.0% loss=0.18418, acc=0.93750
# [19/100] training 23.1% loss=0.34862, acc=0.82812
# [19/100] training 23.4% loss=0.34053, acc=0.85938
# [19/100] training 23.6% loss=0.33527, acc=0.82812
# [19/100] training 23.7% loss=0.32534, acc=0.85938
# [19/100] training 23.9% loss=0.20735, acc=0.92188
# [19/100] training 24.0% loss=0.26078, acc=0.87500
# [19/100] training 24.2% loss=0.29330, acc=0.85938
# [19/100] training 24.3% loss=0.35431, acc=0.85938
# [19/100] training 24.6% loss=0.22134, acc=0.89062
# [19/100] training 24.7% loss=0.37080, acc=0.87500
# [19/100] training 24.9% loss=0.19557, acc=0.92188
# [19/100] training 25.1% loss=0.40560, acc=0.84375
# [19/100] training 25.2% loss=0.16713, acc=0.93750
# [19/100] training 25.4% loss=0.30927, acc=0.84375
# [19/100] training 25.6% loss=0.19212, acc=0.96875
# [19/100] training 25.8% loss=0.33018, acc=0.87500
# [19/100] training 25.9% loss=0.22980, acc=0.87500
# [19/100] training 26.1% loss=0.22756, acc=0.92188
# [19/100] training 26.3% loss=0.25253, acc=0.90625
# [19/100] training 26.4% loss=0.18104, acc=0.93750
# [19/100] training 26.6% loss=0.17541, acc=0.90625
# [19/100] training 26.8% loss=0.33272, acc=0.89062
# [19/100] training 27.0% loss=0.29381, acc=0.90625
# [19/100] training 27.1% loss=0.21790, acc=0.92188
# [19/100] training 27.3% loss=0.26565, acc=0.85938
# [19/100] training 27.4% loss=0.17934, acc=0.93750
# [19/100] training 27.6% loss=0.27199, acc=0.90625
# [19/100] training 27.9% loss=0.21579, acc=0.89062
# [19/100] training 28.0% loss=0.35619, acc=0.82812
# [19/100] training 28.2% loss=0.20998, acc=0.93750
# [19/100] training 28.3% loss=0.14429, acc=0.96875
# [19/100] training 28.5% loss=0.27117, acc=0.87500
# [19/100] training 28.6% loss=0.26271, acc=0.89062
# [19/100] training 28.8% loss=0.14150, acc=0.93750
# [19/100] training 29.1% loss=0.26532, acc=0.87500
# [19/100] training 29.2% loss=0.26288, acc=0.89062
# [19/100] training 29.4% loss=0.32394, acc=0.82812
# [19/100] training 29.5% loss=0.18720, acc=0.92188
# [19/100] training 29.7% loss=0.25142, acc=0.89062
# [19/100] training 29.8% loss=0.14744, acc=0.93750
# [19/100] training 30.0% loss=0.29869, acc=0.84375
# [19/100] training 30.2% loss=0.25763, acc=0.87500
# [19/100] training 30.4% loss=0.20753, acc=0.92188
# [19/100] training 30.6% loss=0.35109, acc=0.89062
# [19/100] training 30.7% loss=0.20265, acc=0.90625
# [19/100] training 30.9% loss=0.39776, acc=0.87500
# [19/100] training 31.0% loss=0.21294, acc=0.92188
# [19/100] training 31.3% loss=0.27494, acc=0.89062
# [19/100] training 31.4% loss=0.41851, acc=0.78125
# [19/100] training 31.6% loss=0.31384, acc=0.85938
# [19/100] training 31.8% loss=0.23110, acc=0.87500
# [19/100] training 31.9% loss=0.31701, acc=0.89062
# [19/100] training 32.1% loss=0.28458, acc=0.84375
# [19/100] training 32.2% loss=0.27732, acc=0.90625
# [19/100] training 32.5% loss=0.20150, acc=0.95312
# [19/100] training 32.6% loss=0.33631, acc=0.84375
# [19/100] training 32.8% loss=0.19304, acc=0.93750
# [19/100] training 32.9% loss=0.35764, acc=0.87500
# [19/100] training 33.1% loss=0.30053, acc=0.89062
# [19/100] training 33.3% loss=0.23049, acc=0.87500
# [19/100] training 33.4% loss=0.29393, acc=0.89062
# [19/100] training 33.7% loss=0.32233, acc=0.84375
# [19/100] training 33.8% loss=0.26949, acc=0.87500
# [19/100] training 34.0% loss=0.25819, acc=0.89062
# [19/100] training 34.1% loss=0.27501, acc=0.90625
# [19/100] training 34.3% loss=0.26574, acc=0.87500
# [19/100] training 34.5% loss=0.34352, acc=0.89062
# [19/100] training 34.7% loss=0.21715, acc=0.89062
# [19/100] training 34.9% loss=0.22888, acc=0.89062
# [19/100] training 35.0% loss=0.18052, acc=0.93750
# [19/100] training 35.2% loss=0.36831, acc=0.81250
# [19/100] training 35.3% loss=0.23368, acc=0.93750
# [19/100] training 35.5% loss=0.25995, acc=0.89062
# [19/100] training 35.6% loss=0.35960, acc=0.84375
# [19/100] training 35.9% loss=0.27458, acc=0.87500
# [19/100] training 36.1% loss=0.28422, acc=0.87500
# [19/100] training 36.2% loss=0.29795, acc=0.87500
# [19/100] training 36.4% loss=0.37788, acc=0.87500
# [19/100] training 36.5% loss=0.23848, acc=0.93750
# [19/100] training 36.7% loss=0.26069, acc=0.90625
# [19/100] training 36.8% loss=0.16888, acc=0.92188
# [19/100] training 37.1% loss=0.26894, acc=0.93750
# [19/100] training 37.3% loss=0.31155, acc=0.89062
# [19/100] training 37.4% loss=0.25276, acc=0.87500
# [19/100] training 37.6% loss=0.21507, acc=0.93750
# [19/100] training 37.7% loss=0.26837, acc=0.89062
# [19/100] training 37.9% loss=0.17015, acc=0.92188
# [19/100] training 38.1% loss=0.40546, acc=0.82812
# [19/100] training 38.3% loss=0.23119, acc=0.92188
# [19/100] training 38.4% loss=0.14453, acc=0.96875
# [19/100] training 38.6% loss=0.20948, acc=0.93750
# [19/100] training 38.8% loss=0.30899, acc=0.84375
# [19/100] training 38.9% loss=0.28308, acc=0.92188
# [19/100] training 39.1% loss=0.20614, acc=0.92188
# [19/100] training 39.3% loss=0.27235, acc=0.87500
# [19/100] training 39.5% loss=0.32238, acc=0.85938
# [19/100] training 39.6% loss=0.33150, acc=0.89062
# [19/100] training 39.8% loss=0.15871, acc=0.96875
# [19/100] training 40.0% loss=0.26781, acc=0.85938
# [19/100] training 40.1% loss=0.22502, acc=0.87500
# [19/100] training 40.4% loss=0.21712, acc=0.85938
# [19/100] training 40.5% loss=0.23758, acc=0.89062
# [19/100] training 40.7% loss=0.25667, acc=0.89062
# [19/100] training 40.8% loss=0.27902, acc=0.84375
# [19/100] training 41.0% loss=0.24747, acc=0.92188
# [19/100] training 41.1% loss=0.35338, acc=0.87500
# [19/100] training 41.3% loss=0.46108, acc=0.75000
# [19/100] training 41.6% loss=0.29115, acc=0.92188
# [19/100] training 41.7% loss=0.36840, acc=0.84375
# [19/100] training 41.9% loss=0.21650, acc=0.92188
# [19/100] training 42.0% loss=0.28920, acc=0.87500
# [19/100] training 42.2% loss=0.25560, acc=0.85938
# [19/100] training 42.3% loss=0.25621, acc=0.89062
# [19/100] training 42.5% loss=0.24357, acc=0.85938
# [19/100] training 42.8% loss=0.15683, acc=0.95312
# [19/100] training 42.9% loss=0.19358, acc=0.90625
# [19/100] training 43.1% loss=0.24105, acc=0.93750
# [19/100] training 43.2% loss=0.16142, acc=0.95312
# [19/100] training 43.4% loss=0.26071, acc=0.90625
# [19/100] training 43.5% loss=0.19290, acc=0.87500
# [19/100] training 43.8% loss=0.30262, acc=0.87500
# [19/100] training 43.9% loss=0.21856, acc=0.90625
# [19/100] training 44.1% loss=0.21351, acc=0.92188
# [19/100] training 44.3% loss=0.30527, acc=0.89062
# [19/100] training 44.4% loss=0.40654, acc=0.82812
# [19/100] training 44.6% loss=0.28058, acc=0.81250
# [19/100] training 44.7% loss=0.39469, acc=0.81250
# [19/100] training 45.0% loss=0.21758, acc=0.92188
# [19/100] training 45.1% loss=0.41034, acc=0.85938
# [19/100] training 45.3% loss=0.21001, acc=0.93750
# [19/100] training 45.5% loss=0.18034, acc=0.92188
# [19/100] training 45.6% loss=0.26942, acc=0.87500
# [19/100] training 45.8% loss=0.17182, acc=0.93750
# [19/100] training 45.9% loss=0.19314, acc=0.85938
# [19/100] training 46.2% loss=0.20441, acc=0.92188
# [19/100] training 46.3% loss=0.16230, acc=0.95312
# [19/100] training 46.5% loss=0.39865, acc=0.82812
# [19/100] training 46.6% loss=0.17143, acc=0.93750
# [19/100] training 46.8% loss=0.20980, acc=0.90625
# [19/100] training 47.0% loss=0.33479, acc=0.85938
# [19/100] training 47.2% loss=0.30731, acc=0.82812
# [19/100] training 47.4% loss=0.19885, acc=0.93750
# [19/100] training 47.5% loss=0.35933, acc=0.87500
# [19/100] training 47.7% loss=0.25981, acc=0.89062
# [19/100] training 47.8% loss=0.27504, acc=0.87500
# [19/100] training 48.0% loss=0.27350, acc=0.87500
# [19/100] training 48.3% loss=0.11851, acc=0.95312
# [19/100] training 48.4% loss=0.13627, acc=0.93750
# [19/100] training 48.6% loss=0.16637, acc=0.92188
# [19/100] training 48.7% loss=0.33431, acc=0.84375
# [19/100] training 48.9% loss=0.27528, acc=0.89062
# [19/100] training 49.0% loss=0.22480, acc=0.90625
# [19/100] training 49.2% loss=0.26171, acc=0.82812
# [19/100] training 49.3% loss=0.21056, acc=0.92188
# [19/100] training 49.6% loss=0.28745, acc=0.92188
# [19/100] training 49.8% loss=0.34455, acc=0.84375
# [19/100] training 49.9% loss=0.28766, acc=0.85938
# [19/100] training 50.1% loss=0.20489, acc=0.89062
# [19/100] training 50.2% loss=0.31811, acc=0.87500
# [19/100] training 50.4% loss=0.39701, acc=0.84375
# [19/100] training 50.6% loss=0.29917, acc=0.85938
# [19/100] training 50.8% loss=0.43497, acc=0.81250
# [19/100] training 51.0% loss=0.35701, acc=0.81250
# [19/100] training 51.1% loss=0.36008, acc=0.84375
# [19/100] training 51.3% loss=0.27845, acc=0.90625
# [19/100] training 51.4% loss=0.33791, acc=0.89062
# [19/100] training 51.7% loss=0.30717, acc=0.84375
# [19/100] training 51.8% loss=0.33432, acc=0.82812
# [19/100] training 52.0% loss=0.19722, acc=0.93750
# [19/100] training 52.1% loss=0.32417, acc=0.85938
# [19/100] training 52.3% loss=0.25299, acc=0.87500
# [19/100] training 52.5% loss=0.20927, acc=0.90625
# [19/100] training 52.6% loss=0.29563, acc=0.89062
# [19/100] training 52.9% loss=0.37972, acc=0.87500
# [19/100] training 53.0% loss=0.19462, acc=0.89062
# [19/100] training 53.2% loss=0.26317, acc=0.90625
# [19/100] training 53.3% loss=0.16018, acc=0.95312
# [19/100] training 53.5% loss=0.25009, acc=0.89062
# [19/100] training 53.7% loss=0.16908, acc=0.95312
# [19/100] training 53.8% loss=0.39725, acc=0.82812
# [19/100] training 54.1% loss=0.29475, acc=0.87500
# [19/100] training 54.2% loss=0.20378, acc=0.92188
# [19/100] training 54.4% loss=0.30814, acc=0.90625
# [19/100] training 54.5% loss=0.28418, acc=0.87500
# [19/100] training 54.7% loss=0.33658, acc=0.82812
# [19/100] training 54.8% loss=0.21427, acc=0.87500
# [19/100] training 55.1% loss=0.21281, acc=0.90625
# [19/100] training 55.3% loss=0.16087, acc=0.93750
# [19/100] training 55.4% loss=0.24794, acc=0.89062
# [19/100] training 55.6% loss=0.26170, acc=0.87500
# [19/100] training 55.7% loss=0.32881, acc=0.84375
# [19/100] training 55.9% loss=0.21116, acc=0.90625
# [19/100] training 56.0% loss=0.28796, acc=0.85938
# [19/100] training 56.3% loss=0.44235, acc=0.81250
# [19/100] training 56.5% loss=0.24804, acc=0.92188
# [19/100] training 56.6% loss=0.27555, acc=0.85938
# [19/100] training 56.8% loss=0.31771, acc=0.89062
# [19/100] training 56.9% loss=0.28947, acc=0.89062
# [19/100] training 57.1% loss=0.33375, acc=0.89062
# [19/100] training 57.2% loss=0.14237, acc=0.95312
# [19/100] training 57.5% loss=0.17363, acc=0.92188
# [19/100] training 57.6% loss=0.34252, acc=0.81250
# [19/100] training 57.8% loss=0.22303, acc=0.89062
# [19/100] training 58.0% loss=0.20582, acc=0.95312
# [19/100] training 58.1% loss=0.32580, acc=0.87500
# [19/100] training 58.3% loss=0.23235, acc=0.92188
# [19/100] training 58.4% loss=0.31486, acc=0.90625
# [19/100] training 58.7% loss=0.27392, acc=0.90625
# [19/100] training 58.8% loss=0.28321, acc=0.82812
# [19/100] training 59.0% loss=0.24153, acc=0.93750
# [19/100] training 59.2% loss=0.26506, acc=0.89062
# [19/100] training 59.3% loss=0.18638, acc=0.90625
# [19/100] training 59.5% loss=0.22065, acc=0.90625
# [19/100] training 59.7% loss=0.23545, acc=0.89062
# [19/100] training 59.9% loss=0.16184, acc=0.96875
# [19/100] training 60.0% loss=0.24573, acc=0.90625
# [19/100] training 60.2% loss=0.22608, acc=0.87500
# [19/100] training 60.3% loss=0.19528, acc=0.93750
# [19/100] training 60.5% loss=0.22220, acc=0.90625
# [19/100] training 60.8% loss=0.31523, acc=0.89062
# [19/100] training 60.9% loss=0.33075, acc=0.87500
# [19/100] training 61.1% loss=0.33549, acc=0.85938
# [19/100] training 61.2% loss=0.35270, acc=0.87500
# [19/100] training 61.4% loss=0.27381, acc=0.85938
# [19/100] training 61.5% loss=0.47297, acc=0.73438
# [19/100] training 61.7% loss=0.22955, acc=0.92188
# [19/100] training 62.0% loss=0.32331, acc=0.84375
# [19/100] training 62.1% loss=0.35699, acc=0.85938
# [19/100] training 62.3% loss=0.18783, acc=0.92188
# [19/100] training 62.4% loss=0.24542, acc=0.92188
# [19/100] training 62.6% loss=0.35927, acc=0.84375
# [19/100] training 62.7% loss=0.27771, acc=0.87500
# [19/100] training 62.9% loss=0.33332, acc=0.85938
# [19/100] training 63.1% loss=0.22852, acc=0.90625
# [19/100] training 63.3% loss=0.22440, acc=0.90625
# [19/100] training 63.5% loss=0.28126, acc=0.89062
# [19/100] training 63.6% loss=0.27324, acc=0.92188
# [19/100] training 63.8% loss=0.32235, acc=0.82812
# [19/100] training 63.9% loss=0.20787, acc=0.98438
# [19/100] training 64.2% loss=0.20387, acc=0.89062
# [19/100] training 64.3% loss=0.31495, acc=0.82812
# [19/100] training 64.5% loss=0.40127, acc=0.82812
# [19/100] training 64.7% loss=0.25541, acc=0.87500
# [19/100] training 64.8% loss=0.43069, acc=0.84375
# [19/100] training 65.0% loss=0.30355, acc=0.87500
# [19/100] training 65.1% loss=0.35827, acc=0.84375
# [19/100] training 65.4% loss=0.19176, acc=0.95312
# [19/100] training 65.5% loss=0.21694, acc=0.93750
# [19/100] training 65.7% loss=0.15653, acc=0.92188
# [19/100] training 65.8% loss=0.25402, acc=0.87500
# [19/100] training 66.0% loss=0.25796, acc=0.90625
# [19/100] training 66.2% loss=0.13074, acc=0.96875
# [19/100] training 66.3% loss=0.37181, acc=0.85938
# [19/100] training 66.6% loss=0.21745, acc=0.93750
# [19/100] training 66.7% loss=0.13232, acc=0.95312
# [19/100] training 66.9% loss=0.29086, acc=0.87500
# [19/100] training 67.0% loss=0.26899, acc=0.87500
# [19/100] training 67.2% loss=0.18329, acc=0.90625
# [19/100] training 67.4% loss=0.25809, acc=0.87500
# [19/100] training 67.6% loss=0.26830, acc=0.90625
# [19/100] training 67.8% loss=0.28551, acc=0.87500
# [19/100] training 67.9% loss=0.19272, acc=0.93750
# [19/100] training 68.1% loss=0.21489, acc=0.87500
# [19/100] training 68.2% loss=0.21536, acc=0.89062
# [19/100] training 68.4% loss=0.15848, acc=0.93750
# [19/100] training 68.5% loss=0.30510, acc=0.81250
# [19/100] training 68.8% loss=0.36433, acc=0.84375
# [19/100] training 69.0% loss=0.34555, acc=0.89062
# [19/100] training 69.1% loss=0.20595, acc=0.90625
# [19/100] training 69.3% loss=0.23762, acc=0.87500
# [19/100] training 69.4% loss=0.26704, acc=0.85938
# [19/100] training 69.6% loss=0.39554, acc=0.76562
# [19/100] training 69.7% loss=0.27027, acc=0.85938
# [19/100] training 70.0% loss=0.33849, acc=0.85938
# [19/100] training 70.2% loss=0.40069, acc=0.87500
# [19/100] training 70.3% loss=0.24222, acc=0.93750
# [19/100] training 70.5% loss=0.28459, acc=0.87500
# [19/100] training 70.6% loss=0.14131, acc=0.93750
# [19/100] training 70.8% loss=0.26028, acc=0.90625
# [19/100] training 71.0% loss=0.25773, acc=0.92188
# [19/100] training 71.2% loss=0.22135, acc=0.92188
# [19/100] training 71.3% loss=0.21982, acc=0.92188
# [19/100] training 71.5% loss=0.31451, acc=0.92188
# [19/100] training 71.7% loss=0.31471, acc=0.87500
# [19/100] training 71.8% loss=0.32935, acc=0.87500
# [19/100] training 72.0% loss=0.24529, acc=0.89062
# [19/100] training 72.2% loss=0.24146, acc=0.87500
# [19/100] training 72.4% loss=0.27811, acc=0.87500
# [19/100] training 72.5% loss=0.43058, acc=0.87500
# [19/100] training 72.7% loss=0.37200, acc=0.81250
# [19/100] training 72.9% loss=0.24141, acc=0.89062
# [19/100] training 73.0% loss=0.18145, acc=0.95312
# [19/100] training 73.3% loss=0.28587, acc=0.87500
# [19/100] training 73.4% loss=0.14865, acc=0.92188
# [19/100] training 73.6% loss=0.18481, acc=0.92188
# [19/100] training 73.7% loss=0.21081, acc=0.90625
# [19/100] training 73.9% loss=0.24486, acc=0.85938
# [19/100] training 74.0% loss=0.23515, acc=0.90625
# [19/100] training 74.2% loss=0.19182, acc=0.93750
# [19/100] training 74.5% loss=0.20347, acc=0.92188
# [19/100] training 74.6% loss=0.41518, acc=0.81250
# [19/100] training 74.8% loss=0.48725, acc=0.81250
# [19/100] training 74.9% loss=0.53101, acc=0.73438
# [19/100] training 75.1% loss=0.25298, acc=0.89062
# [19/100] training 75.2% loss=0.24352, acc=0.90625
# [19/100] training 75.4% loss=0.27686, acc=0.90625
# [19/100] training 75.7% loss=0.33980, acc=0.85938
# [19/100] training 75.8% loss=0.35639, acc=0.79688
# [19/100] training 76.0% loss=0.14475, acc=0.93750
# [19/100] training 76.1% loss=0.35060, acc=0.89062
# [19/100] training 76.3% loss=0.16216, acc=0.95312
# [19/100] training 76.4% loss=0.17712, acc=0.92188
# [19/100] training 76.7% loss=0.17793, acc=0.93750
# [19/100] training 76.8% loss=0.20041, acc=0.93750
# [19/100] training 77.0% loss=0.17406, acc=0.92188
# [19/100] training 77.2% loss=0.33326, acc=0.89062
# [19/100] training 77.3% loss=0.12245, acc=0.95312
# [19/100] training 77.5% loss=0.28419, acc=0.87500
# [19/100] training 77.6% loss=0.20643, acc=0.90625
# [19/100] training 77.9% loss=0.19195, acc=0.90625
# [19/100] training 78.0% loss=0.35659, acc=0.82812
# [19/100] training 78.2% loss=0.34896, acc=0.87500
# [19/100] training 78.4% loss=0.14480, acc=0.95312
# [19/100] training 78.5% loss=0.42721, acc=0.85938
# [19/100] training 78.7% loss=0.23658, acc=0.85938
# [19/100] training 78.8% loss=0.14236, acc=0.95312
# [19/100] training 79.1% loss=0.14083, acc=0.96875
# [19/100] training 79.2% loss=0.17881, acc=0.95312
# [19/100] training 79.4% loss=0.31389, acc=0.81250
# [19/100] training 79.5% loss=0.28073, acc=0.89062
# [19/100] training 79.7% loss=0.08976, acc=0.96875
# [19/100] training 79.9% loss=0.19526, acc=0.92188
# [19/100] training 80.1% loss=0.26271, acc=0.85938
# [19/100] training 80.3% loss=0.36316, acc=0.84375
# [19/100] training 80.4% loss=0.24641, acc=0.89062
# [19/100] training 80.6% loss=0.21085, acc=0.90625
# [19/100] training 80.7% loss=0.22577, acc=0.90625
# [19/100] training 80.9% loss=0.31707, acc=0.85938
# [19/100] training 81.2% loss=0.28465, acc=0.89062
# [19/100] training 81.3% loss=0.27021, acc=0.92188
# [19/100] training 81.5% loss=0.27855, acc=0.90625
# [19/100] training 81.6% loss=0.33162, acc=0.84375
# [19/100] training 81.8% loss=0.20878, acc=0.90625
# [19/100] training 81.9% loss=0.41156, acc=0.87500
# [19/100] training 82.1% loss=0.16662, acc=0.93750
# [19/100] training 82.2% loss=0.25863, acc=0.92188
# [19/100] training 82.5% loss=0.21497, acc=0.93750
# [19/100] training 82.7% loss=0.32401, acc=0.85938
# [19/100] training 82.8% loss=0.30650, acc=0.90625
# [19/100] training 83.0% loss=0.22985, acc=0.89062
# [19/100] training 83.1% loss=0.26555, acc=0.92188
# [19/100] training 83.3% loss=0.19670, acc=0.90625
# [19/100] training 83.5% loss=0.22568, acc=0.93750
# [19/100] training 83.7% loss=0.37121, acc=0.85938
# [19/100] training 83.9% loss=0.22155, acc=0.90625
# [19/100] training 84.0% loss=0.15967, acc=0.95312
# [19/100] training 84.2% loss=0.15453, acc=0.95312
# [19/100] training 84.3% loss=0.27764, acc=0.89062
# [19/100] training 84.5% loss=0.19053, acc=0.92188
# [19/100] training 84.7% loss=0.26261, acc=0.90625
# [19/100] training 84.9% loss=0.23156, acc=0.92188
# [19/100] training 85.0% loss=0.24231, acc=0.92188
# [19/100] training 85.2% loss=0.28991, acc=0.89062
# [19/100] training 85.4% loss=0.32354, acc=0.93750
# [19/100] training 85.5% loss=0.25712, acc=0.87500
# [19/100] training 85.8% loss=0.23507, acc=0.87500
# [19/100] training 85.9% loss=0.29690, acc=0.89062
# [19/100] training 86.1% loss=0.28776, acc=0.85938
# [19/100] training 86.2% loss=0.21470, acc=0.92188
# [19/100] training 86.4% loss=0.36014, acc=0.81250
# [19/100] training 86.6% loss=0.30543, acc=0.87500
# [19/100] training 86.7% loss=0.15551, acc=0.96875
# [19/100] training 87.0% loss=0.32671, acc=0.87500
# [19/100] training 87.1% loss=0.24193, acc=0.93750
# [19/100] training 87.3% loss=0.22217, acc=0.87500
# [19/100] training 87.4% loss=0.29278, acc=0.87500
# [19/100] training 87.6% loss=0.28257, acc=0.89062
# [19/100] training 87.7% loss=0.32531, acc=0.95312
# [19/100] training 87.9% loss=0.29875, acc=0.87500
# [19/100] training 88.2% loss=0.18518, acc=0.90625
# [19/100] training 88.3% loss=0.25669, acc=0.90625
# [19/100] training 88.5% loss=0.24988, acc=0.87500
# [19/100] training 88.6% loss=0.16395, acc=0.93750
# [19/100] training 88.8% loss=0.24069, acc=0.92188
# [19/100] training 88.9% loss=0.29276, acc=0.89062
# [19/100] training 89.2% loss=0.20078, acc=0.90625
# [19/100] training 89.4% loss=0.18074, acc=0.92188
# [19/100] training 89.5% loss=0.29110, acc=0.87500
# [19/100] training 89.7% loss=0.20955, acc=0.87500
# [19/100] training 89.8% loss=0.28124, acc=0.89062
# [19/100] training 90.0% loss=0.21195, acc=0.87500
# [19/100] training 90.1% loss=0.29944, acc=0.85938
# [19/100] training 90.4% loss=0.23492, acc=0.90625
# [19/100] training 90.5% loss=0.13841, acc=0.93750
# [19/100] training 90.7% loss=0.27569, acc=0.92188
# [19/100] training 90.9% loss=0.14552, acc=0.95312
# [19/100] training 91.0% loss=0.25792, acc=0.87500
# [19/100] training 91.2% loss=0.32231, acc=0.87500
# [19/100] training 91.3% loss=0.34785, acc=0.85938
# [19/100] training 91.6% loss=0.42950, acc=0.84375
# [19/100] training 91.7% loss=0.25563, acc=0.84375
# [19/100] training 91.9% loss=0.33144, acc=0.85938
# [19/100] training 92.1% loss=0.26350, acc=0.87500
# [19/100] training 92.2% loss=0.17322, acc=0.96875
# [19/100] training 92.4% loss=0.15548, acc=0.93750
# [19/100] training 92.6% loss=0.30066, acc=0.89062
# [19/100] training 92.8% loss=0.32403, acc=0.85938
# [19/100] training 92.9% loss=0.26098, acc=0.90625
# [19/100] training 93.1% loss=0.34183, acc=0.85938
# [19/100] training 93.2% loss=0.28062, acc=0.89062
# [19/100] training 93.4% loss=0.26416, acc=0.90625
# [19/100] training 93.7% loss=0.21024, acc=0.93750
# [19/100] training 93.8% loss=0.27741, acc=0.87500
# [19/100] training 94.0% loss=0.21884, acc=0.92188
# [19/100] training 94.1% loss=0.28148, acc=0.89062
# [19/100] training 94.3% loss=0.18281, acc=0.90625
# [19/100] training 94.4% loss=0.33338, acc=0.89062
# [19/100] training 94.6% loss=0.11412, acc=0.96875
# [19/100] training 94.9% loss=0.22720, acc=0.84375
# [19/100] training 95.0% loss=0.36174, acc=0.87500
# [19/100] training 95.2% loss=0.32511, acc=0.85938
# [19/100] training 95.3% loss=0.26296, acc=0.92188
# [19/100] training 95.5% loss=0.25063, acc=0.89062
# [19/100] training 95.6% loss=0.29419, acc=0.87500
# [19/100] training 95.8% loss=0.26592, acc=0.90625
# [19/100] training 96.0% loss=0.28461, acc=0.90625
# [19/100] training 96.2% loss=0.16425, acc=0.95312
# [19/100] training 96.4% loss=0.21706, acc=0.92188
# [19/100] training 96.5% loss=0.24352, acc=0.90625
# [19/100] training 96.7% loss=0.21686, acc=0.92188
# [19/100] training 96.8% loss=0.25680, acc=0.93750
# [19/100] training 97.1% loss=0.25807, acc=0.84375
# [19/100] training 97.2% loss=0.23105, acc=0.87500
# [19/100] training 97.4% loss=0.24373, acc=0.90625
# [19/100] training 97.6% loss=0.41690, acc=0.84375
# [19/100] training 97.7% loss=0.24411, acc=0.89062
# [19/100] training 97.9% loss=0.17967, acc=0.89062
# [19/100] training 98.0% loss=0.28780, acc=0.85938
# [19/100] training 98.3% loss=0.29905, acc=0.87500
# [19/100] training 98.4% loss=0.26760, acc=0.87500
# [19/100] training 98.6% loss=0.44209, acc=0.78125
# [19/100] training 98.7% loss=0.26270, acc=0.89062
# [19/100] training 98.9% loss=0.24455, acc=0.93750
# [19/100] training 99.1% loss=0.22488, acc=0.95312
# [19/100] training 99.2% loss=0.23024, acc=0.89062
# [19/100] training 99.5% loss=0.40194, acc=0.78125
# [19/100] training 99.6% loss=0.35757, acc=0.85938
# [19/100] training 99.8% loss=0.19021, acc=0.92188
# [19/100] training 99.9% loss=0.15582, acc=0.93750
# [19/100] testing 0.9% loss=0.18045, acc=0.95312
# [19/100] testing 1.8% loss=0.34906, acc=0.82812
# [19/100] testing 2.2% loss=0.26595, acc=0.87500
# [19/100] testing 3.1% loss=0.29282, acc=0.85938
# [19/100] testing 3.5% loss=0.18252, acc=0.90625
# [19/100] testing 4.4% loss=0.28248, acc=0.87500
# [19/100] testing 4.8% loss=0.39628, acc=0.82812
# [19/100] testing 5.7% loss=0.26506, acc=0.89062
# [19/100] testing 6.6% loss=0.26702, acc=0.87500
# [19/100] testing 7.0% loss=0.21520, acc=0.92188
# [19/100] testing 7.9% loss=0.39159, acc=0.82812
# [19/100] testing 8.3% loss=0.31332, acc=0.84375
# [19/100] testing 9.2% loss=0.27443, acc=0.89062
# [19/100] testing 9.7% loss=0.17670, acc=0.92188
# [19/100] testing 10.5% loss=0.26092, acc=0.90625
# [19/100] testing 11.0% loss=0.22476, acc=0.89062
# [19/100] testing 11.8% loss=0.33175, acc=0.84375
# [19/100] testing 12.7% loss=0.34675, acc=0.85938
# [19/100] testing 13.2% loss=0.26309, acc=0.87500
# [19/100] testing 14.0% loss=0.36303, acc=0.87500
# [19/100] testing 14.5% loss=0.27718, acc=0.89062
# [19/100] testing 15.4% loss=0.33732, acc=0.87500
# [19/100] testing 15.8% loss=0.24272, acc=0.87500
# [19/100] testing 16.7% loss=0.26143, acc=0.89062
# [19/100] testing 17.5% loss=0.29688, acc=0.85938
# [19/100] testing 18.0% loss=0.26512, acc=0.87500
# [19/100] testing 18.9% loss=0.17924, acc=0.96875
# [19/100] testing 19.3% loss=0.31674, acc=0.87500
# [19/100] testing 20.2% loss=0.31498, acc=0.90625
# [19/100] testing 20.6% loss=0.38993, acc=0.81250
# [19/100] testing 21.5% loss=0.28251, acc=0.82812
# [19/100] testing 21.9% loss=0.39427, acc=0.84375
# [19/100] testing 22.8% loss=0.42008, acc=0.85938
# [19/100] testing 23.7% loss=0.27414, acc=0.87500
# [19/100] testing 24.1% loss=0.18099, acc=0.95312
# [19/100] testing 25.0% loss=0.37687, acc=0.92188
# [19/100] testing 25.4% loss=0.13703, acc=0.96875
# [19/100] testing 26.3% loss=0.37379, acc=0.82812
# [19/100] testing 26.8% loss=0.23473, acc=0.92188
# [19/100] testing 27.6% loss=0.27955, acc=0.90625
# [19/100] testing 28.5% loss=0.27381, acc=0.90625
# [19/100] testing 29.0% loss=0.19389, acc=0.93750
# [19/100] testing 29.8% loss=0.42289, acc=0.81250
# [19/100] testing 30.3% loss=0.25953, acc=0.89062
# [19/100] testing 31.1% loss=0.38023, acc=0.85938
# [19/100] testing 31.6% loss=0.29202, acc=0.90625
# [19/100] testing 32.5% loss=0.21774, acc=0.92188
# [19/100] testing 32.9% loss=0.37672, acc=0.82812
# [19/100] testing 33.8% loss=0.33968, acc=0.81250
# [19/100] testing 34.7% loss=0.34681, acc=0.89062
# [19/100] testing 35.1% loss=0.30539, acc=0.84375
# [19/100] testing 36.0% loss=0.25680, acc=0.89062
# [19/100] testing 36.4% loss=0.28472, acc=0.85938
# [19/100] testing 37.3% loss=0.36716, acc=0.85938
# [19/100] testing 37.7% loss=0.42910, acc=0.84375
# [19/100] testing 38.6% loss=0.21091, acc=0.90625
# [19/100] testing 39.5% loss=0.26383, acc=0.89062
# [19/100] testing 39.9% loss=0.28631, acc=0.89062
# [19/100] testing 40.8% loss=0.25009, acc=0.92188
# [19/100] testing 41.2% loss=0.25304, acc=0.92188
# [19/100] testing 42.1% loss=0.32029, acc=0.87500
# [19/100] testing 42.5% loss=0.22720, acc=0.87500
# [19/100] testing 43.4% loss=0.27436, acc=0.87500
# [19/100] testing 43.9% loss=0.20118, acc=0.90625
# [19/100] testing 44.7% loss=0.33799, acc=0.89062
# [19/100] testing 45.6% loss=0.29950, acc=0.87500
# [19/100] testing 46.1% loss=0.32224, acc=0.84375
# [19/100] testing 46.9% loss=0.29226, acc=0.87500
# [19/100] testing 47.4% loss=0.13863, acc=0.93750
# [19/100] testing 48.3% loss=0.32917, acc=0.85938
# [19/100] testing 48.7% loss=0.37631, acc=0.82812
# [19/100] testing 49.6% loss=0.36662, acc=0.85938
# [19/100] testing 50.4% loss=0.21309, acc=0.92188
# [19/100] testing 50.9% loss=0.37508, acc=0.82812
# [19/100] testing 51.8% loss=0.23683, acc=0.89062
# [19/100] testing 52.2% loss=0.35849, acc=0.90625
# [19/100] testing 53.1% loss=0.21305, acc=0.89062
# [19/100] testing 53.5% loss=0.20997, acc=0.90625
# [19/100] testing 54.4% loss=0.41115, acc=0.75000
# [19/100] testing 54.8% loss=0.37861, acc=0.82812
# [19/100] testing 55.7% loss=0.25289, acc=0.92188
# [19/100] testing 56.6% loss=0.30532, acc=0.89062
# [19/100] testing 57.0% loss=0.40742, acc=0.82812
# [19/100] testing 57.9% loss=0.31123, acc=0.84375
# [19/100] testing 58.3% loss=0.31562, acc=0.84375
# [19/100] testing 59.2% loss=0.29755, acc=0.89062
# [19/100] testing 59.7% loss=0.29057, acc=0.84375
# [19/100] testing 60.5% loss=0.35845, acc=0.85938
# [19/100] testing 61.4% loss=0.14031, acc=0.95312
# [19/100] testing 61.9% loss=0.29608, acc=0.89062
# [19/100] testing 62.7% loss=0.18751, acc=0.92188
# [19/100] testing 63.2% loss=0.34028, acc=0.82812
# [19/100] testing 64.0% loss=0.30221, acc=0.85938
# [19/100] testing 64.5% loss=0.22156, acc=0.89062
# [19/100] testing 65.4% loss=0.22450, acc=0.92188
# [19/100] testing 65.8% loss=0.34269, acc=0.82812
# [19/100] testing 66.7% loss=0.26144, acc=0.87500
# [19/100] testing 67.6% loss=0.31729, acc=0.89062
# [19/100] testing 68.0% loss=0.14246, acc=0.95312
# [19/100] testing 68.9% loss=0.25117, acc=0.89062
# [19/100] testing 69.3% loss=0.34083, acc=0.81250
# [19/100] testing 70.2% loss=0.31209, acc=0.89062
# [19/100] testing 70.6% loss=0.33924, acc=0.84375
# [19/100] testing 71.5% loss=0.29730, acc=0.87500
# [19/100] testing 72.4% loss=0.17770, acc=0.93750
# [19/100] testing 72.8% loss=0.25531, acc=0.87500
# [19/100] testing 73.7% loss=0.18114, acc=0.92188
# [19/100] testing 74.1% loss=0.35854, acc=0.89062
# [19/100] testing 75.0% loss=0.24808, acc=0.87500
# [19/100] testing 75.4% loss=0.37215, acc=0.85938
# [19/100] testing 76.3% loss=0.19575, acc=0.89062
# [19/100] testing 76.8% loss=0.32041, acc=0.84375
# [19/100] testing 77.6% loss=0.29797, acc=0.85938
# [19/100] testing 78.5% loss=0.51504, acc=0.78125
# [19/100] testing 79.0% loss=0.29675, acc=0.87500
# [19/100] testing 79.8% loss=0.33807, acc=0.82812
# [19/100] testing 80.3% loss=0.24781, acc=0.89062
# [19/100] testing 81.2% loss=0.37589, acc=0.82812
# [19/100] testing 81.6% loss=0.26627, acc=0.90625
# [19/100] testing 82.5% loss=0.21164, acc=0.89062
# [19/100] testing 83.3% loss=0.25879, acc=0.93750
# [19/100] testing 83.8% loss=0.18094, acc=0.95312
# [19/100] testing 84.7% loss=0.37911, acc=0.81250
# [19/100] testing 85.1% loss=0.27455, acc=0.85938
# [19/100] testing 86.0% loss=0.29983, acc=0.85938
# [19/100] testing 86.4% loss=0.48422, acc=0.81250
# [19/100] testing 87.3% loss=0.24010, acc=0.90625
# [19/100] testing 87.7% loss=0.28124, acc=0.89062
# [19/100] testing 88.6% loss=0.27861, acc=0.89062
# [19/100] testing 89.5% loss=0.37443, acc=0.75000
# [19/100] testing 89.9% loss=0.25468, acc=0.89062
# [19/100] testing 90.8% loss=0.29988, acc=0.87500
# [19/100] testing 91.2% loss=0.11434, acc=0.96875
# [19/100] testing 92.1% loss=0.32248, acc=0.81250
# [19/100] testing 92.6% loss=0.26213, acc=0.90625
# [19/100] testing 93.4% loss=0.36060, acc=0.85938
# [19/100] testing 94.3% loss=0.19495, acc=0.92188
# [19/100] testing 94.7% loss=0.26677, acc=0.90625
# [19/100] testing 95.6% loss=0.28214, acc=0.85938
# [19/100] testing 96.1% loss=0.18831, acc=0.90625
# [19/100] testing 96.9% loss=0.25993, acc=0.89062
# [19/100] testing 97.4% loss=0.21826, acc=0.95312
# [19/100] testing 98.3% loss=0.26282, acc=0.85938
# [19/100] testing 98.7% loss=0.18117, acc=0.90625
# [19/100] testing 99.6% loss=0.22835, acc=0.89062
# [20/100] training 0.2% loss=0.33829, acc=0.82812
# [20/100] training 0.4% loss=0.38933, acc=0.85938
# [20/100] training 0.5% loss=0.21875, acc=0.90625
# [20/100] training 0.8% loss=0.28620, acc=0.87500
# [20/100] training 0.9% loss=0.24601, acc=0.89062
# [20/100] training 1.1% loss=0.27858, acc=0.92188
# [20/100] training 1.2% loss=0.30382, acc=0.87500
# [20/100] training 1.4% loss=0.32217, acc=0.85938
# [20/100] training 1.6% loss=0.18007, acc=0.92188
# [20/100] training 1.8% loss=0.27945, acc=0.87500
# [20/100] training 2.0% loss=0.35727, acc=0.85938
# [20/100] training 2.1% loss=0.30728, acc=0.82812
# [20/100] training 2.3% loss=0.24199, acc=0.89062
# [20/100] training 2.4% loss=0.26226, acc=0.89062
# [20/100] training 2.6% loss=0.20188, acc=0.90625
# [20/100] training 2.7% loss=0.27668, acc=0.90625
# [20/100] training 3.0% loss=0.28848, acc=0.84375
# [20/100] training 3.2% loss=0.28599, acc=0.87500
# [20/100] training 3.3% loss=0.44098, acc=0.84375
# [20/100] training 3.5% loss=0.30513, acc=0.89062
# [20/100] training 3.6% loss=0.29939, acc=0.85938
# [20/100] training 3.8% loss=0.25793, acc=0.89062
# [20/100] training 3.9% loss=0.19715, acc=0.93750
# [20/100] training 4.2% loss=0.18020, acc=0.93750
# [20/100] training 4.4% loss=0.17182, acc=0.95312
# [20/100] training 4.5% loss=0.25382, acc=0.89062
# [20/100] training 4.7% loss=0.28714, acc=0.82812
# [20/100] training 4.8% loss=0.29615, acc=0.90625
# [20/100] training 5.0% loss=0.23591, acc=0.90625
# [20/100] training 5.2% loss=0.35777, acc=0.85938
# [20/100] training 5.4% loss=0.16746, acc=0.93750
# [20/100] training 5.5% loss=0.21493, acc=0.93750
# [20/100] training 5.7% loss=0.27512, acc=0.89062
# [20/100] training 5.9% loss=0.32235, acc=0.85938
# [20/100] training 6.0% loss=0.25367, acc=0.82812
# [20/100] training 6.3% loss=0.20092, acc=0.90625
# [20/100] training 6.4% loss=0.19293, acc=0.95312
# [20/100] training 6.6% loss=0.28717, acc=0.87500
# [20/100] training 6.7% loss=0.33635, acc=0.84375
# [20/100] training 6.9% loss=0.20192, acc=0.93750
# [20/100] training 7.1% loss=0.28973, acc=0.85938
# [20/100] training 7.2% loss=0.27799, acc=0.89062
# [20/100] training 7.5% loss=0.28352, acc=0.89062
# [20/100] training 7.6% loss=0.24538, acc=0.90625
# [20/100] training 7.8% loss=0.33758, acc=0.85938
# [20/100] training 7.9% loss=0.32753, acc=0.87500
# [20/100] training 8.1% loss=0.23446, acc=0.85938
# [20/100] training 8.2% loss=0.22453, acc=0.96875
# [20/100] training 8.4% loss=0.27231, acc=0.90625
# [20/100] training 8.7% loss=0.22140, acc=0.93750
# [20/100] training 8.8% loss=0.25804, acc=0.87500
# [20/100] training 9.0% loss=0.28150, acc=0.85938
# [20/100] training 9.1% loss=0.32351, acc=0.87500
# [20/100] training 9.3% loss=0.36538, acc=0.85938
# [20/100] training 9.4% loss=0.16952, acc=0.95312
# [20/100] training 9.7% loss=0.26518, acc=0.89062
# [20/100] training 9.9% loss=0.32239, acc=0.84375
# [20/100] training 10.0% loss=0.33849, acc=0.84375
# [20/100] training 10.2% loss=0.18302, acc=0.92188
# [20/100] training 10.3% loss=0.16174, acc=0.90625
# [20/100] training 10.5% loss=0.31823, acc=0.87500
# [20/100] training 10.6% loss=0.27087, acc=0.85938
# [20/100] training 10.9% loss=0.14910, acc=0.92188
# [20/100] training 11.0% loss=0.30737, acc=0.87500
# [20/100] training 11.2% loss=0.27153, acc=0.87500
# [20/100] training 11.4% loss=0.29850, acc=0.87500
# [20/100] training 11.5% loss=0.37649, acc=0.85938
# [20/100] training 11.7% loss=0.16405, acc=0.96875
# [20/100] training 11.8% loss=0.26182, acc=0.87500
# [20/100] training 12.1% loss=0.24414, acc=0.90625
# [20/100] training 12.2% loss=0.22875, acc=0.89062
# [20/100] training 12.4% loss=0.22645, acc=0.93750
# [20/100] training 12.6% loss=0.20512, acc=0.92188
# [20/100] training 12.7% loss=0.27026, acc=0.87500
# [20/100] training 12.9% loss=0.35242, acc=0.87500
# [20/100] training 13.0% loss=0.16821, acc=0.92188
# [20/100] training 13.3% loss=0.20308, acc=0.89062
# [20/100] training 13.4% loss=0.27919, acc=0.89062
# [20/100] training 13.6% loss=0.20196, acc=0.89062
# [20/100] training 13.7% loss=0.52418, acc=0.82812
# [20/100] training 13.9% loss=0.31515, acc=0.87500
# [20/100] training 14.1% loss=0.34735, acc=0.84375
# [20/100] training 14.3% loss=0.28525, acc=0.82812
# [20/100] training 14.5% loss=0.25742, acc=0.89062
# [20/100] training 14.6% loss=0.17920, acc=0.95312
# [20/100] training 14.8% loss=0.31911, acc=0.85938
# [20/100] training 14.9% loss=0.20374, acc=0.89062
# [20/100] training 15.1% loss=0.36666, acc=0.87500
# [20/100] training 15.4% loss=0.30720, acc=0.90625
# [20/100] training 15.5% loss=0.38562, acc=0.84375
# [20/100] training 15.7% loss=0.35132, acc=0.89062
# [20/100] training 15.8% loss=0.25592, acc=0.89062
# [20/100] training 16.0% loss=0.33376, acc=0.89062
# [20/100] training 16.1% loss=0.44458, acc=0.78125
# [20/100] training 16.3% loss=0.29476, acc=0.89062
# [20/100] training 16.4% loss=0.25600, acc=0.92188
# [20/100] training 16.7% loss=0.40027, acc=0.82812
# [20/100] training 16.9% loss=0.31607, acc=0.85938
# [20/100] training 17.0% loss=0.23342, acc=0.87500
# [20/100] training 17.2% loss=0.25593, acc=0.90625
# [20/100] training 17.3% loss=0.24807, acc=0.90625
# [20/100] training 17.5% loss=0.29212, acc=0.85938
# [20/100] training 17.7% loss=0.23524, acc=0.87500
# [20/100] training 17.9% loss=0.33933, acc=0.87500
# [20/100] training 18.1% loss=0.30081, acc=0.87500
# [20/100] training 18.2% loss=0.34156, acc=0.85938
# [20/100] training 18.4% loss=0.31108, acc=0.84375
# [20/100] training 18.5% loss=0.21377, acc=0.93750
# [20/100] training 18.8% loss=0.23013, acc=0.92188
# [20/100] training 18.9% loss=0.16811, acc=0.95312
# [20/100] training 19.1% loss=0.31694, acc=0.89062
# [20/100] training 19.2% loss=0.23576, acc=0.87500
# [20/100] training 19.4% loss=0.16978, acc=0.92188
# [20/100] training 19.6% loss=0.33655, acc=0.85938
# [20/100] training 19.7% loss=0.25269, acc=0.89062
# [20/100] training 20.0% loss=0.18245, acc=0.93750
# [20/100] training 20.1% loss=0.28943, acc=0.87500
# [20/100] training 20.3% loss=0.24944, acc=0.92188
# [20/100] training 20.4% loss=0.28218, acc=0.90625
# [20/100] training 20.6% loss=0.40028, acc=0.89062
# [20/100] training 20.8% loss=0.20420, acc=0.92188
# [20/100] training 20.9% loss=0.27389, acc=0.87500
# [20/100] training 21.2% loss=0.36447, acc=0.85938
# [20/100] training 21.3% loss=0.33109, acc=0.81250
# [20/100] training 21.5% loss=0.28978, acc=0.89062
# [20/100] training 21.6% loss=0.16485, acc=0.95312
# [20/100] training 21.8% loss=0.28985, acc=0.85938
# [20/100] training 21.9% loss=0.30117, acc=0.87500
# [20/100] training 22.2% loss=0.27092, acc=0.90625
# [20/100] training 22.4% loss=0.29121, acc=0.85938
# [20/100] training 22.5% loss=0.23031, acc=0.92188
# [20/100] training 22.7% loss=0.25203, acc=0.90625
# [20/100] training 22.8% loss=0.33418, acc=0.82812
# [20/100] training 23.0% loss=0.15744, acc=0.93750
# [20/100] training 23.1% loss=0.29320, acc=0.89062
# [20/100] training 23.4% loss=0.32463, acc=0.82812
# [20/100] training 23.6% loss=0.28730, acc=0.87500
# [20/100] training 23.7% loss=0.18579, acc=0.92188
# [20/100] training 23.9% loss=0.20997, acc=0.89062
# [20/100] training 24.0% loss=0.29357, acc=0.87500
# [20/100] training 24.2% loss=0.26698, acc=0.90625
# [20/100] training 24.3% loss=0.37488, acc=0.82812
# [20/100] training 24.6% loss=0.23450, acc=0.89062
# [20/100] training 24.7% loss=0.40719, acc=0.85938
# [20/100] training 24.9% loss=0.23257, acc=0.92188
# [20/100] training 25.1% loss=0.33488, acc=0.87500
# [20/100] training 25.2% loss=0.15294, acc=0.96875
# [20/100] training 25.4% loss=0.23424, acc=0.90625
# [20/100] training 25.6% loss=0.20736, acc=0.87500
# [20/100] training 25.8% loss=0.42735, acc=0.82812
# [20/100] training 25.9% loss=0.22688, acc=0.90625
# [20/100] training 26.1% loss=0.21800, acc=0.89062
# [20/100] training 26.3% loss=0.19749, acc=0.93750
# [20/100] training 26.4% loss=0.23247, acc=0.90625
# [20/100] training 26.6% loss=0.14371, acc=0.93750
# [20/100] training 26.8% loss=0.30127, acc=0.90625
# [20/100] training 27.0% loss=0.30543, acc=0.90625
# [20/100] training 27.1% loss=0.21143, acc=0.93750
# [20/100] training 27.3% loss=0.25736, acc=0.85938
# [20/100] training 27.4% loss=0.17058, acc=0.95312
# [20/100] training 27.6% loss=0.32618, acc=0.90625
# [20/100] training 27.9% loss=0.26913, acc=0.89062
# [20/100] training 28.0% loss=0.32494, acc=0.84375
# [20/100] training 28.2% loss=0.18762, acc=0.93750
# [20/100] training 28.3% loss=0.14499, acc=0.96875
# [20/100] training 28.5% loss=0.29579, acc=0.84375
# [20/100] training 28.6% loss=0.21112, acc=0.93750
# [20/100] training 28.8% loss=0.11424, acc=0.95312
# [20/100] training 29.1% loss=0.17135, acc=0.96875
# [20/100] training 29.2% loss=0.24490, acc=0.89062
# [20/100] training 29.4% loss=0.37236, acc=0.79688
# [20/100] training 29.5% loss=0.12980, acc=0.96875
# [20/100] training 29.7% loss=0.31316, acc=0.87500
# [20/100] training 29.8% loss=0.18655, acc=0.89062
# [20/100] training 30.0% loss=0.29592, acc=0.82812
# [20/100] training 30.2% loss=0.16513, acc=0.93750
# [20/100] training 30.4% loss=0.14383, acc=0.96875
# [20/100] training 30.6% loss=0.33334, acc=0.90625
# [20/100] training 30.7% loss=0.24873, acc=0.89062
# [20/100] training 30.9% loss=0.32995, acc=0.93750
# [20/100] training 31.0% loss=0.15172, acc=0.95312
# [20/100] training 31.3% loss=0.19192, acc=0.92188
# [20/100] training 31.4% loss=0.41183, acc=0.82812
# [20/100] training 31.6% loss=0.31518, acc=0.87500
# [20/100] training 31.8% loss=0.23844, acc=0.90625
# [20/100] training 31.9% loss=0.22373, acc=0.93750
# [20/100] training 32.1% loss=0.27919, acc=0.87500
# [20/100] training 32.2% loss=0.30661, acc=0.85938
# [20/100] training 32.5% loss=0.17074, acc=0.93750
# [20/100] training 32.6% loss=0.24293, acc=0.90625
# [20/100] training 32.8% loss=0.19195, acc=0.93750
# [20/100] training 32.9% loss=0.30726, acc=0.89062
# [20/100] training 33.1% loss=0.29223, acc=0.87500
# [20/100] training 33.3% loss=0.25305, acc=0.85938
# [20/100] training 33.4% loss=0.18920, acc=0.93750
# [20/100] training 33.7% loss=0.28579, acc=0.85938
# [20/100] training 33.8% loss=0.29149, acc=0.85938
# [20/100] training 34.0% loss=0.27143, acc=0.82812
# [20/100] training 34.1% loss=0.28761, acc=0.90625
# [20/100] training 34.3% loss=0.26465, acc=0.87500
# [20/100] training 34.5% loss=0.40622, acc=0.82812
# [20/100] training 34.7% loss=0.20882, acc=0.90625
# [20/100] training 34.9% loss=0.18900, acc=0.93750
# [20/100] training 35.0% loss=0.20375, acc=0.93750
# [20/100] training 35.2% loss=0.36349, acc=0.84375
# [20/100] training 35.3% loss=0.30062, acc=0.90625
# [20/100] training 35.5% loss=0.19963, acc=0.95312
# [20/100] training 35.6% loss=0.38250, acc=0.85938
# [20/100] training 35.9% loss=0.26873, acc=0.85938
# [20/100] training 36.1% loss=0.40610, acc=0.82812
# [20/100] training 36.2% loss=0.33777, acc=0.85938
# [20/100] training 36.4% loss=0.32852, acc=0.87500
# [20/100] training 36.5% loss=0.31649, acc=0.89062
# [20/100] training 36.7% loss=0.25266, acc=0.87500
# [20/100] training 36.8% loss=0.17443, acc=0.95312
# [20/100] training 37.1% loss=0.35843, acc=0.84375
# [20/100] training 37.3% loss=0.23224, acc=0.93750
# [20/100] training 37.4% loss=0.25015, acc=0.92188
# [20/100] training 37.6% loss=0.23578, acc=0.93750
# [20/100] training 37.7% loss=0.37163, acc=0.87500
# [20/100] training 37.9% loss=0.24006, acc=0.87500
# [20/100] training 38.1% loss=0.38824, acc=0.85938
# [20/100] training 38.3% loss=0.26331, acc=0.89062
# [20/100] training 38.4% loss=0.20303, acc=0.93750
# [20/100] training 38.6% loss=0.25189, acc=0.89062
# [20/100] training 38.8% loss=0.30991, acc=0.87500
# [20/100] training 38.9% loss=0.27852, acc=0.90625
# [20/100] training 39.1% loss=0.29379, acc=0.89062
# [20/100] training 39.3% loss=0.28093, acc=0.89062
# [20/100] training 39.5% loss=0.28239, acc=0.87500
# [20/100] training 39.6% loss=0.23002, acc=0.92188
# [20/100] training 39.8% loss=0.22854, acc=0.92188
# [20/100] training 40.0% loss=0.20875, acc=0.90625
# [20/100] training 40.1% loss=0.32026, acc=0.87500
# [20/100] training 40.4% loss=0.28058, acc=0.89062
# [20/100] training 40.5% loss=0.26297, acc=0.92188
# [20/100] training 40.7% loss=0.30078, acc=0.90625
# [20/100] training 40.8% loss=0.20663, acc=0.90625
# [20/100] training 41.0% loss=0.17729, acc=0.92188
# [20/100] training 41.1% loss=0.41260, acc=0.85938
# [20/100] training 41.3% loss=0.33515, acc=0.82812
# [20/100] training 41.6% loss=0.28706, acc=0.87500
# [20/100] training 41.7% loss=0.28086, acc=0.90625
# [20/100] training 41.9% loss=0.16983, acc=0.92188
# [20/100] training 42.0% loss=0.26237, acc=0.87500
# [20/100] training 42.2% loss=0.23428, acc=0.90625
# [20/100] training 42.3% loss=0.15596, acc=0.96875
# [20/100] training 42.5% loss=0.18382, acc=0.93750
# [20/100] training 42.8% loss=0.18479, acc=0.93750
# [20/100] training 42.9% loss=0.16948, acc=0.92188
# [20/100] training 43.1% loss=0.20365, acc=0.93750
# [20/100] training 43.2% loss=0.17954, acc=0.95312
# [20/100] training 43.4% loss=0.22984, acc=0.93750
# [20/100] training 43.5% loss=0.17092, acc=0.90625
# [20/100] training 43.8% loss=0.34895, acc=0.81250
# [20/100] training 43.9% loss=0.23339, acc=0.89062
# [20/100] training 44.1% loss=0.21336, acc=0.92188
# [20/100] training 44.3% loss=0.31012, acc=0.89062
# [20/100] training 44.4% loss=0.35599, acc=0.84375
# [20/100] training 44.6% loss=0.33103, acc=0.81250
# [20/100] training 44.7% loss=0.32486, acc=0.87500
# [20/100] training 45.0% loss=0.23695, acc=0.90625
# [20/100] training 45.1% loss=0.42803, acc=0.79688
# [20/100] training 45.3% loss=0.27613, acc=0.87500
# [20/100] training 45.5% loss=0.16457, acc=0.95312
# [20/100] training 45.6% loss=0.25275, acc=0.92188
# [20/100] training 45.8% loss=0.16371, acc=0.93750
# [20/100] training 45.9% loss=0.20511, acc=0.90625
# [20/100] training 46.2% loss=0.30681, acc=0.85938
# [20/100] training 46.3% loss=0.17443, acc=0.92188
# [20/100] training 46.5% loss=0.36545, acc=0.87500
# [20/100] training 46.6% loss=0.22416, acc=0.90625
# [20/100] training 46.8% loss=0.18117, acc=0.89062
# [20/100] training 47.0% loss=0.33708, acc=0.87500
# [20/100] training 47.2% loss=0.24228, acc=0.89062
# [20/100] training 47.4% loss=0.20372, acc=0.92188
# [20/100] training 47.5% loss=0.31830, acc=0.85938
# [20/100] training 47.7% loss=0.20432, acc=0.92188
# [20/100] training 47.8% loss=0.28841, acc=0.87500
# [20/100] training 48.0% loss=0.32366, acc=0.87500
# [20/100] training 48.3% loss=0.13827, acc=0.96875
# [20/100] training 48.4% loss=0.09374, acc=1.00000
# [20/100] training 48.6% loss=0.18314, acc=0.93750
# [20/100] training 48.7% loss=0.33536, acc=0.87500
# [20/100] training 48.9% loss=0.21529, acc=0.90625
# [20/100] training 49.0% loss=0.21313, acc=0.90625
# [20/100] training 49.2% loss=0.30781, acc=0.82812
# [20/100] training 49.3% loss=0.16563, acc=0.93750
# [20/100] training 49.6% loss=0.28528, acc=0.85938
# [20/100] training 49.8% loss=0.33187, acc=0.84375
# [20/100] training 49.9% loss=0.31811, acc=0.85938
# [20/100] training 50.1% loss=0.22795, acc=0.93750
# [20/100] training 50.2% loss=0.26657, acc=0.87500
# [20/100] training 50.4% loss=0.36915, acc=0.81250
# [20/100] training 50.6% loss=0.21374, acc=0.87500
# [20/100] training 50.8% loss=0.29505, acc=0.87500
# [20/100] training 51.0% loss=0.29540, acc=0.85938
# [20/100] training 51.1% loss=0.31011, acc=0.81250
# [20/100] training 51.3% loss=0.25902, acc=0.90625
# [20/100] training 51.4% loss=0.26417, acc=0.89062
# [20/100] training 51.7% loss=0.30539, acc=0.87500
# [20/100] training 51.8% loss=0.22639, acc=0.89062
# [20/100] training 52.0% loss=0.17976, acc=0.92188
# [20/100] training 52.1% loss=0.28468, acc=0.90625
# [20/100] training 52.3% loss=0.29685, acc=0.87500
# [20/100] training 52.5% loss=0.22740, acc=0.92188
# [20/100] training 52.6% loss=0.31584, acc=0.85938
# [20/100] training 52.9% loss=0.47345, acc=0.84375
# [20/100] training 53.0% loss=0.17489, acc=0.89062
# [20/100] training 53.2% loss=0.26262, acc=0.89062
# [20/100] training 53.3% loss=0.18324, acc=0.95312
# [20/100] training 53.5% loss=0.18234, acc=0.95312
# [20/100] training 53.7% loss=0.13985, acc=0.95312
# [20/100] training 53.8% loss=0.34687, acc=0.84375
# [20/100] training 54.1% loss=0.34683, acc=0.82812
# [20/100] training 54.2% loss=0.19859, acc=0.89062
# [20/100] training 54.4% loss=0.24297, acc=0.90625
# [20/100] training 54.5% loss=0.27172, acc=0.93750
# [20/100] training 54.7% loss=0.38509, acc=0.84375
# [20/100] training 54.8% loss=0.19888, acc=0.92188
# [20/100] training 55.1% loss=0.22298, acc=0.90625
# [20/100] training 55.3% loss=0.14502, acc=0.92188
# [20/100] training 55.4% loss=0.25312, acc=0.89062
# [20/100] training 55.6% loss=0.26891, acc=0.85938
# [20/100] training 55.7% loss=0.29901, acc=0.87500
# [20/100] training 55.9% loss=0.25832, acc=0.87500
# [20/100] training 56.0% loss=0.24118, acc=0.87500
# [20/100] training 56.3% loss=0.43733, acc=0.84375
# [20/100] training 56.5% loss=0.31037, acc=0.87500
# [20/100] training 56.6% loss=0.23422, acc=0.90625
# [20/100] training 56.8% loss=0.25777, acc=0.92188
# [20/100] training 56.9% loss=0.25949, acc=0.92188
# [20/100] training 57.1% loss=0.34886, acc=0.89062
# [20/100] training 57.2% loss=0.14041, acc=0.96875
# [20/100] training 57.5% loss=0.18565, acc=0.90625
# [20/100] training 57.6% loss=0.29310, acc=0.81250
# [20/100] training 57.8% loss=0.14694, acc=0.95312
# [20/100] training 58.0% loss=0.11846, acc=0.93750
# [20/100] training 58.1% loss=0.25143, acc=0.92188
# [20/100] training 58.3% loss=0.12299, acc=0.96875
# [20/100] training 58.4% loss=0.27003, acc=0.92188
# [20/100] training 58.7% loss=0.24895, acc=0.87500
# [20/100] training 58.8% loss=0.35835, acc=0.82812
# [20/100] training 59.0% loss=0.18033, acc=0.93750
# [20/100] training 59.2% loss=0.33337, acc=0.85938
# [20/100] training 59.3% loss=0.25300, acc=0.90625
# [20/100] training 59.5% loss=0.27706, acc=0.89062
# [20/100] training 59.7% loss=0.35527, acc=0.89062
# [20/100] training 59.9% loss=0.17092, acc=0.96875
# [20/100] training 60.0% loss=0.23550, acc=0.89062
# [20/100] training 60.2% loss=0.33662, acc=0.82812
# [20/100] training 60.3% loss=0.28636, acc=0.90625
# [20/100] training 60.5% loss=0.28163, acc=0.87500
# [20/100] training 60.8% loss=0.35005, acc=0.82812
# [20/100] training 60.9% loss=0.37485, acc=0.84375
# [20/100] training 61.1% loss=0.26822, acc=0.87500
# [20/100] training 61.2% loss=0.28081, acc=0.84375
# [20/100] training 61.4% loss=0.22896, acc=0.90625
# [20/100] training 61.5% loss=0.41555, acc=0.81250
# [20/100] training 61.7% loss=0.23297, acc=0.89062
# [20/100] training 62.0% loss=0.27091, acc=0.87500
# [20/100] training 62.1% loss=0.31419, acc=0.82812
# [20/100] training 62.3% loss=0.15221, acc=0.96875
# [20/100] training 62.4% loss=0.20362, acc=0.87500
# [20/100] training 62.6% loss=0.33022, acc=0.85938
# [20/100] training 62.7% loss=0.18604, acc=0.90625
# [20/100] training 62.9% loss=0.33695, acc=0.87500
# [20/100] training 63.1% loss=0.20152, acc=0.93750
# [20/100] training 63.3% loss=0.23936, acc=0.90625
# [20/100] training 63.5% loss=0.30175, acc=0.85938
# [20/100] training 63.6% loss=0.27778, acc=0.90625
# [20/100] training 63.8% loss=0.35049, acc=0.84375
# [20/100] training 63.9% loss=0.14914, acc=0.95312
# [20/100] training 64.2% loss=0.20305, acc=0.87500
# [20/100] training 64.3% loss=0.32201, acc=0.84375
# [20/100] training 64.5% loss=0.37238, acc=0.81250
# [20/100] training 64.7% loss=0.20657, acc=0.92188
# [20/100] training 64.8% loss=0.42463, acc=0.84375
# [20/100] training 65.0% loss=0.24072, acc=0.89062
# [20/100] training 65.1% loss=0.37486, acc=0.81250
# [20/100] training 65.4% loss=0.19266, acc=0.93750
# [20/100] training 65.5% loss=0.20956, acc=0.93750
# [20/100] training 65.7% loss=0.14627, acc=0.93750
# [20/100] training 65.8% loss=0.23701, acc=0.89062
# [20/100] training 66.0% loss=0.34682, acc=0.82812
# [20/100] training 66.2% loss=0.15059, acc=0.93750
# [20/100] training 66.3% loss=0.29348, acc=0.87500
# [20/100] training 66.6% loss=0.26256, acc=0.89062
# [20/100] training 66.7% loss=0.15933, acc=0.96875
# [20/100] training 66.9% loss=0.29582, acc=0.89062
# [20/100] training 67.0% loss=0.25862, acc=0.87500
# [20/100] training 67.2% loss=0.17543, acc=0.95312
# [20/100] training 67.4% loss=0.25941, acc=0.85938
# [20/100] training 67.6% loss=0.22693, acc=0.93750
# [20/100] training 67.8% loss=0.20919, acc=0.89062
# [20/100] training 67.9% loss=0.12447, acc=0.96875
# [20/100] training 68.1% loss=0.18496, acc=0.93750
# [20/100] training 68.2% loss=0.16668, acc=0.92188
# [20/100] training 68.4% loss=0.16305, acc=0.95312
# [20/100] training 68.5% loss=0.32790, acc=0.82812
# [20/100] training 68.8% loss=0.38489, acc=0.84375
# [20/100] training 69.0% loss=0.32463, acc=0.92188
# [20/100] training 69.1% loss=0.18954, acc=0.92188
# [20/100] training 69.3% loss=0.21642, acc=0.92188
# [20/100] training 69.4% loss=0.14884, acc=0.96875
# [20/100] training 69.6% loss=0.21304, acc=0.90625
# [20/100] training 69.7% loss=0.30258, acc=0.87500
# [20/100] training 70.0% loss=0.30270, acc=0.92188
# [20/100] training 70.2% loss=0.29162, acc=0.87500
# [20/100] training 70.3% loss=0.22936, acc=0.92188
# [20/100] training 70.5% loss=0.27192, acc=0.87500
# [20/100] training 70.6% loss=0.20731, acc=0.90625
# [20/100] training 70.8% loss=0.28227, acc=0.82812
# [20/100] training 71.0% loss=0.29000, acc=0.90625
# [20/100] training 71.2% loss=0.20754, acc=0.89062
# [20/100] training 71.3% loss=0.15400, acc=0.95312
# [20/100] training 71.5% loss=0.24689, acc=0.92188
# [20/100] training 71.7% loss=0.34099, acc=0.85938
# [20/100] training 71.8% loss=0.35968, acc=0.87500
# [20/100] training 72.0% loss=0.14610, acc=0.96875
# [20/100] training 72.2% loss=0.20230, acc=0.92188
# [20/100] training 72.4% loss=0.36204, acc=0.89062
# [20/100] training 72.5% loss=0.42298, acc=0.87500
# [20/100] training 72.7% loss=0.37221, acc=0.84375
# [20/100] training 72.9% loss=0.22265, acc=0.92188
# [20/100] training 73.0% loss=0.16883, acc=0.96875
# [20/100] training 73.3% loss=0.28281, acc=0.89062
# [20/100] training 73.4% loss=0.20670, acc=0.90625
# [20/100] training 73.6% loss=0.15292, acc=0.90625
# [20/100] training 73.7% loss=0.17741, acc=0.93750
# [20/100] training 73.9% loss=0.28817, acc=0.89062
# [20/100] training 74.0% loss=0.25191, acc=0.92188
# [20/100] training 74.2% loss=0.23002, acc=0.89062
# [20/100] training 74.5% loss=0.16124, acc=0.95312
# [20/100] training 74.6% loss=0.53871, acc=0.81250
# [20/100] training 74.8% loss=0.46246, acc=0.81250
# [20/100] training 74.9% loss=0.29623, acc=0.89062
# [20/100] training 75.1% loss=0.26042, acc=0.90625
# [20/100] training 75.2% loss=0.20581, acc=0.93750
# [20/100] training 75.4% loss=0.24988, acc=0.90625
# [20/100] training 75.7% loss=0.25097, acc=0.85938
# [20/100] training 75.8% loss=0.31989, acc=0.89062
# [20/100] training 76.0% loss=0.11095, acc=0.93750
# [20/100] training 76.1% loss=0.40192, acc=0.82812
# [20/100] training 76.3% loss=0.15785, acc=0.95312
# [20/100] training 76.4% loss=0.22122, acc=0.89062
# [20/100] training 76.7% loss=0.25366, acc=0.89062
# [20/100] training 76.8% loss=0.14853, acc=0.93750
# [20/100] training 77.0% loss=0.17524, acc=0.92188
# [20/100] training 77.2% loss=0.30067, acc=0.89062
# [20/100] training 77.3% loss=0.15198, acc=0.92188
# [20/100] training 77.5% loss=0.24823, acc=0.92188
# [20/100] training 77.6% loss=0.28660, acc=0.90625
# [20/100] training 77.9% loss=0.27564, acc=0.87500
# [20/100] training 78.0% loss=0.21276, acc=0.92188
# [20/100] training 78.2% loss=0.27532, acc=0.90625
# [20/100] training 78.4% loss=0.13329, acc=0.93750
# [20/100] training 78.5% loss=0.38004, acc=0.87500
# [20/100] training 78.7% loss=0.30443, acc=0.89062
# [20/100] training 78.8% loss=0.13122, acc=0.96875
# [20/100] training 79.1% loss=0.14707, acc=0.95312
# [20/100] training 79.2% loss=0.19815, acc=0.95312
# [20/100] training 79.4% loss=0.33109, acc=0.85938
# [20/100] training 79.5% loss=0.20259, acc=0.92188
# [20/100] training 79.7% loss=0.08983, acc=0.98438
# [20/100] training 79.9% loss=0.23379, acc=0.87500
# [20/100] training 80.1% loss=0.19224, acc=0.92188
# [20/100] training 80.3% loss=0.27901, acc=0.92188
# [20/100] training 80.4% loss=0.26760, acc=0.89062
# [20/100] training 80.6% loss=0.32228, acc=0.84375
# [20/100] training 80.7% loss=0.15595, acc=0.96875
# [20/100] training 80.9% loss=0.39003, acc=0.82812
# [20/100] training 81.2% loss=0.35863, acc=0.87500
# [20/100] training 81.3% loss=0.22732, acc=0.92188
# [20/100] training 81.5% loss=0.20939, acc=0.90625
# [20/100] training 81.6% loss=0.28691, acc=0.89062
# [20/100] training 81.8% loss=0.20713, acc=0.90625
# [20/100] training 81.9% loss=0.36909, acc=0.87500
# [20/100] training 82.1% loss=0.17239, acc=0.93750
# [20/100] training 82.2% loss=0.23708, acc=0.89062
# [20/100] training 82.5% loss=0.17001, acc=0.93750
# [20/100] training 82.7% loss=0.33683, acc=0.87500
# [20/100] training 82.8% loss=0.27018, acc=0.90625
# [20/100] training 83.0% loss=0.18769, acc=0.92188
# [20/100] training 83.1% loss=0.29367, acc=0.87500
# [20/100] training 83.3% loss=0.15708, acc=0.95312
# [20/100] training 83.5% loss=0.21412, acc=0.90625
# [20/100] training 83.7% loss=0.39216, acc=0.85938
# [20/100] training 83.9% loss=0.36937, acc=0.85938
# [20/100] training 84.0% loss=0.19635, acc=0.93750
# [20/100] training 84.2% loss=0.10984, acc=0.96875
# [20/100] training 84.3% loss=0.30484, acc=0.84375
# [20/100] training 84.5% loss=0.31482, acc=0.87500
# [20/100] training 84.7% loss=0.25041, acc=0.93750
# [20/100] training 84.9% loss=0.22531, acc=0.90625
# [20/100] training 85.0% loss=0.29559, acc=0.84375
# [20/100] training 85.2% loss=0.27103, acc=0.85938
# [20/100] training 85.4% loss=0.20325, acc=0.90625
# [20/100] training 85.5% loss=0.18732, acc=0.92188
# [20/100] training 85.8% loss=0.22867, acc=0.90625
# [20/100] training 85.9% loss=0.24895, acc=0.92188
# [20/100] training 86.1% loss=0.28159, acc=0.87500
# [20/100] training 86.2% loss=0.14317, acc=0.95312
# [20/100] training 86.4% loss=0.37289, acc=0.89062
# [20/100] training 86.6% loss=0.26713, acc=0.87500
# [20/100] training 86.7% loss=0.25992, acc=0.90625
# [20/100] training 87.0% loss=0.29235, acc=0.90625
# [20/100] training 87.1% loss=0.23790, acc=0.92188
# [20/100] training 87.3% loss=0.21357, acc=0.90625
# [20/100] training 87.4% loss=0.22721, acc=0.89062
# [20/100] training 87.6% loss=0.31042, acc=0.87500
# [20/100] training 87.7% loss=0.35764, acc=0.89062
# [20/100] training 87.9% loss=0.30405, acc=0.82812
# [20/100] training 88.2% loss=0.23264, acc=0.92188
# [20/100] training 88.3% loss=0.25443, acc=0.92188
# [20/100] training 88.5% loss=0.30227, acc=0.84375
# [20/100] training 88.6% loss=0.16038, acc=0.93750
# [20/100] training 88.8% loss=0.17690, acc=0.95312
# [20/100] training 88.9% loss=0.26425, acc=0.87500
# [20/100] training 89.2% loss=0.20234, acc=0.93750
# [20/100] training 89.4% loss=0.20349, acc=0.89062
# [20/100] training 89.5% loss=0.31159, acc=0.82812
# [20/100] training 89.7% loss=0.26537, acc=0.85938
# [20/100] training 89.8% loss=0.19684, acc=0.93750
# [20/100] training 90.0% loss=0.25258, acc=0.89062
# [20/100] training 90.1% loss=0.23921, acc=0.87500
# [20/100] training 90.4% loss=0.27225, acc=0.90625
# [20/100] training 90.5% loss=0.22302, acc=0.93750
# [20/100] training 90.7% loss=0.18297, acc=0.93750
# [20/100] training 90.9% loss=0.15200, acc=0.95312
# [20/100] training 91.0% loss=0.19577, acc=0.92188
# [20/100] training 91.2% loss=0.31010, acc=0.87500
# [20/100] training 91.3% loss=0.33050, acc=0.89062
# [20/100] training 91.6% loss=0.34797, acc=0.87500
# [20/100] training 91.7% loss=0.23683, acc=0.87500
# [20/100] training 91.9% loss=0.30283, acc=0.89062
# [20/100] training 92.1% loss=0.24945, acc=0.92188
# [20/100] training 92.2% loss=0.16346, acc=0.92188
# [20/100] training 92.4% loss=0.14625, acc=0.93750
# [20/100] training 92.6% loss=0.25008, acc=0.92188
# [20/100] training 92.8% loss=0.30839, acc=0.89062
# [20/100] training 92.9% loss=0.26611, acc=0.90625
# [20/100] training 93.1% loss=0.36380, acc=0.87500
# [20/100] training 93.2% loss=0.22930, acc=0.89062
# [20/100] training 93.4% loss=0.28945, acc=0.89062
# [20/100] training 93.7% loss=0.18914, acc=0.92188
# [20/100] training 93.8% loss=0.27247, acc=0.87500
# [20/100] training 94.0% loss=0.21018, acc=0.92188
# [20/100] training 94.1% loss=0.30862, acc=0.89062
# [20/100] training 94.3% loss=0.22094, acc=0.92188
# [20/100] training 94.4% loss=0.25592, acc=0.90625
# [20/100] training 94.6% loss=0.14168, acc=0.93750
# [20/100] training 94.9% loss=0.20565, acc=0.90625
# [20/100] training 95.0% loss=0.43298, acc=0.87500
# [20/100] training 95.2% loss=0.37154, acc=0.87500
# [20/100] training 95.3% loss=0.26920, acc=0.89062
# [20/100] training 95.5% loss=0.21221, acc=0.93750
# [20/100] training 95.6% loss=0.31792, acc=0.89062
# [20/100] training 95.8% loss=0.29780, acc=0.85938
# [20/100] training 96.0% loss=0.26079, acc=0.89062
# [20/100] training 96.2% loss=0.17336, acc=0.95312
# [20/100] training 96.4% loss=0.18960, acc=0.95312
# [20/100] training 96.5% loss=0.21971, acc=0.92188
# [20/100] training 96.7% loss=0.20485, acc=0.90625
# [20/100] training 96.8% loss=0.32706, acc=0.85938
# [20/100] training 97.1% loss=0.21891, acc=0.90625
# [20/100] training 97.2% loss=0.24674, acc=0.89062
# [20/100] training 97.4% loss=0.29353, acc=0.90625
# [20/100] training 97.6% loss=0.36825, acc=0.82812
# [20/100] training 97.7% loss=0.19908, acc=0.95312
# [20/100] training 97.9% loss=0.27199, acc=0.90625
# [20/100] training 98.0% loss=0.25099, acc=0.89062
# [20/100] training 98.3% loss=0.19860, acc=0.93750
# [20/100] training 98.4% loss=0.24933, acc=0.89062
# [20/100] training 98.6% loss=0.49981, acc=0.81250
# [20/100] training 98.7% loss=0.30864, acc=0.89062
# [20/100] training 98.9% loss=0.18412, acc=0.93750
# [20/100] training 99.1% loss=0.20894, acc=0.90625
# [20/100] training 99.2% loss=0.23224, acc=0.89062
# [20/100] training 99.5% loss=0.35388, acc=0.84375
# [20/100] training 99.6% loss=0.25567, acc=0.85938
# [20/100] training 99.8% loss=0.18953, acc=0.92188
# [20/100] training 99.9% loss=0.13832, acc=0.93750
# [20/100] testing 0.9% loss=0.17842, acc=0.90625
# [20/100] testing 1.8% loss=0.39952, acc=0.87500
# [20/100] testing 2.2% loss=0.27971, acc=0.90625
# [20/100] testing 3.1% loss=0.42524, acc=0.82812
# [20/100] testing 3.5% loss=0.21529, acc=0.90625
# [20/100] testing 4.4% loss=0.28409, acc=0.92188
# [20/100] testing 4.8% loss=0.37330, acc=0.81250
# [20/100] testing 5.7% loss=0.24362, acc=0.93750
# [20/100] testing 6.6% loss=0.30434, acc=0.84375
# [20/100] testing 7.0% loss=0.15749, acc=0.95312
# [20/100] testing 7.9% loss=0.31759, acc=0.87500
# [20/100] testing 8.3% loss=0.30763, acc=0.84375
# [20/100] testing 9.2% loss=0.26243, acc=0.87500
# [20/100] testing 9.7% loss=0.21825, acc=0.87500
# [20/100] testing 10.5% loss=0.35299, acc=0.85938
# [20/100] testing 11.0% loss=0.32186, acc=0.87500
# [20/100] testing 11.8% loss=0.29362, acc=0.85938
# [20/100] testing 12.7% loss=0.29082, acc=0.89062
# [20/100] testing 13.2% loss=0.28612, acc=0.89062
# [20/100] testing 14.0% loss=0.38204, acc=0.90625
# [20/100] testing 14.5% loss=0.38596, acc=0.81250
# [20/100] testing 15.4% loss=0.31944, acc=0.87500
# [20/100] testing 15.8% loss=0.26857, acc=0.89062
# [20/100] testing 16.7% loss=0.28126, acc=0.85938
# [20/100] testing 17.5% loss=0.26709, acc=0.85938
# [20/100] testing 18.0% loss=0.25344, acc=0.89062
# [20/100] testing 18.9% loss=0.18233, acc=0.93750
# [20/100] testing 19.3% loss=0.32473, acc=0.87500
# [20/100] testing 20.2% loss=0.30820, acc=0.89062
# [20/100] testing 20.6% loss=0.36941, acc=0.85938
# [20/100] testing 21.5% loss=0.25881, acc=0.89062
# [20/100] testing 21.9% loss=0.34535, acc=0.90625
# [20/100] testing 22.8% loss=0.37059, acc=0.84375
# [20/100] testing 23.7% loss=0.23694, acc=0.92188
# [20/100] testing 24.1% loss=0.22445, acc=0.89062
# [20/100] testing 25.0% loss=0.30307, acc=0.89062
# [20/100] testing 25.4% loss=0.17947, acc=0.93750
# [20/100] testing 26.3% loss=0.28992, acc=0.87500
# [20/100] testing 26.8% loss=0.20549, acc=0.90625
# [20/100] testing 27.6% loss=0.26281, acc=0.89062
# [20/100] testing 28.5% loss=0.28995, acc=0.89062
# [20/100] testing 29.0% loss=0.19971, acc=0.90625
# [20/100] testing 29.8% loss=0.39473, acc=0.82812
# [20/100] testing 30.3% loss=0.28967, acc=0.89062
# [20/100] testing 31.1% loss=0.36283, acc=0.84375
# [20/100] testing 31.6% loss=0.26579, acc=0.93750
# [20/100] testing 32.5% loss=0.23555, acc=0.90625
# [20/100] testing 32.9% loss=0.28722, acc=0.89062
# [20/100] testing 33.8% loss=0.28691, acc=0.89062
# [20/100] testing 34.7% loss=0.34836, acc=0.85938
# [20/100] testing 35.1% loss=0.25544, acc=0.89062
# [20/100] testing 36.0% loss=0.25512, acc=0.90625
# [20/100] testing 36.4% loss=0.21278, acc=0.92188
# [20/100] testing 37.3% loss=0.35292, acc=0.92188
# [20/100] testing 37.7% loss=0.44392, acc=0.81250
# [20/100] testing 38.6% loss=0.26965, acc=0.87500
# [20/100] testing 39.5% loss=0.22642, acc=0.93750
# [20/100] testing 39.9% loss=0.33925, acc=0.90625
# [20/100] testing 40.8% loss=0.23155, acc=0.89062
# [20/100] testing 41.2% loss=0.29264, acc=0.90625
# [20/100] testing 42.1% loss=0.27818, acc=0.85938
# [20/100] testing 42.5% loss=0.23531, acc=0.89062
# [20/100] testing 43.4% loss=0.23467, acc=0.90625
# [20/100] testing 43.9% loss=0.18930, acc=0.95312
# [20/100] testing 44.7% loss=0.37137, acc=0.85938
# [20/100] testing 45.6% loss=0.28493, acc=0.87500
# [20/100] testing 46.1% loss=0.23625, acc=0.92188
# [20/100] testing 46.9% loss=0.33082, acc=0.84375
# [20/100] testing 47.4% loss=0.13690, acc=0.95312
# [20/100] testing 48.3% loss=0.35289, acc=0.85938
# [20/100] testing 48.7% loss=0.44338, acc=0.87500
# [20/100] testing 49.6% loss=0.32404, acc=0.89062
# [20/100] testing 50.4% loss=0.32563, acc=0.87500
# [20/100] testing 50.9% loss=0.38166, acc=0.84375
# [20/100] testing 51.8% loss=0.27778, acc=0.82812
# [20/100] testing 52.2% loss=0.29487, acc=0.85938
# [20/100] testing 53.1% loss=0.22840, acc=0.85938
# [20/100] testing 53.5% loss=0.26325, acc=0.90625
# [20/100] testing 54.4% loss=0.41104, acc=0.78125
# [20/100] testing 54.8% loss=0.32088, acc=0.79688
# [20/100] testing 55.7% loss=0.22869, acc=0.85938
# [20/100] testing 56.6% loss=0.24662, acc=0.85938
# [20/100] testing 57.0% loss=0.40821, acc=0.82812
# [20/100] testing 57.9% loss=0.35503, acc=0.84375
# [20/100] testing 58.3% loss=0.39582, acc=0.79688
# [20/100] testing 59.2% loss=0.27880, acc=0.90625
# [20/100] testing 59.7% loss=0.29135, acc=0.87500
# [20/100] testing 60.5% loss=0.30179, acc=0.87500
# [20/100] testing 61.4% loss=0.22905, acc=0.85938
# [20/100] testing 61.9% loss=0.34492, acc=0.84375
# [20/100] testing 62.7% loss=0.22393, acc=0.85938
# [20/100] testing 63.2% loss=0.39681, acc=0.81250
# [20/100] testing 64.0% loss=0.38436, acc=0.85938
# [20/100] testing 64.5% loss=0.19027, acc=0.92188
# [20/100] testing 65.4% loss=0.25247, acc=0.92188
# [20/100] testing 65.8% loss=0.33612, acc=0.87500
# [20/100] testing 66.7% loss=0.25850, acc=0.89062
# [20/100] testing 67.6% loss=0.28413, acc=0.89062
# [20/100] testing 68.0% loss=0.19129, acc=0.90625
# [20/100] testing 68.9% loss=0.31298, acc=0.89062
# [20/100] testing 69.3% loss=0.33953, acc=0.85938
# [20/100] testing 70.2% loss=0.40002, acc=0.82812
# [20/100] testing 70.6% loss=0.41197, acc=0.82812
# [20/100] testing 71.5% loss=0.30022, acc=0.90625
# [20/100] testing 72.4% loss=0.14937, acc=0.95312
# [20/100] testing 72.8% loss=0.27130, acc=0.87500
# [20/100] testing 73.7% loss=0.20548, acc=0.95312
# [20/100] testing 74.1% loss=0.38733, acc=0.87500
# [20/100] testing 75.0% loss=0.27349, acc=0.90625
# [20/100] testing 75.4% loss=0.45983, acc=0.85938
# [20/100] testing 76.3% loss=0.16038, acc=0.92188
# [20/100] testing 76.8% loss=0.44335, acc=0.79688
# [20/100] testing 77.6% loss=0.27401, acc=0.89062
# [20/100] testing 78.5% loss=0.54395, acc=0.79688
# [20/100] testing 79.0% loss=0.27895, acc=0.85938
# [20/100] testing 79.8% loss=0.31483, acc=0.82812
# [20/100] testing 80.3% loss=0.27131, acc=0.87500
# [20/100] testing 81.2% loss=0.38557, acc=0.85938
# [20/100] testing 81.6% loss=0.19660, acc=0.93750
# [20/100] testing 82.5% loss=0.22573, acc=0.92188
# [20/100] testing 83.3% loss=0.23641, acc=0.93750
# [20/100] testing 83.8% loss=0.23233, acc=0.95312
# [20/100] testing 84.7% loss=0.39356, acc=0.81250
# [20/100] testing 85.1% loss=0.26606, acc=0.87500
# [20/100] testing 86.0% loss=0.34002, acc=0.84375
# [20/100] testing 86.4% loss=0.36758, acc=0.84375
# [20/100] testing 87.3% loss=0.22683, acc=0.90625
# [20/100] testing 87.7% loss=0.22865, acc=0.90625
# [20/100] testing 88.6% loss=0.21896, acc=0.93750
# [20/100] testing 89.5% loss=0.46141, acc=0.85938
# [20/100] testing 89.9% loss=0.25728, acc=0.87500
# [20/100] testing 90.8% loss=0.28156, acc=0.95312
# [20/100] testing 91.2% loss=0.13815, acc=0.95312
# [20/100] testing 92.1% loss=0.46384, acc=0.79688
# [20/100] testing 92.6% loss=0.28475, acc=0.87500
# [20/100] testing 93.4% loss=0.36838, acc=0.84375
# [20/100] testing 94.3% loss=0.18638, acc=0.95312
# [20/100] testing 94.7% loss=0.21698, acc=0.95312
# [20/100] testing 95.6% loss=0.26596, acc=0.81250
# [20/100] testing 96.1% loss=0.21768, acc=0.87500
# [20/100] testing 96.9% loss=0.33986, acc=0.82812
# [20/100] testing 97.4% loss=0.16953, acc=0.96875
# [20/100] testing 98.3% loss=0.26415, acc=0.85938
# [20/100] testing 98.7% loss=0.19522, acc=0.93750
# [20/100] testing 99.6% loss=0.23754, acc=0.89062
# [21/100] training 0.2% loss=0.32777, acc=0.84375
# [21/100] training 0.4% loss=0.33418, acc=0.87500
# [21/100] training 0.5% loss=0.19749, acc=0.92188
# [21/100] training 0.8% loss=0.28686, acc=0.85938
# [21/100] training 0.9% loss=0.24564, acc=0.89062
# [21/100] training 1.1% loss=0.32519, acc=0.92188
# [21/100] training 1.2% loss=0.32762, acc=0.87500
# [21/100] training 1.4% loss=0.23029, acc=0.90625
# [21/100] training 1.6% loss=0.13561, acc=0.93750
# [21/100] training 1.8% loss=0.22594, acc=0.93750
# [21/100] training 2.0% loss=0.34200, acc=0.85938
# [21/100] training 2.1% loss=0.29016, acc=0.82812
# [21/100] training 2.3% loss=0.29109, acc=0.84375
# [21/100] training 2.4% loss=0.35590, acc=0.89062
# [21/100] training 2.6% loss=0.20894, acc=0.90625
# [21/100] training 2.7% loss=0.25710, acc=0.93750
# [21/100] training 3.0% loss=0.26714, acc=0.87500
# [21/100] training 3.2% loss=0.17179, acc=0.95312
# [21/100] training 3.3% loss=0.36846, acc=0.84375
# [21/100] training 3.5% loss=0.22264, acc=0.90625
# [21/100] training 3.6% loss=0.37339, acc=0.81250
# [21/100] training 3.8% loss=0.22024, acc=0.90625
# [21/100] training 3.9% loss=0.31159, acc=0.84375
# [21/100] training 4.2% loss=0.13304, acc=0.93750
# [21/100] training 4.4% loss=0.16789, acc=0.95312
# [21/100] training 4.5% loss=0.27910, acc=0.85938
# [21/100] training 4.7% loss=0.29482, acc=0.90625
# [21/100] training 4.8% loss=0.25010, acc=0.90625
# [21/100] training 5.0% loss=0.26300, acc=0.92188
# [21/100] training 5.2% loss=0.33634, acc=0.81250
# [21/100] training 5.4% loss=0.17883, acc=0.93750
# [21/100] training 5.5% loss=0.26292, acc=0.90625
# [21/100] training 5.7% loss=0.30584, acc=0.85938
# [21/100] training 5.9% loss=0.36371, acc=0.84375
# [21/100] training 6.0% loss=0.27618, acc=0.84375
# [21/100] training 6.3% loss=0.25545, acc=0.89062
# [21/100] training 6.4% loss=0.22683, acc=0.95312
# [21/100] training 6.6% loss=0.26773, acc=0.85938
# [21/100] training 6.7% loss=0.27384, acc=0.90625
# [21/100] training 6.9% loss=0.20676, acc=0.93750
# [21/100] training 7.1% loss=0.19652, acc=0.90625
# [21/100] training 7.2% loss=0.25082, acc=0.95312
# [21/100] training 7.5% loss=0.26704, acc=0.87500
# [21/100] training 7.6% loss=0.25607, acc=0.87500
# [21/100] training 7.8% loss=0.31306, acc=0.90625
# [21/100] training 7.9% loss=0.33180, acc=0.84375
# [21/100] training 8.1% loss=0.20283, acc=0.92188
# [21/100] training 8.2% loss=0.28891, acc=0.89062
# [21/100] training 8.4% loss=0.32624, acc=0.85938
# [21/100] training 8.7% loss=0.28911, acc=0.90625
# [21/100] training 8.8% loss=0.27451, acc=0.87500
# [21/100] training 9.0% loss=0.25811, acc=0.92188
# [21/100] training 9.1% loss=0.36138, acc=0.84375
# [21/100] training 9.3% loss=0.41058, acc=0.84375
# [21/100] training 9.4% loss=0.18631, acc=0.93750
# [21/100] training 9.7% loss=0.25810, acc=0.89062
# [21/100] training 9.9% loss=0.39051, acc=0.82812
# [21/100] training 10.0% loss=0.24071, acc=0.89062
# [21/100] training 10.2% loss=0.22748, acc=0.90625
# [21/100] training 10.3% loss=0.28103, acc=0.87500
# [21/100] training 10.5% loss=0.35880, acc=0.87500
# [21/100] training 10.6% loss=0.30081, acc=0.87500
# [21/100] training 10.9% loss=0.19005, acc=0.93750
# [21/100] training 11.0% loss=0.26981, acc=0.92188
# [21/100] training 11.2% loss=0.16067, acc=0.96875
# [21/100] training 11.4% loss=0.24508, acc=0.95312
# [21/100] training 11.5% loss=0.41201, acc=0.85938
# [21/100] training 11.7% loss=0.15425, acc=0.95312
# [21/100] training 11.8% loss=0.23343, acc=0.92188
# [21/100] training 12.1% loss=0.22287, acc=0.89062
# [21/100] training 12.2% loss=0.19723, acc=0.92188
# [21/100] training 12.4% loss=0.22720, acc=0.92188
# [21/100] training 12.6% loss=0.30310, acc=0.85938
# [21/100] training 12.7% loss=0.29392, acc=0.90625
# [21/100] training 12.9% loss=0.34510, acc=0.89062
# [21/100] training 13.0% loss=0.26405, acc=0.90625
# [21/100] training 13.3% loss=0.19779, acc=0.89062
# [21/100] training 13.4% loss=0.25218, acc=0.92188
# [21/100] training 13.6% loss=0.22453, acc=0.87500
# [21/100] training 13.7% loss=0.34055, acc=0.85938
# [21/100] training 13.9% loss=0.21519, acc=0.95312
# [21/100] training 14.1% loss=0.32093, acc=0.90625
# [21/100] training 14.3% loss=0.22383, acc=0.90625
# [21/100] training 14.5% loss=0.21312, acc=0.95312
# [21/100] training 14.6% loss=0.18347, acc=0.95312
# [21/100] training 14.8% loss=0.29353, acc=0.85938
# [21/100] training 14.9% loss=0.20420, acc=0.90625
# [21/100] training 15.1% loss=0.44194, acc=0.81250
# [21/100] training 15.4% loss=0.25473, acc=0.92188
# [21/100] training 15.5% loss=0.27352, acc=0.87500
# [21/100] training 15.7% loss=0.32765, acc=0.93750
# [21/100] training 15.8% loss=0.23392, acc=0.87500
# [21/100] training 16.0% loss=0.36905, acc=0.87500
# [21/100] training 16.1% loss=0.44127, acc=0.78125
# [21/100] training 16.3% loss=0.27977, acc=0.89062
# [21/100] training 16.4% loss=0.28594, acc=0.90625
# [21/100] training 16.7% loss=0.25739, acc=0.90625
# [21/100] training 16.9% loss=0.26556, acc=0.89062
# [21/100] training 17.0% loss=0.36355, acc=0.84375
# [21/100] training 17.2% loss=0.30737, acc=0.89062
# [21/100] training 17.3% loss=0.20359, acc=0.93750
# [21/100] training 17.5% loss=0.24230, acc=0.87500
# [21/100] training 17.7% loss=0.23661, acc=0.85938
# [21/100] training 17.9% loss=0.28175, acc=0.92188
# [21/100] training 18.1% loss=0.26493, acc=0.90625
# [21/100] training 18.2% loss=0.29878, acc=0.90625
# [21/100] training 18.4% loss=0.36814, acc=0.82812
# [21/100] training 18.5% loss=0.19899, acc=0.92188
# [21/100] training 18.8% loss=0.21703, acc=0.93750
# [21/100] training 18.9% loss=0.15911, acc=0.95312
# [21/100] training 19.1% loss=0.35648, acc=0.85938
# [21/100] training 19.2% loss=0.18738, acc=0.90625
# [21/100] training 19.4% loss=0.16181, acc=0.93750
# [21/100] training 19.6% loss=0.23380, acc=0.85938
# [21/100] training 19.7% loss=0.27260, acc=0.90625
# [21/100] training 20.0% loss=0.13739, acc=0.95312
# [21/100] training 20.1% loss=0.22358, acc=0.89062
# [21/100] training 20.3% loss=0.29919, acc=0.90625
# [21/100] training 20.4% loss=0.32868, acc=0.87500
# [21/100] training 20.6% loss=0.21135, acc=0.87500
# [21/100] training 20.8% loss=0.22523, acc=0.90625
# [21/100] training 20.9% loss=0.19594, acc=0.93750
# [21/100] training 21.2% loss=0.27189, acc=0.89062
# [21/100] training 21.3% loss=0.27620, acc=0.87500
# [21/100] training 21.5% loss=0.38693, acc=0.84375
# [21/100] training 21.6% loss=0.14484, acc=0.93750
# [21/100] training 21.8% loss=0.19905, acc=0.92188
# [21/100] training 21.9% loss=0.22289, acc=0.93750
# [21/100] training 22.2% loss=0.28941, acc=0.90625
# [21/100] training 22.4% loss=0.41400, acc=0.87500
# [21/100] training 22.5% loss=0.21687, acc=0.90625
# [21/100] training 22.7% loss=0.26942, acc=0.89062
# [21/100] training 22.8% loss=0.39948, acc=0.81250
# [21/100] training 23.0% loss=0.22978, acc=0.90625
# [21/100] training 23.1% loss=0.31964, acc=0.87500
# [21/100] training 23.4% loss=0.28603, acc=0.89062
# [21/100] training 23.6% loss=0.42022, acc=0.85938
# [21/100] training 23.7% loss=0.32562, acc=0.87500
# [21/100] training 23.9% loss=0.20506, acc=0.89062
# [21/100] training 24.0% loss=0.24157, acc=0.90625
# [21/100] training 24.2% loss=0.22571, acc=0.90625
# [21/100] training 24.3% loss=0.42407, acc=0.82812
# [21/100] training 24.6% loss=0.27886, acc=0.87500
# [21/100] training 24.7% loss=0.37855, acc=0.85938
# [21/100] training 24.9% loss=0.24658, acc=0.92188
# [21/100] training 25.1% loss=0.31971, acc=0.85938
# [21/100] training 25.2% loss=0.15642, acc=0.95312
# [21/100] training 25.4% loss=0.20329, acc=0.93750
# [21/100] training 25.6% loss=0.19614, acc=0.89062
# [21/100] training 25.8% loss=0.27871, acc=0.87500
# [21/100] training 25.9% loss=0.22708, acc=0.89062
# [21/100] training 26.1% loss=0.19090, acc=0.95312
# [21/100] training 26.3% loss=0.18911, acc=0.92188
# [21/100] training 26.4% loss=0.15929, acc=0.92188
# [21/100] training 26.6% loss=0.10486, acc=0.95312
# [21/100] training 26.8% loss=0.28169, acc=0.89062
# [21/100] training 27.0% loss=0.27381, acc=0.92188
# [21/100] training 27.1% loss=0.14875, acc=0.95312
# [21/100] training 27.3% loss=0.21361, acc=0.87500
# [21/100] training 27.4% loss=0.16671, acc=0.89062
# [21/100] training 27.6% loss=0.38496, acc=0.89062
# [21/100] training 27.9% loss=0.16740, acc=0.89062
# [21/100] training 28.0% loss=0.34440, acc=0.87500
# [21/100] training 28.2% loss=0.17102, acc=0.95312
# [21/100] training 28.3% loss=0.14577, acc=0.96875
# [21/100] training 28.5% loss=0.35680, acc=0.82812
# [21/100] training 28.6% loss=0.24532, acc=0.90625
# [21/100] training 28.8% loss=0.16731, acc=0.92188
# [21/100] training 29.1% loss=0.31223, acc=0.84375
# [21/100] training 29.2% loss=0.18606, acc=0.95312
# [21/100] training 29.4% loss=0.31285, acc=0.90625
# [21/100] training 29.5% loss=0.21770, acc=0.90625
# [21/100] training 29.7% loss=0.32091, acc=0.84375
# [21/100] training 29.8% loss=0.19659, acc=0.90625
# [21/100] training 30.0% loss=0.32457, acc=0.84375
# [21/100] training 30.2% loss=0.23120, acc=0.89062
# [21/100] training 30.4% loss=0.16725, acc=0.95312
# [21/100] training 30.6% loss=0.27097, acc=0.89062
# [21/100] training 30.7% loss=0.22968, acc=0.92188
# [21/100] training 30.9% loss=0.29991, acc=0.92188
# [21/100] training 31.0% loss=0.15989, acc=0.96875
# [21/100] training 31.3% loss=0.19021, acc=0.93750
# [21/100] training 31.4% loss=0.39303, acc=0.73438
# [21/100] training 31.6% loss=0.30784, acc=0.87500
# [21/100] training 31.8% loss=0.21144, acc=0.90625
# [21/100] training 31.9% loss=0.23486, acc=0.89062
# [21/100] training 32.1% loss=0.32585, acc=0.89062
# [21/100] training 32.2% loss=0.32073, acc=0.87500
# [21/100] training 32.5% loss=0.23792, acc=0.87500
# [21/100] training 32.6% loss=0.22997, acc=0.90625
# [21/100] training 32.8% loss=0.19267, acc=0.93750
# [21/100] training 32.9% loss=0.25671, acc=0.87500
# [21/100] training 33.1% loss=0.20318, acc=0.90625
# [21/100] training 33.3% loss=0.23187, acc=0.89062
# [21/100] training 33.4% loss=0.16327, acc=0.95312
# [21/100] training 33.7% loss=0.27973, acc=0.87500
# [21/100] training 33.8% loss=0.30138, acc=0.84375
# [21/100] training 34.0% loss=0.24993, acc=0.89062
# [21/100] training 34.1% loss=0.25029, acc=0.89062
# [21/100] training 34.3% loss=0.26871, acc=0.89062
# [21/100] training 34.5% loss=0.36386, acc=0.87500
# [21/100] training 34.7% loss=0.14675, acc=0.93750
# [21/100] training 34.9% loss=0.19187, acc=0.90625
# [21/100] training 35.0% loss=0.20990, acc=0.90625
# [21/100] training 35.2% loss=0.33895, acc=0.81250
# [21/100] training 35.3% loss=0.32352, acc=0.84375
# [21/100] training 35.5% loss=0.24956, acc=0.92188
# [21/100] training 35.6% loss=0.38919, acc=0.89062
# [21/100] training 35.9% loss=0.31336, acc=0.87500
# [21/100] training 36.1% loss=0.36783, acc=0.81250
# [21/100] training 36.2% loss=0.31639, acc=0.82812
# [21/100] training 36.4% loss=0.36798, acc=0.87500
# [21/100] training 36.5% loss=0.27956, acc=0.92188
# [21/100] training 36.7% loss=0.22931, acc=0.87500
# [21/100] training 36.8% loss=0.22064, acc=0.89062
# [21/100] training 37.1% loss=0.32568, acc=0.82812
# [21/100] training 37.3% loss=0.24664, acc=0.90625
# [21/100] training 37.4% loss=0.27863, acc=0.89062
# [21/100] training 37.6% loss=0.26664, acc=0.87500
# [21/100] training 37.7% loss=0.28836, acc=0.89062
# [21/100] training 37.9% loss=0.20159, acc=0.93750
# [21/100] training 38.1% loss=0.34557, acc=0.84375
# [21/100] training 38.3% loss=0.24822, acc=0.92188
# [21/100] training 38.4% loss=0.16381, acc=0.93750
# [21/100] training 38.6% loss=0.26123, acc=0.90625
# [21/100] training 38.8% loss=0.25460, acc=0.90625
# [21/100] training 38.9% loss=0.29042, acc=0.87500
# [21/100] training 39.1% loss=0.23130, acc=0.90625
# [21/100] training 39.3% loss=0.28482, acc=0.87500
# [21/100] training 39.5% loss=0.31525, acc=0.84375
# [21/100] training 39.6% loss=0.32177, acc=0.87500
# [21/100] training 39.8% loss=0.16646, acc=0.92188
# [21/100] training 40.0% loss=0.23087, acc=0.92188
# [21/100] training 40.1% loss=0.27015, acc=0.85938
# [21/100] training 40.4% loss=0.28457, acc=0.85938
# [21/100] training 40.5% loss=0.19939, acc=0.87500
# [21/100] training 40.7% loss=0.26587, acc=0.85938
# [21/100] training 40.8% loss=0.15162, acc=0.93750
# [21/100] training 41.0% loss=0.18570, acc=0.90625
# [21/100] training 41.1% loss=0.41489, acc=0.87500
# [21/100] training 41.3% loss=0.34605, acc=0.85938
# [21/100] training 41.6% loss=0.31336, acc=0.85938
# [21/100] training 41.7% loss=0.29262, acc=0.89062
# [21/100] training 41.9% loss=0.17880, acc=0.90625
# [21/100] training 42.0% loss=0.28851, acc=0.89062
# [21/100] training 42.2% loss=0.22271, acc=0.92188
# [21/100] training 42.3% loss=0.24768, acc=0.90625
# [21/100] training 42.5% loss=0.16291, acc=0.95312
# [21/100] training 42.8% loss=0.14535, acc=0.95312
# [21/100] training 42.9% loss=0.20221, acc=0.90625
# [21/100] training 43.1% loss=0.19954, acc=0.92188
# [21/100] training 43.2% loss=0.15590, acc=0.90625
# [21/100] training 43.4% loss=0.21016, acc=0.95312
# [21/100] training 43.5% loss=0.26928, acc=0.90625
# [21/100] training 43.8% loss=0.31553, acc=0.87500
# [21/100] training 43.9% loss=0.20123, acc=0.92188
# [21/100] training 44.1% loss=0.16584, acc=0.92188
# [21/100] training 44.3% loss=0.21753, acc=0.95312
# [21/100] training 44.4% loss=0.31873, acc=0.85938
# [21/100] training 44.6% loss=0.33401, acc=0.87500
# [21/100] training 44.7% loss=0.34891, acc=0.84375
# [21/100] training 45.0% loss=0.27490, acc=0.87500
# [21/100] training 45.1% loss=0.32238, acc=0.87500
# [21/100] training 45.3% loss=0.22917, acc=0.90625
# [21/100] training 45.5% loss=0.14660, acc=0.98438
# [21/100] training 45.6% loss=0.21801, acc=0.93750
# [21/100] training 45.8% loss=0.19313, acc=0.92188
# [21/100] training 45.9% loss=0.19504, acc=0.87500
# [21/100] training 46.2% loss=0.20948, acc=0.93750
# [21/100] training 46.3% loss=0.15252, acc=0.93750
# [21/100] training 46.5% loss=0.35139, acc=0.85938
# [21/100] training 46.6% loss=0.20827, acc=0.92188
# [21/100] training 46.8% loss=0.30658, acc=0.87500
# [21/100] training 47.0% loss=0.43509, acc=0.85938
# [21/100] training 47.2% loss=0.35919, acc=0.87500
# [21/100] training 47.4% loss=0.25285, acc=0.89062
# [21/100] training 47.5% loss=0.32894, acc=0.84375
# [21/100] training 47.7% loss=0.23749, acc=0.89062
# [21/100] training 47.8% loss=0.38055, acc=0.82812
# [21/100] training 48.0% loss=0.26181, acc=0.90625
# [21/100] training 48.3% loss=0.14249, acc=0.96875
# [21/100] training 48.4% loss=0.17175, acc=0.93750
# [21/100] training 48.6% loss=0.20523, acc=0.95312
# [21/100] training 48.7% loss=0.35310, acc=0.87500
# [21/100] training 48.9% loss=0.20377, acc=0.89062
# [21/100] training 49.0% loss=0.16727, acc=0.92188
# [21/100] training 49.2% loss=0.38050, acc=0.82812
# [21/100] training 49.3% loss=0.26684, acc=0.90625
# [21/100] training 49.6% loss=0.23456, acc=0.89062
# [21/100] training 49.8% loss=0.33156, acc=0.84375
# [21/100] training 49.9% loss=0.24128, acc=0.87500
# [21/100] training 50.1% loss=0.19436, acc=0.95312
# [21/100] training 50.2% loss=0.30186, acc=0.87500
# [21/100] training 50.4% loss=0.39007, acc=0.84375
# [21/100] training 50.6% loss=0.27498, acc=0.89062
# [21/100] training 50.8% loss=0.38042, acc=0.82812
# [21/100] training 51.0% loss=0.28548, acc=0.85938
# [21/100] training 51.1% loss=0.31456, acc=0.81250
# [21/100] training 51.3% loss=0.26578, acc=0.90625
# [21/100] training 51.4% loss=0.27781, acc=0.89062
# [21/100] training 51.7% loss=0.32265, acc=0.82812
# [21/100] training 51.8% loss=0.30270, acc=0.85938
# [21/100] training 52.0% loss=0.23280, acc=0.90625
# [21/100] training 52.1% loss=0.24472, acc=0.90625
# [21/100] training 52.3% loss=0.31556, acc=0.81250
# [21/100] training 52.5% loss=0.16791, acc=0.92188
# [21/100] training 52.6% loss=0.23893, acc=0.84375
# [21/100] training 52.9% loss=0.35556, acc=0.90625
# [21/100] training 53.0% loss=0.18028, acc=0.92188
# [21/100] training 53.2% loss=0.24972, acc=0.89062
# [21/100] training 53.3% loss=0.18365, acc=0.92188
# [21/100] training 53.5% loss=0.24255, acc=0.90625
# [21/100] training 53.7% loss=0.19912, acc=0.92188
# [21/100] training 53.8% loss=0.35104, acc=0.85938
# [21/100] training 54.1% loss=0.28340, acc=0.87500
# [21/100] training 54.2% loss=0.25176, acc=0.90625
# [21/100] training 54.4% loss=0.31174, acc=0.87500
# [21/100] training 54.5% loss=0.29638, acc=0.87500
# [21/100] training 54.7% loss=0.39901, acc=0.79688
# [21/100] training 54.8% loss=0.16809, acc=0.95312
# [21/100] training 55.1% loss=0.22447, acc=0.90625
# [21/100] training 55.3% loss=0.18904, acc=0.93750
# [21/100] training 55.4% loss=0.28458, acc=0.84375
# [21/100] training 55.6% loss=0.20946, acc=0.90625
# [21/100] training 55.7% loss=0.23503, acc=0.87500
# [21/100] training 55.9% loss=0.22684, acc=0.87500
# [21/100] training 56.0% loss=0.27111, acc=0.84375
# [21/100] training 56.3% loss=0.51616, acc=0.79688
# [21/100] training 56.5% loss=0.27819, acc=0.89062
# [21/100] training 56.6% loss=0.28380, acc=0.85938
# [21/100] training 56.8% loss=0.25715, acc=0.89062
# [21/100] training 56.9% loss=0.27520, acc=0.87500
# [21/100] training 57.1% loss=0.33411, acc=0.87500
# [21/100] training 57.2% loss=0.22209, acc=0.87500
# [21/100] training 57.5% loss=0.24008, acc=0.85938
# [21/100] training 57.6% loss=0.26453, acc=0.90625
# [21/100] training 57.8% loss=0.17400, acc=0.95312
# [21/100] training 58.0% loss=0.15999, acc=0.95312
# [21/100] training 58.1% loss=0.17718, acc=0.93750
# [21/100] training 58.3% loss=0.17590, acc=0.93750
# [21/100] training 58.4% loss=0.28806, acc=0.92188
# [21/100] training 58.7% loss=0.37249, acc=0.84375
# [21/100] training 58.8% loss=0.28138, acc=0.90625
# [21/100] training 59.0% loss=0.22563, acc=0.90625
# [21/100] training 59.2% loss=0.25412, acc=0.89062
# [21/100] training 59.3% loss=0.18937, acc=0.92188
# [21/100] training 59.5% loss=0.23727, acc=0.90625
# [21/100] training 59.7% loss=0.26960, acc=0.89062
# [21/100] training 59.9% loss=0.19574, acc=0.93750
# [21/100] training 60.0% loss=0.28298, acc=0.87500
# [21/100] training 60.2% loss=0.21922, acc=0.87500
# [21/100] training 60.3% loss=0.15902, acc=0.92188
# [21/100] training 60.5% loss=0.29233, acc=0.85938
# [21/100] training 60.8% loss=0.28646, acc=0.85938
# [21/100] training 60.9% loss=0.26984, acc=0.85938
# [21/100] training 61.1% loss=0.30971, acc=0.84375
# [21/100] training 61.2% loss=0.18059, acc=0.93750
# [21/100] training 61.4% loss=0.24296, acc=0.89062
# [21/100] training 61.5% loss=0.45431, acc=0.79688
# [21/100] training 61.7% loss=0.33574, acc=0.87500
# [21/100] training 62.0% loss=0.29476, acc=0.84375
# [21/100] training 62.1% loss=0.28579, acc=0.87500
# [21/100] training 62.3% loss=0.18554, acc=0.95312
# [21/100] training 62.4% loss=0.18733, acc=0.92188
# [21/100] training 62.6% loss=0.32181, acc=0.84375
# [21/100] training 62.7% loss=0.24501, acc=0.92188
# [21/100] training 62.9% loss=0.34400, acc=0.87500
# [21/100] training 63.1% loss=0.15941, acc=0.95312
# [21/100] training 63.3% loss=0.27178, acc=0.87500
# [21/100] training 63.5% loss=0.28535, acc=0.92188
# [21/100] training 63.6% loss=0.30583, acc=0.82812
# [21/100] training 63.8% loss=0.31427, acc=0.79688
# [21/100] training 63.9% loss=0.19186, acc=0.95312
# [21/100] training 64.2% loss=0.25526, acc=0.87500
# [21/100] training 64.3% loss=0.27935, acc=0.85938
# [21/100] training 64.5% loss=0.30941, acc=0.85938
# [21/100] training 64.7% loss=0.22207, acc=0.85938
# [21/100] training 64.8% loss=0.37502, acc=0.82812
# [21/100] training 65.0% loss=0.30910, acc=0.85938
# [21/100] training 65.1% loss=0.34017, acc=0.85938
# [21/100] training 65.4% loss=0.23021, acc=0.92188
# [21/100] training 65.5% loss=0.18559, acc=0.93750
# [21/100] training 65.7% loss=0.19138, acc=0.90625
# [21/100] training 65.8% loss=0.26457, acc=0.90625
# [21/100] training 66.0% loss=0.27092, acc=0.84375
# [21/100] training 66.2% loss=0.15065, acc=0.93750
# [21/100] training 66.3% loss=0.31353, acc=0.87500
# [21/100] training 66.6% loss=0.21963, acc=0.89062
# [21/100] training 66.7% loss=0.10867, acc=0.96875
# [21/100] training 66.9% loss=0.21320, acc=0.90625
# [21/100] training 67.0% loss=0.28476, acc=0.89062
# [21/100] training 67.2% loss=0.16328, acc=0.92188
# [21/100] training 67.4% loss=0.26626, acc=0.87500
# [21/100] training 67.6% loss=0.13098, acc=0.98438
# [21/100] training 67.8% loss=0.18136, acc=0.93750
# [21/100] training 67.9% loss=0.13623, acc=0.95312
# [21/100] training 68.1% loss=0.18416, acc=0.93750
# [21/100] training 68.2% loss=0.12416, acc=0.93750
# [21/100] training 68.4% loss=0.19854, acc=0.93750
# [21/100] training 68.5% loss=0.26804, acc=0.89062
# [21/100] training 68.8% loss=0.30204, acc=0.85938
# [21/100] training 69.0% loss=0.36326, acc=0.90625
# [21/100] training 69.1% loss=0.17335, acc=0.93750
# [21/100] training 69.3% loss=0.33571, acc=0.85938
# [21/100] training 69.4% loss=0.20283, acc=0.89062
# [21/100] training 69.6% loss=0.23359, acc=0.90625
# [21/100] training 69.7% loss=0.30945, acc=0.90625
# [21/100] training 70.0% loss=0.31427, acc=0.85938
# [21/100] training 70.2% loss=0.30309, acc=0.93750
# [21/100] training 70.3% loss=0.22029, acc=0.90625
# [21/100] training 70.5% loss=0.24054, acc=0.89062
# [21/100] training 70.6% loss=0.24786, acc=0.87500
# [21/100] training 70.8% loss=0.22664, acc=0.92188
# [21/100] training 71.0% loss=0.26970, acc=0.92188
# [21/100] training 71.2% loss=0.23365, acc=0.93750
# [21/100] training 71.3% loss=0.12484, acc=0.98438
# [21/100] training 71.5% loss=0.31381, acc=0.89062
# [21/100] training 71.7% loss=0.30203, acc=0.87500
# [21/100] training 71.8% loss=0.35180, acc=0.89062
# [21/100] training 72.0% loss=0.20183, acc=0.87500
# [21/100] training 72.2% loss=0.19555, acc=0.90625
# [21/100] training 72.4% loss=0.35110, acc=0.87500
# [21/100] training 72.5% loss=0.38131, acc=0.85938
# [21/100] training 72.7% loss=0.35469, acc=0.82812
# [21/100] training 72.9% loss=0.25210, acc=0.89062
# [21/100] training 73.0% loss=0.20473, acc=0.95312
# [21/100] training 73.3% loss=0.30206, acc=0.82812
# [21/100] training 73.4% loss=0.21206, acc=0.92188
# [21/100] training 73.6% loss=0.19268, acc=0.96875
# [21/100] training 73.7% loss=0.27956, acc=0.89062
# [21/100] training 73.9% loss=0.26131, acc=0.87500
# [21/100] training 74.0% loss=0.32748, acc=0.90625
# [21/100] training 74.2% loss=0.21338, acc=0.90625
# [21/100] training 74.5% loss=0.25407, acc=0.90625
# [21/100] training 74.6% loss=0.45596, acc=0.82812
# [21/100] training 74.8% loss=0.46296, acc=0.85938
# [21/100] training 74.9% loss=0.41704, acc=0.82812
# [21/100] training 75.1% loss=0.25261, acc=0.90625
# [21/100] training 75.2% loss=0.18633, acc=0.95312
# [21/100] training 75.4% loss=0.23952, acc=0.92188
# [21/100] training 75.7% loss=0.31590, acc=0.85938
# [21/100] training 75.8% loss=0.32745, acc=0.87500
# [21/100] training 76.0% loss=0.14531, acc=0.92188
# [21/100] training 76.1% loss=0.28809, acc=0.89062
# [21/100] training 76.3% loss=0.17912, acc=0.93750
# [21/100] training 76.4% loss=0.22436, acc=0.90625
# [21/100] training 76.7% loss=0.23040, acc=0.90625
# [21/100] training 76.8% loss=0.18660, acc=0.95312
# [21/100] training 77.0% loss=0.20596, acc=0.93750
# [21/100] training 77.2% loss=0.29674, acc=0.89062
# [21/100] training 77.3% loss=0.19251, acc=0.92188
# [21/100] training 77.5% loss=0.38132, acc=0.87500
# [21/100] training 77.6% loss=0.18615, acc=0.90625
# [21/100] training 77.9% loss=0.19951, acc=0.89062
# [21/100] training 78.0% loss=0.31985, acc=0.79688
# [21/100] training 78.2% loss=0.27720, acc=0.89062
# [21/100] training 78.4% loss=0.11380, acc=0.96875
# [21/100] training 78.5% loss=0.30148, acc=0.89062
# [21/100] training 78.7% loss=0.27706, acc=0.89062
# [21/100] training 78.8% loss=0.18863, acc=0.93750
# [21/100] training 79.1% loss=0.17413, acc=0.95312
# [21/100] training 79.2% loss=0.16587, acc=0.93750
# [21/100] training 79.4% loss=0.27526, acc=0.87500
# [21/100] training 79.5% loss=0.30135, acc=0.89062
# [21/100] training 79.7% loss=0.09886, acc=0.98438
# [21/100] training 79.9% loss=0.26448, acc=0.89062
# [21/100] training 80.1% loss=0.20637, acc=0.87500
# [21/100] training 80.3% loss=0.33938, acc=0.84375
# [21/100] training 80.4% loss=0.26121, acc=0.90625
# [21/100] training 80.6% loss=0.26727, acc=0.87500
# [21/100] training 80.7% loss=0.20124, acc=0.95312
# [21/100] training 80.9% loss=0.29599, acc=0.87500
# [21/100] training 81.2% loss=0.27754, acc=0.89062
# [21/100] training 81.3% loss=0.28485, acc=0.89062
# [21/100] training 81.5% loss=0.23429, acc=0.90625
# [21/100] training 81.6% loss=0.28677, acc=0.85938
# [21/100] training 81.8% loss=0.27446, acc=0.85938
# [21/100] training 81.9% loss=0.36359, acc=0.87500
# [21/100] training 82.1% loss=0.18767, acc=0.93750
# [21/100] training 82.2% loss=0.30485, acc=0.89062
# [21/100] training 82.5% loss=0.19991, acc=0.90625
# [21/100] training 82.7% loss=0.23571, acc=0.93750
# [21/100] training 82.8% loss=0.29185, acc=0.89062
# [21/100] training 83.0% loss=0.15031, acc=0.95312
# [21/100] training 83.1% loss=0.20535, acc=0.95312
# [21/100] training 83.3% loss=0.16141, acc=0.92188
# [21/100] training 83.5% loss=0.17243, acc=0.96875
# [21/100] training 83.7% loss=0.32528, acc=0.92188
# [21/100] training 83.9% loss=0.29028, acc=0.90625
# [21/100] training 84.0% loss=0.17119, acc=0.92188
# [21/100] training 84.2% loss=0.17942, acc=0.90625
# [21/100] training 84.3% loss=0.26377, acc=0.84375
# [21/100] training 84.5% loss=0.18819, acc=0.92188
# [21/100] training 84.7% loss=0.29501, acc=0.85938
# [21/100] training 84.9% loss=0.20441, acc=0.93750
# [21/100] training 85.0% loss=0.14109, acc=0.98438
# [21/100] training 85.2% loss=0.27769, acc=0.89062
# [21/100] training 85.4% loss=0.33630, acc=0.90625
# [21/100] training 85.5% loss=0.26996, acc=0.87500
# [21/100] training 85.8% loss=0.22516, acc=0.90625
# [21/100] training 85.9% loss=0.25895, acc=0.87500
# [21/100] training 86.1% loss=0.22034, acc=0.85938
# [21/100] training 86.2% loss=0.16023, acc=0.96875
# [21/100] training 86.4% loss=0.37008, acc=0.85938
# [21/100] training 86.6% loss=0.26469, acc=0.90625
# [21/100] training 86.7% loss=0.17668, acc=0.95312
# [21/100] training 87.0% loss=0.33672, acc=0.89062
# [21/100] training 87.1% loss=0.25545, acc=0.92188
# [21/100] training 87.3% loss=0.35765, acc=0.84375
# [21/100] training 87.4% loss=0.27715, acc=0.84375
# [21/100] training 87.6% loss=0.24851, acc=0.85938
# [21/100] training 87.7% loss=0.32802, acc=0.93750
# [21/100] training 87.9% loss=0.25232, acc=0.90625
# [21/100] training 88.2% loss=0.17386, acc=0.89062
# [21/100] training 88.3% loss=0.22782, acc=0.90625
# [21/100] training 88.5% loss=0.18274, acc=0.93750
# [21/100] training 88.6% loss=0.15751, acc=0.93750
# [21/100] training 88.8% loss=0.25094, acc=0.92188
# [21/100] training 88.9% loss=0.28137, acc=0.89062
# [21/100] training 89.2% loss=0.21411, acc=0.89062
# [21/100] training 89.4% loss=0.22960, acc=0.90625
# [21/100] training 89.5% loss=0.22940, acc=0.89062
# [21/100] training 89.7% loss=0.25610, acc=0.89062
# [21/100] training 89.8% loss=0.26240, acc=0.92188
# [21/100] training 90.0% loss=0.18227, acc=0.92188
# [21/100] training 90.1% loss=0.22720, acc=0.92188
# [21/100] training 90.4% loss=0.24653, acc=0.90625
# [21/100] training 90.5% loss=0.13097, acc=0.96875
# [21/100] training 90.7% loss=0.22869, acc=0.90625
# [21/100] training 90.9% loss=0.13307, acc=0.93750
# [21/100] training 91.0% loss=0.25338, acc=0.90625
# [21/100] training 91.2% loss=0.25363, acc=0.90625
# [21/100] training 91.3% loss=0.41124, acc=0.82812
# [21/100] training 91.6% loss=0.39191, acc=0.84375
# [21/100] training 91.7% loss=0.15242, acc=0.96875
# [21/100] training 91.9% loss=0.32133, acc=0.89062
# [21/100] training 92.1% loss=0.26756, acc=0.85938
# [21/100] training 92.2% loss=0.15117, acc=0.93750
# [21/100] training 92.4% loss=0.26220, acc=0.90625
# [21/100] training 92.6% loss=0.34424, acc=0.85938
# [21/100] training 92.8% loss=0.45567, acc=0.82812
# [21/100] training 92.9% loss=0.32036, acc=0.89062
# [21/100] training 93.1% loss=0.33322, acc=0.85938
# [21/100] training 93.2% loss=0.19674, acc=0.93750
# [21/100] training 93.4% loss=0.22864, acc=0.90625
# [21/100] training 93.7% loss=0.22255, acc=0.90625
# [21/100] training 93.8% loss=0.23220, acc=0.90625
# [21/100] training 94.0% loss=0.22942, acc=0.92188
# [21/100] training 94.1% loss=0.29700, acc=0.87500
# [21/100] training 94.3% loss=0.16074, acc=0.95312
# [21/100] training 94.4% loss=0.28383, acc=0.87500
# [21/100] training 94.6% loss=0.13604, acc=0.96875
# [21/100] training 94.9% loss=0.14060, acc=0.96875
# [21/100] training 95.0% loss=0.44321, acc=0.81250
# [21/100] training 95.2% loss=0.36607, acc=0.79688
# [21/100] training 95.3% loss=0.29816, acc=0.92188
# [21/100] training 95.5% loss=0.18483, acc=0.95312
# [21/100] training 95.6% loss=0.39137, acc=0.85938
# [21/100] training 95.8% loss=0.25480, acc=0.87500
# [21/100] training 96.0% loss=0.30581, acc=0.89062
# [21/100] training 96.2% loss=0.17749, acc=0.93750
# [21/100] training 96.4% loss=0.18331, acc=0.96875
# [21/100] training 96.5% loss=0.28108, acc=0.89062
# [21/100] training 96.7% loss=0.18803, acc=0.90625
# [21/100] training 96.8% loss=0.30486, acc=0.90625
# [21/100] training 97.1% loss=0.17830, acc=0.92188
# [21/100] training 97.2% loss=0.26093, acc=0.90625
# [21/100] training 97.4% loss=0.25341, acc=0.90625
# [21/100] training 97.6% loss=0.31110, acc=0.82812
# [21/100] training 97.7% loss=0.27358, acc=0.90625
# [21/100] training 97.9% loss=0.24218, acc=0.89062
# [21/100] training 98.0% loss=0.29121, acc=0.85938
# [21/100] training 98.3% loss=0.23800, acc=0.85938
# [21/100] training 98.4% loss=0.25839, acc=0.92188
# [21/100] training 98.6% loss=0.49120, acc=0.82812
# [21/100] training 98.7% loss=0.28605, acc=0.87500
# [21/100] training 98.9% loss=0.19050, acc=0.92188
# [21/100] training 99.1% loss=0.18588, acc=0.93750
# [21/100] training 99.2% loss=0.19033, acc=0.93750
# [21/100] training 99.5% loss=0.34636, acc=0.85938
# [21/100] training 99.6% loss=0.20990, acc=0.90625
# [21/100] training 99.8% loss=0.18900, acc=0.93750
# [21/100] training 99.9% loss=0.08857, acc=0.96875
# [21/100] testing 0.9% loss=0.20772, acc=0.89062
# [21/100] testing 1.8% loss=0.41143, acc=0.79688
# [21/100] testing 2.2% loss=0.21715, acc=0.87500
# [21/100] testing 3.1% loss=0.34355, acc=0.87500
# [21/100] testing 3.5% loss=0.21816, acc=0.89062
# [21/100] testing 4.4% loss=0.22291, acc=0.89062
# [21/100] testing 4.8% loss=0.46783, acc=0.81250
# [21/100] testing 5.7% loss=0.22847, acc=0.90625
# [21/100] testing 6.6% loss=0.24462, acc=0.90625
# [21/100] testing 7.0% loss=0.13041, acc=0.95312
# [21/100] testing 7.9% loss=0.42201, acc=0.84375
# [21/100] testing 8.3% loss=0.23579, acc=0.90625
# [21/100] testing 9.2% loss=0.22255, acc=0.93750
# [21/100] testing 9.7% loss=0.12202, acc=0.95312
# [21/100] testing 10.5% loss=0.25527, acc=0.89062
# [21/100] testing 11.0% loss=0.26723, acc=0.89062
# [21/100] testing 11.8% loss=0.28446, acc=0.87500
# [21/100] testing 12.7% loss=0.30998, acc=0.89062
# [21/100] testing 13.2% loss=0.23600, acc=0.90625
# [21/100] testing 14.0% loss=0.36660, acc=0.92188
# [21/100] testing 14.5% loss=0.24573, acc=0.90625
# [21/100] testing 15.4% loss=0.44806, acc=0.84375
# [21/100] testing 15.8% loss=0.19433, acc=0.89062
# [21/100] testing 16.7% loss=0.29076, acc=0.87500
# [21/100] testing 17.5% loss=0.26786, acc=0.89062
# [21/100] testing 18.0% loss=0.24743, acc=0.90625
# [21/100] testing 18.9% loss=0.13125, acc=0.96875
# [21/100] testing 19.3% loss=0.30030, acc=0.84375
# [21/100] testing 20.2% loss=0.26415, acc=0.90625
# [21/100] testing 20.6% loss=0.28987, acc=0.87500
# [21/100] testing 21.5% loss=0.24650, acc=0.90625
# [21/100] testing 21.9% loss=0.31852, acc=0.90625
# [21/100] testing 22.8% loss=0.42594, acc=0.85938
# [21/100] testing 23.7% loss=0.34069, acc=0.87500
# [21/100] testing 24.1% loss=0.20889, acc=0.90625
# [21/100] testing 25.0% loss=0.34844, acc=0.90625
# [21/100] testing 25.4% loss=0.19387, acc=0.93750
# [21/100] testing 26.3% loss=0.31699, acc=0.85938
# [21/100] testing 26.8% loss=0.25954, acc=0.85938
# [21/100] testing 27.6% loss=0.31386, acc=0.87500
# [21/100] testing 28.5% loss=0.29353, acc=0.89062
# [21/100] testing 29.0% loss=0.23742, acc=0.87500
# [21/100] testing 29.8% loss=0.35092, acc=0.82812
# [21/100] testing 30.3% loss=0.21962, acc=0.90625
# [21/100] testing 31.1% loss=0.38365, acc=0.82812
# [21/100] testing 31.6% loss=0.26635, acc=0.92188
# [21/100] testing 32.5% loss=0.27969, acc=0.87500
# [21/100] testing 32.9% loss=0.36292, acc=0.92188
# [21/100] testing 33.8% loss=0.23544, acc=0.90625
# [21/100] testing 34.7% loss=0.33173, acc=0.85938
# [21/100] testing 35.1% loss=0.24409, acc=0.85938
# [21/100] testing 36.0% loss=0.28051, acc=0.89062
# [21/100] testing 36.4% loss=0.25062, acc=0.82812
# [21/100] testing 37.3% loss=0.33162, acc=0.89062
# [21/100] testing 37.7% loss=0.48495, acc=0.82812
# [21/100] testing 38.6% loss=0.28012, acc=0.90625
# [21/100] testing 39.5% loss=0.23634, acc=0.95312
# [21/100] testing 39.9% loss=0.33932, acc=0.89062
# [21/100] testing 40.8% loss=0.22140, acc=0.95312
# [21/100] testing 41.2% loss=0.28492, acc=0.92188
# [21/100] testing 42.1% loss=0.23721, acc=0.89062
# [21/100] testing 42.5% loss=0.19449, acc=0.92188
# [21/100] testing 43.4% loss=0.33885, acc=0.87500
# [21/100] testing 43.9% loss=0.18928, acc=0.93750
# [21/100] testing 44.7% loss=0.38085, acc=0.85938
# [21/100] testing 45.6% loss=0.25474, acc=0.90625
# [21/100] testing 46.1% loss=0.26848, acc=0.89062
# [21/100] testing 46.9% loss=0.27707, acc=0.87500
# [21/100] testing 47.4% loss=0.12279, acc=0.93750
# [21/100] testing 48.3% loss=0.39291, acc=0.84375
# [21/100] testing 48.7% loss=0.42133, acc=0.85938
# [21/100] testing 49.6% loss=0.44255, acc=0.78125
# [21/100] testing 50.4% loss=0.35169, acc=0.87500
# [21/100] testing 50.9% loss=0.36459, acc=0.85938
# [21/100] testing 51.8% loss=0.24511, acc=0.87500
# [21/100] testing 52.2% loss=0.24165, acc=0.87500
# [21/100] testing 53.1% loss=0.20280, acc=0.89062
# [21/100] testing 53.5% loss=0.37230, acc=0.87500
# [21/100] testing 54.4% loss=0.43728, acc=0.79688
# [21/100] testing 54.8% loss=0.37989, acc=0.81250
# [21/100] testing 55.7% loss=0.26827, acc=0.92188
# [21/100] testing 56.6% loss=0.21265, acc=0.90625
# [21/100] testing 57.0% loss=0.37351, acc=0.90625
# [21/100] testing 57.9% loss=0.38849, acc=0.84375
# [21/100] testing 58.3% loss=0.41575, acc=0.81250
# [21/100] testing 59.2% loss=0.25171, acc=0.89062
# [21/100] testing 59.7% loss=0.33145, acc=0.85938
# [21/100] testing 60.5% loss=0.28330, acc=0.85938
# [21/100] testing 61.4% loss=0.19888, acc=0.92188
# [21/100] testing 61.9% loss=0.37048, acc=0.85938
# [21/100] testing 62.7% loss=0.14658, acc=0.95312
# [21/100] testing 63.2% loss=0.43455, acc=0.85938
# [21/100] testing 64.0% loss=0.35141, acc=0.82812
# [21/100] testing 64.5% loss=0.18308, acc=0.92188
# [21/100] testing 65.4% loss=0.29318, acc=0.89062
# [21/100] testing 65.8% loss=0.36446, acc=0.82812
# [21/100] testing 66.7% loss=0.24522, acc=0.90625
# [21/100] testing 67.6% loss=0.28102, acc=0.90625
# [21/100] testing 68.0% loss=0.10957, acc=0.95312
# [21/100] testing 68.9% loss=0.37453, acc=0.85938
# [21/100] testing 69.3% loss=0.31612, acc=0.84375
# [21/100] testing 70.2% loss=0.38297, acc=0.85938
# [21/100] testing 70.6% loss=0.38718, acc=0.85938
# [21/100] testing 71.5% loss=0.33770, acc=0.89062
# [21/100] testing 72.4% loss=0.18744, acc=0.92188
# [21/100] testing 72.8% loss=0.20175, acc=0.90625
# [21/100] testing 73.7% loss=0.15277, acc=0.96875
# [21/100] testing 74.1% loss=0.34732, acc=0.89062
# [21/100] testing 75.0% loss=0.23495, acc=0.92188
# [21/100] testing 75.4% loss=0.44581, acc=0.81250
# [21/100] testing 76.3% loss=0.09920, acc=0.98438
# [21/100] testing 76.8% loss=0.31309, acc=0.87500
# [21/100] testing 77.6% loss=0.23521, acc=0.85938
# [21/100] testing 78.5% loss=0.64729, acc=0.78125
# [21/100] testing 79.0% loss=0.36535, acc=0.85938
# [21/100] testing 79.8% loss=0.33848, acc=0.81250
# [21/100] testing 80.3% loss=0.23641, acc=0.84375
# [21/100] testing 81.2% loss=0.44096, acc=0.84375
# [21/100] testing 81.6% loss=0.19505, acc=0.89062
# [21/100] testing 82.5% loss=0.28545, acc=0.85938
# [21/100] testing 83.3% loss=0.21609, acc=0.93750
# [21/100] testing 83.8% loss=0.17053, acc=0.93750
# [21/100] testing 84.7% loss=0.34370, acc=0.87500
# [21/100] testing 85.1% loss=0.25015, acc=0.92188
# [21/100] testing 86.0% loss=0.28919, acc=0.90625
# [21/100] testing 86.4% loss=0.34538, acc=0.81250
# [21/100] testing 87.3% loss=0.29044, acc=0.89062
# [21/100] testing 87.7% loss=0.22182, acc=0.87500
# [21/100] testing 88.6% loss=0.29874, acc=0.90625
# [21/100] testing 89.5% loss=0.39406, acc=0.84375
# [21/100] testing 89.9% loss=0.21673, acc=0.90625
# [21/100] testing 90.8% loss=0.29332, acc=0.90625
# [21/100] testing 91.2% loss=0.15645, acc=0.95312
# [21/100] testing 92.1% loss=0.48420, acc=0.89062
# [21/100] testing 92.6% loss=0.27778, acc=0.89062
# [21/100] testing 93.4% loss=0.34244, acc=0.84375
# [21/100] testing 94.3% loss=0.15919, acc=0.95312
# [21/100] testing 94.7% loss=0.20057, acc=0.93750
# [21/100] testing 95.6% loss=0.34166, acc=0.84375
# [21/100] testing 96.1% loss=0.18986, acc=0.89062
# [21/100] testing 96.9% loss=0.26500, acc=0.87500
# [21/100] testing 97.4% loss=0.12902, acc=0.96875
# [21/100] testing 98.3% loss=0.21079, acc=0.89062
# [21/100] testing 98.7% loss=0.25252, acc=0.90625
# [21/100] testing 99.6% loss=0.25640, acc=0.89062
# [22/100] training 0.2% loss=0.38014, acc=0.82812
# [22/100] training 0.4% loss=0.39914, acc=0.82812
# [22/100] training 0.5% loss=0.20166, acc=0.93750
# [22/100] training 0.8% loss=0.31552, acc=0.84375
# [22/100] training 0.9% loss=0.29579, acc=0.84375
# [22/100] training 1.1% loss=0.31139, acc=0.92188
# [22/100] training 1.2% loss=0.32420, acc=0.87500
# [22/100] training 1.4% loss=0.23158, acc=0.87500
# [22/100] training 1.6% loss=0.14378, acc=0.95312
# [22/100] training 1.8% loss=0.23825, acc=0.92188
# [22/100] training 2.0% loss=0.21180, acc=0.89062
# [22/100] training 2.1% loss=0.31118, acc=0.82812
# [22/100] training 2.3% loss=0.23847, acc=0.85938
# [22/100] training 2.4% loss=0.42019, acc=0.89062
# [22/100] training 2.6% loss=0.19743, acc=0.89062
# [22/100] training 2.7% loss=0.29317, acc=0.90625
# [22/100] training 3.0% loss=0.23644, acc=0.90625
# [22/100] training 3.2% loss=0.18949, acc=0.95312
# [22/100] training 3.3% loss=0.41789, acc=0.82812
# [22/100] training 3.5% loss=0.26590, acc=0.84375
# [22/100] training 3.6% loss=0.30847, acc=0.89062
# [22/100] training 3.8% loss=0.22072, acc=0.90625
# [22/100] training 3.9% loss=0.24370, acc=0.93750
# [22/100] training 4.2% loss=0.20894, acc=0.90625
# [22/100] training 4.4% loss=0.21621, acc=0.89062
# [22/100] training 4.5% loss=0.24602, acc=0.87500
# [22/100] training 4.7% loss=0.30051, acc=0.90625
# [22/100] training 4.8% loss=0.26426, acc=0.90625
# [22/100] training 5.0% loss=0.29615, acc=0.84375
# [22/100] training 5.2% loss=0.24653, acc=0.90625
# [22/100] training 5.4% loss=0.18492, acc=0.93750
# [22/100] training 5.5% loss=0.30940, acc=0.85938
# [22/100] training 5.7% loss=0.26178, acc=0.89062
# [22/100] training 5.9% loss=0.25702, acc=0.89062
# [22/100] training 6.0% loss=0.29833, acc=0.85938
# [22/100] training 6.3% loss=0.31221, acc=0.81250
# [22/100] training 6.4% loss=0.20078, acc=0.92188
# [22/100] training 6.6% loss=0.21750, acc=0.92188
# [22/100] training 6.7% loss=0.36686, acc=0.84375
# [22/100] training 6.9% loss=0.18821, acc=0.90625
# [22/100] training 7.1% loss=0.30089, acc=0.84375
# [22/100] training 7.2% loss=0.28398, acc=0.85938
# [22/100] training 7.5% loss=0.24431, acc=0.90625
# [22/100] training 7.6% loss=0.27110, acc=0.90625
# [22/100] training 7.8% loss=0.28662, acc=0.90625
# [22/100] training 7.9% loss=0.25810, acc=0.90625
# [22/100] training 8.1% loss=0.16332, acc=0.92188
# [22/100] training 8.2% loss=0.19899, acc=0.95312
# [22/100] training 8.4% loss=0.27384, acc=0.85938
# [22/100] training 8.7% loss=0.25013, acc=0.87500
# [22/100] training 8.8% loss=0.25805, acc=0.90625
# [22/100] training 9.0% loss=0.25013, acc=0.89062
# [22/100] training 9.1% loss=0.22699, acc=0.93750
# [22/100] training 9.3% loss=0.33290, acc=0.89062
# [22/100] training 9.4% loss=0.20448, acc=0.92188
# [22/100] training 9.7% loss=0.27647, acc=0.89062
# [22/100] training 9.9% loss=0.33107, acc=0.81250
# [22/100] training 10.0% loss=0.36776, acc=0.85938
# [22/100] training 10.2% loss=0.22639, acc=0.90625
# [22/100] training 10.3% loss=0.20117, acc=0.92188
# [22/100] training 10.5% loss=0.36794, acc=0.84375
# [22/100] training 10.6% loss=0.21562, acc=0.89062
# [22/100] training 10.9% loss=0.14970, acc=0.93750
# [22/100] training 11.0% loss=0.33568, acc=0.82812
# [22/100] training 11.2% loss=0.14634, acc=0.95312
# [22/100] training 11.4% loss=0.29023, acc=0.92188
# [22/100] training 11.5% loss=0.43677, acc=0.82812
# [22/100] training 11.7% loss=0.13308, acc=0.95312
# [22/100] training 11.8% loss=0.20292, acc=0.89062
# [22/100] training 12.1% loss=0.28529, acc=0.87500
# [22/100] training 12.2% loss=0.26129, acc=0.87500
# [22/100] training 12.4% loss=0.23512, acc=0.90625
# [22/100] training 12.6% loss=0.25911, acc=0.92188
# [22/100] training 12.7% loss=0.25164, acc=0.90625
# [22/100] training 12.9% loss=0.27075, acc=0.89062
# [22/100] training 13.0% loss=0.18859, acc=0.95312
# [22/100] training 13.3% loss=0.23040, acc=0.92188
# [22/100] training 13.4% loss=0.26442, acc=0.85938
# [22/100] training 13.6% loss=0.26685, acc=0.89062
# [22/100] training 13.7% loss=0.29748, acc=0.90625
# [22/100] training 13.9% loss=0.30233, acc=0.85938
# [22/100] training 14.1% loss=0.29853, acc=0.90625
# [22/100] training 14.3% loss=0.16180, acc=0.93750
# [22/100] training 14.5% loss=0.33308, acc=0.90625
# [22/100] training 14.6% loss=0.20629, acc=0.95312
# [22/100] training 14.8% loss=0.25491, acc=0.89062
# [22/100] training 14.9% loss=0.19708, acc=0.90625
# [22/100] training 15.1% loss=0.37696, acc=0.84375
# [22/100] training 15.4% loss=0.29241, acc=0.85938
# [22/100] training 15.5% loss=0.26303, acc=0.90625
# [22/100] training 15.7% loss=0.32255, acc=0.89062
# [22/100] training 15.8% loss=0.21473, acc=0.95312
# [22/100] training 16.0% loss=0.36495, acc=0.84375
# [22/100] training 16.1% loss=0.44203, acc=0.79688
# [22/100] training 16.3% loss=0.26943, acc=0.87500
# [22/100] training 16.4% loss=0.23058, acc=0.93750
# [22/100] training 16.7% loss=0.32040, acc=0.87500
# [22/100] training 16.9% loss=0.29529, acc=0.87500
# [22/100] training 17.0% loss=0.22219, acc=0.90625
# [22/100] training 17.2% loss=0.29919, acc=0.90625
# [22/100] training 17.3% loss=0.23926, acc=0.90625
# [22/100] training 17.5% loss=0.28066, acc=0.89062
# [22/100] training 17.7% loss=0.24060, acc=0.89062
# [22/100] training 17.9% loss=0.34481, acc=0.87500
# [22/100] training 18.1% loss=0.24930, acc=0.87500
# [22/100] training 18.2% loss=0.38227, acc=0.81250
# [22/100] training 18.4% loss=0.31832, acc=0.84375
# [22/100] training 18.5% loss=0.22442, acc=0.92188
# [22/100] training 18.8% loss=0.19941, acc=0.92188
# [22/100] training 18.9% loss=0.13622, acc=0.98438
# [22/100] training 19.1% loss=0.31759, acc=0.87500
# [22/100] training 19.2% loss=0.19403, acc=0.90625
# [22/100] training 19.4% loss=0.15740, acc=0.95312
# [22/100] training 19.6% loss=0.26347, acc=0.90625
# [22/100] training 19.7% loss=0.31847, acc=0.81250
# [22/100] training 20.0% loss=0.14755, acc=0.95312
# [22/100] training 20.1% loss=0.23611, acc=0.87500
# [22/100] training 20.3% loss=0.35885, acc=0.87500
# [22/100] training 20.4% loss=0.29259, acc=0.87500
# [22/100] training 20.6% loss=0.24096, acc=0.87500
# [22/100] training 20.8% loss=0.23194, acc=0.92188
# [22/100] training 20.9% loss=0.28260, acc=0.84375
# [22/100] training 21.2% loss=0.23578, acc=0.87500
# [22/100] training 21.3% loss=0.31885, acc=0.87500
# [22/100] training 21.5% loss=0.27068, acc=0.90625
# [22/100] training 21.6% loss=0.19297, acc=0.93750
# [22/100] training 21.8% loss=0.22807, acc=0.87500
# [22/100] training 21.9% loss=0.24493, acc=0.92188
# [22/100] training 22.2% loss=0.32256, acc=0.85938
# [22/100] training 22.4% loss=0.24271, acc=0.89062
# [22/100] training 22.5% loss=0.26719, acc=0.92188
# [22/100] training 22.7% loss=0.24491, acc=0.90625
# [22/100] training 22.8% loss=0.30852, acc=0.85938
# [22/100] training 23.0% loss=0.21031, acc=0.90625
# [22/100] training 23.1% loss=0.36594, acc=0.84375
# [22/100] training 23.4% loss=0.33534, acc=0.85938
# [22/100] training 23.6% loss=0.35405, acc=0.92188
# [22/100] training 23.7% loss=0.34589, acc=0.84375
# [22/100] training 23.9% loss=0.25159, acc=0.87500
# [22/100] training 24.0% loss=0.26410, acc=0.89062
# [22/100] training 24.2% loss=0.27704, acc=0.92188
# [22/100] training 24.3% loss=0.30021, acc=0.89062
# [22/100] training 24.6% loss=0.25398, acc=0.92188
# [22/100] training 24.7% loss=0.32305, acc=0.85938
# [22/100] training 24.9% loss=0.36348, acc=0.92188
# [22/100] training 25.1% loss=0.22598, acc=0.89062
# [22/100] training 25.2% loss=0.14901, acc=0.93750
# [22/100] training 25.4% loss=0.19808, acc=0.92188
# [22/100] training 25.6% loss=0.14729, acc=0.95312
# [22/100] training 25.8% loss=0.32842, acc=0.90625
# [22/100] training 25.9% loss=0.21165, acc=0.85938
# [22/100] training 26.1% loss=0.24310, acc=0.90625
# [22/100] training 26.3% loss=0.22883, acc=0.92188
# [22/100] training 26.4% loss=0.16491, acc=0.92188
# [22/100] training 26.6% loss=0.11391, acc=0.96875
# [22/100] training 26.8% loss=0.26879, acc=0.90625
# [22/100] training 27.0% loss=0.23974, acc=0.92188
# [22/100] training 27.1% loss=0.18128, acc=0.90625
# [22/100] training 27.3% loss=0.18611, acc=0.89062
# [22/100] training 27.4% loss=0.17264, acc=0.93750
# [22/100] training 27.6% loss=0.28208, acc=0.90625
# [22/100] training 27.9% loss=0.16771, acc=0.92188
# [22/100] training 28.0% loss=0.39569, acc=0.81250
# [22/100] training 28.2% loss=0.21065, acc=0.89062
# [22/100] training 28.3% loss=0.16243, acc=0.92188
# [22/100] training 28.5% loss=0.28878, acc=0.90625
# [22/100] training 28.6% loss=0.27531, acc=0.92188
# [22/100] training 28.8% loss=0.16053, acc=0.95312
# [22/100] training 29.1% loss=0.30131, acc=0.85938
# [22/100] training 29.2% loss=0.26514, acc=0.90625
# [22/100] training 29.4% loss=0.30806, acc=0.87500
# [22/100] training 29.5% loss=0.16461, acc=0.95312
# [22/100] training 29.7% loss=0.30844, acc=0.87500
# [22/100] training 29.8% loss=0.16689, acc=0.92188
# [22/100] training 30.0% loss=0.33253, acc=0.84375
# [22/100] training 30.2% loss=0.20067, acc=0.92188
# [22/100] training 30.4% loss=0.22961, acc=0.92188
# [22/100] training 30.6% loss=0.29234, acc=0.90625
# [22/100] training 30.7% loss=0.19892, acc=0.89062
# [22/100] training 30.9% loss=0.37478, acc=0.85938
# [22/100] training 31.0% loss=0.15112, acc=0.93750
# [22/100] training 31.3% loss=0.17247, acc=0.95312
# [22/100] training 31.4% loss=0.34486, acc=0.84375
# [22/100] training 31.6% loss=0.36041, acc=0.85938
# [22/100] training 31.8% loss=0.20732, acc=0.89062
# [22/100] training 31.9% loss=0.23551, acc=0.92188
# [22/100] training 32.1% loss=0.33502, acc=0.85938
# [22/100] training 32.2% loss=0.30795, acc=0.87500
# [22/100] training 32.5% loss=0.16481, acc=0.95312
# [22/100] training 32.6% loss=0.24040, acc=0.89062
# [22/100] training 32.8% loss=0.15646, acc=0.92188
# [22/100] training 32.9% loss=0.23988, acc=0.89062
# [22/100] training 33.1% loss=0.29802, acc=0.90625
# [22/100] training 33.3% loss=0.20207, acc=0.92188
# [22/100] training 33.4% loss=0.26872, acc=0.85938
# [22/100] training 33.7% loss=0.28860, acc=0.90625
# [22/100] training 33.8% loss=0.23316, acc=0.89062
# [22/100] training 34.0% loss=0.22883, acc=0.90625
# [22/100] training 34.1% loss=0.25187, acc=0.90625
# [22/100] training 34.3% loss=0.20396, acc=0.92188
# [22/100] training 34.5% loss=0.39724, acc=0.82812
# [22/100] training 34.7% loss=0.20626, acc=0.92188
# [22/100] training 34.9% loss=0.19101, acc=0.92188
# [22/100] training 35.0% loss=0.18674, acc=0.92188
# [22/100] training 35.2% loss=0.31453, acc=0.89062
# [22/100] training 35.3% loss=0.24698, acc=0.89062
# [22/100] training 35.5% loss=0.29110, acc=0.89062
# [22/100] training 35.6% loss=0.39507, acc=0.84375
# [22/100] training 35.9% loss=0.22135, acc=0.87500
# [22/100] training 36.1% loss=0.35657, acc=0.82812
# [22/100] training 36.2% loss=0.27937, acc=0.87500
# [22/100] training 36.4% loss=0.25177, acc=0.87500
# [22/100] training 36.5% loss=0.24946, acc=0.89062
# [22/100] training 36.7% loss=0.25243, acc=0.90625
# [22/100] training 36.8% loss=0.17006, acc=0.92188
# [22/100] training 37.1% loss=0.30839, acc=0.90625
# [22/100] training 37.3% loss=0.35294, acc=0.84375
# [22/100] training 37.4% loss=0.29745, acc=0.90625
# [22/100] training 37.6% loss=0.21968, acc=0.85938
# [22/100] training 37.7% loss=0.21414, acc=0.92188
# [22/100] training 37.9% loss=0.25952, acc=0.90625
# [22/100] training 38.1% loss=0.37386, acc=0.90625
# [22/100] training 38.3% loss=0.24964, acc=0.90625
# [22/100] training 38.4% loss=0.17228, acc=0.95312
# [22/100] training 38.6% loss=0.29154, acc=0.90625
# [22/100] training 38.8% loss=0.27807, acc=0.87500
# [22/100] training 38.9% loss=0.31819, acc=0.84375
# [22/100] training 39.1% loss=0.18871, acc=0.95312
# [22/100] training 39.3% loss=0.25596, acc=0.89062
# [22/100] training 39.5% loss=0.27511, acc=0.89062
# [22/100] training 39.6% loss=0.25797, acc=0.92188
# [22/100] training 39.8% loss=0.20047, acc=0.92188
# [22/100] training 40.0% loss=0.26584, acc=0.87500
# [22/100] training 40.1% loss=0.29203, acc=0.89062
# [22/100] training 40.4% loss=0.22999, acc=0.89062
# [22/100] training 40.5% loss=0.28560, acc=0.84375
# [22/100] training 40.7% loss=0.30837, acc=0.84375
# [22/100] training 40.8% loss=0.16227, acc=0.90625
# [22/100] training 41.0% loss=0.22647, acc=0.93750
# [22/100] training 41.1% loss=0.36392, acc=0.82812
# [22/100] training 41.3% loss=0.40387, acc=0.85938
# [22/100] training 41.6% loss=0.28906, acc=0.85938
# [22/100] training 41.7% loss=0.34847, acc=0.81250
# [22/100] training 41.9% loss=0.19252, acc=0.93750
# [22/100] training 42.0% loss=0.28295, acc=0.87500
# [22/100] training 42.2% loss=0.31374, acc=0.85938
# [22/100] training 42.3% loss=0.25577, acc=0.90625
# [22/100] training 42.5% loss=0.16160, acc=0.96875
# [22/100] training 42.8% loss=0.14398, acc=0.95312
# [22/100] training 42.9% loss=0.16309, acc=0.92188
# [22/100] training 43.1% loss=0.21992, acc=0.93750
# [22/100] training 43.2% loss=0.17418, acc=0.93750
# [22/100] training 43.4% loss=0.24101, acc=0.92188
# [22/100] training 43.5% loss=0.22196, acc=0.92188
# [22/100] training 43.8% loss=0.29148, acc=0.87500
# [22/100] training 43.9% loss=0.24768, acc=0.92188
# [22/100] training 44.1% loss=0.23502, acc=0.92188
# [22/100] training 44.3% loss=0.29729, acc=0.89062
# [22/100] training 44.4% loss=0.33708, acc=0.84375
# [22/100] training 44.6% loss=0.31464, acc=0.85938
# [22/100] training 44.7% loss=0.32221, acc=0.85938
# [22/100] training 45.0% loss=0.15301, acc=0.92188
# [22/100] training 45.1% loss=0.37503, acc=0.85938
# [22/100] training 45.3% loss=0.30284, acc=0.85938
# [22/100] training 45.5% loss=0.09466, acc=0.98438
# [22/100] training 45.6% loss=0.21283, acc=0.92188
# [22/100] training 45.8% loss=0.14938, acc=0.95312
# [22/100] training 45.9% loss=0.20706, acc=0.89062
# [22/100] training 46.2% loss=0.25773, acc=0.89062
# [22/100] training 46.3% loss=0.22443, acc=0.89062
# [22/100] training 46.5% loss=0.37044, acc=0.89062
# [22/100] training 46.6% loss=0.19394, acc=0.93750
# [22/100] training 46.8% loss=0.20353, acc=0.90625
# [22/100] training 47.0% loss=0.35530, acc=0.85938
# [22/100] training 47.2% loss=0.24518, acc=0.85938
# [22/100] training 47.4% loss=0.21670, acc=0.93750
# [22/100] training 47.5% loss=0.42376, acc=0.82812
# [22/100] training 47.7% loss=0.25647, acc=0.85938
# [22/100] training 47.8% loss=0.28180, acc=0.89062
# [22/100] training 48.0% loss=0.29005, acc=0.89062
# [22/100] training 48.3% loss=0.10469, acc=0.96875
# [22/100] training 48.4% loss=0.14046, acc=0.95312
# [22/100] training 48.6% loss=0.14225, acc=0.96875
# [22/100] training 48.7% loss=0.36556, acc=0.89062
# [22/100] training 48.9% loss=0.24502, acc=0.89062
# [22/100] training 49.0% loss=0.22884, acc=0.93750
# [22/100] training 49.2% loss=0.25561, acc=0.87500
# [22/100] training 49.3% loss=0.14643, acc=0.93750
# [22/100] training 49.6% loss=0.21588, acc=0.92188
# [22/100] training 49.8% loss=0.29794, acc=0.90625
# [22/100] training 49.9% loss=0.29027, acc=0.85938
# [22/100] training 50.1% loss=0.22649, acc=0.90625
# [22/100] training 50.2% loss=0.33499, acc=0.87500
# [22/100] training 50.4% loss=0.33973, acc=0.89062
# [22/100] training 50.6% loss=0.27341, acc=0.87500
# [22/100] training 50.8% loss=0.33520, acc=0.87500
# [22/100] training 51.0% loss=0.30923, acc=0.85938
# [22/100] training 51.1% loss=0.25416, acc=0.89062
# [22/100] training 51.3% loss=0.26406, acc=0.92188
# [22/100] training 51.4% loss=0.24962, acc=0.90625
# [22/100] training 51.7% loss=0.31707, acc=0.87500
# [22/100] training 51.8% loss=0.26515, acc=0.89062
# [22/100] training 52.0% loss=0.18570, acc=0.96875
# [22/100] training 52.1% loss=0.26385, acc=0.89062
# [22/100] training 52.3% loss=0.33360, acc=0.85938
# [22/100] training 52.5% loss=0.15549, acc=0.92188
# [22/100] training 52.6% loss=0.22212, acc=0.87500
# [22/100] training 52.9% loss=0.41184, acc=0.82812
# [22/100] training 53.0% loss=0.24210, acc=0.89062
# [22/100] training 53.2% loss=0.28135, acc=0.87500
# [22/100] training 53.3% loss=0.13087, acc=0.96875
# [22/100] training 53.5% loss=0.22135, acc=0.90625
# [22/100] training 53.7% loss=0.22491, acc=0.93750
# [22/100] training 53.8% loss=0.34261, acc=0.84375
# [22/100] training 54.1% loss=0.37582, acc=0.85938
# [22/100] training 54.2% loss=0.16431, acc=0.92188
# [22/100] training 54.4% loss=0.22817, acc=0.95312
# [22/100] training 54.5% loss=0.29716, acc=0.84375
# [22/100] training 54.7% loss=0.48329, acc=0.79688
# [22/100] training 54.8% loss=0.12846, acc=0.95312
# [22/100] training 55.1% loss=0.23008, acc=0.90625
# [22/100] training 55.3% loss=0.15336, acc=0.93750
# [22/100] training 55.4% loss=0.27665, acc=0.89062
# [22/100] training 55.6% loss=0.28867, acc=0.92188
# [22/100] training 55.7% loss=0.28099, acc=0.90625
# [22/100] training 55.9% loss=0.21265, acc=0.92188
# [22/100] training 56.0% loss=0.26775, acc=0.89062
# [22/100] training 56.3% loss=0.44542, acc=0.75000
# [22/100] training 56.5% loss=0.28952, acc=0.89062
# [22/100] training 56.6% loss=0.27753, acc=0.85938
# [22/100] training 56.8% loss=0.30956, acc=0.87500
# [22/100] training 56.9% loss=0.24715, acc=0.92188
# [22/100] training 57.1% loss=0.38288, acc=0.84375
# [22/100] training 57.2% loss=0.19160, acc=0.92188
# [22/100] training 57.5% loss=0.17186, acc=0.93750
# [22/100] training 57.6% loss=0.28995, acc=0.84375
# [22/100] training 57.8% loss=0.15845, acc=0.95312
# [22/100] training 58.0% loss=0.18401, acc=0.93750
# [22/100] training 58.1% loss=0.22077, acc=0.92188
# [22/100] training 58.3% loss=0.16430, acc=0.92188
# [22/100] training 58.4% loss=0.25550, acc=0.87500
# [22/100] training 58.7% loss=0.28663, acc=0.89062
# [22/100] training 58.8% loss=0.30710, acc=0.85938
# [22/100] training 59.0% loss=0.18202, acc=0.90625
# [22/100] training 59.2% loss=0.23679, acc=0.92188
# [22/100] training 59.3% loss=0.25039, acc=0.85938
# [22/100] training 59.5% loss=0.22735, acc=0.92188
# [22/100] training 59.7% loss=0.32642, acc=0.89062
# [22/100] training 59.9% loss=0.21976, acc=0.92188
# [22/100] training 60.0% loss=0.21011, acc=0.93750
# [22/100] training 60.2% loss=0.23278, acc=0.89062
# [22/100] training 60.3% loss=0.14022, acc=0.95312
# [22/100] training 60.5% loss=0.23196, acc=0.92188
# [22/100] training 60.8% loss=0.26019, acc=0.87500
# [22/100] training 60.9% loss=0.34341, acc=0.85938
# [22/100] training 61.1% loss=0.28757, acc=0.84375
# [22/100] training 61.2% loss=0.22918, acc=0.92188
# [22/100] training 61.4% loss=0.36073, acc=0.79688
# [22/100] training 61.5% loss=0.32283, acc=0.85938
# [22/100] training 61.7% loss=0.27920, acc=0.90625
# [22/100] training 62.0% loss=0.34124, acc=0.84375
# [22/100] training 62.1% loss=0.32989, acc=0.81250
# [22/100] training 62.3% loss=0.11301, acc=0.98438
# [22/100] training 62.4% loss=0.17952, acc=0.92188
# [22/100] training 62.6% loss=0.34889, acc=0.81250
# [22/100] training 62.7% loss=0.19822, acc=0.90625
# [22/100] training 62.9% loss=0.30407, acc=0.82812
# [22/100] training 63.1% loss=0.22453, acc=0.90625
# [22/100] training 63.3% loss=0.17658, acc=0.93750
# [22/100] training 63.5% loss=0.31445, acc=0.85938
# [22/100] training 63.6% loss=0.39959, acc=0.84375
# [22/100] training 63.8% loss=0.38037, acc=0.81250
# [22/100] training 63.9% loss=0.21183, acc=0.93750
# [22/100] training 64.2% loss=0.19566, acc=0.89062
# [22/100] training 64.3% loss=0.25784, acc=0.87500
# [22/100] training 64.5% loss=0.28635, acc=0.87500
# [22/100] training 64.7% loss=0.23113, acc=0.87500
# [22/100] training 64.8% loss=0.43877, acc=0.84375
# [22/100] training 65.0% loss=0.25441, acc=0.93750
# [22/100] training 65.1% loss=0.33446, acc=0.87500
# [22/100] training 65.4% loss=0.20963, acc=0.90625
# [22/100] training 65.5% loss=0.16448, acc=0.95312
# [22/100] training 65.7% loss=0.19455, acc=0.92188
# [22/100] training 65.8% loss=0.22819, acc=0.90625
# [22/100] training 66.0% loss=0.20663, acc=0.92188
# [22/100] training 66.2% loss=0.13436, acc=0.95312
# [22/100] training 66.3% loss=0.36481, acc=0.85938
# [22/100] training 66.6% loss=0.23718, acc=0.90625
# [22/100] training 66.7% loss=0.16879, acc=0.93750
# [22/100] training 66.9% loss=0.22997, acc=0.87500
# [22/100] training 67.0% loss=0.24377, acc=0.93750
# [22/100] training 67.2% loss=0.16791, acc=0.93750
# [22/100] training 67.4% loss=0.21296, acc=0.87500
# [22/100] training 67.6% loss=0.35179, acc=0.85938
# [22/100] training 67.8% loss=0.25780, acc=0.92188
# [22/100] training 67.9% loss=0.17067, acc=0.92188
# [22/100] training 68.1% loss=0.20829, acc=0.90625
# [22/100] training 68.2% loss=0.15996, acc=0.93750
# [22/100] training 68.4% loss=0.21032, acc=0.93750
# [22/100] training 68.5% loss=0.27965, acc=0.87500
# [22/100] training 68.8% loss=0.26221, acc=0.87500
# [22/100] training 69.0% loss=0.28473, acc=0.92188
# [22/100] training 69.1% loss=0.18971, acc=0.90625
# [22/100] training 69.3% loss=0.27508, acc=0.85938
# [22/100] training 69.4% loss=0.18353, acc=0.90625
# [22/100] training 69.6% loss=0.23203, acc=0.87500
# [22/100] training 69.7% loss=0.30167, acc=0.85938
# [22/100] training 70.0% loss=0.36922, acc=0.90625
# [22/100] training 70.2% loss=0.22130, acc=0.92188
# [22/100] training 70.3% loss=0.21065, acc=0.89062
# [22/100] training 70.5% loss=0.25787, acc=0.87500
# [22/100] training 70.6% loss=0.19723, acc=0.95312
# [22/100] training 70.8% loss=0.30477, acc=0.84375
# [22/100] training 71.0% loss=0.43075, acc=0.79688
# [22/100] training 71.2% loss=0.21131, acc=0.90625
# [22/100] training 71.3% loss=0.14433, acc=0.96875
# [22/100] training 71.5% loss=0.25707, acc=0.90625
# [22/100] training 71.7% loss=0.28227, acc=0.89062
# [22/100] training 71.8% loss=0.25962, acc=0.92188
# [22/100] training 72.0% loss=0.16672, acc=0.90625
# [22/100] training 72.2% loss=0.22507, acc=0.90625
# [22/100] training 72.4% loss=0.29951, acc=0.87500
# [22/100] training 72.5% loss=0.36039, acc=0.89062
# [22/100] training 72.7% loss=0.31611, acc=0.85938
# [22/100] training 72.9% loss=0.18452, acc=0.93750
# [22/100] training 73.0% loss=0.17677, acc=0.93750
# [22/100] training 73.3% loss=0.26406, acc=0.90625
# [22/100] training 73.4% loss=0.20336, acc=0.92188
# [22/100] training 73.6% loss=0.13361, acc=0.96875
# [22/100] training 73.7% loss=0.24897, acc=0.92188
# [22/100] training 73.9% loss=0.23967, acc=0.90625
# [22/100] training 74.0% loss=0.31705, acc=0.85938
# [22/100] training 74.2% loss=0.18064, acc=0.93750
# [22/100] training 74.5% loss=0.15727, acc=0.92188
# [22/100] training 74.6% loss=0.47484, acc=0.78125
# [22/100] training 74.8% loss=0.42693, acc=0.79688
# [22/100] training 74.9% loss=0.45102, acc=0.82812
# [22/100] training 75.1% loss=0.22844, acc=0.89062
# [22/100] training 75.2% loss=0.21421, acc=0.96875
# [22/100] training 75.4% loss=0.33913, acc=0.85938
# [22/100] training 75.7% loss=0.30374, acc=0.82812
# [22/100] training 75.8% loss=0.24560, acc=0.90625
# [22/100] training 76.0% loss=0.13535, acc=0.95312
# [22/100] training 76.1% loss=0.39649, acc=0.85938
# [22/100] training 76.3% loss=0.16501, acc=0.95312
# [22/100] training 76.4% loss=0.30862, acc=0.84375
# [22/100] training 76.7% loss=0.23493, acc=0.93750
# [22/100] training 76.8% loss=0.15671, acc=0.96875
# [22/100] training 77.0% loss=0.15042, acc=0.90625
# [22/100] training 77.2% loss=0.32011, acc=0.89062
# [22/100] training 77.3% loss=0.19541, acc=0.89062
# [22/100] training 77.5% loss=0.16129, acc=0.92188
# [22/100] training 77.6% loss=0.21457, acc=0.92188
# [22/100] training 77.9% loss=0.23937, acc=0.89062
# [22/100] training 78.0% loss=0.17127, acc=0.93750
# [22/100] training 78.2% loss=0.33683, acc=0.90625
# [22/100] training 78.4% loss=0.11816, acc=0.95312
# [22/100] training 78.5% loss=0.41616, acc=0.89062
# [22/100] training 78.7% loss=0.26155, acc=0.89062
# [22/100] training 78.8% loss=0.13506, acc=0.96875
# [22/100] training 79.1% loss=0.16332, acc=0.93750
# [22/100] training 79.2% loss=0.19536, acc=0.95312
# [22/100] training 79.4% loss=0.29972, acc=0.89062
# [22/100] training 79.5% loss=0.22181, acc=0.89062
# [22/100] training 79.7% loss=0.13382, acc=0.96875
# [22/100] training 79.9% loss=0.18218, acc=0.92188
# [22/100] training 80.1% loss=0.20671, acc=0.90625
# [22/100] training 80.3% loss=0.35597, acc=0.84375
# [22/100] training 80.4% loss=0.22582, acc=0.92188
# [22/100] training 80.6% loss=0.19963, acc=0.92188
# [22/100] training 80.7% loss=0.22939, acc=0.93750
# [22/100] training 80.9% loss=0.29183, acc=0.85938
# [22/100] training 81.2% loss=0.28290, acc=0.87500
# [22/100] training 81.3% loss=0.26944, acc=0.87500
# [22/100] training 81.5% loss=0.21426, acc=0.90625
# [22/100] training 81.6% loss=0.28552, acc=0.90625
# [22/100] training 81.8% loss=0.28564, acc=0.90625
# [22/100] training 81.9% loss=0.30277, acc=0.90625
# [22/100] training 82.1% loss=0.18888, acc=0.95312
# [22/100] training 82.2% loss=0.24037, acc=0.87500
# [22/100] training 82.5% loss=0.16832, acc=0.90625
# [22/100] training 82.7% loss=0.20061, acc=0.95312
# [22/100] training 82.8% loss=0.17854, acc=0.95312
# [22/100] training 83.0% loss=0.14804, acc=0.95312
# [22/100] training 83.1% loss=0.22338, acc=0.93750
# [22/100] training 83.3% loss=0.20440, acc=0.92188
# [22/100] training 83.5% loss=0.23477, acc=0.92188
# [22/100] training 83.7% loss=0.36186, acc=0.84375
# [22/100] training 83.9% loss=0.28650, acc=0.89062
# [22/100] training 84.0% loss=0.22726, acc=0.90625
# [22/100] training 84.2% loss=0.16544, acc=0.93750
# [22/100] training 84.3% loss=0.22315, acc=0.90625
# [22/100] training 84.5% loss=0.29496, acc=0.90625
# [22/100] training 84.7% loss=0.23959, acc=0.87500
# [22/100] training 84.9% loss=0.19661, acc=0.90625
# [22/100] training 85.0% loss=0.19453, acc=0.90625
# [22/100] training 85.2% loss=0.21115, acc=0.90625
# [22/100] training 85.4% loss=0.18717, acc=0.93750
# [22/100] training 85.5% loss=0.24736, acc=0.85938
# [22/100] training 85.8% loss=0.23929, acc=0.87500
# [22/100] training 85.9% loss=0.30400, acc=0.87500
# [22/100] training 86.1% loss=0.21199, acc=0.89062
# [22/100] training 86.2% loss=0.17701, acc=0.90625
# [22/100] training 86.4% loss=0.32140, acc=0.87500
# [22/100] training 86.6% loss=0.23850, acc=0.89062
# [22/100] training 86.7% loss=0.19179, acc=0.93750
# [22/100] training 87.0% loss=0.27289, acc=0.90625
# [22/100] training 87.1% loss=0.29549, acc=0.87500
# [22/100] training 87.3% loss=0.24846, acc=0.89062
# [22/100] training 87.4% loss=0.30520, acc=0.87500
# [22/100] training 87.6% loss=0.23934, acc=0.90625
# [22/100] training 87.7% loss=0.39496, acc=0.89062
# [22/100] training 87.9% loss=0.30454, acc=0.85938
# [22/100] training 88.2% loss=0.19088, acc=0.89062
# [22/100] training 88.3% loss=0.28250, acc=0.89062
# [22/100] training 88.5% loss=0.24683, acc=0.90625
# [22/100] training 88.6% loss=0.14688, acc=0.93750
# [22/100] training 88.8% loss=0.18622, acc=0.93750
# [22/100] training 88.9% loss=0.28381, acc=0.87500
# [22/100] training 89.2% loss=0.17113, acc=0.95312
# [22/100] training 89.4% loss=0.16248, acc=0.96875
# [22/100] training 89.5% loss=0.32714, acc=0.87500
# [22/100] training 89.7% loss=0.19585, acc=0.95312
# [22/100] training 89.8% loss=0.18127, acc=0.90625
# [22/100] training 90.0% loss=0.14887, acc=0.95312
# [22/100] training 90.1% loss=0.24908, acc=0.89062
# [22/100] training 90.4% loss=0.26620, acc=0.90625
# [22/100] training 90.5% loss=0.12453, acc=0.95312
# [22/100] training 90.7% loss=0.21135, acc=0.92188
# [22/100] training 90.9% loss=0.19254, acc=0.93750
# [22/100] training 91.0% loss=0.22961, acc=0.89062
# [22/100] training 91.2% loss=0.27595, acc=0.87500
# [22/100] training 91.3% loss=0.37887, acc=0.84375
# [22/100] training 91.6% loss=0.44214, acc=0.82812
# [22/100] training 91.7% loss=0.18222, acc=0.93750
# [22/100] training 91.9% loss=0.31846, acc=0.87500
# [22/100] training 92.1% loss=0.27438, acc=0.85938
# [22/100] training 92.2% loss=0.15581, acc=0.92188
# [22/100] training 92.4% loss=0.18107, acc=0.93750
# [22/100] training 92.6% loss=0.26465, acc=0.89062
# [22/100] training 92.8% loss=0.38561, acc=0.87500
# [22/100] training 92.9% loss=0.28202, acc=0.87500
# [22/100] training 93.1% loss=0.34162, acc=0.85938
# [22/100] training 93.2% loss=0.29122, acc=0.87500
# [22/100] training 93.4% loss=0.29629, acc=0.92188
# [22/100] training 93.7% loss=0.16933, acc=0.95312
# [22/100] training 93.8% loss=0.24473, acc=0.84375
# [22/100] training 94.0% loss=0.22597, acc=0.90625
# [22/100] training 94.1% loss=0.29362, acc=0.87500
# [22/100] training 94.3% loss=0.19180, acc=0.92188
# [22/100] training 94.4% loss=0.29264, acc=0.85938
# [22/100] training 94.6% loss=0.27775, acc=0.89062
# [22/100] training 94.9% loss=0.18136, acc=0.87500
# [22/100] training 95.0% loss=0.32545, acc=0.87500
# [22/100] training 95.2% loss=0.35618, acc=0.87500
# [22/100] training 95.3% loss=0.30394, acc=0.92188
# [22/100] training 95.5% loss=0.21125, acc=0.95312
# [22/100] training 95.6% loss=0.32162, acc=0.89062
# [22/100] training 95.8% loss=0.26964, acc=0.84375
# [22/100] training 96.0% loss=0.25393, acc=0.90625
# [22/100] training 96.2% loss=0.17209, acc=0.92188
# [22/100] training 96.4% loss=0.24104, acc=0.93750
# [22/100] training 96.5% loss=0.24436, acc=0.93750
# [22/100] training 96.7% loss=0.17386, acc=0.92188
# [22/100] training 96.8% loss=0.20674, acc=0.92188
# [22/100] training 97.1% loss=0.22332, acc=0.90625
# [22/100] training 97.2% loss=0.17541, acc=0.93750
# [22/100] training 97.4% loss=0.25174, acc=0.92188
# [22/100] training 97.6% loss=0.37481, acc=0.85938
# [22/100] training 97.7% loss=0.21485, acc=0.90625
# [22/100] training 97.9% loss=0.22398, acc=0.92188
# [22/100] training 98.0% loss=0.25154, acc=0.87500
# [22/100] training 98.3% loss=0.21843, acc=0.92188
# [22/100] training 98.4% loss=0.24145, acc=0.89062
# [22/100] training 98.6% loss=0.41713, acc=0.82812
# [22/100] training 98.7% loss=0.35073, acc=0.84375
# [22/100] training 98.9% loss=0.18416, acc=0.92188
# [22/100] training 99.1% loss=0.18911, acc=0.95312
# [22/100] training 99.2% loss=0.21321, acc=0.92188
# [22/100] training 99.5% loss=0.34474, acc=0.87500
# [22/100] training 99.6% loss=0.21452, acc=0.92188
# [22/100] training 99.8% loss=0.18213, acc=0.95312
# [22/100] training 99.9% loss=0.08877, acc=0.95312
# [22/100] testing 0.9% loss=0.13693, acc=0.90625
# [22/100] testing 1.8% loss=0.48186, acc=0.82812
# [22/100] testing 2.2% loss=0.30151, acc=0.89062
# [22/100] testing 3.1% loss=0.32045, acc=0.90625
# [22/100] testing 3.5% loss=0.21976, acc=0.89062
# [22/100] testing 4.4% loss=0.19969, acc=0.90625
# [22/100] testing 4.8% loss=0.59459, acc=0.81250
# [22/100] testing 5.7% loss=0.23999, acc=0.90625
# [22/100] testing 6.6% loss=0.24520, acc=0.93750
# [22/100] testing 7.0% loss=0.18217, acc=0.92188
# [22/100] testing 7.9% loss=0.34306, acc=0.85938
# [22/100] testing 8.3% loss=0.27018, acc=0.92188
# [22/100] testing 9.2% loss=0.24487, acc=0.89062
# [22/100] testing 9.7% loss=0.13943, acc=0.92188
# [22/100] testing 10.5% loss=0.32805, acc=0.87500
# [22/100] testing 11.0% loss=0.27847, acc=0.87500
# [22/100] testing 11.8% loss=0.39249, acc=0.82812
# [22/100] testing 12.7% loss=0.39279, acc=0.89062
# [22/100] testing 13.2% loss=0.26716, acc=0.89062
# [22/100] testing 14.0% loss=0.35355, acc=0.92188
# [22/100] testing 14.5% loss=0.33020, acc=0.90625
# [22/100] testing 15.4% loss=0.35323, acc=0.89062
# [22/100] testing 15.8% loss=0.18658, acc=0.89062
# [22/100] testing 16.7% loss=0.26672, acc=0.89062
# [22/100] testing 17.5% loss=0.25690, acc=0.89062
# [22/100] testing 18.0% loss=0.24487, acc=0.87500
# [22/100] testing 18.9% loss=0.11487, acc=0.93750
# [22/100] testing 19.3% loss=0.38314, acc=0.87500
# [22/100] testing 20.2% loss=0.37193, acc=0.89062
# [22/100] testing 20.6% loss=0.42303, acc=0.84375
# [22/100] testing 21.5% loss=0.23382, acc=0.90625
# [22/100] testing 21.9% loss=0.34901, acc=0.87500
# [22/100] testing 22.8% loss=0.31318, acc=0.87500
# [22/100] testing 23.7% loss=0.39150, acc=0.87500
# [22/100] testing 24.1% loss=0.28041, acc=0.90625
# [22/100] testing 25.0% loss=0.35666, acc=0.90625
# [22/100] testing 25.4% loss=0.14084, acc=0.95312
# [22/100] testing 26.3% loss=0.31977, acc=0.85938
# [22/100] testing 26.8% loss=0.32169, acc=0.89062
# [22/100] testing 27.6% loss=0.27045, acc=0.90625
# [22/100] testing 28.5% loss=0.30746, acc=0.87500
# [22/100] testing 29.0% loss=0.20440, acc=0.93750
# [22/100] testing 29.8% loss=0.46920, acc=0.84375
# [22/100] testing 30.3% loss=0.28242, acc=0.90625
# [22/100] testing 31.1% loss=0.39732, acc=0.85938
# [22/100] testing 31.6% loss=0.22685, acc=0.92188
# [22/100] testing 32.5% loss=0.33856, acc=0.84375
# [22/100] testing 32.9% loss=0.34888, acc=0.90625
# [22/100] testing 33.8% loss=0.23240, acc=0.92188
# [22/100] testing 34.7% loss=0.46687, acc=0.82812
# [22/100] testing 35.1% loss=0.23078, acc=0.90625
# [22/100] testing 36.0% loss=0.27051, acc=0.92188
# [22/100] testing 36.4% loss=0.30308, acc=0.89062
# [22/100] testing 37.3% loss=0.31930, acc=0.92188
# [22/100] testing 37.7% loss=0.42734, acc=0.85938
# [22/100] testing 38.6% loss=0.23827, acc=0.92188
# [22/100] testing 39.5% loss=0.33815, acc=0.89062
# [22/100] testing 39.9% loss=0.30108, acc=0.90625
# [22/100] testing 40.8% loss=0.25471, acc=0.93750
# [22/100] testing 41.2% loss=0.30199, acc=0.92188
# [22/100] testing 42.1% loss=0.38951, acc=0.89062
# [22/100] testing 42.5% loss=0.24973, acc=0.92188
# [22/100] testing 43.4% loss=0.44278, acc=0.85938
# [22/100] testing 43.9% loss=0.15149, acc=0.95312
# [22/100] testing 44.7% loss=0.40994, acc=0.87500
# [22/100] testing 45.6% loss=0.24773, acc=0.87500
# [22/100] testing 46.1% loss=0.29860, acc=0.84375
# [22/100] testing 46.9% loss=0.25655, acc=0.87500
# [22/100] testing 47.4% loss=0.14534, acc=0.90625
# [22/100] testing 48.3% loss=0.38517, acc=0.85938
# [22/100] testing 48.7% loss=0.46121, acc=0.85938
# [22/100] testing 49.6% loss=0.46732, acc=0.84375
# [22/100] testing 50.4% loss=0.25101, acc=0.89062
# [22/100] testing 50.9% loss=0.35414, acc=0.89062
# [22/100] testing 51.8% loss=0.25075, acc=0.84375
# [22/100] testing 52.2% loss=0.26253, acc=0.87500
# [22/100] testing 53.1% loss=0.22687, acc=0.90625
# [22/100] testing 53.5% loss=0.29395, acc=0.89062
# [22/100] testing 54.4% loss=0.51944, acc=0.79688
# [22/100] testing 54.8% loss=0.42835, acc=0.85938
# [22/100] testing 55.7% loss=0.18918, acc=0.93750
# [22/100] testing 56.6% loss=0.40227, acc=0.84375
# [22/100] testing 57.0% loss=0.48321, acc=0.81250
# [22/100] testing 57.9% loss=0.31897, acc=0.87500
# [22/100] testing 58.3% loss=0.50882, acc=0.81250
# [22/100] testing 59.2% loss=0.29645, acc=0.87500
# [22/100] testing 59.7% loss=0.38194, acc=0.85938
# [22/100] testing 60.5% loss=0.39311, acc=0.87500
# [22/100] testing 61.4% loss=0.14989, acc=0.90625
# [22/100] testing 61.9% loss=0.33737, acc=0.90625
# [22/100] testing 62.7% loss=0.17995, acc=0.93750
# [22/100] testing 63.2% loss=0.41018, acc=0.84375
# [22/100] testing 64.0% loss=0.54227, acc=0.78125
# [22/100] testing 64.5% loss=0.16389, acc=0.92188
# [22/100] testing 65.4% loss=0.24770, acc=0.93750
# [22/100] testing 65.8% loss=0.42988, acc=0.82812
# [22/100] testing 66.7% loss=0.32862, acc=0.87500
# [22/100] testing 67.6% loss=0.32599, acc=0.90625
# [22/100] testing 68.0% loss=0.20407, acc=0.92188
# [22/100] testing 68.9% loss=0.38487, acc=0.87500
# [22/100] testing 69.3% loss=0.36830, acc=0.87500
# [22/100] testing 70.2% loss=0.41813, acc=0.82812
# [22/100] testing 70.6% loss=0.50914, acc=0.79688
# [22/100] testing 71.5% loss=0.43550, acc=0.84375
# [22/100] testing 72.4% loss=0.22621, acc=0.90625
# [22/100] testing 72.8% loss=0.24492, acc=0.84375
# [22/100] testing 73.7% loss=0.24199, acc=0.92188
# [22/100] testing 74.1% loss=0.34929, acc=0.90625
# [22/100] testing 75.0% loss=0.15502, acc=0.93750
# [22/100] testing 75.4% loss=0.33239, acc=0.84375
# [22/100] testing 76.3% loss=0.13318, acc=0.93750
# [22/100] testing 76.8% loss=0.23556, acc=0.87500
# [22/100] testing 77.6% loss=0.29438, acc=0.82812
# [22/100] testing 78.5% loss=0.62875, acc=0.81250
# [22/100] testing 79.0% loss=0.34031, acc=0.81250
# [22/100] testing 79.8% loss=0.28159, acc=0.87500
# [22/100] testing 80.3% loss=0.33975, acc=0.89062
# [22/100] testing 81.2% loss=0.49607, acc=0.85938
# [22/100] testing 81.6% loss=0.24023, acc=0.92188
# [22/100] testing 82.5% loss=0.19198, acc=0.92188
# [22/100] testing 83.3% loss=0.16706, acc=0.95312
# [22/100] testing 83.8% loss=0.20891, acc=0.93750
# [22/100] testing 84.7% loss=0.29704, acc=0.93750
# [22/100] testing 85.1% loss=0.35016, acc=0.85938
# [22/100] testing 86.0% loss=0.28379, acc=0.89062
# [22/100] testing 86.4% loss=0.48505, acc=0.84375
# [22/100] testing 87.3% loss=0.35407, acc=0.84375
# [22/100] testing 87.7% loss=0.22141, acc=0.92188
# [22/100] testing 88.6% loss=0.32149, acc=0.85938
# [22/100] testing 89.5% loss=0.49976, acc=0.78125
# [22/100] testing 89.9% loss=0.25690, acc=0.89062
# [22/100] testing 90.8% loss=0.26676, acc=0.93750
# [22/100] testing 91.2% loss=0.12381, acc=0.93750
# [22/100] testing 92.1% loss=0.33618, acc=0.90625
# [22/100] testing 92.6% loss=0.22866, acc=0.90625
# [22/100] testing 93.4% loss=0.34067, acc=0.84375
# [22/100] testing 94.3% loss=0.22158, acc=0.89062
# [22/100] testing 94.7% loss=0.24338, acc=0.93750
# [22/100] testing 95.6% loss=0.34500, acc=0.87500
# [22/100] testing 96.1% loss=0.23229, acc=0.90625
# [22/100] testing 96.9% loss=0.20981, acc=0.90625
# [22/100] testing 97.4% loss=0.11962, acc=0.95312
# [22/100] testing 98.3% loss=0.30530, acc=0.87500
# [22/100] testing 98.7% loss=0.24862, acc=0.92188
# [22/100] testing 99.6% loss=0.24726, acc=0.90625
# [23/100] training 0.2% loss=0.49269, acc=0.84375
# [23/100] training 0.4% loss=0.44082, acc=0.82812
# [23/100] training 0.5% loss=0.15400, acc=0.96875
# [23/100] training 0.8% loss=0.27961, acc=0.85938
# [23/100] training 0.9% loss=0.24344, acc=0.90625
# [23/100] training 1.1% loss=0.22093, acc=0.93750
# [23/100] training 1.2% loss=0.32294, acc=0.85938
# [23/100] training 1.4% loss=0.21443, acc=0.93750
# [23/100] training 1.6% loss=0.15888, acc=0.92188
# [23/100] training 1.8% loss=0.24444, acc=0.90625
# [23/100] training 2.0% loss=0.32166, acc=0.85938
# [23/100] training 2.1% loss=0.26636, acc=0.85938
# [23/100] training 2.3% loss=0.14016, acc=0.93750
# [23/100] training 2.4% loss=0.27328, acc=0.90625
# [23/100] training 2.6% loss=0.29777, acc=0.89062
# [23/100] training 2.7% loss=0.34452, acc=0.85938
# [23/100] training 3.0% loss=0.31564, acc=0.82812
# [23/100] training 3.2% loss=0.16828, acc=0.95312
# [23/100] training 3.3% loss=0.35023, acc=0.84375
# [23/100] training 3.5% loss=0.28013, acc=0.82812
# [23/100] training 3.6% loss=0.36620, acc=0.85938
# [23/100] training 3.8% loss=0.15686, acc=0.92188
# [23/100] training 3.9% loss=0.28046, acc=0.85938
# [23/100] training 4.2% loss=0.13533, acc=0.98438
# [23/100] training 4.4% loss=0.15618, acc=0.95312
# [23/100] training 4.5% loss=0.19938, acc=0.90625
# [23/100] training 4.7% loss=0.36296, acc=0.84375
# [23/100] training 4.8% loss=0.24932, acc=0.92188
# [23/100] training 5.0% loss=0.18777, acc=0.92188
# [23/100] training 5.2% loss=0.21107, acc=0.92188
# [23/100] training 5.4% loss=0.14024, acc=0.95312
# [23/100] training 5.5% loss=0.34063, acc=0.85938
# [23/100] training 5.7% loss=0.23320, acc=0.89062
# [23/100] training 5.9% loss=0.28834, acc=0.84375
# [23/100] training 6.0% loss=0.24784, acc=0.90625
# [23/100] training 6.3% loss=0.23201, acc=0.87500
# [23/100] training 6.4% loss=0.16211, acc=0.93750
# [23/100] training 6.6% loss=0.29941, acc=0.84375
# [23/100] training 6.7% loss=0.31261, acc=0.85938
# [23/100] training 6.9% loss=0.20331, acc=0.89062
# [23/100] training 7.1% loss=0.20939, acc=0.89062
# [23/100] training 7.2% loss=0.20411, acc=0.89062
# [23/100] training 7.5% loss=0.21283, acc=0.93750
# [23/100] training 7.6% loss=0.26980, acc=0.87500
# [23/100] training 7.8% loss=0.22914, acc=0.87500
# [23/100] training 7.9% loss=0.31819, acc=0.87500
# [23/100] training 8.1% loss=0.16090, acc=0.93750
# [23/100] training 8.2% loss=0.27383, acc=0.90625
# [23/100] training 8.4% loss=0.26923, acc=0.85938
# [23/100] training 8.7% loss=0.24917, acc=0.87500
# [23/100] training 8.8% loss=0.24195, acc=0.92188
# [23/100] training 9.0% loss=0.20735, acc=0.89062
# [23/100] training 9.1% loss=0.27625, acc=0.87500
# [23/100] training 9.3% loss=0.30410, acc=0.89062
# [23/100] training 9.4% loss=0.18229, acc=0.87500
# [23/100] training 9.7% loss=0.23691, acc=0.89062
# [23/100] training 9.9% loss=0.28017, acc=0.87500
# [23/100] training 10.0% loss=0.30125, acc=0.87500
# [23/100] training 10.2% loss=0.20809, acc=0.90625
# [23/100] training 10.3% loss=0.20913, acc=0.89062
# [23/100] training 10.5% loss=0.33194, acc=0.87500
# [23/100] training 10.6% loss=0.25758, acc=0.87500
# [23/100] training 10.9% loss=0.15724, acc=0.90625
# [23/100] training 11.0% loss=0.26603, acc=0.90625
# [23/100] training 11.2% loss=0.19188, acc=0.96875
# [23/100] training 11.4% loss=0.30535, acc=0.85938
# [23/100] training 11.5% loss=0.47263, acc=0.78125
# [23/100] training 11.7% loss=0.11996, acc=0.95312
# [23/100] training 11.8% loss=0.19389, acc=0.92188
# [23/100] training 12.1% loss=0.20954, acc=0.89062
# [23/100] training 12.2% loss=0.14912, acc=0.92188
# [23/100] training 12.4% loss=0.26263, acc=0.87500
# [23/100] training 12.6% loss=0.23613, acc=0.92188
# [23/100] training 12.7% loss=0.24972, acc=0.89062
# [23/100] training 12.9% loss=0.26916, acc=0.90625
# [23/100] training 13.0% loss=0.19674, acc=0.90625
# [23/100] training 13.3% loss=0.26886, acc=0.90625
# [23/100] training 13.4% loss=0.24536, acc=0.90625
# [23/100] training 13.6% loss=0.24490, acc=0.89062
# [23/100] training 13.7% loss=0.37755, acc=0.85938
# [23/100] training 13.9% loss=0.26449, acc=0.89062
# [23/100] training 14.1% loss=0.25675, acc=0.92188
# [23/100] training 14.3% loss=0.20330, acc=0.92188
# [23/100] training 14.5% loss=0.24348, acc=0.89062
# [23/100] training 14.6% loss=0.17236, acc=0.90625
# [23/100] training 14.8% loss=0.24498, acc=0.92188
# [23/100] training 14.9% loss=0.23836, acc=0.89062
# [23/100] training 15.1% loss=0.41628, acc=0.81250
# [23/100] training 15.4% loss=0.27019, acc=0.85938
# [23/100] training 15.5% loss=0.32915, acc=0.87500
# [23/100] training 15.7% loss=0.38927, acc=0.81250
# [23/100] training 15.8% loss=0.21332, acc=0.92188
# [23/100] training 16.0% loss=0.28007, acc=0.87500
# [23/100] training 16.1% loss=0.52967, acc=0.82812
# [23/100] training 16.3% loss=0.22038, acc=0.93750
# [23/100] training 16.4% loss=0.20105, acc=0.92188
# [23/100] training 16.7% loss=0.33072, acc=0.85938
# [23/100] training 16.9% loss=0.31974, acc=0.85938
# [23/100] training 17.0% loss=0.21951, acc=0.90625
# [23/100] training 17.2% loss=0.24755, acc=0.89062
# [23/100] training 17.3% loss=0.25706, acc=0.89062
# [23/100] training 17.5% loss=0.27560, acc=0.85938
# [23/100] training 17.7% loss=0.24863, acc=0.92188
# [23/100] training 17.9% loss=0.30119, acc=0.84375
# [23/100] training 18.1% loss=0.38181, acc=0.81250
# [23/100] training 18.2% loss=0.37257, acc=0.85938
# [23/100] training 18.4% loss=0.33341, acc=0.82812
# [23/100] training 18.5% loss=0.26222, acc=0.90625
# [23/100] training 18.8% loss=0.23811, acc=0.92188
# [23/100] training 18.9% loss=0.22387, acc=0.90625
# [23/100] training 19.1% loss=0.31268, acc=0.90625
# [23/100] training 19.2% loss=0.19874, acc=0.92188
# [23/100] training 19.4% loss=0.16995, acc=0.92188
# [23/100] training 19.6% loss=0.25020, acc=0.92188
# [23/100] training 19.7% loss=0.32034, acc=0.85938
# [23/100] training 20.0% loss=0.13157, acc=0.93750
# [23/100] training 20.1% loss=0.31096, acc=0.90625
# [23/100] training 20.3% loss=0.26438, acc=0.89062
# [23/100] training 20.4% loss=0.33659, acc=0.82812
# [23/100] training 20.6% loss=0.24348, acc=0.90625
# [23/100] training 20.8% loss=0.21246, acc=0.90625
# [23/100] training 20.9% loss=0.19982, acc=0.92188
# [23/100] training 21.2% loss=0.30653, acc=0.85938
# [23/100] training 21.3% loss=0.31692, acc=0.89062
# [23/100] training 21.5% loss=0.30300, acc=0.92188
# [23/100] training 21.6% loss=0.11271, acc=0.96875
# [23/100] training 21.8% loss=0.14940, acc=0.93750
# [23/100] training 21.9% loss=0.30032, acc=0.89062
# [23/100] training 22.2% loss=0.20509, acc=0.89062
# [23/100] training 22.4% loss=0.31782, acc=0.87500
# [23/100] training 22.5% loss=0.21765, acc=0.90625
# [23/100] training 22.7% loss=0.24177, acc=0.89062
# [23/100] training 22.8% loss=0.35022, acc=0.87500
# [23/100] training 23.0% loss=0.12503, acc=0.96875
# [23/100] training 23.1% loss=0.33511, acc=0.87500
# [23/100] training 23.4% loss=0.31008, acc=0.82812
# [23/100] training 23.6% loss=0.32935, acc=0.92188
# [23/100] training 23.7% loss=0.30365, acc=0.85938
# [23/100] training 23.9% loss=0.22679, acc=0.90625
# [23/100] training 24.0% loss=0.21740, acc=0.90625
# [23/100] training 24.2% loss=0.22462, acc=0.90625
# [23/100] training 24.3% loss=0.37948, acc=0.84375
# [23/100] training 24.6% loss=0.27597, acc=0.87500
# [23/100] training 24.7% loss=0.36341, acc=0.85938
# [23/100] training 24.9% loss=0.18224, acc=0.92188
# [23/100] training 25.1% loss=0.21961, acc=0.90625
# [23/100] training 25.2% loss=0.16143, acc=0.93750
# [23/100] training 25.4% loss=0.19789, acc=0.93750
# [23/100] training 25.6% loss=0.11341, acc=1.00000
# [23/100] training 25.8% loss=0.27663, acc=0.89062
# [23/100] training 25.9% loss=0.14059, acc=0.93750
# [23/100] training 26.1% loss=0.19730, acc=0.92188
# [23/100] training 26.3% loss=0.23792, acc=0.89062
# [23/100] training 26.4% loss=0.13863, acc=0.93750
# [23/100] training 26.6% loss=0.09939, acc=0.96875
# [23/100] training 26.8% loss=0.26366, acc=0.89062
# [23/100] training 27.0% loss=0.30748, acc=0.89062
# [23/100] training 27.1% loss=0.14346, acc=0.95312
# [23/100] training 27.3% loss=0.23666, acc=0.90625
# [23/100] training 27.4% loss=0.17483, acc=0.92188
# [23/100] training 27.6% loss=0.41653, acc=0.87500
# [23/100] training 27.9% loss=0.20653, acc=0.92188
# [23/100] training 28.0% loss=0.42604, acc=0.81250
# [23/100] training 28.2% loss=0.20174, acc=0.89062
# [23/100] training 28.3% loss=0.16420, acc=0.93750
# [23/100] training 28.5% loss=0.20774, acc=0.90625
# [23/100] training 28.6% loss=0.24181, acc=0.95312
# [23/100] training 28.8% loss=0.17005, acc=0.93750
# [23/100] training 29.1% loss=0.20173, acc=0.93750
# [23/100] training 29.2% loss=0.23901, acc=0.89062
# [23/100] training 29.4% loss=0.35070, acc=0.85938
# [23/100] training 29.5% loss=0.22355, acc=0.89062
# [23/100] training 29.7% loss=0.27957, acc=0.84375
# [23/100] training 29.8% loss=0.21099, acc=0.92188
# [23/100] training 30.0% loss=0.34074, acc=0.81250
# [23/100] training 30.2% loss=0.22023, acc=0.87500
# [23/100] training 30.4% loss=0.14026, acc=0.93750
# [23/100] training 30.6% loss=0.36044, acc=0.89062
# [23/100] training 30.7% loss=0.21361, acc=0.90625
# [23/100] training 30.9% loss=0.33965, acc=0.87500
# [23/100] training 31.0% loss=0.15387, acc=0.92188
# [23/100] training 31.3% loss=0.16382, acc=0.95312
# [23/100] training 31.4% loss=0.38290, acc=0.76562
# [23/100] training 31.6% loss=0.27202, acc=0.85938
# [23/100] training 31.8% loss=0.15717, acc=0.93750
# [23/100] training 31.9% loss=0.22713, acc=0.93750
# [23/100] training 32.1% loss=0.27581, acc=0.90625
# [23/100] training 32.2% loss=0.26572, acc=0.87500
# [23/100] training 32.5% loss=0.17201, acc=0.95312
# [23/100] training 32.6% loss=0.29042, acc=0.90625
# [23/100] training 32.8% loss=0.18151, acc=0.92188
# [23/100] training 32.9% loss=0.28303, acc=0.90625
# [23/100] training 33.1% loss=0.26326, acc=0.92188
# [23/100] training 33.3% loss=0.23237, acc=0.90625
# [23/100] training 33.4% loss=0.24569, acc=0.93750
# [23/100] training 33.7% loss=0.28980, acc=0.87500
# [23/100] training 33.8% loss=0.24150, acc=0.90625
# [23/100] training 34.0% loss=0.20316, acc=0.95312
# [23/100] training 34.1% loss=0.26354, acc=0.90625
# [23/100] training 34.3% loss=0.20781, acc=0.90625
# [23/100] training 34.5% loss=0.29655, acc=0.87500
# [23/100] training 34.7% loss=0.16715, acc=0.95312
# [23/100] training 34.9% loss=0.17549, acc=0.92188
# [23/100] training 35.0% loss=0.23194, acc=0.93750
# [23/100] training 35.2% loss=0.28601, acc=0.89062
# [23/100] training 35.3% loss=0.30984, acc=0.89062
# [23/100] training 35.5% loss=0.19197, acc=0.90625
# [23/100] training 35.6% loss=0.37777, acc=0.84375
# [23/100] training 35.9% loss=0.28489, acc=0.82812
# [23/100] training 36.1% loss=0.33970, acc=0.89062
# [23/100] training 36.2% loss=0.24033, acc=0.89062
# [23/100] training 36.4% loss=0.29558, acc=0.89062
# [23/100] training 36.5% loss=0.29098, acc=0.93750
# [23/100] training 36.7% loss=0.22758, acc=0.89062
# [23/100] training 36.8% loss=0.15077, acc=0.95312
# [23/100] training 37.1% loss=0.30031, acc=0.85938
# [23/100] training 37.3% loss=0.23590, acc=0.92188
# [23/100] training 37.4% loss=0.25367, acc=0.85938
# [23/100] training 37.6% loss=0.19124, acc=0.90625
# [23/100] training 37.7% loss=0.20766, acc=0.90625
# [23/100] training 37.9% loss=0.20073, acc=0.89062
# [23/100] training 38.1% loss=0.35498, acc=0.85938
# [23/100] training 38.3% loss=0.29334, acc=0.89062
# [23/100] training 38.4% loss=0.11724, acc=0.93750
# [23/100] training 38.6% loss=0.22017, acc=0.89062
# [23/100] training 38.8% loss=0.31718, acc=0.84375
# [23/100] training 38.9% loss=0.25461, acc=0.85938
# [23/100] training 39.1% loss=0.23776, acc=0.92188
# [23/100] training 39.3% loss=0.30821, acc=0.84375
# [23/100] training 39.5% loss=0.25523, acc=0.90625
# [23/100] training 39.6% loss=0.27471, acc=0.92188
# [23/100] training 39.8% loss=0.20341, acc=0.92188
# [23/100] training 40.0% loss=0.26390, acc=0.85938
# [23/100] training 40.1% loss=0.27963, acc=0.87500
# [23/100] training 40.4% loss=0.29823, acc=0.87500
# [23/100] training 40.5% loss=0.25073, acc=0.90625
# [23/100] training 40.7% loss=0.22356, acc=0.89062
# [23/100] training 40.8% loss=0.15209, acc=0.92188
# [23/100] training 41.0% loss=0.18779, acc=0.93750
# [23/100] training 41.1% loss=0.27017, acc=0.89062
# [23/100] training 41.3% loss=0.32471, acc=0.79688
# [23/100] training 41.6% loss=0.25580, acc=0.90625
# [23/100] training 41.7% loss=0.33752, acc=0.90625
# [23/100] training 41.9% loss=0.17633, acc=0.95312
# [23/100] training 42.0% loss=0.23848, acc=0.89062
# [23/100] training 42.2% loss=0.25181, acc=0.89062
# [23/100] training 42.3% loss=0.22130, acc=0.90625
# [23/100] training 42.5% loss=0.15094, acc=0.92188
# [23/100] training 42.8% loss=0.12682, acc=0.96875
# [23/100] training 42.9% loss=0.18398, acc=0.92188
# [23/100] training 43.1% loss=0.25705, acc=0.90625
# [23/100] training 43.2% loss=0.21029, acc=0.89062
# [23/100] training 43.4% loss=0.21408, acc=0.93750
# [23/100] training 43.5% loss=0.18853, acc=0.90625
# [23/100] training 43.8% loss=0.27062, acc=0.89062
# [23/100] training 43.9% loss=0.18011, acc=0.93750
# [23/100] training 44.1% loss=0.17725, acc=0.95312
# [23/100] training 44.3% loss=0.24290, acc=0.90625
# [23/100] training 44.4% loss=0.29540, acc=0.84375
# [23/100] training 44.6% loss=0.24240, acc=0.90625
# [23/100] training 44.7% loss=0.31302, acc=0.89062
# [23/100] training 45.0% loss=0.18209, acc=0.92188
# [23/100] training 45.1% loss=0.39305, acc=0.82812
# [23/100] training 45.3% loss=0.24111, acc=0.90625
# [23/100] training 45.5% loss=0.11558, acc=0.96875
# [23/100] training 45.6% loss=0.27917, acc=0.93750
# [23/100] training 45.8% loss=0.15412, acc=0.95312
# [23/100] training 45.9% loss=0.20539, acc=0.90625
# [23/100] training 46.2% loss=0.27837, acc=0.90625
# [23/100] training 46.3% loss=0.16400, acc=0.95312
# [23/100] training 46.5% loss=0.35038, acc=0.85938
# [23/100] training 46.6% loss=0.24398, acc=0.93750
# [23/100] training 46.8% loss=0.16620, acc=0.92188
# [23/100] training 47.0% loss=0.23778, acc=0.90625
# [23/100] training 47.2% loss=0.32515, acc=0.84375
# [23/100] training 47.4% loss=0.17159, acc=0.95312
# [23/100] training 47.5% loss=0.33427, acc=0.84375
# [23/100] training 47.7% loss=0.24748, acc=0.89062
# [23/100] training 47.8% loss=0.36896, acc=0.90625
# [23/100] training 48.0% loss=0.25734, acc=0.89062
# [23/100] training 48.3% loss=0.14147, acc=0.92188
# [23/100] training 48.4% loss=0.10944, acc=0.98438
# [23/100] training 48.6% loss=0.16718, acc=0.90625
# [23/100] training 48.7% loss=0.40118, acc=0.87500
# [23/100] training 48.9% loss=0.29756, acc=0.89062
# [23/100] training 49.0% loss=0.21609, acc=0.90625
# [23/100] training 49.2% loss=0.33085, acc=0.85938
# [23/100] training 49.3% loss=0.19415, acc=0.92188
# [23/100] training 49.6% loss=0.23896, acc=0.89062
# [23/100] training 49.8% loss=0.24668, acc=0.87500
# [23/100] training 49.9% loss=0.24001, acc=0.85938
# [23/100] training 50.1% loss=0.23802, acc=0.90625
# [23/100] training 50.2% loss=0.31882, acc=0.84375
# [23/100] training 50.4% loss=0.35656, acc=0.90625
# [23/100] training 50.6% loss=0.29905, acc=0.85938
# [23/100] training 50.8% loss=0.30879, acc=0.89062
# [23/100] training 51.0% loss=0.35637, acc=0.78125
# [23/100] training 51.1% loss=0.27393, acc=0.87500
# [23/100] training 51.3% loss=0.22317, acc=0.89062
# [23/100] training 51.4% loss=0.26674, acc=0.90625
# [23/100] training 51.7% loss=0.28714, acc=0.90625
# [23/100] training 51.8% loss=0.28029, acc=0.85938
# [23/100] training 52.0% loss=0.23754, acc=0.92188
# [23/100] training 52.1% loss=0.23234, acc=0.92188
# [23/100] training 52.3% loss=0.33515, acc=0.87500
# [23/100] training 52.5% loss=0.15611, acc=0.96875
# [23/100] training 52.6% loss=0.25005, acc=0.92188
# [23/100] training 52.9% loss=0.49411, acc=0.78125
# [23/100] training 53.0% loss=0.21365, acc=0.90625
# [23/100] training 53.2% loss=0.34130, acc=0.89062
# [23/100] training 53.3% loss=0.10592, acc=0.96875
# [23/100] training 53.5% loss=0.19749, acc=0.95312
# [23/100] training 53.7% loss=0.16172, acc=0.93750
# [23/100] training 53.8% loss=0.30607, acc=0.85938
# [23/100] training 54.1% loss=0.38439, acc=0.84375
# [23/100] training 54.2% loss=0.17309, acc=0.93750
# [23/100] training 54.4% loss=0.21375, acc=0.89062
# [23/100] training 54.5% loss=0.26936, acc=0.90625
# [23/100] training 54.7% loss=0.32088, acc=0.85938
# [23/100] training 54.8% loss=0.17191, acc=0.93750
# [23/100] training 55.1% loss=0.21933, acc=0.90625
# [23/100] training 55.3% loss=0.20061, acc=0.93750
# [23/100] training 55.4% loss=0.31644, acc=0.85938
# [23/100] training 55.6% loss=0.22789, acc=0.92188
# [23/100] training 55.7% loss=0.26143, acc=0.85938
# [23/100] training 55.9% loss=0.17565, acc=0.95312
# [23/100] training 56.0% loss=0.26063, acc=0.89062
# [23/100] training 56.3% loss=0.46205, acc=0.81250
# [23/100] training 56.5% loss=0.20206, acc=0.90625
# [23/100] training 56.6% loss=0.22354, acc=0.89062
# [23/100] training 56.8% loss=0.32836, acc=0.89062
# [23/100] training 56.9% loss=0.24982, acc=0.90625
# [23/100] training 57.1% loss=0.30784, acc=0.85938
# [23/100] training 57.2% loss=0.16536, acc=0.93750
# [23/100] training 57.5% loss=0.18780, acc=0.92188
# [23/100] training 57.6% loss=0.29759, acc=0.82812
# [23/100] training 57.8% loss=0.20816, acc=0.92188
# [23/100] training 58.0% loss=0.22001, acc=0.90625
# [23/100] training 58.1% loss=0.18797, acc=0.93750
# [23/100] training 58.3% loss=0.15566, acc=0.95312
# [23/100] training 58.4% loss=0.30630, acc=0.90625
# [23/100] training 58.7% loss=0.25411, acc=0.90625
# [23/100] training 58.8% loss=0.28151, acc=0.84375
# [23/100] training 59.0% loss=0.19850, acc=0.90625
# [23/100] training 59.2% loss=0.22695, acc=0.89062
# [23/100] training 59.3% loss=0.21208, acc=0.93750
# [23/100] training 59.5% loss=0.21247, acc=0.89062
# [23/100] training 59.7% loss=0.24924, acc=0.89062
# [23/100] training 59.9% loss=0.15333, acc=0.92188
# [23/100] training 60.0% loss=0.28116, acc=0.85938
# [23/100] training 60.2% loss=0.21474, acc=0.89062
# [23/100] training 60.3% loss=0.16704, acc=0.93750
# [23/100] training 60.5% loss=0.35249, acc=0.89062
# [23/100] training 60.8% loss=0.24604, acc=0.89062
# [23/100] training 60.9% loss=0.33142, acc=0.87500
# [23/100] training 61.1% loss=0.28648, acc=0.87500
# [23/100] training 61.2% loss=0.22274, acc=0.92188
# [23/100] training 61.4% loss=0.27854, acc=0.90625
# [23/100] training 61.5% loss=0.31414, acc=0.84375
# [23/100] training 61.7% loss=0.32799, acc=0.85938
# [23/100] training 62.0% loss=0.25777, acc=0.89062
# [23/100] training 62.1% loss=0.44669, acc=0.84375
# [23/100] training 62.3% loss=0.11078, acc=0.96875
# [23/100] training 62.4% loss=0.16028, acc=0.93750
# [23/100] training 62.6% loss=0.34129, acc=0.84375
# [23/100] training 62.7% loss=0.21473, acc=0.92188
# [23/100] training 62.9% loss=0.24322, acc=0.89062
# [23/100] training 63.1% loss=0.21743, acc=0.92188
# [23/100] training 63.3% loss=0.20654, acc=0.90625
# [23/100] training 63.5% loss=0.26939, acc=0.92188
# [23/100] training 63.6% loss=0.27828, acc=0.87500
# [23/100] training 63.8% loss=0.32912, acc=0.85938
# [23/100] training 63.9% loss=0.19873, acc=0.92188
# [23/100] training 64.2% loss=0.19194, acc=0.92188
# [23/100] training 64.3% loss=0.22878, acc=0.90625
# [23/100] training 64.5% loss=0.28203, acc=0.87500
# [23/100] training 64.7% loss=0.21632, acc=0.89062
# [23/100] training 64.8% loss=0.36285, acc=0.85938
# [23/100] training 65.0% loss=0.35204, acc=0.82812
# [23/100] training 65.1% loss=0.31008, acc=0.84375
# [23/100] training 65.4% loss=0.30529, acc=0.85938
# [23/100] training 65.5% loss=0.21384, acc=0.90625
# [23/100] training 65.7% loss=0.17674, acc=0.92188
# [23/100] training 65.8% loss=0.36626, acc=0.82812
# [23/100] training 66.0% loss=0.32207, acc=0.85938
# [23/100] training 66.2% loss=0.12516, acc=0.96875
# [23/100] training 66.3% loss=0.38245, acc=0.82812
# [23/100] training 66.6% loss=0.23101, acc=0.89062
# [23/100] training 66.7% loss=0.13214, acc=0.93750
# [23/100] training 66.9% loss=0.27432, acc=0.87500
# [23/100] training 67.0% loss=0.23186, acc=0.90625
# [23/100] training 67.2% loss=0.21273, acc=0.93750
# [23/100] training 67.4% loss=0.20006, acc=0.93750
# [23/100] training 67.6% loss=0.19373, acc=0.93750
# [23/100] training 67.8% loss=0.22360, acc=0.92188
# [23/100] training 67.9% loss=0.16314, acc=0.93750
# [23/100] training 68.1% loss=0.16257, acc=0.93750
# [23/100] training 68.2% loss=0.15194, acc=0.95312
# [23/100] training 68.4% loss=0.20279, acc=0.95312
# [23/100] training 68.5% loss=0.26900, acc=0.85938
# [23/100] training 68.8% loss=0.29435, acc=0.85938
# [23/100] training 69.0% loss=0.33670, acc=0.87500
# [23/100] training 69.1% loss=0.21990, acc=0.90625
# [23/100] training 69.3% loss=0.21977, acc=0.89062
# [23/100] training 69.4% loss=0.24425, acc=0.90625
# [23/100] training 69.6% loss=0.23361, acc=0.85938
# [23/100] training 69.7% loss=0.22039, acc=0.92188
# [23/100] training 70.0% loss=0.34890, acc=0.85938
# [23/100] training 70.2% loss=0.29662, acc=0.84375
# [23/100] training 70.3% loss=0.24529, acc=0.93750
# [23/100] training 70.5% loss=0.25209, acc=0.89062
# [23/100] training 70.6% loss=0.15170, acc=0.93750
# [23/100] training 70.8% loss=0.25073, acc=0.89062
# [23/100] training 71.0% loss=0.30461, acc=0.87500
# [23/100] training 71.2% loss=0.17710, acc=0.89062
# [23/100] training 71.3% loss=0.15338, acc=0.95312
# [23/100] training 71.5% loss=0.22873, acc=0.92188
# [23/100] training 71.7% loss=0.31707, acc=0.92188
# [23/100] training 71.8% loss=0.39208, acc=0.89062
# [23/100] training 72.0% loss=0.14890, acc=0.95312
# [23/100] training 72.2% loss=0.19786, acc=0.95312
# [23/100] training 72.4% loss=0.34432, acc=0.85938
# [23/100] training 72.5% loss=0.31024, acc=0.85938
# [23/100] training 72.7% loss=0.26235, acc=0.90625
# [23/100] training 72.9% loss=0.16218, acc=0.93750
# [23/100] training 73.0% loss=0.17039, acc=0.95312
# [23/100] training 73.3% loss=0.25065, acc=0.89062
# [23/100] training 73.4% loss=0.20388, acc=0.92188
# [23/100] training 73.6% loss=0.12859, acc=0.92188
# [23/100] training 73.7% loss=0.24026, acc=0.87500
# [23/100] training 73.9% loss=0.26034, acc=0.89062
# [23/100] training 74.0% loss=0.33856, acc=0.89062
# [23/100] training 74.2% loss=0.15070, acc=0.93750
# [23/100] training 74.5% loss=0.17637, acc=0.92188
# [23/100] training 74.6% loss=0.47524, acc=0.81250
# [23/100] training 74.8% loss=0.36515, acc=0.84375
# [23/100] training 74.9% loss=0.38244, acc=0.82812
# [23/100] training 75.1% loss=0.22726, acc=0.90625
# [23/100] training 75.2% loss=0.15676, acc=0.96875
# [23/100] training 75.4% loss=0.25478, acc=0.90625
# [23/100] training 75.7% loss=0.25264, acc=0.92188
# [23/100] training 75.8% loss=0.35983, acc=0.84375
# [23/100] training 76.0% loss=0.18065, acc=0.92188
# [23/100] training 76.1% loss=0.32865, acc=0.89062
# [23/100] training 76.3% loss=0.23752, acc=0.93750
# [23/100] training 76.4% loss=0.20637, acc=0.90625
# [23/100] training 76.7% loss=0.20617, acc=0.93750
# [23/100] training 76.8% loss=0.17148, acc=0.92188
# [23/100] training 77.0% loss=0.15007, acc=0.90625
# [23/100] training 77.2% loss=0.33356, acc=0.85938
# [23/100] training 77.3% loss=0.13343, acc=0.98438
# [23/100] training 77.5% loss=0.24589, acc=0.90625
# [23/100] training 77.6% loss=0.22848, acc=0.89062
# [23/100] training 77.9% loss=0.20858, acc=0.93750
# [23/100] training 78.0% loss=0.29081, acc=0.87500
# [23/100] training 78.2% loss=0.27414, acc=0.89062
# [23/100] training 78.4% loss=0.11907, acc=0.98438
# [23/100] training 78.5% loss=0.33361, acc=0.87500
# [23/100] training 78.7% loss=0.22827, acc=0.93750
# [23/100] training 78.8% loss=0.11029, acc=0.96875
# [23/100] training 79.1% loss=0.14376, acc=0.95312
# [23/100] training 79.2% loss=0.20156, acc=0.93750
# [23/100] training 79.4% loss=0.31546, acc=0.84375
# [23/100] training 79.5% loss=0.24444, acc=0.90625
# [23/100] training 79.7% loss=0.11096, acc=0.98438
# [23/100] training 79.9% loss=0.17463, acc=0.95312
# [23/100] training 80.1% loss=0.21985, acc=0.90625
# [23/100] training 80.3% loss=0.31377, acc=0.89062
# [23/100] training 80.4% loss=0.20176, acc=0.95312
# [23/100] training 80.6% loss=0.22391, acc=0.89062
# [23/100] training 80.7% loss=0.20658, acc=0.89062
# [23/100] training 80.9% loss=0.33180, acc=0.84375
# [23/100] training 81.2% loss=0.27746, acc=0.92188
# [23/100] training 81.3% loss=0.22138, acc=0.89062
# [23/100] training 81.5% loss=0.25306, acc=0.90625
# [23/100] training 81.6% loss=0.32511, acc=0.84375
# [23/100] training 81.8% loss=0.20863, acc=0.93750
# [23/100] training 81.9% loss=0.42346, acc=0.90625
# [23/100] training 82.1% loss=0.13121, acc=0.95312
# [23/100] training 82.2% loss=0.26183, acc=0.90625
# [23/100] training 82.5% loss=0.12601, acc=0.93750
# [23/100] training 82.7% loss=0.25611, acc=0.90625
# [23/100] training 82.8% loss=0.18297, acc=0.93750
# [23/100] training 83.0% loss=0.20465, acc=0.90625
# [23/100] training 83.1% loss=0.23621, acc=0.92188
# [23/100] training 83.3% loss=0.23262, acc=0.90625
# [23/100] training 83.5% loss=0.19603, acc=0.90625
# [23/100] training 83.7% loss=0.43605, acc=0.87500
# [23/100] training 83.9% loss=0.26577, acc=0.92188
# [23/100] training 84.0% loss=0.25277, acc=0.90625
# [23/100] training 84.2% loss=0.17828, acc=0.90625
# [23/100] training 84.3% loss=0.19912, acc=0.90625
# [23/100] training 84.5% loss=0.25498, acc=0.93750
# [23/100] training 84.7% loss=0.37804, acc=0.87500
# [23/100] training 84.9% loss=0.17342, acc=0.90625
# [23/100] training 85.0% loss=0.21992, acc=0.89062
# [23/100] training 85.2% loss=0.29329, acc=0.89062
# [23/100] training 85.4% loss=0.24188, acc=0.89062
# [23/100] training 85.5% loss=0.19448, acc=0.90625
# [23/100] training 85.8% loss=0.28244, acc=0.85938
# [23/100] training 85.9% loss=0.23460, acc=0.95312
# [23/100] training 86.1% loss=0.21583, acc=0.90625
# [23/100] training 86.2% loss=0.18365, acc=0.92188
# [23/100] training 86.4% loss=0.31849, acc=0.87500
# [23/100] training 86.6% loss=0.32020, acc=0.87500
# [23/100] training 86.7% loss=0.18609, acc=0.92188
# [23/100] training 87.0% loss=0.26486, acc=0.89062
# [23/100] training 87.1% loss=0.21391, acc=0.90625
# [23/100] training 87.3% loss=0.25915, acc=0.87500
# [23/100] training 87.4% loss=0.23540, acc=0.87500
# [23/100] training 87.6% loss=0.34975, acc=0.82812
# [23/100] training 87.7% loss=0.36449, acc=0.92188
# [23/100] training 87.9% loss=0.26693, acc=0.90625
# [23/100] training 88.2% loss=0.13732, acc=0.96875
# [23/100] training 88.3% loss=0.18900, acc=0.93750
# [23/100] training 88.5% loss=0.18608, acc=0.90625
# [23/100] training 88.6% loss=0.10970, acc=0.95312
# [23/100] training 88.8% loss=0.22073, acc=0.90625
# [23/100] training 88.9% loss=0.36581, acc=0.87500
# [23/100] training 89.2% loss=0.15989, acc=0.95312
# [23/100] training 89.4% loss=0.23037, acc=0.85938
# [23/100] training 89.5% loss=0.39022, acc=0.85938
# [23/100] training 89.7% loss=0.23174, acc=0.92188
# [23/100] training 89.8% loss=0.24560, acc=0.89062
# [23/100] training 90.0% loss=0.25154, acc=0.87500
# [23/100] training 90.1% loss=0.23858, acc=0.89062
# [23/100] training 90.4% loss=0.24387, acc=0.93750
# [23/100] training 90.5% loss=0.14615, acc=0.95312
# [23/100] training 90.7% loss=0.16195, acc=0.96875
# [23/100] training 90.9% loss=0.14456, acc=0.95312
# [23/100] training 91.0% loss=0.16021, acc=0.95312
# [23/100] training 91.2% loss=0.23346, acc=0.89062
# [23/100] training 91.3% loss=0.33324, acc=0.90625
# [23/100] training 91.6% loss=0.34005, acc=0.89062
# [23/100] training 91.7% loss=0.13994, acc=0.95312
# [23/100] training 91.9% loss=0.26460, acc=0.89062
# [23/100] training 92.1% loss=0.19354, acc=0.89062
# [23/100] training 92.2% loss=0.13841, acc=0.93750
# [23/100] training 92.4% loss=0.18386, acc=0.90625
# [23/100] training 92.6% loss=0.21554, acc=0.92188
# [23/100] training 92.8% loss=0.31210, acc=0.89062
# [23/100] training 92.9% loss=0.21726, acc=0.92188
# [23/100] training 93.1% loss=0.31959, acc=0.87500
# [23/100] training 93.2% loss=0.30719, acc=0.90625
# [23/100] training 93.4% loss=0.28517, acc=0.92188
# [23/100] training 93.7% loss=0.22187, acc=0.89062
# [23/100] training 93.8% loss=0.21075, acc=0.93750
# [23/100] training 94.0% loss=0.20891, acc=0.93750
# [23/100] training 94.1% loss=0.28704, acc=0.89062
# [23/100] training 94.3% loss=0.09503, acc=0.98438
# [23/100] training 94.4% loss=0.27425, acc=0.92188
# [23/100] training 94.6% loss=0.13940, acc=0.95312
# [23/100] training 94.9% loss=0.17806, acc=0.92188
# [23/100] training 95.0% loss=0.40604, acc=0.85938
# [23/100] training 95.2% loss=0.39995, acc=0.84375
# [23/100] training 95.3% loss=0.23748, acc=0.90625
# [23/100] training 95.5% loss=0.13134, acc=0.95312
# [23/100] training 95.6% loss=0.32408, acc=0.84375
# [23/100] training 95.8% loss=0.27329, acc=0.87500
# [23/100] training 96.0% loss=0.20344, acc=0.93750
# [23/100] training 96.2% loss=0.19075, acc=0.90625
# [23/100] training 96.4% loss=0.23566, acc=0.93750
# [23/100] training 96.5% loss=0.20984, acc=0.92188
# [23/100] training 96.7% loss=0.15577, acc=0.93750
# [23/100] training 96.8% loss=0.26503, acc=0.90625
# [23/100] training 97.1% loss=0.15815, acc=0.92188
# [23/100] training 97.2% loss=0.25770, acc=0.90625
# [23/100] training 97.4% loss=0.24514, acc=0.93750
# [23/100] training 97.6% loss=0.30281, acc=0.90625
# [23/100] training 97.7% loss=0.17931, acc=0.95312
# [23/100] training 97.9% loss=0.18011, acc=0.92188
# [23/100] training 98.0% loss=0.21761, acc=0.92188
# [23/100] training 98.3% loss=0.23622, acc=0.89062
# [23/100] training 98.4% loss=0.19754, acc=0.95312
# [23/100] training 98.6% loss=0.35581, acc=0.85938
# [23/100] training 98.7% loss=0.35801, acc=0.87500
# [23/100] training 98.9% loss=0.14435, acc=0.90625
# [23/100] training 99.1% loss=0.16752, acc=0.93750
# [23/100] training 99.2% loss=0.20143, acc=0.92188
# [23/100] training 99.5% loss=0.40190, acc=0.82812
# [23/100] training 99.6% loss=0.20932, acc=0.89062
# [23/100] training 99.8% loss=0.19014, acc=0.90625
# [23/100] training 99.9% loss=0.12536, acc=0.95312
# [23/100] testing 0.9% loss=0.12002, acc=0.93750
# [23/100] testing 1.8% loss=0.40263, acc=0.79688
# [23/100] testing 2.2% loss=0.25305, acc=0.90625
# [23/100] testing 3.1% loss=0.28471, acc=0.90625
# [23/100] testing 3.5% loss=0.16947, acc=0.92188
# [23/100] testing 4.4% loss=0.18142, acc=0.92188
# [23/100] testing 4.8% loss=0.37682, acc=0.85938
# [23/100] testing 5.7% loss=0.28758, acc=0.89062
# [23/100] testing 6.6% loss=0.23952, acc=0.90625
# [23/100] testing 7.0% loss=0.20529, acc=0.90625
# [23/100] testing 7.9% loss=0.22550, acc=0.90625
# [23/100] testing 8.3% loss=0.26863, acc=0.90625
# [23/100] testing 9.2% loss=0.36375, acc=0.89062
# [23/100] testing 9.7% loss=0.12302, acc=0.93750
# [23/100] testing 10.5% loss=0.23941, acc=0.90625
# [23/100] testing 11.0% loss=0.26476, acc=0.85938
# [23/100] testing 11.8% loss=0.28213, acc=0.90625
# [23/100] testing 12.7% loss=0.33391, acc=0.85938
# [23/100] testing 13.2% loss=0.26750, acc=0.85938
# [23/100] testing 14.0% loss=0.37216, acc=0.90625
# [23/100] testing 14.5% loss=0.21734, acc=0.92188
# [23/100] testing 15.4% loss=0.32173, acc=0.89062
# [23/100] testing 15.8% loss=0.20110, acc=0.90625
# [23/100] testing 16.7% loss=0.33285, acc=0.85938
# [23/100] testing 17.5% loss=0.21201, acc=0.90625
# [23/100] testing 18.0% loss=0.20474, acc=0.90625
# [23/100] testing 18.9% loss=0.11209, acc=0.93750
# [23/100] testing 19.3% loss=0.31083, acc=0.90625
# [23/100] testing 20.2% loss=0.28441, acc=0.92188
# [23/100] testing 20.6% loss=0.41947, acc=0.85938
# [23/100] testing 21.5% loss=0.21406, acc=0.89062
# [23/100] testing 21.9% loss=0.31885, acc=0.89062
# [23/100] testing 22.8% loss=0.40728, acc=0.87500
# [23/100] testing 23.7% loss=0.34901, acc=0.89062
# [23/100] testing 24.1% loss=0.27191, acc=0.89062
# [23/100] testing 25.0% loss=0.38373, acc=0.85938
# [23/100] testing 25.4% loss=0.20011, acc=0.95312
# [23/100] testing 26.3% loss=0.35599, acc=0.82812
# [23/100] testing 26.8% loss=0.29404, acc=0.87500
# [23/100] testing 27.6% loss=0.31314, acc=0.87500
# [23/100] testing 28.5% loss=0.31091, acc=0.89062
# [23/100] testing 29.0% loss=0.22203, acc=0.89062
# [23/100] testing 29.8% loss=0.37614, acc=0.85938
# [23/100] testing 30.3% loss=0.25921, acc=0.90625
# [23/100] testing 31.1% loss=0.38550, acc=0.85938
# [23/100] testing 31.6% loss=0.20401, acc=0.92188
# [23/100] testing 32.5% loss=0.25043, acc=0.92188
# [23/100] testing 32.9% loss=0.37878, acc=0.87500
# [23/100] testing 33.8% loss=0.20586, acc=0.89062
# [23/100] testing 34.7% loss=0.38302, acc=0.85938
# [23/100] testing 35.1% loss=0.27271, acc=0.90625
# [23/100] testing 36.0% loss=0.28388, acc=0.89062
# [23/100] testing 36.4% loss=0.27039, acc=0.90625
# [23/100] testing 37.3% loss=0.29760, acc=0.92188
# [23/100] testing 37.7% loss=0.36471, acc=0.82812
# [23/100] testing 38.6% loss=0.22298, acc=0.92188
# [23/100] testing 39.5% loss=0.31518, acc=0.90625
# [23/100] testing 39.9% loss=0.31588, acc=0.92188
# [23/100] testing 40.8% loss=0.28613, acc=0.90625
# [23/100] testing 41.2% loss=0.28727, acc=0.92188
# [23/100] testing 42.1% loss=0.27814, acc=0.90625
# [23/100] testing 42.5% loss=0.25127, acc=0.85938
# [23/100] testing 43.4% loss=0.37341, acc=0.85938
# [23/100] testing 43.9% loss=0.20763, acc=0.93750
# [23/100] testing 44.7% loss=0.29645, acc=0.89062
# [23/100] testing 45.6% loss=0.24276, acc=0.87500
# [23/100] testing 46.1% loss=0.25066, acc=0.87500
# [23/100] testing 46.9% loss=0.23800, acc=0.87500
# [23/100] testing 47.4% loss=0.13365, acc=0.92188
# [23/100] testing 48.3% loss=0.37702, acc=0.84375
# [23/100] testing 48.7% loss=0.34320, acc=0.85938
# [23/100] testing 49.6% loss=0.42669, acc=0.84375
# [23/100] testing 50.4% loss=0.21199, acc=0.90625
# [23/100] testing 50.9% loss=0.32607, acc=0.89062
# [23/100] testing 51.8% loss=0.34579, acc=0.84375
# [23/100] testing 52.2% loss=0.28308, acc=0.90625
# [23/100] testing 53.1% loss=0.20170, acc=0.93750
# [23/100] testing 53.5% loss=0.20286, acc=0.92188
# [23/100] testing 54.4% loss=0.40370, acc=0.79688
# [23/100] testing 54.8% loss=0.34785, acc=0.85938
# [23/100] testing 55.7% loss=0.16824, acc=0.93750
# [23/100] testing 56.6% loss=0.35641, acc=0.85938
# [23/100] testing 57.0% loss=0.37294, acc=0.85938
# [23/100] testing 57.9% loss=0.33279, acc=0.81250
# [23/100] testing 58.3% loss=0.43410, acc=0.82812
# [23/100] testing 59.2% loss=0.32651, acc=0.89062
# [23/100] testing 59.7% loss=0.21540, acc=0.89062
# [23/100] testing 60.5% loss=0.33711, acc=0.82812
# [23/100] testing 61.4% loss=0.14872, acc=0.92188
# [23/100] testing 61.9% loss=0.23164, acc=0.90625
# [23/100] testing 62.7% loss=0.26795, acc=0.90625
# [23/100] testing 63.2% loss=0.47065, acc=0.81250
# [23/100] testing 64.0% loss=0.38441, acc=0.87500
# [23/100] testing 64.5% loss=0.17791, acc=0.90625
# [23/100] testing 65.4% loss=0.24472, acc=0.87500
# [23/100] testing 65.8% loss=0.30700, acc=0.84375
# [23/100] testing 66.7% loss=0.30904, acc=0.85938
# [23/100] testing 67.6% loss=0.30669, acc=0.90625
# [23/100] testing 68.0% loss=0.23807, acc=0.92188
# [23/100] testing 68.9% loss=0.25750, acc=0.90625
# [23/100] testing 69.3% loss=0.44073, acc=0.84375
# [23/100] testing 70.2% loss=0.43798, acc=0.85938
# [23/100] testing 70.6% loss=0.38394, acc=0.84375
# [23/100] testing 71.5% loss=0.39945, acc=0.84375
# [23/100] testing 72.4% loss=0.15656, acc=0.93750
# [23/100] testing 72.8% loss=0.18972, acc=0.92188
# [23/100] testing 73.7% loss=0.28798, acc=0.92188
# [23/100] testing 74.1% loss=0.37327, acc=0.87500
# [23/100] testing 75.0% loss=0.24277, acc=0.89062
# [23/100] testing 75.4% loss=0.32274, acc=0.87500
# [23/100] testing 76.3% loss=0.12500, acc=0.93750
# [23/100] testing 76.8% loss=0.29078, acc=0.85938
# [23/100] testing 77.6% loss=0.25546, acc=0.85938
# [23/100] testing 78.5% loss=0.60138, acc=0.82812
# [23/100] testing 79.0% loss=0.22092, acc=0.90625
# [23/100] testing 79.8% loss=0.22177, acc=0.85938
# [23/100] testing 80.3% loss=0.27065, acc=0.89062
# [23/100] testing 81.2% loss=0.42307, acc=0.82812
# [23/100] testing 81.6% loss=0.21271, acc=0.93750
# [23/100] testing 82.5% loss=0.33003, acc=0.90625
# [23/100] testing 83.3% loss=0.18159, acc=0.93750
# [23/100] testing 83.8% loss=0.13408, acc=0.96875
# [23/100] testing 84.7% loss=0.31312, acc=0.84375
# [23/100] testing 85.1% loss=0.25952, acc=0.89062
# [23/100] testing 86.0% loss=0.31900, acc=0.89062
# [23/100] testing 86.4% loss=0.42601, acc=0.81250
# [23/100] testing 87.3% loss=0.25632, acc=0.89062
# [23/100] testing 87.7% loss=0.27458, acc=0.85938
# [23/100] testing 88.6% loss=0.20487, acc=0.90625
# [23/100] testing 89.5% loss=0.50472, acc=0.78125
# [23/100] testing 89.9% loss=0.24694, acc=0.92188
# [23/100] testing 90.8% loss=0.32301, acc=0.89062
# [23/100] testing 91.2% loss=0.13271, acc=0.96875
# [23/100] testing 92.1% loss=0.35197, acc=0.84375
# [23/100] testing 92.6% loss=0.26393, acc=0.85938
# [23/100] testing 93.4% loss=0.36019, acc=0.81250
# [23/100] testing 94.3% loss=0.16866, acc=0.92188
# [23/100] testing 94.7% loss=0.20964, acc=0.93750
# [23/100] testing 95.6% loss=0.35809, acc=0.89062
# [23/100] testing 96.1% loss=0.23948, acc=0.89062
# [23/100] testing 96.9% loss=0.32006, acc=0.89062
# [23/100] testing 97.4% loss=0.13961, acc=0.95312
# [23/100] testing 98.3% loss=0.24627, acc=0.87500
# [23/100] testing 98.7% loss=0.25592, acc=0.89062
# [23/100] testing 99.6% loss=0.22275, acc=0.90625
# [24/100] training 0.2% loss=0.45887, acc=0.81250
# [24/100] training 0.4% loss=0.34618, acc=0.85938
# [24/100] training 0.5% loss=0.14629, acc=0.95312
# [24/100] training 0.8% loss=0.26313, acc=0.89062
# [24/100] training 0.9% loss=0.23890, acc=0.90625
# [24/100] training 1.1% loss=0.25489, acc=0.96875
# [24/100] training 1.2% loss=0.26191, acc=0.90625
# [24/100] training 1.4% loss=0.24127, acc=0.92188
# [24/100] training 1.6% loss=0.13932, acc=0.95312
# [24/100] training 1.8% loss=0.22660, acc=0.90625
# [24/100] training 2.0% loss=0.20062, acc=0.93750
# [24/100] training 2.1% loss=0.26940, acc=0.85938
# [24/100] training 2.3% loss=0.17568, acc=0.95312
# [24/100] training 2.4% loss=0.30023, acc=0.89062
# [24/100] training 2.6% loss=0.16104, acc=0.93750
# [24/100] training 2.7% loss=0.29835, acc=0.90625
# [24/100] training 3.0% loss=0.24053, acc=0.89062
# [24/100] training 3.2% loss=0.20719, acc=0.92188
# [24/100] training 3.3% loss=0.23532, acc=0.90625
# [24/100] training 3.5% loss=0.21659, acc=0.89062
# [24/100] training 3.6% loss=0.19752, acc=0.93750
# [24/100] training 3.8% loss=0.16006, acc=0.95312
# [24/100] training 3.9% loss=0.17300, acc=0.93750
# [24/100] training 4.2% loss=0.18133, acc=0.93750
# [24/100] training 4.4% loss=0.15410, acc=0.96875
# [24/100] training 4.5% loss=0.23081, acc=0.89062
# [24/100] training 4.7% loss=0.37365, acc=0.82812
# [24/100] training 4.8% loss=0.27367, acc=0.85938
# [24/100] training 5.0% loss=0.30135, acc=0.87500
# [24/100] training 5.2% loss=0.19199, acc=0.92188
# [24/100] training 5.4% loss=0.14396, acc=0.95312
# [24/100] training 5.5% loss=0.25623, acc=0.89062
# [24/100] training 5.7% loss=0.28446, acc=0.85938
# [24/100] training 5.9% loss=0.22166, acc=0.89062
# [24/100] training 6.0% loss=0.34323, acc=0.81250
# [24/100] training 6.3% loss=0.20445, acc=0.93750
# [24/100] training 6.4% loss=0.24093, acc=0.89062
# [24/100] training 6.6% loss=0.23298, acc=0.87500
# [24/100] training 6.7% loss=0.36243, acc=0.84375
# [24/100] training 6.9% loss=0.18314, acc=0.92188
# [24/100] training 7.1% loss=0.20411, acc=0.89062
# [24/100] training 7.2% loss=0.24337, acc=0.89062
# [24/100] training 7.5% loss=0.18884, acc=0.90625
# [24/100] training 7.6% loss=0.23124, acc=0.89062
# [24/100] training 7.8% loss=0.34028, acc=0.89062
# [24/100] training 7.9% loss=0.35291, acc=0.82812
# [24/100] training 8.1% loss=0.17467, acc=0.93750
# [24/100] training 8.2% loss=0.21001, acc=0.90625
# [24/100] training 8.4% loss=0.37361, acc=0.87500
# [24/100] training 8.7% loss=0.22585, acc=0.93750
# [24/100] training 8.8% loss=0.26639, acc=0.85938
# [24/100] training 9.0% loss=0.24414, acc=0.93750
# [24/100] training 9.1% loss=0.19434, acc=0.96875
# [24/100] training 9.3% loss=0.36890, acc=0.87500
# [24/100] training 9.4% loss=0.18494, acc=0.93750
# [24/100] training 9.7% loss=0.23536, acc=0.90625
# [24/100] training 9.9% loss=0.35505, acc=0.89062
# [24/100] training 10.0% loss=0.29537, acc=0.87500
# [24/100] training 10.2% loss=0.23005, acc=0.89062
# [24/100] training 10.3% loss=0.16340, acc=0.96875
# [24/100] training 10.5% loss=0.32107, acc=0.87500
# [24/100] training 10.6% loss=0.30426, acc=0.89062
# [24/100] training 10.9% loss=0.14281, acc=0.95312
# [24/100] training 11.0% loss=0.24426, acc=0.93750
# [24/100] training 11.2% loss=0.15345, acc=0.93750
# [24/100] training 11.4% loss=0.23491, acc=0.90625
# [24/100] training 11.5% loss=0.41063, acc=0.89062
# [24/100] training 11.7% loss=0.16978, acc=0.90625
# [24/100] training 11.8% loss=0.18374, acc=0.90625
# [24/100] training 12.1% loss=0.29892, acc=0.89062
# [24/100] training 12.2% loss=0.16858, acc=0.92188
# [24/100] training 12.4% loss=0.22194, acc=0.89062
# [24/100] training 12.6% loss=0.23359, acc=0.92188
# [24/100] training 12.7% loss=0.25204, acc=0.87500
# [24/100] training 12.9% loss=0.27226, acc=0.89062
# [24/100] training 13.0% loss=0.14744, acc=0.93750
# [24/100] training 13.3% loss=0.20337, acc=0.90625
# [24/100] training 13.4% loss=0.23509, acc=0.90625
# [24/100] training 13.6% loss=0.17515, acc=0.93750
# [24/100] training 13.7% loss=0.38358, acc=0.89062
# [24/100] training 13.9% loss=0.24003, acc=0.90625
# [24/100] training 14.1% loss=0.33883, acc=0.90625
# [24/100] training 14.3% loss=0.22064, acc=0.95312
# [24/100] training 14.5% loss=0.23350, acc=0.92188
# [24/100] training 14.6% loss=0.21633, acc=0.92188
# [24/100] training 14.8% loss=0.35224, acc=0.85938
# [24/100] training 14.9% loss=0.19187, acc=0.92188
# [24/100] training 15.1% loss=0.27639, acc=0.82812
# [24/100] training 15.4% loss=0.25793, acc=0.92188
# [24/100] training 15.5% loss=0.26971, acc=0.93750
# [24/100] training 15.7% loss=0.26676, acc=0.90625
# [24/100] training 15.8% loss=0.21181, acc=0.90625
# [24/100] training 16.0% loss=0.28368, acc=0.85938
# [24/100] training 16.1% loss=0.40310, acc=0.85938
# [24/100] training 16.3% loss=0.39711, acc=0.76562
# [24/100] training 16.4% loss=0.23994, acc=0.87500
# [24/100] training 16.7% loss=0.41610, acc=0.84375
# [24/100] training 16.9% loss=0.29519, acc=0.84375
# [24/100] training 17.0% loss=0.19789, acc=0.95312
# [24/100] training 17.2% loss=0.24072, acc=0.90625
# [24/100] training 17.3% loss=0.25848, acc=0.93750
# [24/100] training 17.5% loss=0.23519, acc=0.89062
# [24/100] training 17.7% loss=0.21162, acc=0.87500
# [24/100] training 17.9% loss=0.36125, acc=0.90625
# [24/100] training 18.1% loss=0.30136, acc=0.89062
# [24/100] training 18.2% loss=0.27054, acc=0.87500
# [24/100] training 18.4% loss=0.34261, acc=0.84375
# [24/100] training 18.5% loss=0.18878, acc=0.95312
# [24/100] training 18.8% loss=0.18393, acc=0.93750
# [24/100] training 18.9% loss=0.19467, acc=0.90625
# [24/100] training 19.1% loss=0.24535, acc=0.92188
# [24/100] training 19.2% loss=0.17835, acc=0.92188
# [24/100] training 19.4% loss=0.15536, acc=0.90625
# [24/100] training 19.6% loss=0.22779, acc=0.92188
# [24/100] training 19.7% loss=0.33425, acc=0.85938
# [24/100] training 20.0% loss=0.22577, acc=0.90625
# [24/100] training 20.1% loss=0.26672, acc=0.90625
# [24/100] training 20.3% loss=0.22300, acc=0.90625
# [24/100] training 20.4% loss=0.40255, acc=0.79688
# [24/100] training 20.6% loss=0.32791, acc=0.85938
# [24/100] training 20.8% loss=0.21058, acc=0.93750
# [24/100] training 20.9% loss=0.21711, acc=0.90625
# [24/100] training 21.2% loss=0.19917, acc=0.93750
# [24/100] training 21.3% loss=0.26886, acc=0.87500
# [24/100] training 21.5% loss=0.32499, acc=0.92188
# [24/100] training 21.6% loss=0.15050, acc=0.93750
# [24/100] training 21.8% loss=0.12898, acc=0.92188
# [24/100] training 21.9% loss=0.28545, acc=0.90625
# [24/100] training 22.2% loss=0.24122, acc=0.92188
# [24/100] training 22.4% loss=0.27217, acc=0.87500
# [24/100] training 22.5% loss=0.17218, acc=0.96875
# [24/100] training 22.7% loss=0.17412, acc=0.92188
# [24/100] training 22.8% loss=0.29363, acc=0.89062
# [24/100] training 23.0% loss=0.12151, acc=0.95312
# [24/100] training 23.1% loss=0.30130, acc=0.85938
# [24/100] training 23.4% loss=0.35838, acc=0.82812
# [24/100] training 23.6% loss=0.34547, acc=0.90625
# [24/100] training 23.7% loss=0.21363, acc=0.92188
# [24/100] training 23.9% loss=0.28728, acc=0.85938
# [24/100] training 24.0% loss=0.20091, acc=0.90625
# [24/100] training 24.2% loss=0.19561, acc=0.92188
# [24/100] training 24.3% loss=0.36062, acc=0.81250
# [24/100] training 24.6% loss=0.20280, acc=0.93750
# [24/100] training 24.7% loss=0.30792, acc=0.85938
# [24/100] training 24.9% loss=0.20427, acc=0.90625
# [24/100] training 25.1% loss=0.31542, acc=0.85938
# [24/100] training 25.2% loss=0.19250, acc=0.90625
# [24/100] training 25.4% loss=0.21126, acc=0.92188
# [24/100] training 25.6% loss=0.15652, acc=0.93750
# [24/100] training 25.8% loss=0.30507, acc=0.84375
# [24/100] training 25.9% loss=0.19501, acc=0.92188
# [24/100] training 26.1% loss=0.17375, acc=0.92188
# [24/100] training 26.3% loss=0.16768, acc=0.92188
# [24/100] training 26.4% loss=0.15454, acc=0.96875
# [24/100] training 26.6% loss=0.07030, acc=0.98438
# [24/100] training 26.8% loss=0.36029, acc=0.85938
# [24/100] training 27.0% loss=0.30729, acc=0.90625
# [24/100] training 27.1% loss=0.17799, acc=0.93750
# [24/100] training 27.3% loss=0.22626, acc=0.89062
# [24/100] training 27.4% loss=0.16548, acc=0.89062
# [24/100] training 27.6% loss=0.36125, acc=0.90625
# [24/100] training 27.9% loss=0.21088, acc=0.89062
# [24/100] training 28.0% loss=0.34395, acc=0.84375
# [24/100] training 28.2% loss=0.22236, acc=0.92188
# [24/100] training 28.3% loss=0.12141, acc=0.95312
# [24/100] training 28.5% loss=0.26804, acc=0.85938
# [24/100] training 28.6% loss=0.23398, acc=0.92188
# [24/100] training 28.8% loss=0.17029, acc=0.92188
# [24/100] training 29.1% loss=0.15471, acc=0.95312
# [24/100] training 29.2% loss=0.19239, acc=0.95312
# [24/100] training 29.4% loss=0.29744, acc=0.87500
# [24/100] training 29.5% loss=0.17083, acc=0.92188
# [24/100] training 29.7% loss=0.25569, acc=0.90625
# [24/100] training 29.8% loss=0.17302, acc=0.93750
# [24/100] training 30.0% loss=0.28807, acc=0.84375
# [24/100] training 30.2% loss=0.16728, acc=0.93750
# [24/100] training 30.4% loss=0.19173, acc=0.92188
# [24/100] training 30.6% loss=0.28055, acc=0.89062
# [24/100] training 30.7% loss=0.24602, acc=0.90625
# [24/100] training 30.9% loss=0.37209, acc=0.89062
# [24/100] training 31.0% loss=0.19413, acc=0.90625
# [24/100] training 31.3% loss=0.19434, acc=0.96875
# [24/100] training 31.4% loss=0.29264, acc=0.85938
# [24/100] training 31.6% loss=0.27548, acc=0.89062
# [24/100] training 31.8% loss=0.20140, acc=0.89062
# [24/100] training 31.9% loss=0.27107, acc=0.90625
# [24/100] training 32.1% loss=0.29817, acc=0.85938
# [24/100] training 32.2% loss=0.25942, acc=0.85938
# [24/100] training 32.5% loss=0.16998, acc=0.93750
# [24/100] training 32.6% loss=0.26366, acc=0.89062
# [24/100] training 32.8% loss=0.23293, acc=0.92188
# [24/100] training 32.9% loss=0.24819, acc=0.89062
# [24/100] training 33.1% loss=0.26272, acc=0.89062
# [24/100] training 33.3% loss=0.28518, acc=0.87500
# [24/100] training 33.4% loss=0.17762, acc=0.93750
# [24/100] training 33.7% loss=0.19295, acc=0.95312
# [24/100] training 33.8% loss=0.25567, acc=0.87500
# [24/100] training 34.0% loss=0.26330, acc=0.89062
# [24/100] training 34.1% loss=0.19350, acc=0.90625
# [24/100] training 34.3% loss=0.18769, acc=0.93750
# [24/100] training 34.5% loss=0.27464, acc=0.84375
# [24/100] training 34.7% loss=0.11921, acc=0.95312
# [24/100] training 34.9% loss=0.18037, acc=0.92188
# [24/100] training 35.0% loss=0.15112, acc=0.95312
# [24/100] training 35.2% loss=0.24398, acc=0.89062
# [24/100] training 35.3% loss=0.22860, acc=0.90625
# [24/100] training 35.5% loss=0.20272, acc=0.89062
# [24/100] training 35.6% loss=0.33657, acc=0.87500
# [24/100] training 35.9% loss=0.28933, acc=0.85938
# [24/100] training 36.1% loss=0.33648, acc=0.82812
# [24/100] training 36.2% loss=0.31663, acc=0.89062
# [24/100] training 36.4% loss=0.29783, acc=0.90625
# [24/100] training 36.5% loss=0.29679, acc=0.89062
# [24/100] training 36.7% loss=0.20152, acc=0.92188
# [24/100] training 36.8% loss=0.13199, acc=0.93750
# [24/100] training 37.1% loss=0.30267, acc=0.89062
# [24/100] training 37.3% loss=0.25070, acc=0.93750
# [24/100] training 37.4% loss=0.25155, acc=0.89062
# [24/100] training 37.6% loss=0.17143, acc=0.93750
# [24/100] training 37.7% loss=0.28311, acc=0.90625
# [24/100] training 37.9% loss=0.26571, acc=0.89062
# [24/100] training 38.1% loss=0.33139, acc=0.87500
# [24/100] training 38.3% loss=0.23563, acc=0.90625
# [24/100] training 38.4% loss=0.14666, acc=0.93750
# [24/100] training 38.6% loss=0.23825, acc=0.89062
# [24/100] training 38.8% loss=0.25399, acc=0.87500
# [24/100] training 38.9% loss=0.20469, acc=0.90625
# [24/100] training 39.1% loss=0.19651, acc=0.95312
# [24/100] training 39.3% loss=0.24927, acc=0.92188
# [24/100] training 39.5% loss=0.27896, acc=0.87500
# [24/100] training 39.6% loss=0.17680, acc=0.96875
# [24/100] training 39.8% loss=0.11974, acc=0.93750
# [24/100] training 40.0% loss=0.30722, acc=0.87500
# [24/100] training 40.1% loss=0.31132, acc=0.87500
# [24/100] training 40.4% loss=0.21314, acc=0.87500
# [24/100] training 40.5% loss=0.30979, acc=0.89062
# [24/100] training 40.7% loss=0.30162, acc=0.89062
# [24/100] training 40.8% loss=0.17768, acc=0.93750
# [24/100] training 41.0% loss=0.21567, acc=0.90625
# [24/100] training 41.1% loss=0.42544, acc=0.84375
# [24/100] training 41.3% loss=0.30090, acc=0.89062
# [24/100] training 41.6% loss=0.24535, acc=0.89062
# [24/100] training 41.7% loss=0.39726, acc=0.84375
# [24/100] training 41.9% loss=0.20133, acc=0.95312
# [24/100] training 42.0% loss=0.22904, acc=0.89062
# [24/100] training 42.2% loss=0.30297, acc=0.89062
# [24/100] training 42.3% loss=0.24042, acc=0.92188
# [24/100] training 42.5% loss=0.17459, acc=0.92188
# [24/100] training 42.8% loss=0.12400, acc=0.98438
# [24/100] training 42.9% loss=0.21879, acc=0.92188
# [24/100] training 43.1% loss=0.23635, acc=0.92188
# [24/100] training 43.2% loss=0.14536, acc=0.93750
# [24/100] training 43.4% loss=0.28732, acc=0.90625
# [24/100] training 43.5% loss=0.18206, acc=0.93750
# [24/100] training 43.8% loss=0.36104, acc=0.81250
# [24/100] training 43.9% loss=0.20661, acc=0.90625
# [24/100] training 44.1% loss=0.21053, acc=0.90625
# [24/100] training 44.3% loss=0.23707, acc=0.93750
# [24/100] training 44.4% loss=0.29899, acc=0.84375
# [24/100] training 44.6% loss=0.20084, acc=0.90625
# [24/100] training 44.7% loss=0.32720, acc=0.85938
# [24/100] training 45.0% loss=0.22420, acc=0.90625
# [24/100] training 45.1% loss=0.41328, acc=0.84375
# [24/100] training 45.3% loss=0.25796, acc=0.90625
# [24/100] training 45.5% loss=0.07560, acc=1.00000
# [24/100] training 45.6% loss=0.22770, acc=0.90625
# [24/100] training 45.8% loss=0.19208, acc=0.93750
# [24/100] training 45.9% loss=0.14359, acc=0.90625
# [24/100] training 46.2% loss=0.20667, acc=0.90625
# [24/100] training 46.3% loss=0.17625, acc=0.92188
# [24/100] training 46.5% loss=0.41719, acc=0.81250
# [24/100] training 46.6% loss=0.24087, acc=0.92188
# [24/100] training 46.8% loss=0.24090, acc=0.92188
# [24/100] training 47.0% loss=0.28236, acc=0.89062
# [24/100] training 47.2% loss=0.29395, acc=0.81250
# [24/100] training 47.4% loss=0.21131, acc=0.92188
# [24/100] training 47.5% loss=0.28730, acc=0.89062
# [24/100] training 47.7% loss=0.22818, acc=0.90625
# [24/100] training 47.8% loss=0.32767, acc=0.82812
# [24/100] training 48.0% loss=0.27943, acc=0.89062
# [24/100] training 48.3% loss=0.11332, acc=0.96875
# [24/100] training 48.4% loss=0.13252, acc=0.95312
# [24/100] training 48.6% loss=0.16094, acc=0.96875
# [24/100] training 48.7% loss=0.38362, acc=0.87500
# [24/100] training 48.9% loss=0.16328, acc=0.96875
# [24/100] training 49.0% loss=0.20262, acc=0.95312
# [24/100] training 49.2% loss=0.35202, acc=0.82812
# [24/100] training 49.3% loss=0.14931, acc=0.95312
# [24/100] training 49.6% loss=0.26313, acc=0.90625
# [24/100] training 49.8% loss=0.30946, acc=0.85938
# [24/100] training 49.9% loss=0.26349, acc=0.85938
# [24/100] training 50.1% loss=0.15524, acc=0.93750
# [24/100] training 50.2% loss=0.28381, acc=0.85938
# [24/100] training 50.4% loss=0.34573, acc=0.87500
# [24/100] training 50.6% loss=0.23473, acc=0.92188
# [24/100] training 50.8% loss=0.35335, acc=0.84375
# [24/100] training 51.0% loss=0.32511, acc=0.84375
# [24/100] training 51.1% loss=0.28657, acc=0.87500
# [24/100] training 51.3% loss=0.23646, acc=0.93750
# [24/100] training 51.4% loss=0.21507, acc=0.95312
# [24/100] training 51.7% loss=0.23725, acc=0.89062
# [24/100] training 51.8% loss=0.33783, acc=0.82812
# [24/100] training 52.0% loss=0.22679, acc=0.89062
# [24/100] training 52.1% loss=0.26670, acc=0.89062
# [24/100] training 52.3% loss=0.32603, acc=0.85938
# [24/100] training 52.5% loss=0.14946, acc=0.95312
# [24/100] training 52.6% loss=0.19862, acc=0.87500
# [24/100] training 52.9% loss=0.47804, acc=0.85938
# [24/100] training 53.0% loss=0.17330, acc=0.90625
# [24/100] training 53.2% loss=0.22542, acc=0.90625
# [24/100] training 53.3% loss=0.16183, acc=0.93750
# [24/100] training 53.5% loss=0.18893, acc=0.92188
# [24/100] training 53.7% loss=0.25643, acc=0.90625
# [24/100] training 53.8% loss=0.34378, acc=0.79688
# [24/100] training 54.1% loss=0.31585, acc=0.87500
# [24/100] training 54.2% loss=0.18898, acc=0.92188
# [24/100] training 54.4% loss=0.29600, acc=0.89062
# [24/100] training 54.5% loss=0.26710, acc=0.89062
# [24/100] training 54.7% loss=0.35573, acc=0.85938
# [24/100] training 54.8% loss=0.13395, acc=0.96875
# [24/100] training 55.1% loss=0.17605, acc=0.93750
# [24/100] training 55.3% loss=0.14344, acc=0.93750
# [24/100] training 55.4% loss=0.26893, acc=0.89062
# [24/100] training 55.6% loss=0.25456, acc=0.89062
# [24/100] training 55.7% loss=0.26690, acc=0.87500
# [24/100] training 55.9% loss=0.17244, acc=0.92188
# [24/100] training 56.0% loss=0.22459, acc=0.84375
# [24/100] training 56.3% loss=0.47298, acc=0.78125
# [24/100] training 56.5% loss=0.21822, acc=0.90625
# [24/100] training 56.6% loss=0.23845, acc=0.93750
# [24/100] training 56.8% loss=0.34075, acc=0.84375
# [24/100] training 56.9% loss=0.30175, acc=0.84375
# [24/100] training 57.1% loss=0.31023, acc=0.89062
# [24/100] training 57.2% loss=0.19824, acc=0.93750
# [24/100] training 57.5% loss=0.17711, acc=0.93750
# [24/100] training 57.6% loss=0.25748, acc=0.89062
# [24/100] training 57.8% loss=0.16575, acc=0.95312
# [24/100] training 58.0% loss=0.19721, acc=0.92188
# [24/100] training 58.1% loss=0.19498, acc=0.93750
# [24/100] training 58.3% loss=0.18467, acc=0.92188
# [24/100] training 58.4% loss=0.27386, acc=0.92188
# [24/100] training 58.7% loss=0.30034, acc=0.92188
# [24/100] training 58.8% loss=0.23468, acc=0.89062
# [24/100] training 59.0% loss=0.17223, acc=0.92188
# [24/100] training 59.2% loss=0.19245, acc=0.90625
# [24/100] training 59.3% loss=0.23241, acc=0.92188
# [24/100] training 59.5% loss=0.17060, acc=0.93750
# [24/100] training 59.7% loss=0.23598, acc=0.92188
# [24/100] training 59.9% loss=0.15608, acc=0.98438
# [24/100] training 60.0% loss=0.17893, acc=0.93750
# [24/100] training 60.2% loss=0.33941, acc=0.85938
# [24/100] training 60.3% loss=0.24043, acc=0.90625
# [24/100] training 60.5% loss=0.28077, acc=0.89062
# [24/100] training 60.8% loss=0.18374, acc=0.92188
# [24/100] training 60.9% loss=0.36570, acc=0.79688
# [24/100] training 61.1% loss=0.31372, acc=0.87500
# [24/100] training 61.2% loss=0.20308, acc=0.93750
# [24/100] training 61.4% loss=0.26652, acc=0.92188
# [24/100] training 61.5% loss=0.39087, acc=0.79688
# [24/100] training 61.7% loss=0.33890, acc=0.84375
# [24/100] training 62.0% loss=0.29587, acc=0.85938
# [24/100] training 62.1% loss=0.39380, acc=0.84375
# [24/100] training 62.3% loss=0.15942, acc=0.93750
# [24/100] training 62.4% loss=0.22981, acc=0.90625
# [24/100] training 62.6% loss=0.33146, acc=0.82812
# [24/100] training 62.7% loss=0.12154, acc=0.98438
# [24/100] training 62.9% loss=0.29852, acc=0.89062
# [24/100] training 63.1% loss=0.21187, acc=0.93750
# [24/100] training 63.3% loss=0.24409, acc=0.90625
# [24/100] training 63.5% loss=0.31937, acc=0.89062
# [24/100] training 63.6% loss=0.28522, acc=0.84375
# [24/100] training 63.8% loss=0.31787, acc=0.87500
# [24/100] training 63.9% loss=0.20998, acc=0.92188
# [24/100] training 64.2% loss=0.23613, acc=0.90625
# [24/100] training 64.3% loss=0.28795, acc=0.87500
# [24/100] training 64.5% loss=0.25919, acc=0.90625
# [24/100] training 64.7% loss=0.29168, acc=0.89062
# [24/100] training 64.8% loss=0.39790, acc=0.82812
# [24/100] training 65.0% loss=0.23309, acc=0.89062
# [24/100] training 65.1% loss=0.31873, acc=0.84375
# [24/100] training 65.4% loss=0.26128, acc=0.89062
# [24/100] training 65.5% loss=0.13146, acc=0.96875
# [24/100] training 65.7% loss=0.15234, acc=0.93750
# [24/100] training 65.8% loss=0.23770, acc=0.90625
# [24/100] training 66.0% loss=0.25223, acc=0.90625
# [24/100] training 66.2% loss=0.12918, acc=0.95312
# [24/100] training 66.3% loss=0.31764, acc=0.87500
# [24/100] training 66.6% loss=0.28662, acc=0.89062
# [24/100] training 66.7% loss=0.15980, acc=0.93750
# [24/100] training 66.9% loss=0.22388, acc=0.90625
# [24/100] training 67.0% loss=0.19466, acc=0.90625
# [24/100] training 67.2% loss=0.17626, acc=0.89062
# [24/100] training 67.4% loss=0.21727, acc=0.92188
# [24/100] training 67.6% loss=0.39094, acc=0.85938
# [24/100] training 67.8% loss=0.20332, acc=0.92188
# [24/100] training 67.9% loss=0.23420, acc=0.84375
# [24/100] training 68.1% loss=0.12829, acc=0.96875
# [24/100] training 68.2% loss=0.16630, acc=0.93750
# [24/100] training 68.4% loss=0.15233, acc=0.96875
# [24/100] training 68.5% loss=0.29922, acc=0.89062
# [24/100] training 68.8% loss=0.26527, acc=0.89062
# [24/100] training 69.0% loss=0.27727, acc=0.87500
# [24/100] training 69.1% loss=0.16788, acc=0.93750
# [24/100] training 69.3% loss=0.31105, acc=0.87500
# [24/100] training 69.4% loss=0.26840, acc=0.89062
# [24/100] training 69.6% loss=0.26970, acc=0.85938
# [24/100] training 69.7% loss=0.28219, acc=0.89062
# [24/100] training 70.0% loss=0.31791, acc=0.87500
# [24/100] training 70.2% loss=0.27213, acc=0.92188
# [24/100] training 70.3% loss=0.23886, acc=0.92188
# [24/100] training 70.5% loss=0.29022, acc=0.85938
# [24/100] training 70.6% loss=0.16850, acc=0.90625
# [24/100] training 70.8% loss=0.30878, acc=0.87500
# [24/100] training 71.0% loss=0.25390, acc=0.90625
# [24/100] training 71.2% loss=0.20825, acc=0.92188
# [24/100] training 71.3% loss=0.18414, acc=0.93750
# [24/100] training 71.5% loss=0.39878, acc=0.84375
# [24/100] training 71.7% loss=0.31702, acc=0.85938
# [24/100] training 71.8% loss=0.36410, acc=0.90625
# [24/100] training 72.0% loss=0.25252, acc=0.85938
# [24/100] training 72.2% loss=0.17926, acc=0.90625
# [24/100] training 72.4% loss=0.30948, acc=0.87500
# [24/100] training 72.5% loss=0.26444, acc=0.90625
# [24/100] training 72.7% loss=0.30560, acc=0.84375
# [24/100] training 72.9% loss=0.25481, acc=0.85938
# [24/100] training 73.0% loss=0.12377, acc=0.95312
# [24/100] training 73.3% loss=0.31409, acc=0.87500
# [24/100] training 73.4% loss=0.11673, acc=0.95312
# [24/100] training 73.6% loss=0.19303, acc=0.93750
# [24/100] training 73.7% loss=0.27246, acc=0.92188
# [24/100] training 73.9% loss=0.17623, acc=0.92188
# [24/100] training 74.0% loss=0.34418, acc=0.87500
# [24/100] training 74.2% loss=0.17143, acc=0.90625
# [24/100] training 74.5% loss=0.12715, acc=0.95312
# [24/100] training 74.6% loss=0.46998, acc=0.84375
# [24/100] training 74.8% loss=0.43193, acc=0.81250
# [24/100] training 74.9% loss=0.40409, acc=0.79688
# [24/100] training 75.1% loss=0.24307, acc=0.90625
# [24/100] training 75.2% loss=0.25727, acc=0.92188
# [24/100] training 75.4% loss=0.29217, acc=0.90625
# [24/100] training 75.7% loss=0.31137, acc=0.89062
# [24/100] training 75.8% loss=0.39185, acc=0.87500
# [24/100] training 76.0% loss=0.17299, acc=0.92188
# [24/100] training 76.1% loss=0.24911, acc=0.92188
# [24/100] training 76.3% loss=0.16472, acc=0.93750
# [24/100] training 76.4% loss=0.25341, acc=0.89062
# [24/100] training 76.7% loss=0.23392, acc=0.93750
# [24/100] training 76.8% loss=0.14074, acc=0.93750
# [24/100] training 77.0% loss=0.20620, acc=0.92188
# [24/100] training 77.2% loss=0.29359, acc=0.90625
# [24/100] training 77.3% loss=0.10616, acc=0.95312
# [24/100] training 77.5% loss=0.27800, acc=0.89062
# [24/100] training 77.6% loss=0.23585, acc=0.89062
# [24/100] training 77.9% loss=0.18490, acc=0.92188
# [24/100] training 78.0% loss=0.27415, acc=0.85938
# [24/100] training 78.2% loss=0.22188, acc=0.92188
# [24/100] training 78.4% loss=0.12491, acc=0.95312
# [24/100] training 78.5% loss=0.28460, acc=0.90625
# [24/100] training 78.7% loss=0.25782, acc=0.87500
# [24/100] training 78.8% loss=0.18015, acc=0.93750
# [24/100] training 79.1% loss=0.14713, acc=0.93750
# [24/100] training 79.2% loss=0.15589, acc=0.93750
# [24/100] training 79.4% loss=0.31891, acc=0.82812
# [24/100] training 79.5% loss=0.21840, acc=0.92188
# [24/100] training 79.7% loss=0.11518, acc=0.95312
# [24/100] training 79.9% loss=0.18110, acc=0.92188
# [24/100] training 80.1% loss=0.21041, acc=0.92188
# [24/100] training 80.3% loss=0.31630, acc=0.85938
# [24/100] training 80.4% loss=0.19595, acc=0.93750
# [24/100] training 80.6% loss=0.27364, acc=0.87500
# [24/100] training 80.7% loss=0.13749, acc=0.96875
# [24/100] training 80.9% loss=0.28271, acc=0.85938
# [24/100] training 81.2% loss=0.27161, acc=0.89062
# [24/100] training 81.3% loss=0.22834, acc=0.87500
# [24/100] training 81.5% loss=0.21515, acc=0.90625
# [24/100] training 81.6% loss=0.28147, acc=0.87500
# [24/100] training 81.8% loss=0.26351, acc=0.87500
# [24/100] training 81.9% loss=0.34503, acc=0.90625
# [24/100] training 82.1% loss=0.16001, acc=0.95312
# [24/100] training 82.2% loss=0.22205, acc=0.90625
# [24/100] training 82.5% loss=0.23441, acc=0.87500
# [24/100] training 82.7% loss=0.22441, acc=0.92188
# [24/100] training 82.8% loss=0.17900, acc=0.93750
# [24/100] training 83.0% loss=0.19231, acc=0.93750
# [24/100] training 83.1% loss=0.24270, acc=0.93750
# [24/100] training 83.3% loss=0.18056, acc=0.92188
# [24/100] training 83.5% loss=0.16805, acc=0.93750
# [24/100] training 83.7% loss=0.41024, acc=0.84375
# [24/100] training 83.9% loss=0.26927, acc=0.87500
# [24/100] training 84.0% loss=0.12860, acc=0.98438
# [24/100] training 84.2% loss=0.19226, acc=0.92188
# [24/100] training 84.3% loss=0.22304, acc=0.90625
# [24/100] training 84.5% loss=0.12784, acc=0.95312
# [24/100] training 84.7% loss=0.21558, acc=0.90625
# [24/100] training 84.9% loss=0.19990, acc=0.90625
# [24/100] training 85.0% loss=0.20438, acc=0.92188
# [24/100] training 85.2% loss=0.25027, acc=0.93750
# [24/100] training 85.4% loss=0.25114, acc=0.90625
# [24/100] training 85.5% loss=0.23126, acc=0.90625
# [24/100] training 85.8% loss=0.28110, acc=0.89062
# [24/100] training 85.9% loss=0.26110, acc=0.89062
# [24/100] training 86.1% loss=0.22550, acc=0.89062
# [24/100] training 86.2% loss=0.17896, acc=0.93750
# [24/100] training 86.4% loss=0.29226, acc=0.85938
# [24/100] training 86.6% loss=0.31312, acc=0.90625
# [24/100] training 86.7% loss=0.16794, acc=0.93750
# [24/100] training 87.0% loss=0.30403, acc=0.90625
# [24/100] training 87.1% loss=0.24206, acc=0.90625
# [24/100] training 87.3% loss=0.28202, acc=0.87500
# [24/100] training 87.4% loss=0.28470, acc=0.87500
# [24/100] training 87.6% loss=0.32380, acc=0.84375
# [24/100] training 87.7% loss=0.35716, acc=0.90625
# [24/100] training 87.9% loss=0.27885, acc=0.87500
# [24/100] training 88.2% loss=0.11906, acc=0.96875
# [24/100] training 88.3% loss=0.19052, acc=0.93750
# [24/100] training 88.5% loss=0.30037, acc=0.87500
# [24/100] training 88.6% loss=0.07311, acc=1.00000
# [24/100] training 88.8% loss=0.21314, acc=0.92188
# [24/100] training 88.9% loss=0.27774, acc=0.90625
# [24/100] training 89.2% loss=0.20311, acc=0.90625
# [24/100] training 89.4% loss=0.18768, acc=0.92188
# [24/100] training 89.5% loss=0.31984, acc=0.81250
# [24/100] training 89.7% loss=0.20827, acc=0.90625
# [24/100] training 89.8% loss=0.20679, acc=0.93750
# [24/100] training 90.0% loss=0.19208, acc=0.92188
# [24/100] training 90.1% loss=0.19744, acc=0.92188
# [24/100] training 90.4% loss=0.17791, acc=0.95312
# [24/100] training 90.5% loss=0.17236, acc=0.92188
# [24/100] training 90.7% loss=0.20834, acc=0.90625
# [24/100] training 90.9% loss=0.17327, acc=0.92188
# [24/100] training 91.0% loss=0.21557, acc=0.93750
# [24/100] training 91.2% loss=0.31373, acc=0.82812
# [24/100] training 91.3% loss=0.22878, acc=0.90625
# [24/100] training 91.6% loss=0.38713, acc=0.82812
# [24/100] training 91.7% loss=0.28039, acc=0.89062
# [24/100] training 91.9% loss=0.34194, acc=0.89062
# [24/100] training 92.1% loss=0.20689, acc=0.90625
# [24/100] training 92.2% loss=0.11672, acc=0.95312
# [24/100] training 92.4% loss=0.19421, acc=0.90625
# [24/100] training 92.6% loss=0.31025, acc=0.87500
# [24/100] training 92.8% loss=0.28743, acc=0.90625
# [24/100] training 92.9% loss=0.25683, acc=0.90625
# [24/100] training 93.1% loss=0.33914, acc=0.87500
# [24/100] training 93.2% loss=0.32088, acc=0.87500
# [24/100] training 93.4% loss=0.15112, acc=0.95312
# [24/100] training 93.7% loss=0.19688, acc=0.90625
# [24/100] training 93.8% loss=0.15891, acc=0.93750
# [24/100] training 94.0% loss=0.16133, acc=0.92188
# [24/100] training 94.1% loss=0.32150, acc=0.85938
# [24/100] training 94.3% loss=0.15932, acc=0.93750
# [24/100] training 94.4% loss=0.29139, acc=0.89062
# [24/100] training 94.6% loss=0.15869, acc=0.95312
# [24/100] training 94.9% loss=0.25618, acc=0.87500
# [24/100] training 95.0% loss=0.33075, acc=0.87500
# [24/100] training 95.2% loss=0.38788, acc=0.87500
# [24/100] training 95.3% loss=0.23643, acc=0.93750
# [24/100] training 95.5% loss=0.17844, acc=0.93750
# [24/100] training 95.6% loss=0.34863, acc=0.85938
# [24/100] training 95.8% loss=0.40970, acc=0.79688
# [24/100] training 96.0% loss=0.28729, acc=0.92188
# [24/100] training 96.2% loss=0.18721, acc=0.93750
# [24/100] training 96.4% loss=0.20089, acc=0.95312
# [24/100] training 96.5% loss=0.24011, acc=0.92188
# [24/100] training 96.7% loss=0.16991, acc=0.95312
# [24/100] training 96.8% loss=0.22245, acc=0.92188
# [24/100] training 97.1% loss=0.21824, acc=0.89062
# [24/100] training 97.2% loss=0.17535, acc=0.92188
# [24/100] training 97.4% loss=0.18955, acc=0.93750
# [24/100] training 97.6% loss=0.33372, acc=0.87500
# [24/100] training 97.7% loss=0.15083, acc=0.92188
# [24/100] training 97.9% loss=0.19121, acc=0.89062
# [24/100] training 98.0% loss=0.21682, acc=0.89062
# [24/100] training 98.3% loss=0.22195, acc=0.90625
# [24/100] training 98.4% loss=0.23750, acc=0.89062
# [24/100] training 98.6% loss=0.38322, acc=0.78125
# [24/100] training 98.7% loss=0.27409, acc=0.90625
# [24/100] training 98.9% loss=0.21230, acc=0.90625
# [24/100] training 99.1% loss=0.12964, acc=0.96875
# [24/100] training 99.2% loss=0.24292, acc=0.87500
# [24/100] training 99.5% loss=0.33343, acc=0.84375
# [24/100] training 99.6% loss=0.23778, acc=0.90625
# [24/100] training 99.8% loss=0.18280, acc=0.93750
# [24/100] training 99.9% loss=0.10453, acc=0.93750
# [24/100] testing 0.9% loss=0.19747, acc=0.90625
# [24/100] testing 1.8% loss=0.50627, acc=0.79688
# [24/100] testing 2.2% loss=0.29409, acc=0.87500
# [24/100] testing 3.1% loss=0.28267, acc=0.87500
# [24/100] testing 3.5% loss=0.19362, acc=0.90625
# [24/100] testing 4.4% loss=0.31785, acc=0.89062
# [24/100] testing 4.8% loss=0.41385, acc=0.87500
# [24/100] testing 5.7% loss=0.27926, acc=0.85938
# [24/100] testing 6.6% loss=0.27680, acc=0.87500
# [24/100] testing 7.0% loss=0.17678, acc=0.93750
# [24/100] testing 7.9% loss=0.31391, acc=0.87500
# [24/100] testing 8.3% loss=0.23364, acc=0.93750
# [24/100] testing 9.2% loss=0.38629, acc=0.87500
# [24/100] testing 9.7% loss=0.23385, acc=0.92188
# [24/100] testing 10.5% loss=0.36972, acc=0.78125
# [24/100] testing 11.0% loss=0.34725, acc=0.85938
# [24/100] testing 11.8% loss=0.27244, acc=0.93750
# [24/100] testing 12.7% loss=0.40288, acc=0.87500
# [24/100] testing 13.2% loss=0.27978, acc=0.87500
# [24/100] testing 14.0% loss=0.34324, acc=0.89062
# [24/100] testing 14.5% loss=0.28559, acc=0.85938
# [24/100] testing 15.4% loss=0.33794, acc=0.89062
# [24/100] testing 15.8% loss=0.19990, acc=0.90625
# [24/100] testing 16.7% loss=0.31871, acc=0.90625
# [24/100] testing 17.5% loss=0.26724, acc=0.87500
# [24/100] testing 18.0% loss=0.17936, acc=0.93750
# [24/100] testing 18.9% loss=0.11514, acc=0.93750
# [24/100] testing 19.3% loss=0.37040, acc=0.90625
# [24/100] testing 20.2% loss=0.35340, acc=0.87500
# [24/100] testing 20.6% loss=0.33300, acc=0.87500
# [24/100] testing 21.5% loss=0.22610, acc=0.89062
# [24/100] testing 21.9% loss=0.52500, acc=0.84375
# [24/100] testing 22.8% loss=0.49712, acc=0.85938
# [24/100] testing 23.7% loss=0.40033, acc=0.89062
# [24/100] testing 24.1% loss=0.25314, acc=0.92188
# [24/100] testing 25.0% loss=0.42038, acc=0.87500
# [24/100] testing 25.4% loss=0.15648, acc=0.96875
# [24/100] testing 26.3% loss=0.34537, acc=0.84375
# [24/100] testing 26.8% loss=0.31180, acc=0.92188
# [24/100] testing 27.6% loss=0.36466, acc=0.89062
# [24/100] testing 28.5% loss=0.28715, acc=0.89062
# [24/100] testing 29.0% loss=0.11400, acc=0.93750
# [24/100] testing 29.8% loss=0.43621, acc=0.85938
# [24/100] testing 30.3% loss=0.25494, acc=0.85938
# [24/100] testing 31.1% loss=0.42599, acc=0.85938
# [24/100] testing 31.6% loss=0.18262, acc=0.90625
# [24/100] testing 32.5% loss=0.28762, acc=0.89062
# [24/100] testing 32.9% loss=0.53547, acc=0.89062
# [24/100] testing 33.8% loss=0.31280, acc=0.84375
# [24/100] testing 34.7% loss=0.47719, acc=0.79688
# [24/100] testing 35.1% loss=0.20285, acc=0.90625
# [24/100] testing 36.0% loss=0.32089, acc=0.87500
# [24/100] testing 36.4% loss=0.32032, acc=0.87500
# [24/100] testing 37.3% loss=0.35107, acc=0.92188
# [24/100] testing 37.7% loss=0.43075, acc=0.82812
# [24/100] testing 38.6% loss=0.22841, acc=0.92188
# [24/100] testing 39.5% loss=0.34641, acc=0.92188
# [24/100] testing 39.9% loss=0.39375, acc=0.89062
# [24/100] testing 40.8% loss=0.39897, acc=0.87500
# [24/100] testing 41.2% loss=0.30600, acc=0.90625
# [24/100] testing 42.1% loss=0.27696, acc=0.87500
# [24/100] testing 42.5% loss=0.24316, acc=0.89062
# [24/100] testing 43.4% loss=0.47102, acc=0.85938
# [24/100] testing 43.9% loss=0.15870, acc=0.96875
# [24/100] testing 44.7% loss=0.42115, acc=0.85938
# [24/100] testing 45.6% loss=0.29758, acc=0.90625
# [24/100] testing 46.1% loss=0.24781, acc=0.85938
# [24/100] testing 46.9% loss=0.19020, acc=0.92188
# [24/100] testing 47.4% loss=0.11405, acc=0.96875
# [24/100] testing 48.3% loss=0.44463, acc=0.81250
# [24/100] testing 48.7% loss=0.56591, acc=0.82812
# [24/100] testing 49.6% loss=0.45663, acc=0.81250
# [24/100] testing 50.4% loss=0.21121, acc=0.89062
# [24/100] testing 50.9% loss=0.45594, acc=0.85938
# [24/100] testing 51.8% loss=0.26671, acc=0.82812
# [24/100] testing 52.2% loss=0.35459, acc=0.87500
# [24/100] testing 53.1% loss=0.14538, acc=0.95312
# [24/100] testing 53.5% loss=0.27674, acc=0.89062
# [24/100] testing 54.4% loss=0.36644, acc=0.82812
# [24/100] testing 54.8% loss=0.47865, acc=0.84375
# [24/100] testing 55.7% loss=0.16724, acc=0.95312
# [24/100] testing 56.6% loss=0.44084, acc=0.82812
# [24/100] testing 57.0% loss=0.54583, acc=0.82812
# [24/100] testing 57.9% loss=0.32562, acc=0.85938
# [24/100] testing 58.3% loss=0.61024, acc=0.78125
# [24/100] testing 59.2% loss=0.33030, acc=0.87500
# [24/100] testing 59.7% loss=0.28892, acc=0.89062
# [24/100] testing 60.5% loss=0.45310, acc=0.79688
# [24/100] testing 61.4% loss=0.15055, acc=0.92188
# [24/100] testing 61.9% loss=0.22950, acc=0.89062
# [24/100] testing 62.7% loss=0.23497, acc=0.89062
# [24/100] testing 63.2% loss=0.43724, acc=0.82812
# [24/100] testing 64.0% loss=0.38271, acc=0.81250
# [24/100] testing 64.5% loss=0.15686, acc=0.93750
# [24/100] testing 65.4% loss=0.20689, acc=0.90625
# [24/100] testing 65.8% loss=0.36499, acc=0.84375
# [24/100] testing 66.7% loss=0.25117, acc=0.90625
# [24/100] testing 67.6% loss=0.29832, acc=0.89062
# [24/100] testing 68.0% loss=0.23607, acc=0.92188
# [24/100] testing 68.9% loss=0.28307, acc=0.87500
# [24/100] testing 69.3% loss=0.34123, acc=0.85938
# [24/100] testing 70.2% loss=0.46298, acc=0.84375
# [24/100] testing 70.6% loss=0.50983, acc=0.81250
# [24/100] testing 71.5% loss=0.51848, acc=0.84375
# [24/100] testing 72.4% loss=0.13238, acc=0.93750
# [24/100] testing 72.8% loss=0.15738, acc=0.95312
# [24/100] testing 73.7% loss=0.18340, acc=0.93750
# [24/100] testing 74.1% loss=0.49231, acc=0.85938
# [24/100] testing 75.0% loss=0.23180, acc=0.90625
# [24/100] testing 75.4% loss=0.36000, acc=0.92188
# [24/100] testing 76.3% loss=0.11239, acc=0.96875
# [24/100] testing 76.8% loss=0.20811, acc=0.92188
# [24/100] testing 77.6% loss=0.25216, acc=0.93750
# [24/100] testing 78.5% loss=0.64435, acc=0.75000
# [24/100] testing 79.0% loss=0.29455, acc=0.89062
# [24/100] testing 79.8% loss=0.22051, acc=0.90625
# [24/100] testing 80.3% loss=0.37001, acc=0.87500
# [24/100] testing 81.2% loss=0.42240, acc=0.87500
# [24/100] testing 81.6% loss=0.31058, acc=0.92188
# [24/100] testing 82.5% loss=0.25001, acc=0.89062
# [24/100] testing 83.3% loss=0.20051, acc=0.95312
# [24/100] testing 83.8% loss=0.14727, acc=0.95312
# [24/100] testing 84.7% loss=0.37596, acc=0.85938
# [24/100] testing 85.1% loss=0.36143, acc=0.85938
# [24/100] testing 86.0% loss=0.41225, acc=0.87500
# [24/100] testing 86.4% loss=0.46762, acc=0.84375
# [24/100] testing 87.3% loss=0.33093, acc=0.82812
# [24/100] testing 87.7% loss=0.34714, acc=0.85938
# [24/100] testing 88.6% loss=0.27930, acc=0.90625
# [24/100] testing 89.5% loss=0.60840, acc=0.78125
# [24/100] testing 89.9% loss=0.22970, acc=0.92188
# [24/100] testing 90.8% loss=0.34028, acc=0.89062
# [24/100] testing 91.2% loss=0.13565, acc=0.90625
# [24/100] testing 92.1% loss=0.50382, acc=0.84375
# [24/100] testing 92.6% loss=0.28085, acc=0.85938
# [24/100] testing 93.4% loss=0.33592, acc=0.85938
# [24/100] testing 94.3% loss=0.19313, acc=0.92188
# [24/100] testing 94.7% loss=0.26506, acc=0.89062
# [24/100] testing 95.6% loss=0.42010, acc=0.84375
# [24/100] testing 96.1% loss=0.24119, acc=0.79688
# [24/100] testing 96.9% loss=0.26608, acc=0.89062
# [24/100] testing 97.4% loss=0.16512, acc=0.92188
# [24/100] testing 98.3% loss=0.35249, acc=0.84375
# [24/100] testing 98.7% loss=0.22490, acc=0.93750
# [24/100] testing 99.6% loss=0.36698, acc=0.84375
# [25/100] training 0.2% loss=0.40114, acc=0.84375
# [25/100] training 0.4% loss=0.46090, acc=0.84375
# [25/100] training 0.5% loss=0.14610, acc=0.92188
# [25/100] training 0.8% loss=0.25706, acc=0.90625
# [25/100] training 0.9% loss=0.35132, acc=0.84375
# [25/100] training 1.1% loss=0.20010, acc=0.95312
# [25/100] training 1.2% loss=0.29478, acc=0.87500
# [25/100] training 1.4% loss=0.24469, acc=0.92188
# [25/100] training 1.6% loss=0.18854, acc=0.92188
# [25/100] training 1.8% loss=0.25031, acc=0.92188
# [25/100] training 2.0% loss=0.36178, acc=0.84375
# [25/100] training 2.1% loss=0.28957, acc=0.82812
# [25/100] training 2.3% loss=0.18692, acc=0.92188
# [25/100] training 2.4% loss=0.25976, acc=0.87500
# [25/100] training 2.6% loss=0.17280, acc=0.96875
# [25/100] training 2.7% loss=0.31766, acc=0.89062
# [25/100] training 3.0% loss=0.25974, acc=0.84375
# [25/100] training 3.2% loss=0.24578, acc=0.92188
# [25/100] training 3.3% loss=0.38697, acc=0.90625
# [25/100] training 3.5% loss=0.27799, acc=0.85938
# [25/100] training 3.6% loss=0.29862, acc=0.84375
# [25/100] training 3.8% loss=0.21957, acc=0.87500
# [25/100] training 3.9% loss=0.24863, acc=0.85938
# [25/100] training 4.2% loss=0.13205, acc=0.93750
# [25/100] training 4.4% loss=0.19748, acc=0.92188
# [25/100] training 4.5% loss=0.15204, acc=0.93750
# [25/100] training 4.7% loss=0.28695, acc=0.85938
# [25/100] training 4.8% loss=0.35390, acc=0.87500
# [25/100] training 5.0% loss=0.34105, acc=0.89062
# [25/100] training 5.2% loss=0.42114, acc=0.85938
# [25/100] training 5.4% loss=0.25278, acc=0.87500
# [25/100] training 5.5% loss=0.28615, acc=0.87500
# [25/100] training 5.7% loss=0.20535, acc=0.92188
# [25/100] training 5.9% loss=0.35432, acc=0.81250
# [25/100] training 6.0% loss=0.31864, acc=0.82812
# [25/100] training 6.3% loss=0.24004, acc=0.87500
# [25/100] training 6.4% loss=0.19775, acc=0.95312
# [25/100] training 6.6% loss=0.21477, acc=0.92188
# [25/100] training 6.7% loss=0.32466, acc=0.84375
# [25/100] training 6.9% loss=0.20104, acc=0.92188
# [25/100] training 7.1% loss=0.20494, acc=0.90625
# [25/100] training 7.2% loss=0.29429, acc=0.87500
# [25/100] training 7.5% loss=0.22759, acc=0.89062
# [25/100] training 7.6% loss=0.26935, acc=0.89062
# [25/100] training 7.8% loss=0.25548, acc=0.87500
# [25/100] training 7.9% loss=0.31869, acc=0.87500
# [25/100] training 8.1% loss=0.16000, acc=0.92188
# [25/100] training 8.2% loss=0.20839, acc=0.93750
# [25/100] training 8.4% loss=0.29573, acc=0.90625
# [25/100] training 8.7% loss=0.21238, acc=0.89062
# [25/100] training 8.8% loss=0.21918, acc=0.90625
# [25/100] training 9.0% loss=0.28501, acc=0.89062
# [25/100] training 9.1% loss=0.23142, acc=0.93750
# [25/100] training 9.3% loss=0.29277, acc=0.87500
# [25/100] training 9.4% loss=0.19814, acc=0.93750
# [25/100] training 9.7% loss=0.21876, acc=0.90625
# [25/100] training 9.9% loss=0.28979, acc=0.90625
# [25/100] training 10.0% loss=0.39468, acc=0.87500
# [25/100] training 10.2% loss=0.22649, acc=0.90625
# [25/100] training 10.3% loss=0.15665, acc=0.92188
# [25/100] training 10.5% loss=0.32967, acc=0.90625
# [25/100] training 10.6% loss=0.32472, acc=0.85938
# [25/100] training 10.9% loss=0.13129, acc=0.95312
# [25/100] training 11.0% loss=0.31253, acc=0.89062
# [25/100] training 11.2% loss=0.14725, acc=0.95312
# [25/100] training 11.4% loss=0.27600, acc=0.90625
# [25/100] training 11.5% loss=0.39005, acc=0.81250
# [25/100] training 11.7% loss=0.14987, acc=0.96875
# [25/100] training 11.8% loss=0.25520, acc=0.92188
# [25/100] training 12.1% loss=0.18162, acc=0.92188
# [25/100] training 12.2% loss=0.25531, acc=0.85938
# [25/100] training 12.4% loss=0.27745, acc=0.87500
# [25/100] training 12.6% loss=0.22602, acc=0.92188
# [25/100] training 12.7% loss=0.26062, acc=0.92188
# [25/100] training 12.9% loss=0.26804, acc=0.90625
# [25/100] training 13.0% loss=0.19776, acc=0.95312
# [25/100] training 13.3% loss=0.20905, acc=0.89062
# [25/100] training 13.4% loss=0.19269, acc=0.90625
# [25/100] training 13.6% loss=0.26942, acc=0.90625
# [25/100] training 13.7% loss=0.35481, acc=0.89062
# [25/100] training 13.9% loss=0.40775, acc=0.89062
# [25/100] training 14.1% loss=0.22552, acc=0.95312
# [25/100] training 14.3% loss=0.21371, acc=0.89062
# [25/100] training 14.5% loss=0.19561, acc=0.92188
# [25/100] training 14.6% loss=0.22606, acc=0.92188
# [25/100] training 14.8% loss=0.33093, acc=0.87500
# [25/100] training 14.9% loss=0.23452, acc=0.90625
# [25/100] training 15.1% loss=0.24525, acc=0.89062
# [25/100] training 15.4% loss=0.27245, acc=0.85938
# [25/100] training 15.5% loss=0.20923, acc=0.90625
# [25/100] training 15.7% loss=0.32302, acc=0.84375
# [25/100] training 15.8% loss=0.18158, acc=0.92188
# [25/100] training 16.0% loss=0.41054, acc=0.87500
# [25/100] training 16.1% loss=0.47339, acc=0.81250
# [25/100] training 16.3% loss=0.23544, acc=0.92188
# [25/100] training 16.4% loss=0.25452, acc=0.90625
# [25/100] training 16.7% loss=0.30365, acc=0.82812
# [25/100] training 16.9% loss=0.35348, acc=0.87500
# [25/100] training 17.0% loss=0.21134, acc=0.93750
# [25/100] training 17.2% loss=0.25077, acc=0.92188
# [25/100] training 17.3% loss=0.25740, acc=0.92188
# [25/100] training 17.5% loss=0.24653, acc=0.89062
# [25/100] training 17.7% loss=0.19274, acc=0.89062
# [25/100] training 17.9% loss=0.29236, acc=0.89062
# [25/100] training 18.1% loss=0.31269, acc=0.85938
# [25/100] training 18.2% loss=0.29054, acc=0.84375
# [25/100] training 18.4% loss=0.29650, acc=0.87500
# [25/100] training 18.5% loss=0.21818, acc=0.93750
# [25/100] training 18.8% loss=0.19024, acc=0.92188
# [25/100] training 18.9% loss=0.12634, acc=0.96875
# [25/100] training 19.1% loss=0.25488, acc=0.92188
# [25/100] training 19.2% loss=0.22345, acc=0.89062
# [25/100] training 19.4% loss=0.19241, acc=0.93750
# [25/100] training 19.6% loss=0.30292, acc=0.90625
# [25/100] training 19.7% loss=0.24014, acc=0.90625
# [25/100] training 20.0% loss=0.14295, acc=0.95312
# [25/100] training 20.1% loss=0.29590, acc=0.84375
# [25/100] training 20.3% loss=0.27650, acc=0.93750
# [25/100] training 20.4% loss=0.25596, acc=0.87500
# [25/100] training 20.6% loss=0.35072, acc=0.87500
# [25/100] training 20.8% loss=0.22394, acc=0.90625
# [25/100] training 20.9% loss=0.21673, acc=0.89062
# [25/100] training 21.2% loss=0.21388, acc=0.92188
# [25/100] training 21.3% loss=0.24054, acc=0.95312
# [25/100] training 21.5% loss=0.33998, acc=0.85938
# [25/100] training 21.6% loss=0.16138, acc=0.93750
# [25/100] training 21.8% loss=0.25632, acc=0.89062
# [25/100] training 21.9% loss=0.26088, acc=0.87500
# [25/100] training 22.2% loss=0.24377, acc=0.90625
# [25/100] training 22.4% loss=0.28555, acc=0.89062
# [25/100] training 22.5% loss=0.14804, acc=0.98438
# [25/100] training 22.7% loss=0.26634, acc=0.90625
# [25/100] training 22.8% loss=0.36528, acc=0.85938
# [25/100] training 23.0% loss=0.18816, acc=0.89062
# [25/100] training 23.1% loss=0.28411, acc=0.87500
# [25/100] training 23.4% loss=0.28814, acc=0.82812
# [25/100] training 23.6% loss=0.35590, acc=0.87500
# [25/100] training 23.7% loss=0.28172, acc=0.90625
# [25/100] training 23.9% loss=0.20871, acc=0.90625
# [25/100] training 24.0% loss=0.25097, acc=0.89062
# [25/100] training 24.2% loss=0.27223, acc=0.89062
# [25/100] training 24.3% loss=0.34453, acc=0.87500
# [25/100] training 24.6% loss=0.27305, acc=0.89062
# [25/100] training 24.7% loss=0.34186, acc=0.90625
# [25/100] training 24.9% loss=0.18741, acc=0.93750
# [25/100] training 25.1% loss=0.32383, acc=0.84375
# [25/100] training 25.2% loss=0.16245, acc=0.98438
# [25/100] training 25.4% loss=0.24896, acc=0.89062
# [25/100] training 25.6% loss=0.22626, acc=0.93750
# [25/100] training 25.8% loss=0.30624, acc=0.89062
# [25/100] training 25.9% loss=0.19833, acc=0.89062
# [25/100] training 26.1% loss=0.18332, acc=0.93750
# [25/100] training 26.3% loss=0.26804, acc=0.89062
# [25/100] training 26.4% loss=0.19133, acc=0.92188
# [25/100] training 26.6% loss=0.15947, acc=0.93750
# [25/100] training 26.8% loss=0.33484, acc=0.90625
# [25/100] training 27.0% loss=0.26902, acc=0.92188
# [25/100] training 27.1% loss=0.19275, acc=0.89062
# [25/100] training 27.3% loss=0.20579, acc=0.90625
# [25/100] training 27.4% loss=0.14214, acc=0.96875
# [25/100] training 27.6% loss=0.37507, acc=0.90625
# [25/100] training 27.9% loss=0.19907, acc=0.89062
# [25/100] training 28.0% loss=0.23419, acc=0.92188
# [25/100] training 28.2% loss=0.13795, acc=0.93750
# [25/100] training 28.3% loss=0.14382, acc=0.95312
# [25/100] training 28.5% loss=0.27105, acc=0.90625
# [25/100] training 28.6% loss=0.20814, acc=0.92188
# [25/100] training 28.8% loss=0.16647, acc=0.93750
# [25/100] training 29.1% loss=0.19199, acc=0.95312
# [25/100] training 29.2% loss=0.19580, acc=0.95312
# [25/100] training 29.4% loss=0.30071, acc=0.85938
# [25/100] training 29.5% loss=0.14502, acc=0.92188
# [25/100] training 29.7% loss=0.43640, acc=0.82812
# [25/100] training 29.8% loss=0.29270, acc=0.89062
# [25/100] training 30.0% loss=0.43707, acc=0.84375
# [25/100] training 30.2% loss=0.17531, acc=0.92188
# [25/100] training 30.4% loss=0.18066, acc=0.93750
# [25/100] training 30.6% loss=0.29804, acc=0.89062
# [25/100] training 30.7% loss=0.27269, acc=0.85938
# [25/100] training 30.9% loss=0.27375, acc=0.90625
# [25/100] training 31.0% loss=0.20308, acc=0.93750
# [25/100] training 31.3% loss=0.23744, acc=0.85938
# [25/100] training 31.4% loss=0.32130, acc=0.81250
# [25/100] training 31.6% loss=0.35842, acc=0.87500
# [25/100] training 31.8% loss=0.17310, acc=0.93750
# [25/100] training 31.9% loss=0.30338, acc=0.87500
# [25/100] training 32.1% loss=0.30519, acc=0.90625
# [25/100] training 32.2% loss=0.31403, acc=0.89062
# [25/100] training 32.5% loss=0.19666, acc=0.87500
# [25/100] training 32.6% loss=0.18469, acc=0.95312
# [25/100] training 32.8% loss=0.26184, acc=0.92188
# [25/100] training 32.9% loss=0.22401, acc=0.90625
# [25/100] training 33.1% loss=0.18805, acc=0.90625
# [25/100] training 33.3% loss=0.21826, acc=0.90625
# [25/100] training 33.4% loss=0.17281, acc=0.93750
# [25/100] training 33.7% loss=0.23134, acc=0.92188
# [25/100] training 33.8% loss=0.20607, acc=0.87500
# [25/100] training 34.0% loss=0.21470, acc=0.89062
# [25/100] training 34.1% loss=0.20607, acc=0.89062
# [25/100] training 34.3% loss=0.18566, acc=0.92188
# [25/100] training 34.5% loss=0.29379, acc=0.84375
# [25/100] training 34.7% loss=0.14391, acc=0.95312
# [25/100] training 34.9% loss=0.17689, acc=0.93750
# [25/100] training 35.0% loss=0.19789, acc=0.92188
# [25/100] training 35.2% loss=0.30943, acc=0.84375
# [25/100] training 35.3% loss=0.22928, acc=0.92188
# [25/100] training 35.5% loss=0.20587, acc=0.92188
# [25/100] training 35.6% loss=0.21589, acc=0.90625
# [25/100] training 35.9% loss=0.21752, acc=0.90625
# [25/100] training 36.1% loss=0.37916, acc=0.81250
# [25/100] training 36.2% loss=0.30776, acc=0.89062
# [25/100] training 36.4% loss=0.31100, acc=0.87500
# [25/100] training 36.5% loss=0.25088, acc=0.93750
# [25/100] training 36.7% loss=0.31184, acc=0.90625
# [25/100] training 36.8% loss=0.13625, acc=0.93750
# [25/100] training 37.1% loss=0.31715, acc=0.87500
# [25/100] training 37.3% loss=0.25762, acc=0.92188
# [25/100] training 37.4% loss=0.36728, acc=0.81250
# [25/100] training 37.6% loss=0.21367, acc=0.89062
# [25/100] training 37.7% loss=0.28302, acc=0.90625
# [25/100] training 37.9% loss=0.23867, acc=0.87500
# [25/100] training 38.1% loss=0.25570, acc=0.90625
# [25/100] training 38.3% loss=0.30132, acc=0.87500
# [25/100] training 38.4% loss=0.13046, acc=0.95312
# [25/100] training 38.6% loss=0.23940, acc=0.92188
# [25/100] training 38.8% loss=0.34746, acc=0.82812
# [25/100] training 38.9% loss=0.26012, acc=0.90625
# [25/100] training 39.1% loss=0.24679, acc=0.92188
# [25/100] training 39.3% loss=0.27809, acc=0.85938
# [25/100] training 39.5% loss=0.24632, acc=0.90625
# [25/100] training 39.6% loss=0.24551, acc=0.92188
# [25/100] training 39.8% loss=0.18594, acc=0.90625
# [25/100] training 40.0% loss=0.30323, acc=0.89062
# [25/100] training 40.1% loss=0.27907, acc=0.85938
# [25/100] training 40.4% loss=0.15915, acc=0.95312
# [25/100] training 40.5% loss=0.26244, acc=0.87500
# [25/100] training 40.7% loss=0.18465, acc=0.90625
# [25/100] training 40.8% loss=0.17596, acc=0.92188
# [25/100] training 41.0% loss=0.19581, acc=0.93750
# [25/100] training 41.1% loss=0.42559, acc=0.84375
# [25/100] training 41.3% loss=0.27936, acc=0.84375
# [25/100] training 41.6% loss=0.32773, acc=0.84375
# [25/100] training 41.7% loss=0.38313, acc=0.82812
# [25/100] training 41.9% loss=0.19244, acc=0.93750
# [25/100] training 42.0% loss=0.26382, acc=0.87500
# [25/100] training 42.2% loss=0.30106, acc=0.85938
# [25/100] training 42.3% loss=0.20988, acc=0.93750
# [25/100] training 42.5% loss=0.21648, acc=0.95312
# [25/100] training 42.8% loss=0.23536, acc=0.90625
# [25/100] training 42.9% loss=0.16919, acc=0.90625
# [25/100] training 43.1% loss=0.19593, acc=0.93750
# [25/100] training 43.2% loss=0.15318, acc=0.90625
# [25/100] training 43.4% loss=0.26237, acc=0.92188
# [25/100] training 43.5% loss=0.20309, acc=0.90625
# [25/100] training 43.8% loss=0.33161, acc=0.85938
# [25/100] training 43.9% loss=0.22461, acc=0.93750
# [25/100] training 44.1% loss=0.23571, acc=0.92188
# [25/100] training 44.3% loss=0.21482, acc=0.89062
# [25/100] training 44.4% loss=0.31391, acc=0.82812
# [25/100] training 44.6% loss=0.25811, acc=0.89062
# [25/100] training 44.7% loss=0.37704, acc=0.87500
# [25/100] training 45.0% loss=0.20325, acc=0.92188
# [25/100] training 45.1% loss=0.40787, acc=0.87500
# [25/100] training 45.3% loss=0.23646, acc=0.85938
# [25/100] training 45.5% loss=0.09851, acc=0.98438
# [25/100] training 45.6% loss=0.23921, acc=0.89062
# [25/100] training 45.8% loss=0.12713, acc=0.92188
# [25/100] training 45.9% loss=0.20812, acc=0.92188
# [25/100] training 46.2% loss=0.18926, acc=0.93750
# [25/100] training 46.3% loss=0.12481, acc=0.93750
# [25/100] training 46.5% loss=0.32823, acc=0.85938
# [25/100] training 46.6% loss=0.25023, acc=0.93750
# [25/100] training 46.8% loss=0.18579, acc=0.93750
# [25/100] training 47.0% loss=0.20370, acc=0.90625
# [25/100] training 47.2% loss=0.25016, acc=0.89062
# [25/100] training 47.4% loss=0.18344, acc=0.95312
# [25/100] training 47.5% loss=0.23252, acc=0.92188
# [25/100] training 47.7% loss=0.21626, acc=0.90625
# [25/100] training 47.8% loss=0.28596, acc=0.89062
# [25/100] training 48.0% loss=0.33098, acc=0.87500
# [25/100] training 48.3% loss=0.15040, acc=0.93750
# [25/100] training 48.4% loss=0.15577, acc=0.93750
# [25/100] training 48.6% loss=0.22908, acc=0.90625
# [25/100] training 48.7% loss=0.24050, acc=0.84375
# [25/100] training 48.9% loss=0.25602, acc=0.93750
# [25/100] training 49.0% loss=0.16082, acc=0.93750
# [25/100] training 49.2% loss=0.35621, acc=0.81250
# [25/100] training 49.3% loss=0.22851, acc=0.90625
# [25/100] training 49.6% loss=0.26234, acc=0.87500
# [25/100] training 49.8% loss=0.28425, acc=0.84375
# [25/100] training 49.9% loss=0.24737, acc=0.87500
# [25/100] training 50.1% loss=0.19745, acc=0.92188
# [25/100] training 50.2% loss=0.24167, acc=0.90625
# [25/100] training 50.4% loss=0.38143, acc=0.85938
# [25/100] training 50.6% loss=0.33234, acc=0.85938
# [25/100] training 50.8% loss=0.33547, acc=0.84375
# [25/100] training 51.0% loss=0.33642, acc=0.85938
# [25/100] training 51.1% loss=0.25256, acc=0.87500
# [25/100] training 51.3% loss=0.31238, acc=0.85938
# [25/100] training 51.4% loss=0.20584, acc=0.93750
# [25/100] training 51.7% loss=0.30577, acc=0.84375
# [25/100] training 51.8% loss=0.27241, acc=0.89062
# [25/100] training 52.0% loss=0.17360, acc=0.93750
# [25/100] training 52.1% loss=0.30696, acc=0.87500
# [25/100] training 52.3% loss=0.30369, acc=0.85938
# [25/100] training 52.5% loss=0.12289, acc=0.95312
# [25/100] training 52.6% loss=0.26283, acc=0.85938
# [25/100] training 52.9% loss=0.33436, acc=0.90625
# [25/100] training 53.0% loss=0.13266, acc=0.95312
# [25/100] training 53.2% loss=0.21805, acc=0.93750
# [25/100] training 53.3% loss=0.16729, acc=0.93750
# [25/100] training 53.5% loss=0.21611, acc=0.93750
# [25/100] training 53.7% loss=0.18874, acc=0.92188
# [25/100] training 53.8% loss=0.23210, acc=0.87500
# [25/100] training 54.1% loss=0.37979, acc=0.87500
# [25/100] training 54.2% loss=0.18547, acc=0.92188
# [25/100] training 54.4% loss=0.20387, acc=0.92188
# [25/100] training 54.5% loss=0.26432, acc=0.89062
# [25/100] training 54.7% loss=0.36172, acc=0.84375
# [25/100] training 54.8% loss=0.18558, acc=0.92188
# [25/100] training 55.1% loss=0.22598, acc=0.90625
# [25/100] training 55.3% loss=0.24529, acc=0.89062
# [25/100] training 55.4% loss=0.28359, acc=0.89062
# [25/100] training 55.6% loss=0.20105, acc=0.92188
# [25/100] training 55.7% loss=0.18139, acc=0.90625
# [25/100] training 55.9% loss=0.21508, acc=0.85938
# [25/100] training 56.0% loss=0.17606, acc=0.93750
# [25/100] training 56.3% loss=0.51433, acc=0.78125
# [25/100] training 56.5% loss=0.20252, acc=0.93750
# [25/100] training 56.6% loss=0.26460, acc=0.92188
# [25/100] training 56.8% loss=0.29742, acc=0.84375
# [25/100] training 56.9% loss=0.27980, acc=0.85938
# [25/100] training 57.1% loss=0.31843, acc=0.87500
# [25/100] training 57.2% loss=0.19017, acc=0.93750
# [25/100] training 57.5% loss=0.15081, acc=0.92188
# [25/100] training 57.6% loss=0.25166, acc=0.87500
# [25/100] training 57.8% loss=0.16303, acc=0.93750
# [25/100] training 58.0% loss=0.15804, acc=0.95312
# [25/100] training 58.1% loss=0.17598, acc=0.93750
# [25/100] training 58.3% loss=0.15197, acc=0.95312
# [25/100] training 58.4% loss=0.28171, acc=0.92188
# [25/100] training 58.7% loss=0.26811, acc=0.92188
# [25/100] training 58.8% loss=0.27753, acc=0.89062
# [25/100] training 59.0% loss=0.22120, acc=0.90625
# [25/100] training 59.2% loss=0.20598, acc=0.89062
# [25/100] training 59.3% loss=0.21986, acc=0.93750
# [25/100] training 59.5% loss=0.24259, acc=0.92188
# [25/100] training 59.7% loss=0.32573, acc=0.84375
# [25/100] training 59.9% loss=0.19617, acc=0.93750
# [25/100] training 60.0% loss=0.27825, acc=0.85938
# [25/100] training 60.2% loss=0.27490, acc=0.89062
# [25/100] training 60.3% loss=0.19581, acc=0.93750
# [25/100] training 60.5% loss=0.27689, acc=0.89062
# [25/100] training 60.8% loss=0.22888, acc=0.90625
# [25/100] training 60.9% loss=0.33031, acc=0.89062
# [25/100] training 61.1% loss=0.29728, acc=0.87500
# [25/100] training 61.2% loss=0.17748, acc=0.93750
# [25/100] training 61.4% loss=0.27954, acc=0.87500
# [25/100] training 61.5% loss=0.37193, acc=0.84375
# [25/100] training 61.7% loss=0.26817, acc=0.90625
# [25/100] training 62.0% loss=0.30917, acc=0.84375
# [25/100] training 62.1% loss=0.43296, acc=0.82812
# [25/100] training 62.3% loss=0.15975, acc=0.93750
# [25/100] training 62.4% loss=0.17878, acc=0.92188
# [25/100] training 62.6% loss=0.35628, acc=0.81250
# [25/100] training 62.7% loss=0.18472, acc=0.89062
# [25/100] training 62.9% loss=0.24670, acc=0.90625
# [25/100] training 63.1% loss=0.17171, acc=0.95312
# [25/100] training 63.3% loss=0.22393, acc=0.90625
# [25/100] training 63.5% loss=0.26081, acc=0.92188
# [25/100] training 63.6% loss=0.28281, acc=0.92188
# [25/100] training 63.8% loss=0.45292, acc=0.84375
# [25/100] training 63.9% loss=0.25704, acc=0.89062
# [25/100] training 64.2% loss=0.13259, acc=0.92188
# [25/100] training 64.3% loss=0.28229, acc=0.85938
# [25/100] training 64.5% loss=0.32020, acc=0.84375
# [25/100] training 64.7% loss=0.25005, acc=0.84375
# [25/100] training 64.8% loss=0.38586, acc=0.84375
# [25/100] training 65.0% loss=0.28972, acc=0.87500
# [25/100] training 65.1% loss=0.27950, acc=0.87500
# [25/100] training 65.4% loss=0.19796, acc=0.93750
# [25/100] training 65.5% loss=0.24860, acc=0.90625
# [25/100] training 65.7% loss=0.11468, acc=0.95312
# [25/100] training 65.8% loss=0.22250, acc=0.89062
# [25/100] training 66.0% loss=0.37095, acc=0.89062
# [25/100] training 66.2% loss=0.14386, acc=0.96875
# [25/100] training 66.3% loss=0.27468, acc=0.89062
# [25/100] training 66.6% loss=0.25396, acc=0.90625
# [25/100] training 66.7% loss=0.15388, acc=0.93750
# [25/100] training 66.9% loss=0.20905, acc=0.92188
# [25/100] training 67.0% loss=0.25945, acc=0.87500
# [25/100] training 67.2% loss=0.21896, acc=0.93750
# [25/100] training 67.4% loss=0.12450, acc=0.96875
# [25/100] training 67.6% loss=0.21009, acc=0.92188
# [25/100] training 67.8% loss=0.20756, acc=0.92188
# [25/100] training 67.9% loss=0.13786, acc=0.95312
# [25/100] training 68.1% loss=0.13381, acc=0.95312
# [25/100] training 68.2% loss=0.17458, acc=0.89062
# [25/100] training 68.4% loss=0.18464, acc=0.95312
# [25/100] training 68.5% loss=0.18285, acc=0.93750
# [25/100] training 68.8% loss=0.28021, acc=0.89062
# [25/100] training 69.0% loss=0.27199, acc=0.90625
# [25/100] training 69.1% loss=0.13197, acc=0.93750
# [25/100] training 69.3% loss=0.27561, acc=0.89062
# [25/100] training 69.4% loss=0.13696, acc=0.92188
# [25/100] training 69.6% loss=0.31833, acc=0.85938
# [25/100] training 69.7% loss=0.28384, acc=0.87500
# [25/100] training 70.0% loss=0.30637, acc=0.85938
# [25/100] training 70.2% loss=0.25629, acc=0.89062
# [25/100] training 70.3% loss=0.25542, acc=0.89062
# [25/100] training 70.5% loss=0.28152, acc=0.85938
# [25/100] training 70.6% loss=0.20358, acc=0.90625
# [25/100] training 70.8% loss=0.22678, acc=0.89062
# [25/100] training 71.0% loss=0.30161, acc=0.84375
# [25/100] training 71.2% loss=0.15062, acc=0.92188
# [25/100] training 71.3% loss=0.18559, acc=0.93750
# [25/100] training 71.5% loss=0.25209, acc=0.93750
# [25/100] training 71.7% loss=0.30833, acc=0.84375
# [25/100] training 71.8% loss=0.33926, acc=0.89062
# [25/100] training 72.0% loss=0.18215, acc=0.92188
# [25/100] training 72.2% loss=0.23797, acc=0.89062
# [25/100] training 72.4% loss=0.30846, acc=0.89062
# [25/100] training 72.5% loss=0.32607, acc=0.87500
# [25/100] training 72.7% loss=0.35027, acc=0.82812
# [25/100] training 72.9% loss=0.19274, acc=0.90625
# [25/100] training 73.0% loss=0.18100, acc=0.92188
# [25/100] training 73.3% loss=0.23807, acc=0.89062
# [25/100] training 73.4% loss=0.15364, acc=0.92188
# [25/100] training 73.6% loss=0.12592, acc=0.93750
# [25/100] training 73.7% loss=0.23766, acc=0.92188
# [25/100] training 73.9% loss=0.19801, acc=0.89062
# [25/100] training 74.0% loss=0.27392, acc=0.87500
# [25/100] training 74.2% loss=0.19448, acc=0.93750
# [25/100] training 74.5% loss=0.17093, acc=0.92188
# [25/100] training 74.6% loss=0.40178, acc=0.79688
# [25/100] training 74.8% loss=0.49771, acc=0.82812
# [25/100] training 74.9% loss=0.37219, acc=0.79688
# [25/100] training 75.1% loss=0.23010, acc=0.89062
# [25/100] training 75.2% loss=0.19009, acc=0.95312
# [25/100] training 75.4% loss=0.26324, acc=0.90625
# [25/100] training 75.7% loss=0.27264, acc=0.89062
# [25/100] training 75.8% loss=0.31603, acc=0.89062
# [25/100] training 76.0% loss=0.15371, acc=0.92188
# [25/100] training 76.1% loss=0.32219, acc=0.87500
# [25/100] training 76.3% loss=0.24938, acc=0.90625
# [25/100] training 76.4% loss=0.22076, acc=0.93750
# [25/100] training 76.7% loss=0.19846, acc=0.93750
# [25/100] training 76.8% loss=0.16290, acc=0.96875
# [25/100] training 77.0% loss=0.10596, acc=0.96875
# [25/100] training 77.2% loss=0.22792, acc=0.93750
# [25/100] training 77.3% loss=0.12824, acc=0.95312
# [25/100] training 77.5% loss=0.18403, acc=0.95312
# [25/100] training 77.6% loss=0.25756, acc=0.90625
# [25/100] training 77.9% loss=0.21329, acc=0.90625
# [25/100] training 78.0% loss=0.20779, acc=0.92188
# [25/100] training 78.2% loss=0.27233, acc=0.87500
# [25/100] training 78.4% loss=0.08693, acc=0.96875
# [25/100] training 78.5% loss=0.33354, acc=0.90625
# [25/100] training 78.7% loss=0.26435, acc=0.85938
# [25/100] training 78.8% loss=0.11867, acc=0.96875
# [25/100] training 79.1% loss=0.15426, acc=0.90625
# [25/100] training 79.2% loss=0.15987, acc=0.96875
# [25/100] training 79.4% loss=0.25139, acc=0.87500
# [25/100] training 79.5% loss=0.22516, acc=0.90625
# [25/100] training 79.7% loss=0.08899, acc=0.96875
# [25/100] training 79.9% loss=0.18514, acc=0.90625
# [25/100] training 80.1% loss=0.15892, acc=0.93750
# [25/100] training 80.3% loss=0.28986, acc=0.87500
# [25/100] training 80.4% loss=0.18492, acc=0.95312
# [25/100] training 80.6% loss=0.19863, acc=0.90625
# [25/100] training 80.7% loss=0.14366, acc=0.98438
# [25/100] training 80.9% loss=0.29002, acc=0.87500
# [25/100] training 81.2% loss=0.30927, acc=0.85938
# [25/100] training 81.3% loss=0.33841, acc=0.82812
# [25/100] training 81.5% loss=0.24639, acc=0.89062
# [25/100] training 81.6% loss=0.29743, acc=0.85938
# [25/100] training 81.8% loss=0.27504, acc=0.89062
# [25/100] training 81.9% loss=0.28713, acc=0.92188
# [25/100] training 82.1% loss=0.15935, acc=0.96875
# [25/100] training 82.2% loss=0.23833, acc=0.89062
# [25/100] training 82.5% loss=0.13536, acc=0.95312
# [25/100] training 82.7% loss=0.21638, acc=0.93750
# [25/100] training 82.8% loss=0.21168, acc=0.93750
# [25/100] training 83.0% loss=0.23173, acc=0.90625
# [25/100] training 83.1% loss=0.26697, acc=0.89062
# [25/100] training 83.3% loss=0.11609, acc=0.96875
# [25/100] training 83.5% loss=0.13316, acc=0.93750
# [25/100] training 83.7% loss=0.44260, acc=0.85938
# [25/100] training 83.9% loss=0.30501, acc=0.85938
# [25/100] training 84.0% loss=0.16057, acc=0.93750
# [25/100] training 84.2% loss=0.19487, acc=0.92188
# [25/100] training 84.3% loss=0.21664, acc=0.90625
# [25/100] training 84.5% loss=0.15529, acc=0.95312
# [25/100] training 84.7% loss=0.16554, acc=0.92188
# [25/100] training 84.9% loss=0.23511, acc=0.92188
# [25/100] training 85.0% loss=0.29399, acc=0.87500
# [25/100] training 85.2% loss=0.22891, acc=0.89062
# [25/100] training 85.4% loss=0.22903, acc=0.95312
# [25/100] training 85.5% loss=0.20247, acc=0.90625
# [25/100] training 85.8% loss=0.30602, acc=0.89062
# [25/100] training 85.9% loss=0.26153, acc=0.90625
# [25/100] training 86.1% loss=0.19936, acc=0.92188
# [25/100] training 86.2% loss=0.16766, acc=0.92188
# [25/100] training 86.4% loss=0.31665, acc=0.87500
# [25/100] training 86.6% loss=0.22314, acc=0.93750
# [25/100] training 86.7% loss=0.17684, acc=0.92188
# [25/100] training 87.0% loss=0.26082, acc=0.87500
# [25/100] training 87.1% loss=0.22959, acc=0.87500
# [25/100] training 87.3% loss=0.23509, acc=0.90625
# [25/100] training 87.4% loss=0.30223, acc=0.90625
# [25/100] training 87.6% loss=0.22274, acc=0.90625
# [25/100] training 87.7% loss=0.33889, acc=0.92188
# [25/100] training 87.9% loss=0.28144, acc=0.89062
# [25/100] training 88.2% loss=0.20863, acc=0.89062
# [25/100] training 88.3% loss=0.24506, acc=0.93750
# [25/100] training 88.5% loss=0.27974, acc=0.87500
# [25/100] training 88.6% loss=0.11066, acc=0.98438
# [25/100] training 88.8% loss=0.23020, acc=0.92188
# [25/100] training 88.9% loss=0.27432, acc=0.89062
# [25/100] training 89.2% loss=0.17868, acc=0.92188
# [25/100] training 89.4% loss=0.20558, acc=0.89062
# [25/100] training 89.5% loss=0.26922, acc=0.87500
# [25/100] training 89.7% loss=0.25387, acc=0.89062
# [25/100] training 89.8% loss=0.26837, acc=0.90625
# [25/100] training 90.0% loss=0.19777, acc=0.92188
# [25/100] training 90.1% loss=0.27071, acc=0.89062
# [25/100] training 90.4% loss=0.26729, acc=0.84375
# [25/100] training 90.5% loss=0.16679, acc=0.92188
# [25/100] training 90.7% loss=0.19698, acc=0.92188
# [25/100] training 90.9% loss=0.19096, acc=0.92188
# [25/100] training 91.0% loss=0.16375, acc=0.96875
# [25/100] training 91.2% loss=0.26239, acc=0.87500
# [25/100] training 91.3% loss=0.28289, acc=0.89062
# [25/100] training 91.6% loss=0.34249, acc=0.90625
# [25/100] training 91.7% loss=0.23723, acc=0.92188
# [25/100] training 91.9% loss=0.22809, acc=0.92188
# [25/100] training 92.1% loss=0.19549, acc=0.90625
# [25/100] training 92.2% loss=0.10975, acc=0.93750
# [25/100] training 92.4% loss=0.23601, acc=0.89062
# [25/100] training 92.6% loss=0.24411, acc=0.89062
# [25/100] training 92.8% loss=0.26738, acc=0.89062
# [25/100] training 92.9% loss=0.22164, acc=0.89062
# [25/100] training 93.1% loss=0.28109, acc=0.85938
# [25/100] training 93.2% loss=0.30021, acc=0.90625
# [25/100] training 93.4% loss=0.23091, acc=0.92188
# [25/100] training 93.7% loss=0.20463, acc=0.95312
# [25/100] training 93.8% loss=0.21520, acc=0.93750
# [25/100] training 94.0% loss=0.21860, acc=0.87500
# [25/100] training 94.1% loss=0.22252, acc=0.90625
# [25/100] training 94.3% loss=0.12722, acc=0.95312
# [25/100] training 94.4% loss=0.30620, acc=0.90625
# [25/100] training 94.6% loss=0.12945, acc=0.93750
# [25/100] training 94.9% loss=0.15565, acc=0.95312
# [25/100] training 95.0% loss=0.35788, acc=0.85938
# [25/100] training 95.2% loss=0.41550, acc=0.84375
# [25/100] training 95.3% loss=0.23896, acc=0.93750
# [25/100] training 95.5% loss=0.20697, acc=0.93750
# [25/100] training 95.6% loss=0.28665, acc=0.87500
# [25/100] training 95.8% loss=0.22656, acc=0.92188
# [25/100] training 96.0% loss=0.22432, acc=0.92188
# [25/100] training 96.2% loss=0.16187, acc=0.93750
# [25/100] training 96.4% loss=0.19491, acc=0.92188
# [25/100] training 96.5% loss=0.24643, acc=0.90625
# [25/100] training 96.7% loss=0.12783, acc=0.96875
# [25/100] training 96.8% loss=0.21997, acc=0.93750
# [25/100] training 97.1% loss=0.16838, acc=0.90625
# [25/100] training 97.2% loss=0.17891, acc=0.92188
# [25/100] training 97.4% loss=0.22221, acc=0.90625
# [25/100] training 97.6% loss=0.43056, acc=0.82812
# [25/100] training 97.7% loss=0.23133, acc=0.89062
# [25/100] training 97.9% loss=0.14654, acc=0.93750
# [25/100] training 98.0% loss=0.25804, acc=0.90625
# [25/100] training 98.3% loss=0.23119, acc=0.89062
# [25/100] training 98.4% loss=0.25861, acc=0.90625
# [25/100] training 98.6% loss=0.35338, acc=0.84375
# [25/100] training 98.7% loss=0.30471, acc=0.85938
# [25/100] training 98.9% loss=0.19499, acc=0.92188
# [25/100] training 99.1% loss=0.16619, acc=0.92188
# [25/100] training 99.2% loss=0.18895, acc=0.92188
# [25/100] training 99.5% loss=0.28815, acc=0.89062
# [25/100] training 99.6% loss=0.24367, acc=0.90625
# [25/100] training 99.8% loss=0.13515, acc=0.93750
# [25/100] training 99.9% loss=0.08843, acc=0.96875
# [25/100] testing 0.9% loss=0.18590, acc=0.93750
# [25/100] testing 1.8% loss=0.40700, acc=0.84375
# [25/100] testing 2.2% loss=0.22186, acc=0.90625
# [25/100] testing 3.1% loss=0.46834, acc=0.79688
# [25/100] testing 3.5% loss=0.21195, acc=0.92188
# [25/100] testing 4.4% loss=0.22800, acc=0.89062
# [25/100] testing 4.8% loss=0.36739, acc=0.84375
# [25/100] testing 5.7% loss=0.24018, acc=0.89062
# [25/100] testing 6.6% loss=0.24385, acc=0.92188
# [25/100] testing 7.0% loss=0.29408, acc=0.87500
# [25/100] testing 7.9% loss=0.31593, acc=0.87500
# [25/100] testing 8.3% loss=0.28721, acc=0.89062
# [25/100] testing 9.2% loss=0.26299, acc=0.89062
# [25/100] testing 9.7% loss=0.14338, acc=0.92188
# [25/100] testing 10.5% loss=0.25537, acc=0.89062
# [25/100] testing 11.0% loss=0.27674, acc=0.87500
# [25/100] testing 11.8% loss=0.33482, acc=0.85938
# [25/100] testing 12.7% loss=0.35871, acc=0.85938
# [25/100] testing 13.2% loss=0.24858, acc=0.90625
# [25/100] testing 14.0% loss=0.43951, acc=0.89062
# [25/100] testing 14.5% loss=0.36164, acc=0.89062
# [25/100] testing 15.4% loss=0.26318, acc=0.95312
# [25/100] testing 15.8% loss=0.26035, acc=0.87500
# [25/100] testing 16.7% loss=0.16711, acc=0.93750
# [25/100] testing 17.5% loss=0.22980, acc=0.89062
# [25/100] testing 18.0% loss=0.29564, acc=0.89062
# [25/100] testing 18.9% loss=0.15621, acc=0.93750
# [25/100] testing 19.3% loss=0.34493, acc=0.87500
# [25/100] testing 20.2% loss=0.29054, acc=0.87500
# [25/100] testing 20.6% loss=0.37672, acc=0.84375
# [25/100] testing 21.5% loss=0.20098, acc=0.92188
# [25/100] testing 21.9% loss=0.34165, acc=0.87500
# [25/100] testing 22.8% loss=0.57188, acc=0.79688
# [25/100] testing 23.7% loss=0.32653, acc=0.85938
# [25/100] testing 24.1% loss=0.22797, acc=0.90625
# [25/100] testing 25.0% loss=0.35446, acc=0.92188
# [25/100] testing 25.4% loss=0.15282, acc=0.95312
# [25/100] testing 26.3% loss=0.24996, acc=0.89062
# [25/100] testing 26.8% loss=0.24512, acc=0.89062
# [25/100] testing 27.6% loss=0.31811, acc=0.85938
# [25/100] testing 28.5% loss=0.28639, acc=0.90625
# [25/100] testing 29.0% loss=0.22866, acc=0.95312
# [25/100] testing 29.8% loss=0.36072, acc=0.92188
# [25/100] testing 30.3% loss=0.23778, acc=0.90625
# [25/100] testing 31.1% loss=0.36039, acc=0.84375
# [25/100] testing 31.6% loss=0.25742, acc=0.87500
# [25/100] testing 32.5% loss=0.25049, acc=0.89062
# [25/100] testing 32.9% loss=0.46782, acc=0.84375
# [25/100] testing 33.8% loss=0.30121, acc=0.89062
# [25/100] testing 34.7% loss=0.36639, acc=0.89062
# [25/100] testing 35.1% loss=0.16975, acc=0.89062
# [25/100] testing 36.0% loss=0.31095, acc=0.90625
# [25/100] testing 36.4% loss=0.31493, acc=0.89062
# [25/100] testing 37.3% loss=0.41642, acc=0.89062
# [25/100] testing 37.7% loss=0.36514, acc=0.84375
# [25/100] testing 38.6% loss=0.28590, acc=0.92188
# [25/100] testing 39.5% loss=0.28237, acc=0.96875
# [25/100] testing 39.9% loss=0.37608, acc=0.85938
# [25/100] testing 40.8% loss=0.33302, acc=0.90625
# [25/100] testing 41.2% loss=0.27565, acc=0.92188
# [25/100] testing 42.1% loss=0.27930, acc=0.89062
# [25/100] testing 42.5% loss=0.25126, acc=0.90625
# [25/100] testing 43.4% loss=0.26929, acc=0.90625
# [25/100] testing 43.9% loss=0.19486, acc=0.96875
# [25/100] testing 44.7% loss=0.35341, acc=0.85938
# [25/100] testing 45.6% loss=0.24578, acc=0.93750
# [25/100] testing 46.1% loss=0.30517, acc=0.85938
# [25/100] testing 46.9% loss=0.28229, acc=0.89062
# [25/100] testing 47.4% loss=0.11379, acc=0.93750
# [25/100] testing 48.3% loss=0.32997, acc=0.87500
# [25/100] testing 48.7% loss=0.51422, acc=0.84375
# [25/100] testing 49.6% loss=0.59332, acc=0.79688
# [25/100] testing 50.4% loss=0.27266, acc=0.92188
# [25/100] testing 50.9% loss=0.28795, acc=0.92188
# [25/100] testing 51.8% loss=0.30947, acc=0.87500
# [25/100] testing 52.2% loss=0.30538, acc=0.85938
# [25/100] testing 53.1% loss=0.15505, acc=0.95312
# [25/100] testing 53.5% loss=0.34774, acc=0.89062
# [25/100] testing 54.4% loss=0.43132, acc=0.87500
# [25/100] testing 54.8% loss=0.37376, acc=0.82812
# [25/100] testing 55.7% loss=0.21312, acc=0.92188
# [25/100] testing 56.6% loss=0.33535, acc=0.89062
# [25/100] testing 57.0% loss=0.44536, acc=0.84375
# [25/100] testing 57.9% loss=0.32180, acc=0.84375
# [25/100] testing 58.3% loss=0.32242, acc=0.89062
# [25/100] testing 59.2% loss=0.33986, acc=0.84375
# [25/100] testing 59.7% loss=0.29080, acc=0.90625
# [25/100] testing 60.5% loss=0.42187, acc=0.82812
# [25/100] testing 61.4% loss=0.12488, acc=0.95312
# [25/100] testing 61.9% loss=0.32441, acc=0.87500
# [25/100] testing 62.7% loss=0.23455, acc=0.92188
# [25/100] testing 63.2% loss=0.52416, acc=0.82812
# [25/100] testing 64.0% loss=0.37840, acc=0.87500
# [25/100] testing 64.5% loss=0.23397, acc=0.87500
# [25/100] testing 65.4% loss=0.20869, acc=0.92188
# [25/100] testing 65.8% loss=0.38117, acc=0.85938
# [25/100] testing 66.7% loss=0.25596, acc=0.89062
# [25/100] testing 67.6% loss=0.31063, acc=0.92188
# [25/100] testing 68.0% loss=0.17741, acc=0.90625
# [25/100] testing 68.9% loss=0.19750, acc=0.93750
# [25/100] testing 69.3% loss=0.28489, acc=0.85938
# [25/100] testing 70.2% loss=0.47341, acc=0.82812
# [25/100] testing 70.6% loss=0.44405, acc=0.87500
# [25/100] testing 71.5% loss=0.34501, acc=0.85938
# [25/100] testing 72.4% loss=0.22362, acc=0.92188
# [25/100] testing 72.8% loss=0.25558, acc=0.89062
# [25/100] testing 73.7% loss=0.26687, acc=0.90625
# [25/100] testing 74.1% loss=0.35223, acc=0.84375
# [25/100] testing 75.0% loss=0.18297, acc=0.89062
# [25/100] testing 75.4% loss=0.48646, acc=0.79688
# [25/100] testing 76.3% loss=0.07832, acc=1.00000
# [25/100] testing 76.8% loss=0.26184, acc=0.89062
# [25/100] testing 77.6% loss=0.22640, acc=0.89062
# [25/100] testing 78.5% loss=0.47562, acc=0.81250
# [25/100] testing 79.0% loss=0.25827, acc=0.89062
# [25/100] testing 79.8% loss=0.34747, acc=0.85938
# [25/100] testing 80.3% loss=0.31877, acc=0.82812
# [25/100] testing 81.2% loss=0.43852, acc=0.85938
# [25/100] testing 81.6% loss=0.17650, acc=0.96875
# [25/100] testing 82.5% loss=0.31747, acc=0.89062
# [25/100] testing 83.3% loss=0.26254, acc=0.93750
# [25/100] testing 83.8% loss=0.27322, acc=0.87500
# [25/100] testing 84.7% loss=0.27422, acc=0.82812
# [25/100] testing 85.1% loss=0.31587, acc=0.85938
# [25/100] testing 86.0% loss=0.31833, acc=0.89062
# [25/100] testing 86.4% loss=0.31932, acc=0.89062
# [25/100] testing 87.3% loss=0.26032, acc=0.87500
# [25/100] testing 87.7% loss=0.21103, acc=0.90625
# [25/100] testing 88.6% loss=0.33428, acc=0.89062
# [25/100] testing 89.5% loss=0.43243, acc=0.85938
# [25/100] testing 89.9% loss=0.24693, acc=0.81250
# [25/100] testing 90.8% loss=0.25732, acc=0.92188
# [25/100] testing 91.2% loss=0.14191, acc=0.92188
# [25/100] testing 92.1% loss=0.45689, acc=0.85938
# [25/100] testing 92.6% loss=0.26739, acc=0.85938
# [25/100] testing 93.4% loss=0.46164, acc=0.81250
# [25/100] testing 94.3% loss=0.19925, acc=0.90625
# [25/100] testing 94.7% loss=0.20476, acc=0.95312
# [25/100] testing 95.6% loss=0.39079, acc=0.84375
# [25/100] testing 96.1% loss=0.22549, acc=0.87500
# [25/100] testing 96.9% loss=0.29568, acc=0.84375
# [25/100] testing 97.4% loss=0.15582, acc=0.95312
# [25/100] testing 98.3% loss=0.33934, acc=0.82812
# [25/100] testing 98.7% loss=0.31251, acc=0.90625
# [25/100] testing 99.6% loss=0.23581, acc=0.90625
# [26/100] training 0.2% loss=0.33643, acc=0.89062
# [26/100] training 0.4% loss=0.43883, acc=0.85938
# [26/100] training 0.5% loss=0.19695, acc=0.92188
# [26/100] training 0.8% loss=0.18518, acc=0.95312
# [26/100] training 0.9% loss=0.22070, acc=0.90625
# [26/100] training 1.1% loss=0.28014, acc=0.92188
# [26/100] training 1.2% loss=0.24109, acc=0.92188
# [26/100] training 1.4% loss=0.26739, acc=0.92188
# [26/100] training 1.6% loss=0.22131, acc=0.90625
# [26/100] training 1.8% loss=0.21817, acc=0.92188
# [26/100] training 2.0% loss=0.27279, acc=0.90625
# [26/100] training 2.1% loss=0.23984, acc=0.90625
# [26/100] training 2.3% loss=0.21560, acc=0.90625
# [26/100] training 2.4% loss=0.30065, acc=0.87500
# [26/100] training 2.6% loss=0.17867, acc=0.92188
# [26/100] training 2.7% loss=0.23359, acc=0.92188
# [26/100] training 3.0% loss=0.18332, acc=0.93750
# [26/100] training 3.2% loss=0.18300, acc=0.92188
# [26/100] training 3.3% loss=0.41355, acc=0.82812
# [26/100] training 3.5% loss=0.22387, acc=0.92188
# [26/100] training 3.6% loss=0.32176, acc=0.85938
# [26/100] training 3.8% loss=0.24309, acc=0.90625
# [26/100] training 3.9% loss=0.18369, acc=0.92188
# [26/100] training 4.2% loss=0.13364, acc=0.98438
# [26/100] training 4.4% loss=0.16989, acc=0.90625
# [26/100] training 4.5% loss=0.18478, acc=0.89062
# [26/100] training 4.7% loss=0.30701, acc=0.89062
# [26/100] training 4.8% loss=0.28795, acc=0.87500
# [26/100] training 5.0% loss=0.17368, acc=0.93750
# [26/100] training 5.2% loss=0.27869, acc=0.89062
# [26/100] training 5.4% loss=0.16118, acc=0.93750
# [26/100] training 5.5% loss=0.28125, acc=0.90625
# [26/100] training 5.7% loss=0.25787, acc=0.89062
# [26/100] training 5.9% loss=0.25324, acc=0.89062
# [26/100] training 6.0% loss=0.23242, acc=0.87500
# [26/100] training 6.3% loss=0.24869, acc=0.87500
# [26/100] training 6.4% loss=0.18718, acc=0.92188
# [26/100] training 6.6% loss=0.27149, acc=0.89062
# [26/100] training 6.7% loss=0.34458, acc=0.85938
# [26/100] training 6.9% loss=0.20015, acc=0.92188
# [26/100] training 7.1% loss=0.21735, acc=0.90625
# [26/100] training 7.2% loss=0.32282, acc=0.81250
# [26/100] training 7.5% loss=0.21198, acc=0.92188
# [26/100] training 7.6% loss=0.24856, acc=0.90625
# [26/100] training 7.8% loss=0.20251, acc=0.90625
# [26/100] training 7.9% loss=0.32691, acc=0.85938
# [26/100] training 8.1% loss=0.16767, acc=0.96875
# [26/100] training 8.2% loss=0.26426, acc=0.92188
# [26/100] training 8.4% loss=0.32160, acc=0.85938
# [26/100] training 8.7% loss=0.20272, acc=0.92188
# [26/100] training 8.8% loss=0.18664, acc=0.92188
# [26/100] training 9.0% loss=0.26350, acc=0.87500
# [26/100] training 9.1% loss=0.25953, acc=0.93750
# [26/100] training 9.3% loss=0.46455, acc=0.81250
# [26/100] training 9.4% loss=0.22917, acc=0.92188
# [26/100] training 9.7% loss=0.27572, acc=0.92188
# [26/100] training 9.9% loss=0.31556, acc=0.87500
# [26/100] training 10.0% loss=0.28033, acc=0.89062
# [26/100] training 10.2% loss=0.18745, acc=0.93750
# [26/100] training 10.3% loss=0.22328, acc=0.87500
# [26/100] training 10.5% loss=0.32144, acc=0.90625
# [26/100] training 10.6% loss=0.20003, acc=0.90625
# [26/100] training 10.9% loss=0.13263, acc=0.96875
# [26/100] training 11.0% loss=0.27912, acc=0.89062
# [26/100] training 11.2% loss=0.18541, acc=0.92188
# [26/100] training 11.4% loss=0.29761, acc=0.87500
# [26/100] training 11.5% loss=0.30702, acc=0.93750
# [26/100] training 11.7% loss=0.10899, acc=0.98438
# [26/100] training 11.8% loss=0.21231, acc=0.87500
# [26/100] training 12.1% loss=0.19174, acc=0.90625
# [26/100] training 12.2% loss=0.11845, acc=0.96875
# [26/100] training 12.4% loss=0.31738, acc=0.87500
# [26/100] training 12.6% loss=0.28036, acc=0.93750
# [26/100] training 12.7% loss=0.31658, acc=0.84375
# [26/100] training 12.9% loss=0.30760, acc=0.85938
# [26/100] training 13.0% loss=0.17680, acc=0.95312
# [26/100] training 13.3% loss=0.18330, acc=0.92188
# [26/100] training 13.4% loss=0.27228, acc=0.87500
# [26/100] training 13.6% loss=0.21632, acc=0.92188
# [26/100] training 13.7% loss=0.32469, acc=0.90625
# [26/100] training 13.9% loss=0.29962, acc=0.89062
# [26/100] training 14.1% loss=0.34910, acc=0.92188
# [26/100] training 14.3% loss=0.13138, acc=0.92188
# [26/100] training 14.5% loss=0.29862, acc=0.92188
# [26/100] training 14.6% loss=0.20687, acc=0.93750
# [26/100] training 14.8% loss=0.22642, acc=0.90625
# [26/100] training 14.9% loss=0.14716, acc=0.95312
# [26/100] training 15.1% loss=0.38884, acc=0.82812
# [26/100] training 15.4% loss=0.25350, acc=0.84375
# [26/100] training 15.5% loss=0.24786, acc=0.87500
# [26/100] training 15.7% loss=0.43775, acc=0.87500
# [26/100] training 15.8% loss=0.14263, acc=0.93750
# [26/100] training 16.0% loss=0.38646, acc=0.84375
# [26/100] training 16.1% loss=0.35519, acc=0.85938
# [26/100] training 16.3% loss=0.21780, acc=0.92188
# [26/100] training 16.4% loss=0.28683, acc=0.89062
# [26/100] training 16.7% loss=0.33068, acc=0.82812
# [26/100] training 16.9% loss=0.34144, acc=0.84375
# [26/100] training 17.0% loss=0.25395, acc=0.90625
# [26/100] training 17.2% loss=0.24637, acc=0.90625
# [26/100] training 17.3% loss=0.27832, acc=0.92188
# [26/100] training 17.5% loss=0.25613, acc=0.90625
# [26/100] training 17.7% loss=0.20055, acc=0.92188
# [26/100] training 17.9% loss=0.26337, acc=0.92188
# [26/100] training 18.1% loss=0.33091, acc=0.89062
# [26/100] training 18.2% loss=0.34216, acc=0.87500
# [26/100] training 18.4% loss=0.39375, acc=0.81250
# [26/100] training 18.5% loss=0.19702, acc=0.95312
# [26/100] training 18.8% loss=0.18727, acc=0.95312
# [26/100] training 18.9% loss=0.21559, acc=0.90625
# [26/100] training 19.1% loss=0.35375, acc=0.87500
# [26/100] training 19.2% loss=0.13221, acc=0.95312
# [26/100] training 19.4% loss=0.16707, acc=0.92188
# [26/100] training 19.6% loss=0.29202, acc=0.87500
# [26/100] training 19.7% loss=0.26790, acc=0.90625
# [26/100] training 20.0% loss=0.12096, acc=0.96875
# [26/100] training 20.1% loss=0.20571, acc=0.92188
# [26/100] training 20.3% loss=0.18728, acc=0.93750
# [26/100] training 20.4% loss=0.32283, acc=0.84375
# [26/100] training 20.6% loss=0.23689, acc=0.90625
# [26/100] training 20.8% loss=0.19440, acc=0.92188
# [26/100] training 20.9% loss=0.19470, acc=0.92188
# [26/100] training 21.2% loss=0.24776, acc=0.89062
# [26/100] training 21.3% loss=0.29417, acc=0.87500
# [26/100] training 21.5% loss=0.28918, acc=0.92188
# [26/100] training 21.6% loss=0.16308, acc=0.96875
# [26/100] training 21.8% loss=0.16918, acc=0.93750
# [26/100] training 21.9% loss=0.24027, acc=0.92188
# [26/100] training 22.2% loss=0.34027, acc=0.84375
# [26/100] training 22.4% loss=0.35282, acc=0.87500
# [26/100] training 22.5% loss=0.18461, acc=0.93750
# [26/100] training 22.7% loss=0.26768, acc=0.89062
# [26/100] training 22.8% loss=0.32044, acc=0.89062
# [26/100] training 23.0% loss=0.18247, acc=0.93750
# [26/100] training 23.1% loss=0.36562, acc=0.87500
# [26/100] training 23.4% loss=0.33462, acc=0.85938
# [26/100] training 23.6% loss=0.34585, acc=0.90625
# [26/100] training 23.7% loss=0.22747, acc=0.90625
# [26/100] training 23.9% loss=0.23868, acc=0.89062
# [26/100] training 24.0% loss=0.24632, acc=0.89062
# [26/100] training 24.2% loss=0.33060, acc=0.87500
# [26/100] training 24.3% loss=0.27410, acc=0.89062
# [26/100] training 24.6% loss=0.20834, acc=0.90625
# [26/100] training 24.7% loss=0.28065, acc=0.89062
# [26/100] training 24.9% loss=0.17920, acc=0.92188
# [26/100] training 25.1% loss=0.23377, acc=0.87500
# [26/100] training 25.2% loss=0.13911, acc=0.95312
# [26/100] training 25.4% loss=0.23199, acc=0.89062
# [26/100] training 25.6% loss=0.17468, acc=0.93750
# [26/100] training 25.8% loss=0.22656, acc=0.90625
# [26/100] training 25.9% loss=0.15691, acc=0.96875
# [26/100] training 26.1% loss=0.21746, acc=0.89062
# [26/100] training 26.3% loss=0.20436, acc=0.92188
# [26/100] training 26.4% loss=0.18981, acc=0.92188
# [26/100] training 26.6% loss=0.16069, acc=0.93750
# [26/100] training 26.8% loss=0.16903, acc=0.93750
# [26/100] training 27.0% loss=0.22890, acc=0.90625
# [26/100] training 27.1% loss=0.10845, acc=0.98438
# [26/100] training 27.3% loss=0.23601, acc=0.89062
# [26/100] training 27.4% loss=0.15783, acc=0.93750
# [26/100] training 27.6% loss=0.24310, acc=0.93750
# [26/100] training 27.9% loss=0.20006, acc=0.93750
# [26/100] training 28.0% loss=0.24582, acc=0.90625
# [26/100] training 28.2% loss=0.23221, acc=0.90625
# [26/100] training 28.3% loss=0.12867, acc=0.95312
# [26/100] training 28.5% loss=0.19677, acc=0.90625
# [26/100] training 28.6% loss=0.19167, acc=0.92188
# [26/100] training 28.8% loss=0.09780, acc=0.96875
# [26/100] training 29.1% loss=0.27880, acc=0.85938
# [26/100] training 29.2% loss=0.15857, acc=0.93750
# [26/100] training 29.4% loss=0.35319, acc=0.82812
# [26/100] training 29.5% loss=0.17098, acc=0.92188
# [26/100] training 29.7% loss=0.30396, acc=0.87500
# [26/100] training 29.8% loss=0.20050, acc=0.93750
# [26/100] training 30.0% loss=0.34887, acc=0.81250
# [26/100] training 30.2% loss=0.18517, acc=0.89062
# [26/100] training 30.4% loss=0.16539, acc=0.95312
# [26/100] training 30.6% loss=0.26836, acc=0.89062
# [26/100] training 30.7% loss=0.23598, acc=0.92188
# [26/100] training 30.9% loss=0.31631, acc=0.92188
# [26/100] training 31.0% loss=0.18076, acc=0.95312
# [26/100] training 31.3% loss=0.15744, acc=0.95312
# [26/100] training 31.4% loss=0.32862, acc=0.85938
# [26/100] training 31.6% loss=0.26968, acc=0.93750
# [26/100] training 31.8% loss=0.17499, acc=0.93750
# [26/100] training 31.9% loss=0.24576, acc=0.90625
# [26/100] training 32.1% loss=0.24751, acc=0.92188
# [26/100] training 32.2% loss=0.28146, acc=0.89062
# [26/100] training 32.5% loss=0.13913, acc=0.96875
# [26/100] training 32.6% loss=0.18672, acc=0.93750
# [26/100] training 32.8% loss=0.19478, acc=0.89062
# [26/100] training 32.9% loss=0.26028, acc=0.89062
# [26/100] training 33.1% loss=0.22362, acc=0.92188
# [26/100] training 33.3% loss=0.23208, acc=0.85938
# [26/100] training 33.4% loss=0.21542, acc=0.90625
# [26/100] training 33.7% loss=0.24490, acc=0.92188
# [26/100] training 33.8% loss=0.25185, acc=0.92188
# [26/100] training 34.0% loss=0.28340, acc=0.84375
# [26/100] training 34.1% loss=0.22172, acc=0.90625
# [26/100] training 34.3% loss=0.18779, acc=0.90625
# [26/100] training 34.5% loss=0.26416, acc=0.87500
# [26/100] training 34.7% loss=0.13357, acc=0.95312
# [26/100] training 34.9% loss=0.16428, acc=0.89062
# [26/100] training 35.0% loss=0.16429, acc=0.96875
# [26/100] training 35.2% loss=0.34469, acc=0.81250
# [26/100] training 35.3% loss=0.17770, acc=0.95312
# [26/100] training 35.5% loss=0.19153, acc=0.92188
# [26/100] training 35.6% loss=0.36482, acc=0.85938
# [26/100] training 35.9% loss=0.34404, acc=0.84375
# [26/100] training 36.1% loss=0.30256, acc=0.85938
# [26/100] training 36.2% loss=0.25626, acc=0.87500
# [26/100] training 36.4% loss=0.23213, acc=0.92188
# [26/100] training 36.5% loss=0.26420, acc=0.90625
# [26/100] training 36.7% loss=0.23189, acc=0.89062
# [26/100] training 36.8% loss=0.15273, acc=0.95312
# [26/100] training 37.1% loss=0.29738, acc=0.85938
# [26/100] training 37.3% loss=0.28124, acc=0.90625
# [26/100] training 37.4% loss=0.22647, acc=0.89062
# [26/100] training 37.6% loss=0.21530, acc=0.92188
# [26/100] training 37.7% loss=0.36537, acc=0.84375
# [26/100] training 37.9% loss=0.28615, acc=0.89062
# [26/100] training 38.1% loss=0.26429, acc=0.90625
# [26/100] training 38.3% loss=0.22937, acc=0.93750
# [26/100] training 38.4% loss=0.09765, acc=0.93750
# [26/100] training 38.6% loss=0.21600, acc=0.87500
# [26/100] training 38.8% loss=0.21611, acc=0.87500
# [26/100] training 38.9% loss=0.20746, acc=0.90625
# [26/100] training 39.1% loss=0.26981, acc=0.92188
# [26/100] training 39.3% loss=0.30618, acc=0.84375
# [26/100] training 39.5% loss=0.22286, acc=0.89062
# [26/100] training 39.6% loss=0.16143, acc=0.96875
# [26/100] training 39.8% loss=0.10204, acc=0.96875
# [26/100] training 40.0% loss=0.24158, acc=0.89062
# [26/100] training 40.1% loss=0.41402, acc=0.84375
# [26/100] training 40.4% loss=0.16069, acc=0.92188
# [26/100] training 40.5% loss=0.26860, acc=0.87500
# [26/100] training 40.7% loss=0.23502, acc=0.92188
# [26/100] training 40.8% loss=0.15289, acc=0.96875
# [26/100] training 41.0% loss=0.15544, acc=0.93750
# [26/100] training 41.1% loss=0.29584, acc=0.87500
# [26/100] training 41.3% loss=0.28903, acc=0.87500
# [26/100] training 41.6% loss=0.30537, acc=0.87500
# [26/100] training 41.7% loss=0.33944, acc=0.87500
# [26/100] training 41.9% loss=0.18257, acc=0.93750
# [26/100] training 42.0% loss=0.22602, acc=0.90625
# [26/100] training 42.2% loss=0.22488, acc=0.89062
# [26/100] training 42.3% loss=0.16669, acc=0.92188
# [26/100] training 42.5% loss=0.13963, acc=0.98438
# [26/100] training 42.8% loss=0.17537, acc=0.89062
# [26/100] training 42.9% loss=0.19667, acc=0.90625
# [26/100] training 43.1% loss=0.22970, acc=0.92188
# [26/100] training 43.2% loss=0.17933, acc=0.90625
# [26/100] training 43.4% loss=0.26794, acc=0.92188
# [26/100] training 43.5% loss=0.23769, acc=0.92188
# [26/100] training 43.8% loss=0.36779, acc=0.84375
# [26/100] training 43.9% loss=0.22884, acc=0.89062
# [26/100] training 44.1% loss=0.17973, acc=0.95312
# [26/100] training 44.3% loss=0.16887, acc=0.92188
# [26/100] training 44.4% loss=0.23586, acc=0.89062
# [26/100] training 44.6% loss=0.28503, acc=0.84375
# [26/100] training 44.7% loss=0.29127, acc=0.90625
# [26/100] training 45.0% loss=0.15546, acc=0.93750
# [26/100] training 45.1% loss=0.43173, acc=0.85938
# [26/100] training 45.3% loss=0.26295, acc=0.87500
# [26/100] training 45.5% loss=0.07528, acc=1.00000
# [26/100] training 45.6% loss=0.26211, acc=0.89062
# [26/100] training 45.8% loss=0.15426, acc=0.92188
# [26/100] training 45.9% loss=0.19565, acc=0.90625
# [26/100] training 46.2% loss=0.25128, acc=0.92188
# [26/100] training 46.3% loss=0.19876, acc=0.90625
# [26/100] training 46.5% loss=0.35035, acc=0.85938
# [26/100] training 46.6% loss=0.17128, acc=0.92188
# [26/100] training 46.8% loss=0.21635, acc=0.90625
# [26/100] training 47.0% loss=0.28934, acc=0.90625
# [26/100] training 47.2% loss=0.24451, acc=0.92188
# [26/100] training 47.4% loss=0.22011, acc=0.85938
# [26/100] training 47.5% loss=0.27341, acc=0.89062
# [26/100] training 47.7% loss=0.17553, acc=0.89062
# [26/100] training 47.8% loss=0.30365, acc=0.87500
# [26/100] training 48.0% loss=0.17696, acc=0.93750
# [26/100] training 48.3% loss=0.10891, acc=0.96875
# [26/100] training 48.4% loss=0.11328, acc=0.96875
# [26/100] training 48.6% loss=0.19957, acc=0.92188
# [26/100] training 48.7% loss=0.28881, acc=0.85938
# [26/100] training 48.9% loss=0.18365, acc=0.90625
# [26/100] training 49.0% loss=0.14719, acc=0.93750
# [26/100] training 49.2% loss=0.25428, acc=0.84375
# [26/100] training 49.3% loss=0.17765, acc=0.96875
# [26/100] training 49.6% loss=0.25665, acc=0.90625
# [26/100] training 49.8% loss=0.29357, acc=0.84375
# [26/100] training 49.9% loss=0.30054, acc=0.84375
# [26/100] training 50.1% loss=0.19019, acc=0.92188
# [26/100] training 50.2% loss=0.26577, acc=0.85938
# [26/100] training 50.4% loss=0.39335, acc=0.87500
# [26/100] training 50.6% loss=0.27269, acc=0.85938
# [26/100] training 50.8% loss=0.31020, acc=0.87500
# [26/100] training 51.0% loss=0.25415, acc=0.90625
# [26/100] training 51.1% loss=0.31749, acc=0.84375
# [26/100] training 51.3% loss=0.26557, acc=0.93750
# [26/100] training 51.4% loss=0.24584, acc=0.93750
# [26/100] training 51.7% loss=0.26499, acc=0.87500
# [26/100] training 51.8% loss=0.24960, acc=0.92188
# [26/100] training 52.0% loss=0.21633, acc=0.90625
# [26/100] training 52.1% loss=0.25960, acc=0.84375
# [26/100] training 52.3% loss=0.28249, acc=0.89062
# [26/100] training 52.5% loss=0.14986, acc=0.93750
# [26/100] training 52.6% loss=0.21777, acc=0.90625
# [26/100] training 52.9% loss=0.39141, acc=0.92188
# [26/100] training 53.0% loss=0.16476, acc=0.95312
# [26/100] training 53.2% loss=0.23414, acc=0.92188
# [26/100] training 53.3% loss=0.14224, acc=0.96875
# [26/100] training 53.5% loss=0.25860, acc=0.90625
# [26/100] training 53.7% loss=0.14914, acc=0.95312
# [26/100] training 53.8% loss=0.27373, acc=0.85938
# [26/100] training 54.1% loss=0.26836, acc=0.85938
# [26/100] training 54.2% loss=0.17256, acc=0.95312
# [26/100] training 54.4% loss=0.23465, acc=0.90625
# [26/100] training 54.5% loss=0.23372, acc=0.92188
# [26/100] training 54.7% loss=0.35916, acc=0.84375
# [26/100] training 54.8% loss=0.14980, acc=0.95312
# [26/100] training 55.1% loss=0.17706, acc=0.92188
# [26/100] training 55.3% loss=0.20564, acc=0.92188
# [26/100] training 55.4% loss=0.25302, acc=0.85938
# [26/100] training 55.6% loss=0.19415, acc=0.89062
# [26/100] training 55.7% loss=0.20925, acc=0.93750
# [26/100] training 55.9% loss=0.26171, acc=0.90625
# [26/100] training 56.0% loss=0.19048, acc=0.90625
# [26/100] training 56.3% loss=0.54779, acc=0.84375
# [26/100] training 56.5% loss=0.25467, acc=0.92188
# [26/100] training 56.6% loss=0.18071, acc=0.90625
# [26/100] training 56.8% loss=0.31175, acc=0.85938
# [26/100] training 56.9% loss=0.25829, acc=0.89062
# [26/100] training 57.1% loss=0.30857, acc=0.84375
# [26/100] training 57.2% loss=0.14155, acc=0.96875
# [26/100] training 57.5% loss=0.16762, acc=0.95312
# [26/100] training 57.6% loss=0.23049, acc=0.89062
# [26/100] training 57.8% loss=0.12798, acc=0.96875
# [26/100] training 58.0% loss=0.18892, acc=0.92188
# [26/100] training 58.1% loss=0.22276, acc=0.89062
# [26/100] training 58.3% loss=0.14625, acc=0.95312
# [26/100] training 58.4% loss=0.30360, acc=0.89062
# [26/100] training 58.7% loss=0.19660, acc=0.92188
# [26/100] training 58.8% loss=0.28927, acc=0.84375
# [26/100] training 59.0% loss=0.17187, acc=0.92188
# [26/100] training 59.2% loss=0.21702, acc=0.92188
# [26/100] training 59.3% loss=0.21192, acc=0.90625
# [26/100] training 59.5% loss=0.27307, acc=0.87500
# [26/100] training 59.7% loss=0.18995, acc=0.93750
# [26/100] training 59.9% loss=0.18893, acc=0.93750
# [26/100] training 60.0% loss=0.20941, acc=0.89062
# [26/100] training 60.2% loss=0.16170, acc=0.93750
# [26/100] training 60.3% loss=0.24383, acc=0.90625
# [26/100] training 60.5% loss=0.26347, acc=0.85938
# [26/100] training 60.8% loss=0.15840, acc=0.95312
# [26/100] training 60.9% loss=0.30052, acc=0.85938
# [26/100] training 61.1% loss=0.28866, acc=0.84375
# [26/100] training 61.2% loss=0.16277, acc=0.92188
# [26/100] training 61.4% loss=0.20785, acc=0.92188
# [26/100] training 61.5% loss=0.40989, acc=0.79688
# [26/100] training 61.7% loss=0.20889, acc=0.92188
# [26/100] training 62.0% loss=0.30594, acc=0.87500
# [26/100] training 62.1% loss=0.40504, acc=0.81250
# [26/100] training 62.3% loss=0.12241, acc=1.00000
# [26/100] training 62.4% loss=0.26706, acc=0.85938
# [26/100] training 62.6% loss=0.36512, acc=0.82812
# [26/100] training 62.7% loss=0.15725, acc=0.93750
# [26/100] training 62.9% loss=0.27288, acc=0.90625
# [26/100] training 63.1% loss=0.18988, acc=0.95312
# [26/100] training 63.3% loss=0.21939, acc=0.90625
# [26/100] training 63.5% loss=0.22812, acc=0.90625
# [26/100] training 63.6% loss=0.36798, acc=0.84375
# [26/100] training 63.8% loss=0.32538, acc=0.85938
# [26/100] training 63.9% loss=0.17744, acc=0.93750
# [26/100] training 64.2% loss=0.20684, acc=0.90625
# [26/100] training 64.3% loss=0.31416, acc=0.89062
# [26/100] training 64.5% loss=0.26046, acc=0.89062
# [26/100] training 64.7% loss=0.20835, acc=0.90625
# [26/100] training 64.8% loss=0.44592, acc=0.81250
# [26/100] training 65.0% loss=0.22965, acc=0.89062
# [26/100] training 65.1% loss=0.30202, acc=0.85938
# [26/100] training 65.4% loss=0.17689, acc=0.95312
# [26/100] training 65.5% loss=0.15838, acc=0.96875
# [26/100] training 65.7% loss=0.16528, acc=0.90625
# [26/100] training 65.8% loss=0.25170, acc=0.89062
# [26/100] training 66.0% loss=0.19040, acc=0.90625
# [26/100] training 66.2% loss=0.11086, acc=0.96875
# [26/100] training 66.3% loss=0.25577, acc=0.90625
# [26/100] training 66.6% loss=0.20413, acc=0.92188
# [26/100] training 66.7% loss=0.13826, acc=0.93750
# [26/100] training 66.9% loss=0.22893, acc=0.89062
# [26/100] training 67.0% loss=0.27862, acc=0.87500
# [26/100] training 67.2% loss=0.17761, acc=0.92188
# [26/100] training 67.4% loss=0.15973, acc=0.93750
# [26/100] training 67.6% loss=0.20033, acc=0.95312
# [26/100] training 67.8% loss=0.16524, acc=0.93750
# [26/100] training 67.9% loss=0.17380, acc=0.90625
# [26/100] training 68.1% loss=0.19355, acc=0.89062
# [26/100] training 68.2% loss=0.20974, acc=0.90625
# [26/100] training 68.4% loss=0.15280, acc=0.92188
# [26/100] training 68.5% loss=0.37064, acc=0.87500
# [26/100] training 68.8% loss=0.23767, acc=0.87500
# [26/100] training 69.0% loss=0.39051, acc=0.87500
# [26/100] training 69.1% loss=0.09345, acc=0.96875
# [26/100] training 69.3% loss=0.34162, acc=0.79688
# [26/100] training 69.4% loss=0.24320, acc=0.90625
# [26/100] training 69.6% loss=0.16490, acc=0.92188
# [26/100] training 69.7% loss=0.14773, acc=0.93750
# [26/100] training 70.0% loss=0.33104, acc=0.89062
# [26/100] training 70.2% loss=0.22090, acc=0.90625
# [26/100] training 70.3% loss=0.25493, acc=0.92188
# [26/100] training 70.5% loss=0.23239, acc=0.89062
# [26/100] training 70.6% loss=0.17353, acc=0.95312
# [26/100] training 70.8% loss=0.25216, acc=0.85938
# [26/100] training 71.0% loss=0.24261, acc=0.89062
# [26/100] training 71.2% loss=0.19278, acc=0.92188
# [26/100] training 71.3% loss=0.12902, acc=0.95312
# [26/100] training 71.5% loss=0.29151, acc=0.90625
# [26/100] training 71.7% loss=0.24626, acc=0.89062
# [26/100] training 71.8% loss=0.27531, acc=0.87500
# [26/100] training 72.0% loss=0.11849, acc=0.93750
# [26/100] training 72.2% loss=0.28064, acc=0.89062
# [26/100] training 72.4% loss=0.25499, acc=0.89062
# [26/100] training 72.5% loss=0.36169, acc=0.87500
# [26/100] training 72.7% loss=0.31124, acc=0.85938
# [26/100] training 72.9% loss=0.21491, acc=0.89062
# [26/100] training 73.0% loss=0.14193, acc=0.96875
# [26/100] training 73.3% loss=0.26975, acc=0.89062
# [26/100] training 73.4% loss=0.14157, acc=0.93750
# [26/100] training 73.6% loss=0.19162, acc=0.93750
# [26/100] training 73.7% loss=0.23744, acc=0.90625
# [26/100] training 73.9% loss=0.19346, acc=0.93750
# [26/100] training 74.0% loss=0.26941, acc=0.87500
# [26/100] training 74.2% loss=0.17095, acc=0.92188
# [26/100] training 74.5% loss=0.20885, acc=0.89062
# [26/100] training 74.6% loss=0.36515, acc=0.81250
# [26/100] training 74.8% loss=0.39184, acc=0.82812
# [26/100] training 74.9% loss=0.35470, acc=0.82812
# [26/100] training 75.1% loss=0.21183, acc=0.92188
# [26/100] training 75.2% loss=0.19520, acc=0.90625
# [26/100] training 75.4% loss=0.25860, acc=0.92188
# [26/100] training 75.7% loss=0.25994, acc=0.85938
# [26/100] training 75.8% loss=0.30326, acc=0.85938
# [26/100] training 76.0% loss=0.11706, acc=0.95312
# [26/100] training 76.1% loss=0.29412, acc=0.87500
# [26/100] training 76.3% loss=0.12293, acc=0.96875
# [26/100] training 76.4% loss=0.23241, acc=0.90625
# [26/100] training 76.7% loss=0.27559, acc=0.90625
# [26/100] training 76.8% loss=0.15759, acc=0.95312
# [26/100] training 77.0% loss=0.11365, acc=0.96875
# [26/100] training 77.2% loss=0.33007, acc=0.89062
# [26/100] training 77.3% loss=0.10363, acc=0.96875
# [26/100] training 77.5% loss=0.28967, acc=0.87500
# [26/100] training 77.6% loss=0.24709, acc=0.90625
# [26/100] training 77.9% loss=0.17998, acc=0.92188
# [26/100] training 78.0% loss=0.27039, acc=0.85938
# [26/100] training 78.2% loss=0.20417, acc=0.92188
# [26/100] training 78.4% loss=0.15377, acc=0.92188
# [26/100] training 78.5% loss=0.34540, acc=0.85938
# [26/100] training 78.7% loss=0.24926, acc=0.89062
# [26/100] training 78.8% loss=0.08750, acc=0.96875
# [26/100] training 79.1% loss=0.12746, acc=0.95312
# [26/100] training 79.2% loss=0.15258, acc=0.95312
# [26/100] training 79.4% loss=0.20125, acc=0.92188
# [26/100] training 79.5% loss=0.23112, acc=0.92188
# [26/100] training 79.7% loss=0.09121, acc=0.96875
# [26/100] training 79.9% loss=0.20369, acc=0.89062
# [26/100] training 80.1% loss=0.14354, acc=0.93750
# [26/100] training 80.3% loss=0.23639, acc=0.90625
# [26/100] training 80.4% loss=0.19944, acc=0.93750
# [26/100] training 80.6% loss=0.23295, acc=0.93750
# [26/100] training 80.7% loss=0.19027, acc=0.98438
# [26/100] training 80.9% loss=0.26564, acc=0.87500
# [26/100] training 81.2% loss=0.24330, acc=0.92188
# [26/100] training 81.3% loss=0.21237, acc=0.90625
# [26/100] training 81.5% loss=0.27763, acc=0.90625
# [26/100] training 81.6% loss=0.37812, acc=0.89062
# [26/100] training 81.8% loss=0.21096, acc=0.90625
# [26/100] training 81.9% loss=0.29290, acc=0.92188
# [26/100] training 82.1% loss=0.14576, acc=0.96875
# [26/100] training 82.2% loss=0.21326, acc=0.92188
# [26/100] training 82.5% loss=0.16731, acc=0.98438
# [26/100] training 82.7% loss=0.20946, acc=0.93750
# [26/100] training 82.8% loss=0.20216, acc=0.95312
# [26/100] training 83.0% loss=0.25918, acc=0.89062
# [26/100] training 83.1% loss=0.13163, acc=0.95312
# [26/100] training 83.3% loss=0.11810, acc=0.93750
# [26/100] training 83.5% loss=0.18159, acc=0.92188
# [26/100] training 83.7% loss=0.40027, acc=0.85938
# [26/100] training 83.9% loss=0.28036, acc=0.89062
# [26/100] training 84.0% loss=0.17646, acc=0.95312
# [26/100] training 84.2% loss=0.14375, acc=0.95312
# [26/100] training 84.3% loss=0.22681, acc=0.84375
# [26/100] training 84.5% loss=0.15159, acc=0.95312
# [26/100] training 84.7% loss=0.26299, acc=0.87500
# [26/100] training 84.9% loss=0.14275, acc=0.93750
# [26/100] training 85.0% loss=0.18114, acc=0.92188
# [26/100] training 85.2% loss=0.19796, acc=0.92188
# [26/100] training 85.4% loss=0.13555, acc=0.96875
# [26/100] training 85.5% loss=0.20494, acc=0.90625
# [26/100] training 85.8% loss=0.22400, acc=0.90625
# [26/100] training 85.9% loss=0.21984, acc=0.89062
# [26/100] training 86.1% loss=0.23928, acc=0.90625
# [26/100] training 86.2% loss=0.20332, acc=0.90625
# [26/100] training 86.4% loss=0.27432, acc=0.89062
# [26/100] training 86.6% loss=0.23993, acc=0.89062
# [26/100] training 86.7% loss=0.14119, acc=0.95312
# [26/100] training 87.0% loss=0.30450, acc=0.85938
# [26/100] training 87.1% loss=0.20636, acc=0.93750
# [26/100] training 87.3% loss=0.24338, acc=0.93750
# [26/100] training 87.4% loss=0.21595, acc=0.90625
# [26/100] training 87.6% loss=0.24229, acc=0.87500
# [26/100] training 87.7% loss=0.39414, acc=0.90625
# [26/100] training 87.9% loss=0.21623, acc=0.87500
# [26/100] training 88.2% loss=0.17414, acc=0.90625
# [26/100] training 88.3% loss=0.28133, acc=0.89062
# [26/100] training 88.5% loss=0.19094, acc=0.95312
# [26/100] training 88.6% loss=0.16370, acc=0.92188
# [26/100] training 88.8% loss=0.21386, acc=0.92188
# [26/100] training 88.9% loss=0.28322, acc=0.85938
# [26/100] training 89.2% loss=0.16424, acc=0.92188
# [26/100] training 89.4% loss=0.18068, acc=0.93750
# [26/100] training 89.5% loss=0.19181, acc=0.90625
# [26/100] training 89.7% loss=0.27046, acc=0.89062
# [26/100] training 89.8% loss=0.22054, acc=0.93750
# [26/100] training 90.0% loss=0.19725, acc=0.92188
# [26/100] training 90.1% loss=0.20862, acc=0.89062
# [26/100] training 90.4% loss=0.19252, acc=0.93750
# [26/100] training 90.5% loss=0.12688, acc=0.96875
# [26/100] training 90.7% loss=0.11951, acc=0.98438
# [26/100] training 90.9% loss=0.07754, acc=1.00000
# [26/100] training 91.0% loss=0.13364, acc=0.92188
# [26/100] training 91.2% loss=0.17821, acc=0.92188
# [26/100] training 91.3% loss=0.43007, acc=0.89062
# [26/100] training 91.6% loss=0.32615, acc=0.89062
# [26/100] training 91.7% loss=0.19795, acc=0.92188
# [26/100] training 91.9% loss=0.26336, acc=0.85938
# [26/100] training 92.1% loss=0.23514, acc=0.89062
# [26/100] training 92.2% loss=0.10842, acc=0.96875
# [26/100] training 92.4% loss=0.23355, acc=0.90625
# [26/100] training 92.6% loss=0.33006, acc=0.84375
# [26/100] training 92.8% loss=0.25809, acc=0.92188
# [26/100] training 92.9% loss=0.17135, acc=0.93750
# [26/100] training 93.1% loss=0.40338, acc=0.85938
# [26/100] training 93.2% loss=0.27563, acc=0.85938
# [26/100] training 93.4% loss=0.24000, acc=0.92188
# [26/100] training 93.7% loss=0.19321, acc=0.93750
# [26/100] training 93.8% loss=0.23722, acc=0.87500
# [26/100] training 94.0% loss=0.21342, acc=0.90625
# [26/100] training 94.1% loss=0.24503, acc=0.90625
# [26/100] training 94.3% loss=0.11759, acc=0.96875
# [26/100] training 94.4% loss=0.21960, acc=0.90625
# [26/100] training 94.6% loss=0.14531, acc=0.96875
# [26/100] training 94.9% loss=0.19341, acc=0.89062
# [26/100] training 95.0% loss=0.29187, acc=0.85938
# [26/100] training 95.2% loss=0.39347, acc=0.85938
# [26/100] training 95.3% loss=0.40106, acc=0.87500
# [26/100] training 95.5% loss=0.20523, acc=0.90625
# [26/100] training 95.6% loss=0.30787, acc=0.85938
# [26/100] training 95.8% loss=0.37098, acc=0.81250
# [26/100] training 96.0% loss=0.24007, acc=0.90625
# [26/100] training 96.2% loss=0.20467, acc=0.92188
# [26/100] training 96.4% loss=0.22699, acc=0.96875
# [26/100] training 96.5% loss=0.27697, acc=0.92188
# [26/100] training 96.7% loss=0.15502, acc=0.90625
# [26/100] training 96.8% loss=0.19855, acc=0.89062
# [26/100] training 97.1% loss=0.21227, acc=0.89062
# [26/100] training 97.2% loss=0.30670, acc=0.87500
# [26/100] training 97.4% loss=0.32171, acc=0.90625
# [26/100] training 97.6% loss=0.32927, acc=0.85938
# [26/100] training 97.7% loss=0.25294, acc=0.87500
# [26/100] training 97.9% loss=0.14173, acc=0.92188
# [26/100] training 98.0% loss=0.25520, acc=0.89062
# [26/100] training 98.3% loss=0.19145, acc=0.93750
# [26/100] training 98.4% loss=0.18775, acc=0.93750
# [26/100] training 98.6% loss=0.49948, acc=0.78125
# [26/100] training 98.7% loss=0.23324, acc=0.92188
# [26/100] training 98.9% loss=0.17259, acc=0.92188
# [26/100] training 99.1% loss=0.27458, acc=0.89062
# [26/100] training 99.2% loss=0.24669, acc=0.89062
# [26/100] training 99.5% loss=0.28147, acc=0.84375
# [26/100] training 99.6% loss=0.23119, acc=0.92188
# [26/100] training 99.8% loss=0.16212, acc=0.92188
# [26/100] training 99.9% loss=0.13486, acc=0.93750
# [26/100] testing 0.9% loss=0.14158, acc=0.92188
# [26/100] testing 1.8% loss=0.46341, acc=0.84375
# [26/100] testing 2.2% loss=0.23117, acc=0.90625
# [26/100] testing 3.1% loss=0.34060, acc=0.85938
# [26/100] testing 3.5% loss=0.16477, acc=0.92188
# [26/100] testing 4.4% loss=0.23323, acc=0.89062
# [26/100] testing 4.8% loss=0.40927, acc=0.82812
# [26/100] testing 5.7% loss=0.16130, acc=0.96875
# [26/100] testing 6.6% loss=0.24498, acc=0.90625
# [26/100] testing 7.0% loss=0.16865, acc=0.92188
# [26/100] testing 7.9% loss=0.35483, acc=0.82812
# [26/100] testing 8.3% loss=0.27131, acc=0.92188
# [26/100] testing 9.2% loss=0.32244, acc=0.87500
# [26/100] testing 9.7% loss=0.17748, acc=0.93750
# [26/100] testing 10.5% loss=0.21780, acc=0.87500
# [26/100] testing 11.0% loss=0.40412, acc=0.84375
# [26/100] testing 11.8% loss=0.22869, acc=0.89062
# [26/100] testing 12.7% loss=0.40998, acc=0.84375
# [26/100] testing 13.2% loss=0.27510, acc=0.85938
# [26/100] testing 14.0% loss=0.42117, acc=0.90625
# [26/100] testing 14.5% loss=0.27166, acc=0.84375
# [26/100] testing 15.4% loss=0.38107, acc=0.84375
# [26/100] testing 15.8% loss=0.17468, acc=0.93750
# [26/100] testing 16.7% loss=0.23881, acc=0.87500
# [26/100] testing 17.5% loss=0.15799, acc=0.92188
# [26/100] testing 18.0% loss=0.18253, acc=0.90625
# [26/100] testing 18.9% loss=0.12145, acc=0.93750
# [26/100] testing 19.3% loss=0.39383, acc=0.85938
# [26/100] testing 20.2% loss=0.26660, acc=0.92188
# [26/100] testing 20.6% loss=0.34803, acc=0.81250
# [26/100] testing 21.5% loss=0.22794, acc=0.92188
# [26/100] testing 21.9% loss=0.36826, acc=0.87500
# [26/100] testing 22.8% loss=0.27829, acc=0.90625
# [26/100] testing 23.7% loss=0.41552, acc=0.87500
# [26/100] testing 24.1% loss=0.30349, acc=0.89062
# [26/100] testing 25.0% loss=0.35661, acc=0.87500
# [26/100] testing 25.4% loss=0.19350, acc=0.92188
# [26/100] testing 26.3% loss=0.27955, acc=0.87500
# [26/100] testing 26.8% loss=0.33366, acc=0.87500
# [26/100] testing 27.6% loss=0.28697, acc=0.89062
# [26/100] testing 28.5% loss=0.28836, acc=0.87500
# [26/100] testing 29.0% loss=0.22591, acc=0.92188
# [26/100] testing 29.8% loss=0.38708, acc=0.85938
# [26/100] testing 30.3% loss=0.28261, acc=0.84375
# [26/100] testing 31.1% loss=0.23431, acc=0.87500
# [26/100] testing 31.6% loss=0.18886, acc=0.92188
# [26/100] testing 32.5% loss=0.25154, acc=0.90625
# [26/100] testing 32.9% loss=0.46805, acc=0.85938
# [26/100] testing 33.8% loss=0.23517, acc=0.89062
# [26/100] testing 34.7% loss=0.32268, acc=0.85938
# [26/100] testing 35.1% loss=0.19359, acc=0.95312
# [26/100] testing 36.0% loss=0.25161, acc=0.92188
# [26/100] testing 36.4% loss=0.23278, acc=0.90625
# [26/100] testing 37.3% loss=0.30329, acc=0.92188
# [26/100] testing 37.7% loss=0.39766, acc=0.85938
# [26/100] testing 38.6% loss=0.19666, acc=0.93750
# [26/100] testing 39.5% loss=0.32910, acc=0.89062
# [26/100] testing 39.9% loss=0.33501, acc=0.87500
# [26/100] testing 40.8% loss=0.30736, acc=0.90625
# [26/100] testing 41.2% loss=0.25570, acc=0.93750
# [26/100] testing 42.1% loss=0.28632, acc=0.84375
# [26/100] testing 42.5% loss=0.18000, acc=0.90625
# [26/100] testing 43.4% loss=0.41343, acc=0.85938
# [26/100] testing 43.9% loss=0.12845, acc=0.93750
# [26/100] testing 44.7% loss=0.33141, acc=0.87500
# [26/100] testing 45.6% loss=0.23929, acc=0.89062
# [26/100] testing 46.1% loss=0.20579, acc=0.87500
# [26/100] testing 46.9% loss=0.22795, acc=0.89062
# [26/100] testing 47.4% loss=0.11299, acc=0.96875
# [26/100] testing 48.3% loss=0.41087, acc=0.87500
# [26/100] testing 48.7% loss=0.42071, acc=0.84375
# [26/100] testing 49.6% loss=0.44431, acc=0.82812
# [26/100] testing 50.4% loss=0.23947, acc=0.90625
# [26/100] testing 50.9% loss=0.26836, acc=0.90625
# [26/100] testing 51.8% loss=0.25853, acc=0.87500
# [26/100] testing 52.2% loss=0.29028, acc=0.87500
# [26/100] testing 53.1% loss=0.19417, acc=0.90625
# [26/100] testing 53.5% loss=0.28135, acc=0.89062
# [26/100] testing 54.4% loss=0.43788, acc=0.81250
# [26/100] testing 54.8% loss=0.32382, acc=0.85938
# [26/100] testing 55.7% loss=0.21537, acc=0.93750
# [26/100] testing 56.6% loss=0.33896, acc=0.85938
# [26/100] testing 57.0% loss=0.42294, acc=0.85938
# [26/100] testing 57.9% loss=0.27136, acc=0.89062
# [26/100] testing 58.3% loss=0.31567, acc=0.84375
# [26/100] testing 59.2% loss=0.28687, acc=0.85938
# [26/100] testing 59.7% loss=0.19906, acc=0.90625
# [26/100] testing 60.5% loss=0.33389, acc=0.84375
# [26/100] testing 61.4% loss=0.12315, acc=0.95312
# [26/100] testing 61.9% loss=0.21466, acc=0.89062
# [26/100] testing 62.7% loss=0.15655, acc=0.95312
# [26/100] testing 63.2% loss=0.44108, acc=0.87500
# [26/100] testing 64.0% loss=0.36019, acc=0.84375
# [26/100] testing 64.5% loss=0.22284, acc=0.90625
# [26/100] testing 65.4% loss=0.24467, acc=0.92188
# [26/100] testing 65.8% loss=0.34985, acc=0.84375
# [26/100] testing 66.7% loss=0.22130, acc=0.89062
# [26/100] testing 67.6% loss=0.28030, acc=0.89062
# [26/100] testing 68.0% loss=0.10170, acc=0.95312
# [26/100] testing 68.9% loss=0.23937, acc=0.92188
# [26/100] testing 69.3% loss=0.36603, acc=0.85938
# [26/100] testing 70.2% loss=0.37251, acc=0.85938
# [26/100] testing 70.6% loss=0.36892, acc=0.81250
# [26/100] testing 71.5% loss=0.40124, acc=0.89062
# [26/100] testing 72.4% loss=0.18924, acc=0.92188
# [26/100] testing 72.8% loss=0.14293, acc=0.93750
# [26/100] testing 73.7% loss=0.20319, acc=0.93750
# [26/100] testing 74.1% loss=0.42599, acc=0.89062
# [26/100] testing 75.0% loss=0.17832, acc=0.93750
# [26/100] testing 75.4% loss=0.35264, acc=0.87500
# [26/100] testing 76.3% loss=0.12396, acc=0.93750
# [26/100] testing 76.8% loss=0.28902, acc=0.84375
# [26/100] testing 77.6% loss=0.29461, acc=0.85938
# [26/100] testing 78.5% loss=0.40703, acc=0.85938
# [26/100] testing 79.0% loss=0.32014, acc=0.85938
# [26/100] testing 79.8% loss=0.30060, acc=0.85938
# [26/100] testing 80.3% loss=0.36642, acc=0.87500
# [26/100] testing 81.2% loss=0.36969, acc=0.84375
# [26/100] testing 81.6% loss=0.23653, acc=0.93750
# [26/100] testing 82.5% loss=0.19715, acc=0.92188
# [26/100] testing 83.3% loss=0.19372, acc=0.95312
# [26/100] testing 83.8% loss=0.17497, acc=0.92188
# [26/100] testing 84.7% loss=0.27609, acc=0.87500
# [26/100] testing 85.1% loss=0.20522, acc=0.92188
# [26/100] testing 86.0% loss=0.24212, acc=0.92188
# [26/100] testing 86.4% loss=0.39410, acc=0.79688
# [26/100] testing 87.3% loss=0.46322, acc=0.81250
# [26/100] testing 87.7% loss=0.22031, acc=0.92188
# [26/100] testing 88.6% loss=0.25290, acc=0.92188
# [26/100] testing 89.5% loss=0.41253, acc=0.79688
# [26/100] testing 89.9% loss=0.22765, acc=0.84375
# [26/100] testing 90.8% loss=0.25221, acc=0.89062
# [26/100] testing 91.2% loss=0.14207, acc=0.93750
# [26/100] testing 92.1% loss=0.19855, acc=0.87500
# [26/100] testing 92.6% loss=0.30800, acc=0.85938
# [26/100] testing 93.4% loss=0.34003, acc=0.81250
# [26/100] testing 94.3% loss=0.14774, acc=0.93750
# [26/100] testing 94.7% loss=0.18303, acc=0.93750
# [26/100] testing 95.6% loss=0.37985, acc=0.87500
# [26/100] testing 96.1% loss=0.20987, acc=0.90625
# [26/100] testing 96.9% loss=0.25194, acc=0.90625
# [26/100] testing 97.4% loss=0.19298, acc=0.93750
# [26/100] testing 98.3% loss=0.23368, acc=0.90625
# [26/100] testing 98.7% loss=0.20945, acc=0.92188
# [26/100] testing 99.6% loss=0.31774, acc=0.84375
# [27/100] training 0.2% loss=0.31020, acc=0.87500
# [27/100] training 0.4% loss=0.37182, acc=0.81250
# [27/100] training 0.5% loss=0.16264, acc=0.92188
# [27/100] training 0.8% loss=0.19108, acc=0.93750
# [27/100] training 0.9% loss=0.23560, acc=0.85938
# [27/100] training 1.1% loss=0.33422, acc=0.93750
# [27/100] training 1.2% loss=0.30831, acc=0.92188
# [27/100] training 1.4% loss=0.27446, acc=0.92188
# [27/100] training 1.6% loss=0.16481, acc=0.92188
# [27/100] training 1.8% loss=0.26153, acc=0.95312
# [27/100] training 2.0% loss=0.27294, acc=0.90625
# [27/100] training 2.1% loss=0.25820, acc=0.87500
# [27/100] training 2.3% loss=0.19498, acc=0.95312
# [27/100] training 2.4% loss=0.34378, acc=0.85938
# [27/100] training 2.6% loss=0.19980, acc=0.89062
# [27/100] training 2.7% loss=0.27881, acc=0.89062
# [27/100] training 3.0% loss=0.24845, acc=0.89062
# [27/100] training 3.2% loss=0.17408, acc=0.92188
# [27/100] training 3.3% loss=0.33581, acc=0.87500
# [27/100] training 3.5% loss=0.18076, acc=0.92188
# [27/100] training 3.6% loss=0.29248, acc=0.89062
# [27/100] training 3.8% loss=0.22768, acc=0.90625
# [27/100] training 3.9% loss=0.19549, acc=0.93750
# [27/100] training 4.2% loss=0.11306, acc=0.96875
# [27/100] training 4.4% loss=0.15105, acc=0.93750
# [27/100] training 4.5% loss=0.19512, acc=0.89062
# [27/100] training 4.7% loss=0.26601, acc=0.87500
# [27/100] training 4.8% loss=0.29142, acc=0.92188
# [27/100] training 5.0% loss=0.20297, acc=0.92188
# [27/100] training 5.2% loss=0.29484, acc=0.89062
# [27/100] training 5.4% loss=0.13038, acc=0.96875
# [27/100] training 5.5% loss=0.31693, acc=0.87500
# [27/100] training 5.7% loss=0.25527, acc=0.89062
# [27/100] training 5.9% loss=0.26163, acc=0.87500
# [27/100] training 6.0% loss=0.37163, acc=0.82812
# [27/100] training 6.3% loss=0.21304, acc=0.92188
# [27/100] training 6.4% loss=0.17530, acc=0.93750
# [27/100] training 6.6% loss=0.23495, acc=0.89062
# [27/100] training 6.7% loss=0.33346, acc=0.84375
# [27/100] training 6.9% loss=0.18198, acc=0.93750
# [27/100] training 7.1% loss=0.21658, acc=0.89062
# [27/100] training 7.2% loss=0.31619, acc=0.84375
# [27/100] training 7.5% loss=0.28034, acc=0.85938
# [27/100] training 7.6% loss=0.25496, acc=0.85938
# [27/100] training 7.8% loss=0.21763, acc=0.89062
# [27/100] training 7.9% loss=0.36574, acc=0.85938
# [27/100] training 8.1% loss=0.16718, acc=0.90625
# [27/100] training 8.2% loss=0.23313, acc=0.87500
# [27/100] training 8.4% loss=0.29775, acc=0.87500
# [27/100] training 8.7% loss=0.20982, acc=0.92188
# [27/100] training 8.8% loss=0.32602, acc=0.81250
# [27/100] training 9.0% loss=0.24067, acc=0.89062
# [27/100] training 9.1% loss=0.19106, acc=0.96875
# [27/100] training 9.3% loss=0.34773, acc=0.85938
# [27/100] training 9.4% loss=0.15971, acc=0.95312
# [27/100] training 9.7% loss=0.21297, acc=0.90625
# [27/100] training 9.9% loss=0.32058, acc=0.85938
# [27/100] training 10.0% loss=0.29273, acc=0.90625
# [27/100] training 10.2% loss=0.17544, acc=0.93750
# [27/100] training 10.3% loss=0.20556, acc=0.90625
# [27/100] training 10.5% loss=0.28628, acc=0.90625
# [27/100] training 10.6% loss=0.22772, acc=0.92188
# [27/100] training 10.9% loss=0.15403, acc=0.92188
# [27/100] training 11.0% loss=0.28625, acc=0.87500
# [27/100] training 11.2% loss=0.12869, acc=0.98438
# [27/100] training 11.4% loss=0.21483, acc=0.90625
# [27/100] training 11.5% loss=0.39314, acc=0.85938
# [27/100] training 11.7% loss=0.13171, acc=0.95312
# [27/100] training 11.8% loss=0.25231, acc=0.93750
# [27/100] training 12.1% loss=0.22474, acc=0.90625
# [27/100] training 12.2% loss=0.22120, acc=0.90625
# [27/100] training 12.4% loss=0.26273, acc=0.89062
# [27/100] training 12.6% loss=0.23330, acc=0.90625
# [27/100] training 12.7% loss=0.20673, acc=0.90625
# [27/100] training 12.9% loss=0.25744, acc=0.87500
# [27/100] training 13.0% loss=0.18164, acc=0.95312
# [27/100] training 13.3% loss=0.26573, acc=0.87500
# [27/100] training 13.4% loss=0.22563, acc=0.92188
# [27/100] training 13.6% loss=0.19318, acc=0.93750
# [27/100] training 13.7% loss=0.27821, acc=0.92188
# [27/100] training 13.9% loss=0.25798, acc=0.87500
# [27/100] training 14.1% loss=0.31631, acc=0.92188
# [27/100] training 14.3% loss=0.21497, acc=0.90625
# [27/100] training 14.5% loss=0.28243, acc=0.89062
# [27/100] training 14.6% loss=0.22350, acc=0.95312
# [27/100] training 14.8% loss=0.22194, acc=0.87500
# [27/100] training 14.9% loss=0.15665, acc=0.93750
# [27/100] training 15.1% loss=0.29277, acc=0.87500
# [27/100] training 15.4% loss=0.30802, acc=0.79688
# [27/100] training 15.5% loss=0.31452, acc=0.85938
# [27/100] training 15.7% loss=0.42362, acc=0.87500
# [27/100] training 15.8% loss=0.17963, acc=0.92188
# [27/100] training 16.0% loss=0.26583, acc=0.89062
# [27/100] training 16.1% loss=0.40093, acc=0.87500
# [27/100] training 16.3% loss=0.25653, acc=0.85938
# [27/100] training 16.4% loss=0.20044, acc=0.92188
# [27/100] training 16.7% loss=0.31567, acc=0.85938
# [27/100] training 16.9% loss=0.29297, acc=0.89062
# [27/100] training 17.0% loss=0.27836, acc=0.89062
# [27/100] training 17.2% loss=0.22841, acc=0.87500
# [27/100] training 17.3% loss=0.19947, acc=0.90625
# [27/100] training 17.5% loss=0.24018, acc=0.90625
# [27/100] training 17.7% loss=0.17069, acc=0.93750
# [27/100] training 17.9% loss=0.25574, acc=0.89062
# [27/100] training 18.1% loss=0.30688, acc=0.85938
# [27/100] training 18.2% loss=0.33983, acc=0.85938
# [27/100] training 18.4% loss=0.30618, acc=0.87500
# [27/100] training 18.5% loss=0.21621, acc=0.90625
# [27/100] training 18.8% loss=0.21173, acc=0.92188
# [27/100] training 18.9% loss=0.19762, acc=0.92188
# [27/100] training 19.1% loss=0.32043, acc=0.89062
# [27/100] training 19.2% loss=0.17442, acc=0.89062
# [27/100] training 19.4% loss=0.12718, acc=0.93750
# [27/100] training 19.6% loss=0.32903, acc=0.85938
# [27/100] training 19.7% loss=0.20564, acc=0.90625
# [27/100] training 20.0% loss=0.18583, acc=0.90625
# [27/100] training 20.1% loss=0.26635, acc=0.89062
# [27/100] training 20.3% loss=0.28095, acc=0.90625
# [27/100] training 20.4% loss=0.25312, acc=0.89062
# [27/100] training 20.6% loss=0.24995, acc=0.90625
# [27/100] training 20.8% loss=0.21348, acc=0.89062
# [27/100] training 20.9% loss=0.20070, acc=0.89062
# [27/100] training 21.2% loss=0.37235, acc=0.87500
# [27/100] training 21.3% loss=0.25281, acc=0.90625
# [27/100] training 21.5% loss=0.41009, acc=0.85938
# [27/100] training 21.6% loss=0.13676, acc=0.92188
# [27/100] training 21.8% loss=0.15965, acc=0.95312
# [27/100] training 21.9% loss=0.28527, acc=0.89062
# [27/100] training 22.2% loss=0.29170, acc=0.87500
# [27/100] training 22.4% loss=0.28693, acc=0.89062
# [27/100] training 22.5% loss=0.24618, acc=0.90625
# [27/100] training 22.7% loss=0.20840, acc=0.93750
# [27/100] training 22.8% loss=0.23961, acc=0.93750
# [27/100] training 23.0% loss=0.15291, acc=0.95312
# [27/100] training 23.1% loss=0.30613, acc=0.85938
# [27/100] training 23.4% loss=0.26902, acc=0.84375
# [27/100] training 23.6% loss=0.41499, acc=0.87500
# [27/100] training 23.7% loss=0.24018, acc=0.89062
# [27/100] training 23.9% loss=0.20997, acc=0.92188
# [27/100] training 24.0% loss=0.18582, acc=0.93750
# [27/100] training 24.2% loss=0.20446, acc=0.89062
# [27/100] training 24.3% loss=0.23978, acc=0.92188
# [27/100] training 24.6% loss=0.30543, acc=0.87500
# [27/100] training 24.7% loss=0.34095, acc=0.85938
# [27/100] training 24.9% loss=0.21864, acc=0.90625
# [27/100] training 25.1% loss=0.26838, acc=0.89062
# [27/100] training 25.2% loss=0.15998, acc=0.96875
# [27/100] training 25.4% loss=0.25514, acc=0.89062
# [27/100] training 25.6% loss=0.19516, acc=0.92188
# [27/100] training 25.8% loss=0.23873, acc=0.87500
# [27/100] training 25.9% loss=0.19945, acc=0.90625
# [27/100] training 26.1% loss=0.20584, acc=0.89062
# [27/100] training 26.3% loss=0.15311, acc=0.92188
# [27/100] training 26.4% loss=0.14021, acc=0.95312
# [27/100] training 26.6% loss=0.12844, acc=0.93750
# [27/100] training 26.8% loss=0.25159, acc=0.92188
# [27/100] training 27.0% loss=0.31564, acc=0.87500
# [27/100] training 27.1% loss=0.22275, acc=0.93750
# [27/100] training 27.3% loss=0.29615, acc=0.84375
# [27/100] training 27.4% loss=0.11490, acc=0.98438
# [27/100] training 27.6% loss=0.27192, acc=0.92188
# [27/100] training 27.9% loss=0.19376, acc=0.93750
# [27/100] training 28.0% loss=0.29149, acc=0.85938
# [27/100] training 28.2% loss=0.16484, acc=0.93750
# [27/100] training 28.3% loss=0.11389, acc=0.96875
# [27/100] training 28.5% loss=0.28311, acc=0.89062
# [27/100] training 28.6% loss=0.22476, acc=0.92188
# [27/100] training 28.8% loss=0.10651, acc=0.96875
# [27/100] training 29.1% loss=0.18357, acc=0.90625
# [27/100] training 29.2% loss=0.18867, acc=0.96875
# [27/100] training 29.4% loss=0.31376, acc=0.85938
# [27/100] training 29.5% loss=0.17376, acc=0.92188
# [27/100] training 29.7% loss=0.40998, acc=0.82812
# [27/100] training 29.8% loss=0.18926, acc=0.93750
# [27/100] training 30.0% loss=0.30589, acc=0.82812
# [27/100] training 30.2% loss=0.19107, acc=0.90625
# [27/100] training 30.4% loss=0.18704, acc=0.90625
# [27/100] training 30.6% loss=0.25595, acc=0.89062
# [27/100] training 30.7% loss=0.22895, acc=0.89062
# [27/100] training 30.9% loss=0.34429, acc=0.87500
# [27/100] training 31.0% loss=0.23803, acc=0.87500
# [27/100] training 31.3% loss=0.21275, acc=0.95312
# [27/100] training 31.4% loss=0.35204, acc=0.82812
# [27/100] training 31.6% loss=0.24765, acc=0.92188
# [27/100] training 31.8% loss=0.20937, acc=0.90625
# [27/100] training 31.9% loss=0.26501, acc=0.92188
# [27/100] training 32.1% loss=0.28270, acc=0.87500
# [27/100] training 32.2% loss=0.24404, acc=0.90625
# [27/100] training 32.5% loss=0.15686, acc=0.95312
# [27/100] training 32.6% loss=0.17597, acc=0.93750
# [27/100] training 32.8% loss=0.15772, acc=0.95312
# [27/100] training 32.9% loss=0.29010, acc=0.90625
# [27/100] training 33.1% loss=0.18572, acc=0.90625
# [27/100] training 33.3% loss=0.26626, acc=0.90625
# [27/100] training 33.4% loss=0.21705, acc=0.92188
# [27/100] training 33.7% loss=0.16938, acc=0.90625
# [27/100] training 33.8% loss=0.19897, acc=0.92188
# [27/100] training 34.0% loss=0.24003, acc=0.85938
# [27/100] training 34.1% loss=0.22437, acc=0.92188
# [27/100] training 34.3% loss=0.15158, acc=0.93750
# [27/100] training 34.5% loss=0.32169, acc=0.84375
# [27/100] training 34.7% loss=0.17131, acc=0.90625
# [27/100] training 34.9% loss=0.15635, acc=0.95312
# [27/100] training 35.0% loss=0.22677, acc=0.93750
# [27/100] training 35.2% loss=0.34498, acc=0.87500
# [27/100] training 35.3% loss=0.31086, acc=0.84375
# [27/100] training 35.5% loss=0.21310, acc=0.92188
# [27/100] training 35.6% loss=0.32909, acc=0.82812
# [27/100] training 35.9% loss=0.20097, acc=0.92188
# [27/100] training 36.1% loss=0.27800, acc=0.89062
# [27/100] training 36.2% loss=0.30581, acc=0.89062
# [27/100] training 36.4% loss=0.33779, acc=0.89062
# [27/100] training 36.5% loss=0.27602, acc=0.92188
# [27/100] training 36.7% loss=0.22531, acc=0.93750
# [27/100] training 36.8% loss=0.16431, acc=0.96875
# [27/100] training 37.1% loss=0.29188, acc=0.87500
# [27/100] training 37.3% loss=0.23834, acc=0.93750
# [27/100] training 37.4% loss=0.28192, acc=0.89062
# [27/100] training 37.6% loss=0.19794, acc=0.90625
# [27/100] training 37.7% loss=0.23851, acc=0.93750
# [27/100] training 37.9% loss=0.19181, acc=0.89062
# [27/100] training 38.1% loss=0.26163, acc=0.87500
# [27/100] training 38.3% loss=0.23654, acc=0.92188
# [27/100] training 38.4% loss=0.08847, acc=0.95312
# [27/100] training 38.6% loss=0.19789, acc=0.92188
# [27/100] training 38.8% loss=0.30268, acc=0.84375
# [27/100] training 38.9% loss=0.20445, acc=0.90625
# [27/100] training 39.1% loss=0.23488, acc=0.92188
# [27/100] training 39.3% loss=0.24871, acc=0.89062
# [27/100] training 39.5% loss=0.33671, acc=0.87500
# [27/100] training 39.6% loss=0.30581, acc=0.89062
# [27/100] training 39.8% loss=0.16155, acc=0.92188
# [27/100] training 40.0% loss=0.19120, acc=0.92188
# [27/100] training 40.1% loss=0.29312, acc=0.82812
# [27/100] training 40.4% loss=0.20509, acc=0.93750
# [27/100] training 40.5% loss=0.25370, acc=0.90625
# [27/100] training 40.7% loss=0.17522, acc=0.90625
# [27/100] training 40.8% loss=0.12504, acc=0.96875
# [27/100] training 41.0% loss=0.17711, acc=0.92188
# [27/100] training 41.1% loss=0.32519, acc=0.90625
# [27/100] training 41.3% loss=0.29869, acc=0.84375
# [27/100] training 41.6% loss=0.23786, acc=0.92188
# [27/100] training 41.7% loss=0.33872, acc=0.84375
# [27/100] training 41.9% loss=0.19982, acc=0.92188
# [27/100] training 42.0% loss=0.25774, acc=0.92188
# [27/100] training 42.2% loss=0.25171, acc=0.89062
# [27/100] training 42.3% loss=0.19227, acc=0.93750
# [27/100] training 42.5% loss=0.12893, acc=0.96875
# [27/100] training 42.8% loss=0.17038, acc=0.90625
# [27/100] training 42.9% loss=0.18989, acc=0.92188
# [27/100] training 43.1% loss=0.17578, acc=0.96875
# [27/100] training 43.2% loss=0.13691, acc=0.93750
# [27/100] training 43.4% loss=0.19837, acc=0.92188
# [27/100] training 43.5% loss=0.22993, acc=0.89062
# [27/100] training 43.8% loss=0.29237, acc=0.90625
# [27/100] training 43.9% loss=0.14799, acc=0.93750
# [27/100] training 44.1% loss=0.24316, acc=0.90625
# [27/100] training 44.3% loss=0.17313, acc=0.92188
# [27/100] training 44.4% loss=0.29777, acc=0.82812
# [27/100] training 44.6% loss=0.24924, acc=0.89062
# [27/100] training 44.7% loss=0.36761, acc=0.85938
# [27/100] training 45.0% loss=0.20149, acc=0.90625
# [27/100] training 45.1% loss=0.32459, acc=0.89062
# [27/100] training 45.3% loss=0.26671, acc=0.93750
# [27/100] training 45.5% loss=0.13344, acc=0.96875
# [27/100] training 45.6% loss=0.17168, acc=0.93750
# [27/100] training 45.8% loss=0.15191, acc=0.96875
# [27/100] training 45.9% loss=0.19509, acc=0.93750
# [27/100] training 46.2% loss=0.13442, acc=0.92188
# [27/100] training 46.3% loss=0.11784, acc=0.95312
# [27/100] training 46.5% loss=0.40880, acc=0.87500
# [27/100] training 46.6% loss=0.17275, acc=0.92188
# [27/100] training 46.8% loss=0.23535, acc=0.87500
# [27/100] training 47.0% loss=0.24413, acc=0.90625
# [27/100] training 47.2% loss=0.20516, acc=0.92188
# [27/100] training 47.4% loss=0.18675, acc=0.92188
# [27/100] training 47.5% loss=0.39069, acc=0.87500
# [27/100] training 47.7% loss=0.30272, acc=0.89062
# [27/100] training 47.8% loss=0.32815, acc=0.85938
# [27/100] training 48.0% loss=0.18945, acc=0.95312
# [27/100] training 48.3% loss=0.08362, acc=0.98438
# [27/100] training 48.4% loss=0.08253, acc=0.98438
# [27/100] training 48.6% loss=0.16794, acc=0.95312
# [27/100] training 48.7% loss=0.44104, acc=0.87500
# [27/100] training 48.9% loss=0.22389, acc=0.89062
# [27/100] training 49.0% loss=0.14916, acc=0.92188
# [27/100] training 49.2% loss=0.27340, acc=0.82812
# [27/100] training 49.3% loss=0.19105, acc=0.96875
# [27/100] training 49.6% loss=0.22951, acc=0.92188
# [27/100] training 49.8% loss=0.25112, acc=0.85938
# [27/100] training 49.9% loss=0.24873, acc=0.92188
# [27/100] training 50.1% loss=0.15523, acc=0.93750
# [27/100] training 50.2% loss=0.41434, acc=0.84375
# [27/100] training 50.4% loss=0.36327, acc=0.89062
# [27/100] training 50.6% loss=0.26758, acc=0.87500
# [27/100] training 50.8% loss=0.35840, acc=0.82812
# [27/100] training 51.0% loss=0.29309, acc=0.89062
# [27/100] training 51.1% loss=0.24527, acc=0.85938
# [27/100] training 51.3% loss=0.35123, acc=0.85938
# [27/100] training 51.4% loss=0.24767, acc=0.93750
# [27/100] training 51.7% loss=0.18813, acc=0.95312
# [27/100] training 51.8% loss=0.25968, acc=0.90625
# [27/100] training 52.0% loss=0.15802, acc=0.95312
# [27/100] training 52.1% loss=0.17165, acc=0.87500
# [27/100] training 52.3% loss=0.30757, acc=0.89062
# [27/100] training 52.5% loss=0.12204, acc=0.95312
# [27/100] training 52.6% loss=0.25446, acc=0.84375
# [27/100] training 52.9% loss=0.32721, acc=0.90625
# [27/100] training 53.0% loss=0.09884, acc=0.96875
# [27/100] training 53.2% loss=0.25208, acc=0.90625
# [27/100] training 53.3% loss=0.12377, acc=0.96875
# [27/100] training 53.5% loss=0.15508, acc=0.93750
# [27/100] training 53.7% loss=0.10948, acc=0.96875
# [27/100] training 53.8% loss=0.27589, acc=0.85938
# [27/100] training 54.1% loss=0.31890, acc=0.87500
# [27/100] training 54.2% loss=0.13883, acc=0.95312
# [27/100] training 54.4% loss=0.22743, acc=0.92188
# [27/100] training 54.5% loss=0.29214, acc=0.87500
# [27/100] training 54.7% loss=0.34914, acc=0.81250
# [27/100] training 54.8% loss=0.16988, acc=0.93750
# [27/100] training 55.1% loss=0.16548, acc=0.93750
# [27/100] training 55.3% loss=0.15489, acc=0.98438
# [27/100] training 55.4% loss=0.28519, acc=0.84375
# [27/100] training 55.6% loss=0.31602, acc=0.87500
# [27/100] training 55.7% loss=0.26168, acc=0.85938
# [27/100] training 55.9% loss=0.20217, acc=0.90625
# [27/100] training 56.0% loss=0.19136, acc=0.90625
# [27/100] training 56.3% loss=0.48555, acc=0.79688
# [27/100] training 56.5% loss=0.21913, acc=0.92188
# [27/100] training 56.6% loss=0.22766, acc=0.89062
# [27/100] training 56.8% loss=0.22271, acc=0.90625
# [27/100] training 56.9% loss=0.30157, acc=0.85938
# [27/100] training 57.1% loss=0.32730, acc=0.87500
# [27/100] training 57.2% loss=0.15001, acc=0.93750
# [27/100] training 57.5% loss=0.20099, acc=0.89062
# [27/100] training 57.6% loss=0.22835, acc=0.89062
# [27/100] training 57.8% loss=0.18290, acc=0.96875
# [27/100] training 58.0% loss=0.22204, acc=0.92188
# [27/100] training 58.1% loss=0.19146, acc=0.90625
# [27/100] training 58.3% loss=0.19621, acc=0.93750
# [27/100] training 58.4% loss=0.26928, acc=0.89062
# [27/100] training 58.7% loss=0.26041, acc=0.90625
# [27/100] training 58.8% loss=0.26382, acc=0.87500
# [27/100] training 59.0% loss=0.16283, acc=0.92188
# [27/100] training 59.2% loss=0.16220, acc=0.92188
# [27/100] training 59.3% loss=0.24545, acc=0.87500
# [27/100] training 59.5% loss=0.26181, acc=0.93750
# [27/100] training 59.7% loss=0.22550, acc=0.92188
# [27/100] training 59.9% loss=0.16885, acc=0.96875
# [27/100] training 60.0% loss=0.22884, acc=0.90625
# [27/100] training 60.2% loss=0.15978, acc=0.90625
# [27/100] training 60.3% loss=0.14745, acc=0.93750
# [27/100] training 60.5% loss=0.22825, acc=0.90625
# [27/100] training 60.8% loss=0.16579, acc=0.95312
# [27/100] training 60.9% loss=0.33895, acc=0.87500
# [27/100] training 61.1% loss=0.29624, acc=0.84375
# [27/100] training 61.2% loss=0.26889, acc=0.89062
# [27/100] training 61.4% loss=0.31562, acc=0.85938
# [27/100] training 61.5% loss=0.31654, acc=0.79688
# [27/100] training 61.7% loss=0.25139, acc=0.89062
# [27/100] training 62.0% loss=0.33686, acc=0.85938
# [27/100] training 62.1% loss=0.27138, acc=0.89062
# [27/100] training 62.3% loss=0.15890, acc=0.95312
# [27/100] training 62.4% loss=0.23382, acc=0.87500
# [27/100] training 62.6% loss=0.30408, acc=0.82812
# [27/100] training 62.7% loss=0.20022, acc=0.92188
# [27/100] training 62.9% loss=0.32793, acc=0.85938
# [27/100] training 63.1% loss=0.16689, acc=0.95312
# [27/100] training 63.3% loss=0.20592, acc=0.93750
# [27/100] training 63.5% loss=0.31248, acc=0.87500
# [27/100] training 63.6% loss=0.22260, acc=0.95312
# [27/100] training 63.8% loss=0.27936, acc=0.87500
# [27/100] training 63.9% loss=0.23985, acc=0.90625
# [27/100] training 64.2% loss=0.16155, acc=0.92188
# [27/100] training 64.3% loss=0.26899, acc=0.89062
# [27/100] training 64.5% loss=0.27116, acc=0.87500
# [27/100] training 64.7% loss=0.20759, acc=0.92188
# [27/100] training 64.8% loss=0.31036, acc=0.90625
# [27/100] training 65.0% loss=0.21563, acc=0.93750
# [27/100] training 65.1% loss=0.30776, acc=0.84375
# [27/100] training 65.4% loss=0.16789, acc=0.93750
# [27/100] training 65.5% loss=0.09158, acc=0.96875
# [27/100] training 65.7% loss=0.15279, acc=0.93750
# [27/100] training 65.8% loss=0.30098, acc=0.89062
# [27/100] training 66.0% loss=0.22106, acc=0.90625
# [27/100] training 66.2% loss=0.09613, acc=0.98438
# [27/100] training 66.3% loss=0.41186, acc=0.87500
# [27/100] training 66.6% loss=0.22336, acc=0.89062
# [27/100] training 66.7% loss=0.10305, acc=0.95312
# [27/100] training 66.9% loss=0.27299, acc=0.87500
# [27/100] training 67.0% loss=0.25583, acc=0.89062
# [27/100] training 67.2% loss=0.26244, acc=0.89062
# [27/100] training 67.4% loss=0.22833, acc=0.92188
# [27/100] training 67.6% loss=0.15979, acc=0.93750
# [27/100] training 67.8% loss=0.17159, acc=0.93750
# [27/100] training 67.9% loss=0.13062, acc=0.96875
# [27/100] training 68.1% loss=0.20236, acc=0.92188
# [27/100] training 68.2% loss=0.13563, acc=0.95312
# [27/100] training 68.4% loss=0.23298, acc=0.92188
# [27/100] training 68.5% loss=0.35409, acc=0.84375
# [27/100] training 68.8% loss=0.32839, acc=0.84375
# [27/100] training 69.0% loss=0.32762, acc=0.90625
# [27/100] training 69.1% loss=0.14793, acc=0.93750
# [27/100] training 69.3% loss=0.29197, acc=0.82812
# [27/100] training 69.4% loss=0.28553, acc=0.87500
# [27/100] training 69.6% loss=0.24994, acc=0.90625
# [27/100] training 69.7% loss=0.26150, acc=0.89062
# [27/100] training 70.0% loss=0.31862, acc=0.90625
# [27/100] training 70.2% loss=0.22291, acc=0.93750
# [27/100] training 70.3% loss=0.19183, acc=0.90625
# [27/100] training 70.5% loss=0.26019, acc=0.87500
# [27/100] training 70.6% loss=0.17266, acc=0.93750
# [27/100] training 70.8% loss=0.23869, acc=0.89062
# [27/100] training 71.0% loss=0.22470, acc=0.90625
# [27/100] training 71.2% loss=0.13537, acc=0.92188
# [27/100] training 71.3% loss=0.15251, acc=0.93750
# [27/100] training 71.5% loss=0.24749, acc=0.92188
# [27/100] training 71.7% loss=0.22401, acc=0.92188
# [27/100] training 71.8% loss=0.27852, acc=0.93750
# [27/100] training 72.0% loss=0.13044, acc=0.93750
# [27/100] training 72.2% loss=0.25814, acc=0.89062
# [27/100] training 72.4% loss=0.24921, acc=0.89062
# [27/100] training 72.5% loss=0.26336, acc=0.87500
# [27/100] training 72.7% loss=0.34993, acc=0.84375
# [27/100] training 72.9% loss=0.14470, acc=0.95312
# [27/100] training 73.0% loss=0.15558, acc=0.93750
# [27/100] training 73.3% loss=0.31696, acc=0.87500
# [27/100] training 73.4% loss=0.15985, acc=0.93750
# [27/100] training 73.6% loss=0.08557, acc=0.98438
# [27/100] training 73.7% loss=0.21159, acc=0.89062
# [27/100] training 73.9% loss=0.18276, acc=0.92188
# [27/100] training 74.0% loss=0.32150, acc=0.82812
# [27/100] training 74.2% loss=0.15545, acc=0.93750
# [27/100] training 74.5% loss=0.16051, acc=0.93750
# [27/100] training 74.6% loss=0.40433, acc=0.76562
# [27/100] training 74.8% loss=0.38912, acc=0.81250
# [27/100] training 74.9% loss=0.36366, acc=0.87500
# [27/100] training 75.1% loss=0.24634, acc=0.92188
# [27/100] training 75.2% loss=0.23793, acc=0.89062
# [27/100] training 75.4% loss=0.29850, acc=0.85938
# [27/100] training 75.7% loss=0.26154, acc=0.89062
# [27/100] training 75.8% loss=0.29817, acc=0.85938
# [27/100] training 76.0% loss=0.14582, acc=0.95312
# [27/100] training 76.1% loss=0.39457, acc=0.82812
# [27/100] training 76.3% loss=0.18029, acc=0.93750
# [27/100] training 76.4% loss=0.27754, acc=0.87500
# [27/100] training 76.7% loss=0.20116, acc=0.95312
# [27/100] training 76.8% loss=0.17332, acc=0.93750
# [27/100] training 77.0% loss=0.13115, acc=0.95312
# [27/100] training 77.2% loss=0.18421, acc=0.93750
# [27/100] training 77.3% loss=0.09146, acc=1.00000
# [27/100] training 77.5% loss=0.26882, acc=0.85938
# [27/100] training 77.6% loss=0.20218, acc=0.90625
# [27/100] training 77.9% loss=0.27650, acc=0.92188
# [27/100] training 78.0% loss=0.19955, acc=0.87500
# [27/100] training 78.2% loss=0.23510, acc=0.89062
# [27/100] training 78.4% loss=0.07410, acc=0.98438
# [27/100] training 78.5% loss=0.23588, acc=0.92188
# [27/100] training 78.7% loss=0.24159, acc=0.87500
# [27/100] training 78.8% loss=0.10668, acc=0.98438
# [27/100] training 79.1% loss=0.12372, acc=0.98438
# [27/100] training 79.2% loss=0.18906, acc=0.95312
# [27/100] training 79.4% loss=0.21896, acc=0.90625
# [27/100] training 79.5% loss=0.17667, acc=0.93750
# [27/100] training 79.7% loss=0.06935, acc=0.98438
# [27/100] training 79.9% loss=0.16998, acc=0.92188
# [27/100] training 80.1% loss=0.14134, acc=0.93750
# [27/100] training 80.3% loss=0.32644, acc=0.87500
# [27/100] training 80.4% loss=0.20084, acc=0.92188
# [27/100] training 80.6% loss=0.25737, acc=0.87500
# [27/100] training 80.7% loss=0.28627, acc=0.89062
# [27/100] training 80.9% loss=0.28535, acc=0.92188
# [27/100] training 81.2% loss=0.25299, acc=0.90625
# [27/100] training 81.3% loss=0.22542, acc=0.89062
# [27/100] training 81.5% loss=0.24710, acc=0.89062
# [27/100] training 81.6% loss=0.30465, acc=0.90625
# [27/100] training 81.8% loss=0.19684, acc=0.90625
# [27/100] training 81.9% loss=0.35779, acc=0.84375
# [27/100] training 82.1% loss=0.14502, acc=0.95312
# [27/100] training 82.2% loss=0.21580, acc=0.92188
# [27/100] training 82.5% loss=0.16208, acc=0.95312
# [27/100] training 82.7% loss=0.28232, acc=0.87500
# [27/100] training 82.8% loss=0.19385, acc=0.95312
# [27/100] training 83.0% loss=0.21430, acc=0.90625
# [27/100] training 83.1% loss=0.17262, acc=0.93750
# [27/100] training 83.3% loss=0.13899, acc=0.96875
# [27/100] training 83.5% loss=0.14789, acc=0.93750
# [27/100] training 83.7% loss=0.35021, acc=0.90625
# [27/100] training 83.9% loss=0.24254, acc=0.90625
# [27/100] training 84.0% loss=0.17995, acc=0.92188
# [27/100] training 84.2% loss=0.11465, acc=0.96875
# [27/100] training 84.3% loss=0.18830, acc=0.89062
# [27/100] training 84.5% loss=0.11273, acc=0.96875
# [27/100] training 84.7% loss=0.34700, acc=0.87500
# [27/100] training 84.9% loss=0.15674, acc=0.92188
# [27/100] training 85.0% loss=0.22097, acc=0.90625
# [27/100] training 85.2% loss=0.20362, acc=0.92188
# [27/100] training 85.4% loss=0.19364, acc=0.92188
# [27/100] training 85.5% loss=0.26238, acc=0.87500
# [27/100] training 85.8% loss=0.21945, acc=0.87500
# [27/100] training 85.9% loss=0.20750, acc=0.89062
# [27/100] training 86.1% loss=0.14317, acc=0.93750
# [27/100] training 86.2% loss=0.19599, acc=0.93750
# [27/100] training 86.4% loss=0.25207, acc=0.93750
# [27/100] training 86.6% loss=0.17718, acc=0.92188
# [27/100] training 86.7% loss=0.19843, acc=0.90625
# [27/100] training 87.0% loss=0.31385, acc=0.87500
# [27/100] training 87.1% loss=0.17280, acc=0.93750
# [27/100] training 87.3% loss=0.17076, acc=0.93750
# [27/100] training 87.4% loss=0.16886, acc=0.92188
# [27/100] training 87.6% loss=0.24295, acc=0.90625
# [27/100] training 87.7% loss=0.27423, acc=0.95312
# [27/100] training 87.9% loss=0.28590, acc=0.87500
# [27/100] training 88.2% loss=0.20766, acc=0.89062
# [27/100] training 88.3% loss=0.23788, acc=0.92188
# [27/100] training 88.5% loss=0.26725, acc=0.89062
# [27/100] training 88.6% loss=0.14334, acc=0.95312
# [27/100] training 88.8% loss=0.21999, acc=0.92188
# [27/100] training 88.9% loss=0.23269, acc=0.92188
# [27/100] training 89.2% loss=0.16010, acc=0.96875
# [27/100] training 89.4% loss=0.24893, acc=0.89062
# [27/100] training 89.5% loss=0.28635, acc=0.90625
# [27/100] training 89.7% loss=0.26138, acc=0.85938
# [27/100] training 89.8% loss=0.15754, acc=0.96875
# [27/100] training 90.0% loss=0.12065, acc=0.96875
# [27/100] training 90.1% loss=0.22762, acc=0.92188
# [27/100] training 90.4% loss=0.18469, acc=0.93750
# [27/100] training 90.5% loss=0.13444, acc=0.93750
# [27/100] training 90.7% loss=0.15290, acc=0.92188
# [27/100] training 90.9% loss=0.17160, acc=0.95312
# [27/100] training 91.0% loss=0.14295, acc=0.95312
# [27/100] training 91.2% loss=0.21629, acc=0.92188
# [27/100] training 91.3% loss=0.40926, acc=0.85938
# [27/100] training 91.6% loss=0.22010, acc=0.87500
# [27/100] training 91.7% loss=0.21369, acc=0.89062
# [27/100] training 91.9% loss=0.29782, acc=0.89062
# [27/100] training 92.1% loss=0.24913, acc=0.87500
# [27/100] training 92.2% loss=0.16113, acc=0.90625
# [27/100] training 92.4% loss=0.26650, acc=0.87500
# [27/100] training 92.6% loss=0.39438, acc=0.81250
# [27/100] training 92.8% loss=0.23253, acc=0.93750
# [27/100] training 92.9% loss=0.19764, acc=0.92188
# [27/100] training 93.1% loss=0.36607, acc=0.82812
# [27/100] training 93.2% loss=0.22850, acc=0.93750
# [27/100] training 93.4% loss=0.23452, acc=0.89062
# [27/100] training 93.7% loss=0.16304, acc=0.93750
# [27/100] training 93.8% loss=0.21024, acc=0.90625
# [27/100] training 94.0% loss=0.19203, acc=0.93750
# [27/100] training 94.1% loss=0.26402, acc=0.89062
# [27/100] training 94.3% loss=0.12197, acc=0.98438
# [27/100] training 94.4% loss=0.22224, acc=0.92188
# [27/100] training 94.6% loss=0.16331, acc=0.96875
# [27/100] training 94.9% loss=0.15766, acc=0.93750
# [27/100] training 95.0% loss=0.32543, acc=0.90625
# [27/100] training 95.2% loss=0.30819, acc=0.87500
# [27/100] training 95.3% loss=0.21471, acc=0.92188
# [27/100] training 95.5% loss=0.21587, acc=0.93750
# [27/100] training 95.6% loss=0.34457, acc=0.85938
# [27/100] training 95.8% loss=0.20856, acc=0.89062
# [27/100] training 96.0% loss=0.21937, acc=0.90625
# [27/100] training 96.2% loss=0.18396, acc=0.93750
# [27/100] training 96.4% loss=0.24200, acc=0.90625
# [27/100] training 96.5% loss=0.23427, acc=0.89062
# [27/100] training 96.7% loss=0.14125, acc=0.93750
# [27/100] training 96.8% loss=0.23175, acc=0.90625
# [27/100] training 97.1% loss=0.17909, acc=0.92188
# [27/100] training 97.2% loss=0.28306, acc=0.92188
# [27/100] training 97.4% loss=0.19744, acc=0.95312
# [27/100] training 97.6% loss=0.23233, acc=0.92188
# [27/100] training 97.7% loss=0.14033, acc=0.95312
# [27/100] training 97.9% loss=0.18766, acc=0.92188
# [27/100] training 98.0% loss=0.16047, acc=0.93750
# [27/100] training 98.3% loss=0.19859, acc=0.95312
# [27/100] training 98.4% loss=0.17744, acc=0.93750
# [27/100] training 98.6% loss=0.45833, acc=0.81250
# [27/100] training 98.7% loss=0.28881, acc=0.90625
# [27/100] training 98.9% loss=0.17412, acc=0.90625
# [27/100] training 99.1% loss=0.20204, acc=0.90625
# [27/100] training 99.2% loss=0.23748, acc=0.85938
# [27/100] training 99.5% loss=0.26640, acc=0.87500
# [27/100] training 99.6% loss=0.21895, acc=0.89062
# [27/100] training 99.8% loss=0.16851, acc=0.93750
# [27/100] training 99.9% loss=0.12860, acc=0.95312
# [27/100] testing 0.9% loss=0.16013, acc=0.90625
# [27/100] testing 1.8% loss=0.44247, acc=0.82812
# [27/100] testing 2.2% loss=0.21049, acc=0.92188
# [27/100] testing 3.1% loss=0.35124, acc=0.87500
# [27/100] testing 3.5% loss=0.15683, acc=0.93750
# [27/100] testing 4.4% loss=0.30391, acc=0.85938
# [27/100] testing 4.8% loss=0.38559, acc=0.85938
# [27/100] testing 5.7% loss=0.18426, acc=0.92188
# [27/100] testing 6.6% loss=0.24781, acc=0.89062
# [27/100] testing 7.0% loss=0.16135, acc=0.93750
# [27/100] testing 7.9% loss=0.28445, acc=0.84375
# [27/100] testing 8.3% loss=0.31493, acc=0.92188
# [27/100] testing 9.2% loss=0.33545, acc=0.89062
# [27/100] testing 9.7% loss=0.13051, acc=0.92188
# [27/100] testing 10.5% loss=0.35966, acc=0.82812
# [27/100] testing 11.0% loss=0.29044, acc=0.90625
# [27/100] testing 11.8% loss=0.31324, acc=0.89062
# [27/100] testing 12.7% loss=0.35739, acc=0.84375
# [27/100] testing 13.2% loss=0.30371, acc=0.89062
# [27/100] testing 14.0% loss=0.40883, acc=0.90625
# [27/100] testing 14.5% loss=0.24431, acc=0.90625
# [27/100] testing 15.4% loss=0.26041, acc=0.89062
# [27/100] testing 15.8% loss=0.23552, acc=0.87500
# [27/100] testing 16.7% loss=0.21820, acc=0.89062
# [27/100] testing 17.5% loss=0.21028, acc=0.89062
# [27/100] testing 18.0% loss=0.23121, acc=0.87500
# [27/100] testing 18.9% loss=0.17062, acc=0.93750
# [27/100] testing 19.3% loss=0.35824, acc=0.90625
# [27/100] testing 20.2% loss=0.30774, acc=0.90625
# [27/100] testing 20.6% loss=0.38625, acc=0.85938
# [27/100] testing 21.5% loss=0.35539, acc=0.84375
# [27/100] testing 21.9% loss=0.36836, acc=0.85938
# [27/100] testing 22.8% loss=0.37796, acc=0.89062
# [27/100] testing 23.7% loss=0.37848, acc=0.85938
# [27/100] testing 24.1% loss=0.23837, acc=0.90625
# [27/100] testing 25.0% loss=0.36792, acc=0.90625
# [27/100] testing 25.4% loss=0.12473, acc=0.95312
# [27/100] testing 26.3% loss=0.35627, acc=0.87500
# [27/100] testing 26.8% loss=0.23338, acc=0.89062
# [27/100] testing 27.6% loss=0.27695, acc=0.87500
# [27/100] testing 28.5% loss=0.27242, acc=0.87500
# [27/100] testing 29.0% loss=0.23450, acc=0.93750
# [27/100] testing 29.8% loss=0.38525, acc=0.89062
# [27/100] testing 30.3% loss=0.20159, acc=0.89062
# [27/100] testing 31.1% loss=0.32658, acc=0.85938
# [27/100] testing 31.6% loss=0.31425, acc=0.87500
# [27/100] testing 32.5% loss=0.32598, acc=0.87500
# [27/100] testing 32.9% loss=0.54595, acc=0.84375
# [27/100] testing 33.8% loss=0.27481, acc=0.87500
# [27/100] testing 34.7% loss=0.38386, acc=0.87500
# [27/100] testing 35.1% loss=0.29319, acc=0.89062
# [27/100] testing 36.0% loss=0.34777, acc=0.89062
# [27/100] testing 36.4% loss=0.27170, acc=0.92188
# [27/100] testing 37.3% loss=0.33831, acc=0.89062
# [27/100] testing 37.7% loss=0.41125, acc=0.84375
# [27/100] testing 38.6% loss=0.22357, acc=0.92188
# [27/100] testing 39.5% loss=0.42931, acc=0.82812
# [27/100] testing 39.9% loss=0.30892, acc=0.89062
# [27/100] testing 40.8% loss=0.35294, acc=0.89062
# [27/100] testing 41.2% loss=0.27940, acc=0.90625
# [27/100] testing 42.1% loss=0.33308, acc=0.90625
# [27/100] testing 42.5% loss=0.21616, acc=0.90625
# [27/100] testing 43.4% loss=0.30757, acc=0.89062
# [27/100] testing 43.9% loss=0.13164, acc=0.93750
# [27/100] testing 44.7% loss=0.44085, acc=0.82812
# [27/100] testing 45.6% loss=0.35098, acc=0.87500
# [27/100] testing 46.1% loss=0.31882, acc=0.84375
# [27/100] testing 46.9% loss=0.22465, acc=0.92188
# [27/100] testing 47.4% loss=0.14953, acc=0.93750
# [27/100] testing 48.3% loss=0.49157, acc=0.81250
# [27/100] testing 48.7% loss=0.54669, acc=0.79688
# [27/100] testing 49.6% loss=0.45641, acc=0.82812
# [27/100] testing 50.4% loss=0.24429, acc=0.92188
# [27/100] testing 50.9% loss=0.28639, acc=0.87500
# [27/100] testing 51.8% loss=0.30031, acc=0.85938
# [27/100] testing 52.2% loss=0.31430, acc=0.90625
# [27/100] testing 53.1% loss=0.20515, acc=0.92188
# [27/100] testing 53.5% loss=0.32269, acc=0.89062
# [27/100] testing 54.4% loss=0.38582, acc=0.81250
# [27/100] testing 54.8% loss=0.31603, acc=0.89062
# [27/100] testing 55.7% loss=0.18672, acc=0.95312
# [27/100] testing 56.6% loss=0.38047, acc=0.90625
# [27/100] testing 57.0% loss=0.40380, acc=0.87500
# [27/100] testing 57.9% loss=0.30644, acc=0.89062
# [27/100] testing 58.3% loss=0.38401, acc=0.85938
# [27/100] testing 59.2% loss=0.28980, acc=0.85938
# [27/100] testing 59.7% loss=0.27772, acc=0.87500
# [27/100] testing 60.5% loss=0.41654, acc=0.82812
# [27/100] testing 61.4% loss=0.13758, acc=0.93750
# [27/100] testing 61.9% loss=0.23529, acc=0.92188
# [27/100] testing 62.7% loss=0.37832, acc=0.90625
# [27/100] testing 63.2% loss=0.38892, acc=0.85938
# [27/100] testing 64.0% loss=0.41951, acc=0.85938
# [27/100] testing 64.5% loss=0.19374, acc=0.89062
# [27/100] testing 65.4% loss=0.27281, acc=0.92188
# [27/100] testing 65.8% loss=0.46192, acc=0.84375
# [27/100] testing 66.7% loss=0.21847, acc=0.89062
# [27/100] testing 67.6% loss=0.48443, acc=0.87500
# [27/100] testing 68.0% loss=0.13352, acc=0.96875
# [27/100] testing 68.9% loss=0.27765, acc=0.89062
# [27/100] testing 69.3% loss=0.43018, acc=0.82812
# [27/100] testing 70.2% loss=0.49016, acc=0.84375
# [27/100] testing 70.6% loss=0.40573, acc=0.82812
# [27/100] testing 71.5% loss=0.30413, acc=0.90625
# [27/100] testing 72.4% loss=0.22545, acc=0.92188
# [27/100] testing 72.8% loss=0.25906, acc=0.90625
# [27/100] testing 73.7% loss=0.27338, acc=0.87500
# [27/100] testing 74.1% loss=0.40991, acc=0.85938
# [27/100] testing 75.0% loss=0.23855, acc=0.90625
# [27/100] testing 75.4% loss=0.41209, acc=0.89062
# [27/100] testing 76.3% loss=0.07238, acc=0.98438
# [27/100] testing 76.8% loss=0.26048, acc=0.84375
# [27/100] testing 77.6% loss=0.25752, acc=0.85938
# [27/100] testing 78.5% loss=0.53274, acc=0.84375
# [27/100] testing 79.0% loss=0.29966, acc=0.85938
# [27/100] testing 79.8% loss=0.22814, acc=0.87500
# [27/100] testing 80.3% loss=0.36732, acc=0.82812
# [27/100] testing 81.2% loss=0.51829, acc=0.84375
# [27/100] testing 81.6% loss=0.18450, acc=0.95312
# [27/100] testing 82.5% loss=0.23138, acc=0.89062
# [27/100] testing 83.3% loss=0.21489, acc=0.93750
# [27/100] testing 83.8% loss=0.19081, acc=0.90625
# [27/100] testing 84.7% loss=0.37815, acc=0.84375
# [27/100] testing 85.1% loss=0.30042, acc=0.85938
# [27/100] testing 86.0% loss=0.30305, acc=0.89062
# [27/100] testing 86.4% loss=0.39433, acc=0.85938
# [27/100] testing 87.3% loss=0.42847, acc=0.79688
# [27/100] testing 87.7% loss=0.17866, acc=0.95312
# [27/100] testing 88.6% loss=0.21592, acc=0.93750
# [27/100] testing 89.5% loss=0.44750, acc=0.79688
# [27/100] testing 89.9% loss=0.25191, acc=0.87500
# [27/100] testing 90.8% loss=0.28188, acc=0.92188
# [27/100] testing 91.2% loss=0.08887, acc=0.96875
# [27/100] testing 92.1% loss=0.57831, acc=0.79688
# [27/100] testing 92.6% loss=0.35628, acc=0.87500
# [27/100] testing 93.4% loss=0.38576, acc=0.87500
# [27/100] testing 94.3% loss=0.25954, acc=0.85938
# [27/100] testing 94.7% loss=0.24146, acc=0.87500
# [27/100] testing 95.6% loss=0.39514, acc=0.89062
# [27/100] testing 96.1% loss=0.19132, acc=0.92188
# [27/100] testing 96.9% loss=0.24736, acc=0.85938
# [27/100] testing 97.4% loss=0.22816, acc=0.95312
# [27/100] testing 98.3% loss=0.26259, acc=0.85938
# [27/100] testing 98.7% loss=0.25021, acc=0.89062
# [27/100] testing 99.6% loss=0.31602, acc=0.84375
# [28/100] training 0.2% loss=0.38629, acc=0.84375
# [28/100] training 0.4% loss=0.40555, acc=0.85938
# [28/100] training 0.5% loss=0.20218, acc=0.93750
# [28/100] training 0.8% loss=0.21080, acc=0.92188
# [28/100] training 0.9% loss=0.24992, acc=0.90625
# [28/100] training 1.1% loss=0.27528, acc=0.90625
# [28/100] training 1.2% loss=0.37355, acc=0.85938
# [28/100] training 1.4% loss=0.20836, acc=0.93750
# [28/100] training 1.6% loss=0.15120, acc=0.95312
# [28/100] training 1.8% loss=0.25580, acc=0.89062
# [28/100] training 2.0% loss=0.21732, acc=0.90625
# [28/100] training 2.1% loss=0.29494, acc=0.87500
# [28/100] training 2.3% loss=0.10936, acc=0.98438
# [28/100] training 2.4% loss=0.28537, acc=0.89062
# [28/100] training 2.6% loss=0.16944, acc=0.95312
# [28/100] training 2.7% loss=0.21443, acc=0.93750
# [28/100] training 3.0% loss=0.23818, acc=0.95312
# [28/100] training 3.2% loss=0.26095, acc=0.92188
# [28/100] training 3.3% loss=0.37224, acc=0.85938
# [28/100] training 3.5% loss=0.21327, acc=0.90625
# [28/100] training 3.6% loss=0.39092, acc=0.84375
# [28/100] training 3.8% loss=0.24055, acc=0.89062
# [28/100] training 3.9% loss=0.24438, acc=0.89062
# [28/100] training 4.2% loss=0.15715, acc=0.93750
# [28/100] training 4.4% loss=0.15395, acc=0.95312
# [28/100] training 4.5% loss=0.19125, acc=0.90625
# [28/100] training 4.7% loss=0.32757, acc=0.85938
# [28/100] training 4.8% loss=0.25611, acc=0.92188
# [28/100] training 5.0% loss=0.17985, acc=0.93750
# [28/100] training 5.2% loss=0.24311, acc=0.92188
# [28/100] training 5.4% loss=0.16656, acc=0.92188
# [28/100] training 5.5% loss=0.24402, acc=0.90625
# [28/100] training 5.7% loss=0.17171, acc=0.93750
# [28/100] training 5.9% loss=0.26351, acc=0.89062
# [28/100] training 6.0% loss=0.26927, acc=0.85938
# [28/100] training 6.3% loss=0.21832, acc=0.89062
# [28/100] training 6.4% loss=0.17735, acc=0.92188
# [28/100] training 6.6% loss=0.26096, acc=0.89062
# [28/100] training 6.7% loss=0.38688, acc=0.82812
# [28/100] training 6.9% loss=0.10418, acc=0.95312
# [28/100] training 7.1% loss=0.16255, acc=0.95312
# [28/100] training 7.2% loss=0.23839, acc=0.89062
# [28/100] training 7.5% loss=0.19749, acc=0.90625
# [28/100] training 7.6% loss=0.25688, acc=0.89062
# [28/100] training 7.8% loss=0.29686, acc=0.85938
# [28/100] training 7.9% loss=0.37934, acc=0.85938
# [28/100] training 8.1% loss=0.15395, acc=0.95312
# [28/100] training 8.2% loss=0.23793, acc=0.90625
# [28/100] training 8.4% loss=0.24849, acc=0.87500
# [28/100] training 8.7% loss=0.22667, acc=0.93750
# [28/100] training 8.8% loss=0.23501, acc=0.89062
# [28/100] training 9.0% loss=0.21653, acc=0.89062
# [28/100] training 9.1% loss=0.24689, acc=0.93750
# [28/100] training 9.3% loss=0.33144, acc=0.89062
# [28/100] training 9.4% loss=0.15696, acc=0.93750
# [28/100] training 9.7% loss=0.26874, acc=0.90625
# [28/100] training 9.9% loss=0.29443, acc=0.89062
# [28/100] training 10.0% loss=0.26853, acc=0.87500
# [28/100] training 10.2% loss=0.17317, acc=0.96875
# [28/100] training 10.3% loss=0.15755, acc=0.92188
# [28/100] training 10.5% loss=0.28082, acc=0.90625
# [28/100] training 10.6% loss=0.28557, acc=0.89062
# [28/100] training 10.9% loss=0.18456, acc=0.90625
# [28/100] training 11.0% loss=0.20719, acc=0.93750
# [28/100] training 11.2% loss=0.18803, acc=0.90625
# [28/100] training 11.4% loss=0.25073, acc=0.90625
# [28/100] training 11.5% loss=0.34267, acc=0.87500
# [28/100] training 11.7% loss=0.10305, acc=0.98438
# [28/100] training 11.8% loss=0.17957, acc=0.93750
# [28/100] training 12.1% loss=0.20503, acc=0.92188
# [28/100] training 12.2% loss=0.18479, acc=0.95312
# [28/100] training 12.4% loss=0.21383, acc=0.89062
# [28/100] training 12.6% loss=0.24216, acc=0.90625
# [28/100] training 12.7% loss=0.22469, acc=0.92188
# [28/100] training 12.9% loss=0.31049, acc=0.87500
# [28/100] training 13.0% loss=0.21968, acc=0.95312
# [28/100] training 13.3% loss=0.15372, acc=0.93750
# [28/100] training 13.4% loss=0.24789, acc=0.87500
# [28/100] training 13.6% loss=0.19511, acc=0.89062
# [28/100] training 13.7% loss=0.32503, acc=0.85938
# [28/100] training 13.9% loss=0.20716, acc=0.93750
# [28/100] training 14.1% loss=0.20957, acc=0.92188
# [28/100] training 14.3% loss=0.18051, acc=0.93750
# [28/100] training 14.5% loss=0.26810, acc=0.92188
# [28/100] training 14.6% loss=0.19991, acc=0.95312
# [28/100] training 14.8% loss=0.31799, acc=0.85938
# [28/100] training 14.9% loss=0.15710, acc=0.93750
# [28/100] training 15.1% loss=0.37482, acc=0.82812
# [28/100] training 15.4% loss=0.27002, acc=0.90625
# [28/100] training 15.5% loss=0.29829, acc=0.90625
# [28/100] training 15.7% loss=0.35372, acc=0.89062
# [28/100] training 15.8% loss=0.18859, acc=0.93750
# [28/100] training 16.0% loss=0.28819, acc=0.89062
# [28/100] training 16.1% loss=0.41363, acc=0.82812
# [28/100] training 16.3% loss=0.28134, acc=0.87500
# [28/100] training 16.4% loss=0.19610, acc=0.92188
# [28/100] training 16.7% loss=0.28807, acc=0.85938
# [28/100] training 16.9% loss=0.29782, acc=0.90625
# [28/100] training 17.0% loss=0.25066, acc=0.89062
# [28/100] training 17.2% loss=0.28810, acc=0.89062
# [28/100] training 17.3% loss=0.22558, acc=0.92188
# [28/100] training 17.5% loss=0.22973, acc=0.90625
# [28/100] training 17.7% loss=0.18745, acc=0.90625
# [28/100] training 17.9% loss=0.28943, acc=0.89062
# [28/100] training 18.1% loss=0.30990, acc=0.85938
# [28/100] training 18.2% loss=0.23466, acc=0.90625
# [28/100] training 18.4% loss=0.26970, acc=0.89062
# [28/100] training 18.5% loss=0.20662, acc=0.95312
# [28/100] training 18.8% loss=0.16709, acc=0.95312
# [28/100] training 18.9% loss=0.09451, acc=0.96875
# [28/100] training 19.1% loss=0.24507, acc=0.89062
# [28/100] training 19.2% loss=0.15017, acc=0.92188
# [28/100] training 19.4% loss=0.18968, acc=0.90625
# [28/100] training 19.6% loss=0.27351, acc=0.87500
# [28/100] training 19.7% loss=0.31379, acc=0.85938
# [28/100] training 20.0% loss=0.14741, acc=0.93750
# [28/100] training 20.1% loss=0.23606, acc=0.92188
# [28/100] training 20.3% loss=0.24476, acc=0.92188
# [28/100] training 20.4% loss=0.24531, acc=0.87500
# [28/100] training 20.6% loss=0.33491, acc=0.85938
# [28/100] training 20.8% loss=0.18054, acc=0.93750
# [28/100] training 20.9% loss=0.18440, acc=0.92188
# [28/100] training 21.2% loss=0.42009, acc=0.87500
# [28/100] training 21.3% loss=0.31978, acc=0.90625
# [28/100] training 21.5% loss=0.28056, acc=0.92188
# [28/100] training 21.6% loss=0.11465, acc=0.96875
# [28/100] training 21.8% loss=0.24554, acc=0.89062
# [28/100] training 21.9% loss=0.31083, acc=0.85938
# [28/100] training 22.2% loss=0.17675, acc=0.92188
# [28/100] training 22.4% loss=0.35221, acc=0.81250
# [28/100] training 22.5% loss=0.21932, acc=0.89062
# [28/100] training 22.7% loss=0.21815, acc=0.90625
# [28/100] training 22.8% loss=0.23175, acc=0.89062
# [28/100] training 23.0% loss=0.12635, acc=0.96875
# [28/100] training 23.1% loss=0.25333, acc=0.92188
# [28/100] training 23.4% loss=0.23820, acc=0.89062
# [28/100] training 23.6% loss=0.36925, acc=0.89062
# [28/100] training 23.7% loss=0.28763, acc=0.89062
# [28/100] training 23.9% loss=0.19902, acc=0.92188
# [28/100] training 24.0% loss=0.22800, acc=0.90625
# [28/100] training 24.2% loss=0.24595, acc=0.89062
# [28/100] training 24.3% loss=0.30321, acc=0.92188
# [28/100] training 24.6% loss=0.21183, acc=0.84375
# [28/100] training 24.7% loss=0.33547, acc=0.87500
# [28/100] training 24.9% loss=0.17975, acc=0.92188
# [28/100] training 25.1% loss=0.22890, acc=0.87500
# [28/100] training 25.2% loss=0.19973, acc=0.93750
# [28/100] training 25.4% loss=0.22993, acc=0.93750
# [28/100] training 25.6% loss=0.16879, acc=0.95312
# [28/100] training 25.8% loss=0.21127, acc=0.92188
# [28/100] training 25.9% loss=0.20062, acc=0.92188
# [28/100] training 26.1% loss=0.20996, acc=0.92188
# [28/100] training 26.3% loss=0.22057, acc=0.90625
# [28/100] training 26.4% loss=0.23447, acc=0.90625
# [28/100] training 26.6% loss=0.08091, acc=0.98438
# [28/100] training 26.8% loss=0.30373, acc=0.87500
# [28/100] training 27.0% loss=0.22109, acc=0.89062
# [28/100] training 27.1% loss=0.13641, acc=0.95312
# [28/100] training 27.3% loss=0.15761, acc=0.93750
# [28/100] training 27.4% loss=0.18048, acc=0.95312
# [28/100] training 27.6% loss=0.37240, acc=0.84375
# [28/100] training 27.9% loss=0.14898, acc=0.95312
# [28/100] training 28.0% loss=0.37545, acc=0.84375
# [28/100] training 28.2% loss=0.13300, acc=0.95312
# [28/100] training 28.3% loss=0.09581, acc=0.98438
# [28/100] training 28.5% loss=0.24321, acc=0.92188
# [28/100] training 28.6% loss=0.22634, acc=0.92188
# [28/100] training 28.8% loss=0.15649, acc=0.93750
# [28/100] training 29.1% loss=0.16462, acc=0.93750
# [28/100] training 29.2% loss=0.14597, acc=0.95312
# [28/100] training 29.4% loss=0.34153, acc=0.82812
# [28/100] training 29.5% loss=0.08901, acc=0.96875
# [28/100] training 29.7% loss=0.23760, acc=0.87500
# [28/100] training 29.8% loss=0.15682, acc=0.92188
# [28/100] training 30.0% loss=0.31983, acc=0.84375
# [28/100] training 30.2% loss=0.15445, acc=0.92188
# [28/100] training 30.4% loss=0.17117, acc=0.93750
# [28/100] training 30.6% loss=0.29900, acc=0.85938
# [28/100] training 30.7% loss=0.18837, acc=0.92188
# [28/100] training 30.9% loss=0.34298, acc=0.89062
# [28/100] training 31.0% loss=0.13088, acc=0.96875
# [28/100] training 31.3% loss=0.22058, acc=0.90625
# [28/100] training 31.4% loss=0.44324, acc=0.79688
# [28/100] training 31.6% loss=0.25819, acc=0.92188
# [28/100] training 31.8% loss=0.23187, acc=0.87500
# [28/100] training 31.9% loss=0.25249, acc=0.84375
# [28/100] training 32.1% loss=0.23651, acc=0.89062
# [28/100] training 32.2% loss=0.23168, acc=0.92188
# [28/100] training 32.5% loss=0.12987, acc=0.95312
# [28/100] training 32.6% loss=0.29508, acc=0.87500
# [28/100] training 32.8% loss=0.12837, acc=0.95312
# [28/100] training 32.9% loss=0.28626, acc=0.89062
# [28/100] training 33.1% loss=0.22632, acc=0.87500
# [28/100] training 33.3% loss=0.25657, acc=0.89062
# [28/100] training 33.4% loss=0.18518, acc=0.95312
# [28/100] training 33.7% loss=0.24751, acc=0.90625
# [28/100] training 33.8% loss=0.21006, acc=0.90625
# [28/100] training 34.0% loss=0.21992, acc=0.90625
# [28/100] training 34.1% loss=0.27455, acc=0.89062
# [28/100] training 34.3% loss=0.17959, acc=0.90625
# [28/100] training 34.5% loss=0.25075, acc=0.89062
# [28/100] training 34.7% loss=0.11654, acc=0.93750
# [28/100] training 34.9% loss=0.15519, acc=0.93750
# [28/100] training 35.0% loss=0.20134, acc=0.92188
# [28/100] training 35.2% loss=0.34236, acc=0.79688
# [28/100] training 35.3% loss=0.28083, acc=0.89062
# [28/100] training 35.5% loss=0.20424, acc=0.93750
# [28/100] training 35.6% loss=0.31378, acc=0.87500
# [28/100] training 35.9% loss=0.23640, acc=0.85938
# [28/100] training 36.1% loss=0.31431, acc=0.85938
# [28/100] training 36.2% loss=0.21535, acc=0.90625
# [28/100] training 36.4% loss=0.30223, acc=0.90625
# [28/100] training 36.5% loss=0.24904, acc=0.93750
# [28/100] training 36.7% loss=0.26479, acc=0.89062
# [28/100] training 36.8% loss=0.12070, acc=0.95312
# [28/100] training 37.1% loss=0.25157, acc=0.89062
# [28/100] training 37.3% loss=0.22540, acc=0.92188
# [28/100] training 37.4% loss=0.18011, acc=0.92188
# [28/100] training 37.6% loss=0.18717, acc=0.90625
# [28/100] training 37.7% loss=0.29495, acc=0.87500
# [28/100] training 37.9% loss=0.19046, acc=0.90625
# [28/100] training 38.1% loss=0.17634, acc=0.92188
# [28/100] training 38.3% loss=0.23694, acc=0.89062
# [28/100] training 38.4% loss=0.09143, acc=0.95312
# [28/100] training 38.6% loss=0.25031, acc=0.90625
# [28/100] training 38.8% loss=0.33021, acc=0.87500
# [28/100] training 38.9% loss=0.29884, acc=0.87500
# [28/100] training 39.1% loss=0.21874, acc=0.90625
# [28/100] training 39.3% loss=0.23738, acc=0.85938
# [28/100] training 39.5% loss=0.32199, acc=0.87500
# [28/100] training 39.6% loss=0.31117, acc=0.90625
# [28/100] training 39.8% loss=0.14423, acc=0.95312
# [28/100] training 40.0% loss=0.20220, acc=0.89062
# [28/100] training 40.1% loss=0.27189, acc=0.89062
# [28/100] training 40.4% loss=0.22757, acc=0.89062
# [28/100] training 40.5% loss=0.24766, acc=0.90625
# [28/100] training 40.7% loss=0.23404, acc=0.92188
# [28/100] training 40.8% loss=0.13952, acc=0.95312
# [28/100] training 41.0% loss=0.19562, acc=0.95312
# [28/100] training 41.1% loss=0.35580, acc=0.85938
# [28/100] training 41.3% loss=0.31050, acc=0.84375
# [28/100] training 41.6% loss=0.27176, acc=0.92188
# [28/100] training 41.7% loss=0.31864, acc=0.89062
# [28/100] training 41.9% loss=0.19945, acc=0.95312
# [28/100] training 42.0% loss=0.26215, acc=0.89062
# [28/100] training 42.2% loss=0.29262, acc=0.89062
# [28/100] training 42.3% loss=0.22801, acc=0.89062
# [28/100] training 42.5% loss=0.16286, acc=0.98438
# [28/100] training 42.8% loss=0.11375, acc=0.95312
# [28/100] training 42.9% loss=0.16981, acc=0.92188
# [28/100] training 43.1% loss=0.17970, acc=0.95312
# [28/100] training 43.2% loss=0.12899, acc=0.95312
# [28/100] training 43.4% loss=0.25867, acc=0.92188
# [28/100] training 43.5% loss=0.21584, acc=0.87500
# [28/100] training 43.8% loss=0.27379, acc=0.89062
# [28/100] training 43.9% loss=0.22664, acc=0.93750
# [28/100] training 44.1% loss=0.15969, acc=0.92188
# [28/100] training 44.3% loss=0.22679, acc=0.90625
# [28/100] training 44.4% loss=0.24309, acc=0.85938
# [28/100] training 44.6% loss=0.19495, acc=0.92188
# [28/100] training 44.7% loss=0.34529, acc=0.85938
# [28/100] training 45.0% loss=0.11943, acc=0.93750
# [28/100] training 45.1% loss=0.38447, acc=0.84375
# [28/100] training 45.3% loss=0.29577, acc=0.85938
# [28/100] training 45.5% loss=0.09252, acc=0.98438
# [28/100] training 45.6% loss=0.20433, acc=0.95312
# [28/100] training 45.8% loss=0.11091, acc=0.96875
# [28/100] training 45.9% loss=0.25096, acc=0.82812
# [28/100] training 46.2% loss=0.13357, acc=0.93750
# [28/100] training 46.3% loss=0.12913, acc=0.92188
# [28/100] training 46.5% loss=0.43417, acc=0.85938
# [28/100] training 46.6% loss=0.18919, acc=0.90625
# [28/100] training 46.8% loss=0.20720, acc=0.90625
# [28/100] training 47.0% loss=0.34063, acc=0.90625
# [28/100] training 47.2% loss=0.23768, acc=0.89062
# [28/100] training 47.4% loss=0.16134, acc=0.96875
# [28/100] training 47.5% loss=0.29523, acc=0.90625
# [28/100] training 47.7% loss=0.22181, acc=0.87500
# [28/100] training 47.8% loss=0.25735, acc=0.84375
# [28/100] training 48.0% loss=0.20823, acc=0.92188
# [28/100] training 48.3% loss=0.13522, acc=0.96875
# [28/100] training 48.4% loss=0.09652, acc=0.96875
# [28/100] training 48.6% loss=0.20949, acc=0.90625
# [28/100] training 48.7% loss=0.31128, acc=0.87500
# [28/100] training 48.9% loss=0.24792, acc=0.90625
# [28/100] training 49.0% loss=0.17717, acc=0.92188
# [28/100] training 49.2% loss=0.23516, acc=0.87500
# [28/100] training 49.3% loss=0.18626, acc=0.93750
# [28/100] training 49.6% loss=0.22897, acc=0.92188
# [28/100] training 49.8% loss=0.32256, acc=0.87500
# [28/100] training 49.9% loss=0.23813, acc=0.90625
# [28/100] training 50.1% loss=0.15582, acc=0.92188
# [28/100] training 50.2% loss=0.25276, acc=0.87500
# [28/100] training 50.4% loss=0.39302, acc=0.85938
# [28/100] training 50.6% loss=0.22678, acc=0.89062
# [28/100] training 50.8% loss=0.30210, acc=0.90625
# [28/100] training 51.0% loss=0.33624, acc=0.87500
# [28/100] training 51.1% loss=0.27921, acc=0.87500
# [28/100] training 51.3% loss=0.23797, acc=0.92188
# [28/100] training 51.4% loss=0.29443, acc=0.92188
# [28/100] training 51.7% loss=0.25868, acc=0.85938
# [28/100] training 51.8% loss=0.24807, acc=0.89062
# [28/100] training 52.0% loss=0.20925, acc=0.92188
# [28/100] training 52.1% loss=0.22163, acc=0.90625
# [28/100] training 52.3% loss=0.26770, acc=0.87500
# [28/100] training 52.5% loss=0.11161, acc=0.96875
# [28/100] training 52.6% loss=0.30579, acc=0.89062
# [28/100] training 52.9% loss=0.39385, acc=0.89062
# [28/100] training 53.0% loss=0.13166, acc=0.95312
# [28/100] training 53.2% loss=0.22768, acc=0.92188
# [28/100] training 53.3% loss=0.15838, acc=0.93750
# [28/100] training 53.5% loss=0.22719, acc=0.92188
# [28/100] training 53.7% loss=0.12103, acc=0.95312
# [28/100] training 53.8% loss=0.24036, acc=0.85938
# [28/100] training 54.1% loss=0.28774, acc=0.89062
# [28/100] training 54.2% loss=0.19075, acc=0.96875
# [28/100] training 54.4% loss=0.17598, acc=0.92188
# [28/100] training 54.5% loss=0.24810, acc=0.93750
# [28/100] training 54.7% loss=0.39387, acc=0.84375
# [28/100] training 54.8% loss=0.16194, acc=0.93750
# [28/100] training 55.1% loss=0.16948, acc=0.92188
# [28/100] training 55.3% loss=0.19796, acc=0.92188
# [28/100] training 55.4% loss=0.21382, acc=0.89062
# [28/100] training 55.6% loss=0.17402, acc=0.92188
# [28/100] training 55.7% loss=0.15077, acc=0.92188
# [28/100] training 55.9% loss=0.21273, acc=0.90625
# [28/100] training 56.0% loss=0.10668, acc=0.96875
# [28/100] training 56.3% loss=0.54725, acc=0.81250
# [28/100] training 56.5% loss=0.31771, acc=0.89062
# [28/100] training 56.6% loss=0.14163, acc=0.98438
# [28/100] training 56.8% loss=0.30437, acc=0.89062
# [28/100] training 56.9% loss=0.26610, acc=0.90625
# [28/100] training 57.1% loss=0.34156, acc=0.87500
# [28/100] training 57.2% loss=0.12437, acc=0.96875
# [28/100] training 57.5% loss=0.19020, acc=0.93750
# [28/100] training 57.6% loss=0.21485, acc=0.89062
# [28/100] training 57.8% loss=0.16281, acc=0.93750
# [28/100] training 58.0% loss=0.18902, acc=0.93750
# [28/100] training 58.1% loss=0.16224, acc=0.93750
# [28/100] training 58.3% loss=0.12001, acc=0.95312
# [28/100] training 58.4% loss=0.24931, acc=0.92188
# [28/100] training 58.7% loss=0.24646, acc=0.92188
# [28/100] training 58.8% loss=0.23078, acc=0.92188
# [28/100] training 59.0% loss=0.20153, acc=0.93750
# [28/100] training 59.2% loss=0.17790, acc=0.92188
# [28/100] training 59.3% loss=0.21468, acc=0.90625
# [28/100] training 59.5% loss=0.19618, acc=0.90625
# [28/100] training 59.7% loss=0.27581, acc=0.84375
# [28/100] training 59.9% loss=0.16445, acc=0.95312
# [28/100] training 60.0% loss=0.24713, acc=0.90625
# [28/100] training 60.2% loss=0.18516, acc=0.92188
# [28/100] training 60.3% loss=0.21828, acc=0.90625
# [28/100] training 60.5% loss=0.23353, acc=0.85938
# [28/100] training 60.8% loss=0.24707, acc=0.90625
# [28/100] training 60.9% loss=0.25481, acc=0.89062
# [28/100] training 61.1% loss=0.23011, acc=0.90625
# [28/100] training 61.2% loss=0.26081, acc=0.90625
# [28/100] training 61.4% loss=0.28719, acc=0.89062
# [28/100] training 61.5% loss=0.37069, acc=0.82812
# [28/100] training 61.7% loss=0.22155, acc=0.90625
# [28/100] training 62.0% loss=0.29055, acc=0.87500
# [28/100] training 62.1% loss=0.38029, acc=0.85938
# [28/100] training 62.3% loss=0.12100, acc=0.96875
# [28/100] training 62.4% loss=0.21722, acc=0.90625
# [28/100] training 62.6% loss=0.28015, acc=0.84375
# [28/100] training 62.7% loss=0.18982, acc=0.92188
# [28/100] training 62.9% loss=0.29878, acc=0.84375
# [28/100] training 63.1% loss=0.20979, acc=0.89062
# [28/100] training 63.3% loss=0.15817, acc=0.93750
# [28/100] training 63.5% loss=0.35215, acc=0.90625
# [28/100] training 63.6% loss=0.32236, acc=0.85938
# [28/100] training 63.8% loss=0.29508, acc=0.82812
# [28/100] training 63.9% loss=0.19241, acc=0.90625
# [28/100] training 64.2% loss=0.19402, acc=0.92188
# [28/100] training 64.3% loss=0.21191, acc=0.93750
# [28/100] training 64.5% loss=0.22068, acc=0.89062
# [28/100] training 64.7% loss=0.25881, acc=0.89062
# [28/100] training 64.8% loss=0.42075, acc=0.81250
# [28/100] training 65.0% loss=0.31816, acc=0.89062
# [28/100] training 65.1% loss=0.33665, acc=0.84375
# [28/100] training 65.4% loss=0.18841, acc=0.92188
# [28/100] training 65.5% loss=0.14615, acc=0.96875
# [28/100] training 65.7% loss=0.16811, acc=0.92188
# [28/100] training 65.8% loss=0.31574, acc=0.89062
# [28/100] training 66.0% loss=0.26667, acc=0.82812
# [28/100] training 66.2% loss=0.10691, acc=0.96875
# [28/100] training 66.3% loss=0.28846, acc=0.89062
# [28/100] training 66.6% loss=0.19841, acc=0.92188
# [28/100] training 66.7% loss=0.15253, acc=0.93750
# [28/100] training 66.9% loss=0.22995, acc=0.87500
# [28/100] training 67.0% loss=0.23311, acc=0.90625
# [28/100] training 67.2% loss=0.18996, acc=0.89062
# [28/100] training 67.4% loss=0.19475, acc=0.93750
# [28/100] training 67.6% loss=0.21199, acc=0.92188
# [28/100] training 67.8% loss=0.16342, acc=0.93750
# [28/100] training 67.9% loss=0.13735, acc=0.95312
# [28/100] training 68.1% loss=0.18562, acc=0.92188
# [28/100] training 68.2% loss=0.13899, acc=0.92188
# [28/100] training 68.4% loss=0.12150, acc=0.96875
# [28/100] training 68.5% loss=0.25135, acc=0.93750
# [28/100] training 68.8% loss=0.30590, acc=0.84375
# [28/100] training 69.0% loss=0.23494, acc=0.93750
# [28/100] training 69.1% loss=0.14446, acc=0.92188
# [28/100] training 69.3% loss=0.26460, acc=0.90625
# [28/100] training 69.4% loss=0.14527, acc=0.93750
# [28/100] training 69.6% loss=0.32232, acc=0.79688
# [28/100] training 69.7% loss=0.20282, acc=0.92188
# [28/100] training 70.0% loss=0.30022, acc=0.87500
# [28/100] training 70.2% loss=0.20325, acc=0.92188
# [28/100] training 70.3% loss=0.26157, acc=0.90625
# [28/100] training 70.5% loss=0.22696, acc=0.93750
# [28/100] training 70.6% loss=0.11739, acc=0.96875
# [28/100] training 70.8% loss=0.32580, acc=0.84375
# [28/100] training 71.0% loss=0.24193, acc=0.90625
# [28/100] training 71.2% loss=0.17208, acc=0.92188
# [28/100] training 71.3% loss=0.13074, acc=0.96875
# [28/100] training 71.5% loss=0.23110, acc=0.95312
# [28/100] training 71.7% loss=0.21719, acc=0.93750
# [28/100] training 71.8% loss=0.23286, acc=0.90625
# [28/100] training 72.0% loss=0.10328, acc=0.96875
# [28/100] training 72.2% loss=0.16322, acc=0.93750
# [28/100] training 72.4% loss=0.26896, acc=0.85938
# [28/100] training 72.5% loss=0.35035, acc=0.87500
# [28/100] training 72.7% loss=0.23959, acc=0.85938
# [28/100] training 72.9% loss=0.14873, acc=0.93750
# [28/100] training 73.0% loss=0.09753, acc=0.96875
# [28/100] training 73.3% loss=0.22809, acc=0.90625
# [28/100] training 73.4% loss=0.17075, acc=0.92188
# [28/100] training 73.6% loss=0.13126, acc=0.93750
# [28/100] training 73.7% loss=0.19695, acc=0.95312
# [28/100] training 73.9% loss=0.19211, acc=0.92188
# [28/100] training 74.0% loss=0.19210, acc=0.92188
# [28/100] training 74.2% loss=0.16420, acc=0.92188
# [28/100] training 74.5% loss=0.21087, acc=0.89062
# [28/100] training 74.6% loss=0.35144, acc=0.82812
# [28/100] training 74.8% loss=0.40295, acc=0.87500
# [28/100] training 74.9% loss=0.32358, acc=0.89062
# [28/100] training 75.1% loss=0.22567, acc=0.90625
# [28/100] training 75.2% loss=0.21071, acc=0.93750
# [28/100] training 75.4% loss=0.24098, acc=0.90625
# [28/100] training 75.7% loss=0.28428, acc=0.87500
# [28/100] training 75.8% loss=0.30371, acc=0.89062
# [28/100] training 76.0% loss=0.16136, acc=0.93750
# [28/100] training 76.1% loss=0.34491, acc=0.87500
# [28/100] training 76.3% loss=0.22597, acc=0.85938
# [28/100] training 76.4% loss=0.25995, acc=0.90625
# [28/100] training 76.7% loss=0.20485, acc=0.92188
# [28/100] training 76.8% loss=0.17298, acc=0.95312
# [28/100] training 77.0% loss=0.15603, acc=0.93750
# [28/100] training 77.2% loss=0.25242, acc=0.89062
# [28/100] training 77.3% loss=0.14554, acc=0.95312
# [28/100] training 77.5% loss=0.22722, acc=0.87500
# [28/100] training 77.6% loss=0.24049, acc=0.89062
# [28/100] training 77.9% loss=0.24402, acc=0.85938
# [28/100] training 78.0% loss=0.18881, acc=0.93750
# [28/100] training 78.2% loss=0.28089, acc=0.87500
# [28/100] training 78.4% loss=0.12175, acc=0.93750
# [28/100] training 78.5% loss=0.27381, acc=0.90625
# [28/100] training 78.7% loss=0.22090, acc=0.87500
# [28/100] training 78.8% loss=0.13983, acc=0.98438
# [28/100] training 79.1% loss=0.14405, acc=0.98438
# [28/100] training 79.2% loss=0.15193, acc=0.96875
# [28/100] training 79.4% loss=0.21542, acc=0.92188
# [28/100] training 79.5% loss=0.18122, acc=0.93750
# [28/100] training 79.7% loss=0.11017, acc=0.96875
# [28/100] training 79.9% loss=0.10455, acc=1.00000
# [28/100] training 80.1% loss=0.14740, acc=0.93750
# [28/100] training 80.3% loss=0.27307, acc=0.92188
# [28/100] training 80.4% loss=0.17988, acc=0.93750
# [28/100] training 80.6% loss=0.17878, acc=0.92188
# [28/100] training 80.7% loss=0.22570, acc=0.92188
# [28/100] training 80.9% loss=0.28180, acc=0.84375
# [28/100] training 81.2% loss=0.31726, acc=0.87500
# [28/100] training 81.3% loss=0.28393, acc=0.85938
# [28/100] training 81.5% loss=0.22245, acc=0.92188
# [28/100] training 81.6% loss=0.32538, acc=0.85938
# [28/100] training 81.8% loss=0.25213, acc=0.89062
# [28/100] training 81.9% loss=0.33217, acc=0.92188
# [28/100] training 82.1% loss=0.18157, acc=0.93750
# [28/100] training 82.2% loss=0.22857, acc=0.92188
# [28/100] training 82.5% loss=0.11563, acc=0.96875
# [28/100] training 82.7% loss=0.19400, acc=0.93750
# [28/100] training 82.8% loss=0.16890, acc=0.93750
# [28/100] training 83.0% loss=0.21233, acc=0.92188
# [28/100] training 83.1% loss=0.18060, acc=0.90625
# [28/100] training 83.3% loss=0.20446, acc=0.92188
# [28/100] training 83.5% loss=0.25858, acc=0.92188
# [28/100] training 83.7% loss=0.26881, acc=0.90625
# [28/100] training 83.9% loss=0.26233, acc=0.87500
# [28/100] training 84.0% loss=0.14561, acc=0.95312
# [28/100] training 84.2% loss=0.17349, acc=0.92188
# [28/100] training 84.3% loss=0.21650, acc=0.87500
# [28/100] training 84.5% loss=0.14026, acc=0.96875
# [28/100] training 84.7% loss=0.28279, acc=0.90625
# [28/100] training 84.9% loss=0.13789, acc=0.93750
# [28/100] training 85.0% loss=0.24696, acc=0.90625
# [28/100] training 85.2% loss=0.37087, acc=0.82812
# [28/100] training 85.4% loss=0.20526, acc=0.92188
# [28/100] training 85.5% loss=0.19121, acc=0.87500
# [28/100] training 85.8% loss=0.28113, acc=0.87500
# [28/100] training 85.9% loss=0.20316, acc=0.92188
# [28/100] training 86.1% loss=0.19569, acc=0.89062
# [28/100] training 86.2% loss=0.16046, acc=0.95312
# [28/100] training 86.4% loss=0.26748, acc=0.90625
# [28/100] training 86.6% loss=0.19112, acc=0.92188
# [28/100] training 86.7% loss=0.11695, acc=0.95312
# [28/100] training 87.0% loss=0.36036, acc=0.82812
# [28/100] training 87.1% loss=0.17807, acc=0.92188
# [28/100] training 87.3% loss=0.17012, acc=0.92188
# [28/100] training 87.4% loss=0.25162, acc=0.90625
# [28/100] training 87.6% loss=0.21238, acc=0.89062
# [28/100] training 87.7% loss=0.35383, acc=0.90625
# [28/100] training 87.9% loss=0.26286, acc=0.90625
# [28/100] training 88.2% loss=0.24823, acc=0.90625
# [28/100] training 88.3% loss=0.28582, acc=0.90625
# [28/100] training 88.5% loss=0.25027, acc=0.87500
# [28/100] training 88.6% loss=0.21318, acc=0.95312
# [28/100] training 88.8% loss=0.18743, acc=0.95312
# [28/100] training 88.9% loss=0.21097, acc=0.89062
# [28/100] training 89.2% loss=0.22962, acc=0.93750
# [28/100] training 89.4% loss=0.26379, acc=0.90625
# [28/100] training 89.5% loss=0.25349, acc=0.93750
# [28/100] training 89.7% loss=0.27637, acc=0.87500
# [28/100] training 89.8% loss=0.24519, acc=0.95312
# [28/100] training 90.0% loss=0.18295, acc=0.93750
# [28/100] training 90.1% loss=0.22169, acc=0.87500
# [28/100] training 90.4% loss=0.25410, acc=0.90625
# [28/100] training 90.5% loss=0.13613, acc=0.93750
# [28/100] training 90.7% loss=0.14588, acc=0.93750
# [28/100] training 90.9% loss=0.10684, acc=0.96875
# [28/100] training 91.0% loss=0.19629, acc=0.93750
# [28/100] training 91.2% loss=0.15990, acc=0.93750
# [28/100] training 91.3% loss=0.31283, acc=0.87500
# [28/100] training 91.6% loss=0.36264, acc=0.87500
# [28/100] training 91.7% loss=0.21839, acc=0.93750
# [28/100] training 91.9% loss=0.35112, acc=0.87500
# [28/100] training 92.1% loss=0.18493, acc=0.92188
# [28/100] training 92.2% loss=0.12242, acc=0.96875
# [28/100] training 92.4% loss=0.19346, acc=0.90625
# [28/100] training 92.6% loss=0.26263, acc=0.87500
# [28/100] training 92.8% loss=0.26547, acc=0.92188
# [28/100] training 92.9% loss=0.19793, acc=0.90625
# [28/100] training 93.1% loss=0.36493, acc=0.87500
# [28/100] training 93.2% loss=0.26035, acc=0.89062
# [28/100] training 93.4% loss=0.18156, acc=0.92188
# [28/100] training 93.7% loss=0.22732, acc=0.92188
# [28/100] training 93.8% loss=0.13490, acc=0.95312
# [28/100] training 94.0% loss=0.12518, acc=0.95312
# [28/100] training 94.1% loss=0.25258, acc=0.90625
# [28/100] training 94.3% loss=0.12995, acc=0.95312
# [28/100] training 94.4% loss=0.18365, acc=0.96875
# [28/100] training 94.6% loss=0.09784, acc=0.96875
# [28/100] training 94.9% loss=0.17018, acc=0.92188
# [28/100] training 95.0% loss=0.37859, acc=0.89062
# [28/100] training 95.2% loss=0.41659, acc=0.87500
# [28/100] training 95.3% loss=0.26082, acc=0.90625
# [28/100] training 95.5% loss=0.14344, acc=0.95312
# [28/100] training 95.6% loss=0.24698, acc=0.92188
# [28/100] training 95.8% loss=0.30992, acc=0.84375
# [28/100] training 96.0% loss=0.25600, acc=0.89062
# [28/100] training 96.2% loss=0.14356, acc=0.96875
# [28/100] training 96.4% loss=0.18276, acc=0.96875
# [28/100] training 96.5% loss=0.28150, acc=0.89062
# [28/100] training 96.7% loss=0.20371, acc=0.90625
# [28/100] training 96.8% loss=0.26440, acc=0.90625
# [28/100] training 97.1% loss=0.17778, acc=0.90625
# [28/100] training 97.2% loss=0.16313, acc=0.90625
# [28/100] training 97.4% loss=0.22025, acc=0.92188
# [28/100] training 97.6% loss=0.28027, acc=0.87500
# [28/100] training 97.7% loss=0.23415, acc=0.90625
# [28/100] training 97.9% loss=0.16604, acc=0.90625
# [28/100] training 98.0% loss=0.23397, acc=0.87500
# [28/100] training 98.3% loss=0.28863, acc=0.82812
# [28/100] training 98.4% loss=0.18989, acc=0.93750
# [28/100] training 98.6% loss=0.37127, acc=0.85938
# [28/100] training 98.7% loss=0.27024, acc=0.85938
# [28/100] training 98.9% loss=0.16329, acc=0.92188
# [28/100] training 99.1% loss=0.21417, acc=0.92188
# [28/100] training 99.2% loss=0.16774, acc=0.92188
# [28/100] training 99.5% loss=0.34297, acc=0.84375
# [28/100] training 99.6% loss=0.24529, acc=0.89062
# [28/100] training 99.8% loss=0.15596, acc=0.93750
# [28/100] training 99.9% loss=0.11754, acc=0.95312
# [28/100] testing 0.9% loss=0.15994, acc=0.89062
# [28/100] testing 1.8% loss=0.35419, acc=0.81250
# [28/100] testing 2.2% loss=0.21628, acc=0.89062
# [28/100] testing 3.1% loss=0.28566, acc=0.85938
# [28/100] testing 3.5% loss=0.21972, acc=0.92188
# [28/100] testing 4.4% loss=0.19189, acc=0.89062
# [28/100] testing 4.8% loss=0.35645, acc=0.84375
# [28/100] testing 5.7% loss=0.28410, acc=0.85938
# [28/100] testing 6.6% loss=0.21002, acc=0.90625
# [28/100] testing 7.0% loss=0.15878, acc=0.92188
# [28/100] testing 7.9% loss=0.23932, acc=0.87500
# [28/100] testing 8.3% loss=0.26561, acc=0.89062
# [28/100] testing 9.2% loss=0.23798, acc=0.90625
# [28/100] testing 9.7% loss=0.09636, acc=0.96875
# [28/100] testing 10.5% loss=0.27461, acc=0.85938
# [28/100] testing 11.0% loss=0.31410, acc=0.87500
# [28/100] testing 11.8% loss=0.18018, acc=0.92188
# [28/100] testing 12.7% loss=0.35668, acc=0.85938
# [28/100] testing 13.2% loss=0.19164, acc=0.92188
# [28/100] testing 14.0% loss=0.40678, acc=0.87500
# [28/100] testing 14.5% loss=0.26579, acc=0.90625
# [28/100] testing 15.4% loss=0.29218, acc=0.90625
# [28/100] testing 15.8% loss=0.17276, acc=0.89062
# [28/100] testing 16.7% loss=0.22960, acc=0.90625
# [28/100] testing 17.5% loss=0.19709, acc=0.93750
# [28/100] testing 18.0% loss=0.25839, acc=0.87500
# [28/100] testing 18.9% loss=0.15926, acc=0.93750
# [28/100] testing 19.3% loss=0.29374, acc=0.90625
# [28/100] testing 20.2% loss=0.30943, acc=0.90625
# [28/100] testing 20.6% loss=0.29073, acc=0.87500
# [28/100] testing 21.5% loss=0.23155, acc=0.90625
# [28/100] testing 21.9% loss=0.28938, acc=0.90625
# [28/100] testing 22.8% loss=0.37745, acc=0.85938
# [28/100] testing 23.7% loss=0.35259, acc=0.85938
# [28/100] testing 24.1% loss=0.22039, acc=0.90625
# [28/100] testing 25.0% loss=0.34113, acc=0.89062
# [28/100] testing 25.4% loss=0.16644, acc=0.95312
# [28/100] testing 26.3% loss=0.35241, acc=0.89062
# [28/100] testing 26.8% loss=0.22438, acc=0.93750
# [28/100] testing 27.6% loss=0.26185, acc=0.90625
# [28/100] testing 28.5% loss=0.21699, acc=0.95312
# [28/100] testing 29.0% loss=0.23796, acc=0.90625
# [28/100] testing 29.8% loss=0.32719, acc=0.89062
# [28/100] testing 30.3% loss=0.19987, acc=0.92188
# [28/100] testing 31.1% loss=0.24779, acc=0.87500
# [28/100] testing 31.6% loss=0.27127, acc=0.92188
# [28/100] testing 32.5% loss=0.22771, acc=0.90625
# [28/100] testing 32.9% loss=0.38733, acc=0.90625
# [28/100] testing 33.8% loss=0.24639, acc=0.90625
# [28/100] testing 34.7% loss=0.33099, acc=0.85938
# [28/100] testing 35.1% loss=0.15363, acc=0.93750
# [28/100] testing 36.0% loss=0.31901, acc=0.90625
# [28/100] testing 36.4% loss=0.25213, acc=0.92188
# [28/100] testing 37.3% loss=0.30549, acc=0.90625
# [28/100] testing 37.7% loss=0.34749, acc=0.87500
# [28/100] testing 38.6% loss=0.23363, acc=0.90625
# [28/100] testing 39.5% loss=0.20603, acc=0.93750
# [28/100] testing 39.9% loss=0.25177, acc=0.92188
# [28/100] testing 40.8% loss=0.28158, acc=0.87500
# [28/100] testing 41.2% loss=0.25557, acc=0.93750
# [28/100] testing 42.1% loss=0.28056, acc=0.87500
# [28/100] testing 42.5% loss=0.19958, acc=0.92188
# [28/100] testing 43.4% loss=0.28440, acc=0.87500
# [28/100] testing 43.9% loss=0.16781, acc=0.92188
# [28/100] testing 44.7% loss=0.40040, acc=0.85938
# [28/100] testing 45.6% loss=0.28957, acc=0.87500
# [28/100] testing 46.1% loss=0.25247, acc=0.85938
# [28/100] testing 46.9% loss=0.24545, acc=0.89062
# [28/100] testing 47.4% loss=0.13361, acc=0.96875
# [28/100] testing 48.3% loss=0.38501, acc=0.82812
# [28/100] testing 48.7% loss=0.44497, acc=0.82812
# [28/100] testing 49.6% loss=0.39131, acc=0.82812
# [28/100] testing 50.4% loss=0.18592, acc=0.93750
# [28/100] testing 50.9% loss=0.29041, acc=0.89062
# [28/100] testing 51.8% loss=0.28994, acc=0.87500
# [28/100] testing 52.2% loss=0.27490, acc=0.84375
# [28/100] testing 53.1% loss=0.23871, acc=0.90625
# [28/100] testing 53.5% loss=0.29406, acc=0.89062
# [28/100] testing 54.4% loss=0.36746, acc=0.84375
# [28/100] testing 54.8% loss=0.33274, acc=0.82812
# [28/100] testing 55.7% loss=0.17924, acc=0.92188
# [28/100] testing 56.6% loss=0.29024, acc=0.85938
# [28/100] testing 57.0% loss=0.32203, acc=0.90625
# [28/100] testing 57.9% loss=0.29458, acc=0.85938
# [28/100] testing 58.3% loss=0.43029, acc=0.84375
# [28/100] testing 59.2% loss=0.21644, acc=0.87500
# [28/100] testing 59.7% loss=0.27352, acc=0.87500
# [28/100] testing 60.5% loss=0.29450, acc=0.87500
# [28/100] testing 61.4% loss=0.15917, acc=0.92188
# [28/100] testing 61.9% loss=0.28147, acc=0.89062
# [28/100] testing 62.7% loss=0.19364, acc=0.92188
# [28/100] testing 63.2% loss=0.44254, acc=0.87500
# [28/100] testing 64.0% loss=0.41120, acc=0.87500
# [28/100] testing 64.5% loss=0.17705, acc=0.92188
# [28/100] testing 65.4% loss=0.20852, acc=0.95312
# [28/100] testing 65.8% loss=0.37732, acc=0.85938
# [28/100] testing 66.7% loss=0.26070, acc=0.90625
# [28/100] testing 67.6% loss=0.30984, acc=0.92188
# [28/100] testing 68.0% loss=0.13755, acc=0.93750
# [28/100] testing 68.9% loss=0.29644, acc=0.89062
# [28/100] testing 69.3% loss=0.28126, acc=0.85938
# [28/100] testing 70.2% loss=0.37583, acc=0.87500
# [28/100] testing 70.6% loss=0.28857, acc=0.84375
# [28/100] testing 71.5% loss=0.34644, acc=0.87500
# [28/100] testing 72.4% loss=0.20020, acc=0.90625
# [28/100] testing 72.8% loss=0.17053, acc=0.92188
# [28/100] testing 73.7% loss=0.21083, acc=0.90625
# [28/100] testing 74.1% loss=0.43152, acc=0.85938
# [28/100] testing 75.0% loss=0.22275, acc=0.90625
# [28/100] testing 75.4% loss=0.46616, acc=0.81250
# [28/100] testing 76.3% loss=0.07757, acc=1.00000
# [28/100] testing 76.8% loss=0.26845, acc=0.87500
# [28/100] testing 77.6% loss=0.25409, acc=0.84375
# [28/100] testing 78.5% loss=0.49318, acc=0.84375
# [28/100] testing 79.0% loss=0.22364, acc=0.92188
# [28/100] testing 79.8% loss=0.35227, acc=0.82812
# [28/100] testing 80.3% loss=0.22186, acc=0.89062
# [28/100] testing 81.2% loss=0.43577, acc=0.81250
# [28/100] testing 81.6% loss=0.22443, acc=0.89062
# [28/100] testing 82.5% loss=0.24446, acc=0.90625
# [28/100] testing 83.3% loss=0.21714, acc=0.93750
# [28/100] testing 83.8% loss=0.19416, acc=0.92188
# [28/100] testing 84.7% loss=0.32298, acc=0.87500
# [28/100] testing 85.1% loss=0.26715, acc=0.89062
# [28/100] testing 86.0% loss=0.34220, acc=0.89062
# [28/100] testing 86.4% loss=0.29978, acc=0.89062
# [28/100] testing 87.3% loss=0.26611, acc=0.84375
# [28/100] testing 87.7% loss=0.18545, acc=0.92188
# [28/100] testing 88.6% loss=0.23977, acc=0.90625
# [28/100] testing 89.5% loss=0.38829, acc=0.82812
# [28/100] testing 89.9% loss=0.21238, acc=0.89062
# [28/100] testing 90.8% loss=0.26280, acc=0.93750
# [28/100] testing 91.2% loss=0.13372, acc=0.95312
# [28/100] testing 92.1% loss=0.39024, acc=0.89062
# [28/100] testing 92.6% loss=0.20206, acc=0.89062
# [28/100] testing 93.4% loss=0.37940, acc=0.82812
# [28/100] testing 94.3% loss=0.18217, acc=0.92188
# [28/100] testing 94.7% loss=0.21549, acc=0.93750
# [28/100] testing 95.6% loss=0.23091, acc=0.89062
# [28/100] testing 96.1% loss=0.19688, acc=0.87500
# [28/100] testing 96.9% loss=0.22370, acc=0.92188
# [28/100] testing 97.4% loss=0.13643, acc=0.98438
# [28/100] testing 98.3% loss=0.22077, acc=0.90625
# [28/100] testing 98.7% loss=0.21216, acc=0.92188
# [28/100] testing 99.6% loss=0.25758, acc=0.90625
# [29/100] training 0.2% loss=0.21607, acc=0.95312
# [29/100] training 0.4% loss=0.33408, acc=0.89062
# [29/100] training 0.5% loss=0.14006, acc=0.93750
# [29/100] training 0.8% loss=0.18123, acc=0.92188
# [29/100] training 0.9% loss=0.20299, acc=0.92188
# [29/100] training 1.1% loss=0.30705, acc=0.96875
# [29/100] training 1.2% loss=0.28713, acc=0.89062
# [29/100] training 1.4% loss=0.18639, acc=0.92188
# [29/100] training 1.6% loss=0.16659, acc=0.90625
# [29/100] training 1.8% loss=0.23687, acc=0.90625
# [29/100] training 2.0% loss=0.30555, acc=0.85938
# [29/100] training 2.1% loss=0.29915, acc=0.82812
# [29/100] training 2.3% loss=0.16371, acc=0.92188
# [29/100] training 2.4% loss=0.29342, acc=0.89062
# [29/100] training 2.6% loss=0.17712, acc=0.90625
# [29/100] training 2.7% loss=0.25875, acc=0.93750
# [29/100] training 3.0% loss=0.27700, acc=0.89062
# [29/100] training 3.2% loss=0.20201, acc=0.92188
# [29/100] training 3.3% loss=0.35184, acc=0.89062
# [29/100] training 3.5% loss=0.22731, acc=0.89062
# [29/100] training 3.6% loss=0.27770, acc=0.90625
# [29/100] training 3.8% loss=0.22891, acc=0.89062
# [29/100] training 3.9% loss=0.18764, acc=0.90625
# [29/100] training 4.2% loss=0.09593, acc=0.96875
# [29/100] training 4.4% loss=0.13866, acc=0.95312
# [29/100] training 4.5% loss=0.13214, acc=0.95312
# [29/100] training 4.7% loss=0.33324, acc=0.85938
# [29/100] training 4.8% loss=0.19928, acc=0.95312
# [29/100] training 5.0% loss=0.13227, acc=0.95312
# [29/100] training 5.2% loss=0.26235, acc=0.87500
# [29/100] training 5.4% loss=0.13330, acc=0.98438
# [29/100] training 5.5% loss=0.22374, acc=0.90625
# [29/100] training 5.7% loss=0.27084, acc=0.90625
# [29/100] training 5.9% loss=0.26827, acc=0.87500
# [29/100] training 6.0% loss=0.28160, acc=0.85938
# [29/100] training 6.3% loss=0.31791, acc=0.89062
# [29/100] training 6.4% loss=0.12901, acc=0.93750
# [29/100] training 6.6% loss=0.17230, acc=0.92188
# [29/100] training 6.7% loss=0.30552, acc=0.87500
# [29/100] training 6.9% loss=0.19742, acc=0.90625
# [29/100] training 7.1% loss=0.24493, acc=0.90625
# [29/100] training 7.2% loss=0.32863, acc=0.85938
# [29/100] training 7.5% loss=0.16095, acc=0.96875
# [29/100] training 7.6% loss=0.25771, acc=0.92188
# [29/100] training 7.8% loss=0.31968, acc=0.89062
# [29/100] training 7.9% loss=0.25201, acc=0.89062
# [29/100] training 8.1% loss=0.16289, acc=0.93750
# [29/100] training 8.2% loss=0.24601, acc=0.90625
# [29/100] training 8.4% loss=0.19834, acc=0.93750
# [29/100] training 8.7% loss=0.18689, acc=0.95312
# [29/100] training 8.8% loss=0.29532, acc=0.84375
# [29/100] training 9.0% loss=0.20718, acc=0.93750
# [29/100] training 9.1% loss=0.19596, acc=0.92188
# [29/100] training 9.3% loss=0.37840, acc=0.87500
# [29/100] training 9.4% loss=0.14215, acc=0.95312
# [29/100] training 9.7% loss=0.15092, acc=0.95312
# [29/100] training 9.9% loss=0.21742, acc=0.93750
# [29/100] training 10.0% loss=0.32239, acc=0.90625
# [29/100] training 10.2% loss=0.16081, acc=0.93750
# [29/100] training 10.3% loss=0.18556, acc=0.89062
# [29/100] training 10.5% loss=0.33697, acc=0.90625
# [29/100] training 10.6% loss=0.21124, acc=0.90625
# [29/100] training 10.9% loss=0.17662, acc=0.90625
# [29/100] training 11.0% loss=0.24033, acc=0.87500
# [29/100] training 11.2% loss=0.22444, acc=0.90625
# [29/100] training 11.4% loss=0.28777, acc=0.87500
# [29/100] training 11.5% loss=0.40678, acc=0.81250
# [29/100] training 11.7% loss=0.14205, acc=0.95312
# [29/100] training 11.8% loss=0.25202, acc=0.87500
# [29/100] training 12.1% loss=0.17505, acc=0.95312
# [29/100] training 12.2% loss=0.17253, acc=0.93750
# [29/100] training 12.4% loss=0.21412, acc=0.89062
# [29/100] training 12.6% loss=0.27342, acc=0.93750
# [29/100] training 12.7% loss=0.25405, acc=0.89062
# [29/100] training 12.9% loss=0.23858, acc=0.89062
# [29/100] training 13.0% loss=0.22634, acc=0.90625
# [29/100] training 13.3% loss=0.19138, acc=0.89062
# [29/100] training 13.4% loss=0.19277, acc=0.92188
# [29/100] training 13.6% loss=0.26816, acc=0.87500
# [29/100] training 13.7% loss=0.30426, acc=0.85938
# [29/100] training 13.9% loss=0.23092, acc=0.89062
# [29/100] training 14.1% loss=0.28492, acc=0.95312
# [29/100] training 14.3% loss=0.16329, acc=0.92188
# [29/100] training 14.5% loss=0.19748, acc=0.92188
# [29/100] training 14.6% loss=0.18919, acc=0.95312
# [29/100] training 14.8% loss=0.28716, acc=0.90625
# [29/100] training 14.9% loss=0.15418, acc=0.92188
# [29/100] training 15.1% loss=0.37797, acc=0.85938
# [29/100] training 15.4% loss=0.24478, acc=0.85938
# [29/100] training 15.5% loss=0.28981, acc=0.89062
# [29/100] training 15.7% loss=0.28136, acc=0.92188
# [29/100] training 15.8% loss=0.18277, acc=0.93750
# [29/100] training 16.0% loss=0.31721, acc=0.85938
# [29/100] training 16.1% loss=0.45324, acc=0.78125
# [29/100] training 16.3% loss=0.26949, acc=0.87500
# [29/100] training 16.4% loss=0.22384, acc=0.90625
# [29/100] training 16.7% loss=0.30013, acc=0.85938
# [29/100] training 16.9% loss=0.24737, acc=0.89062
# [29/100] training 17.0% loss=0.25562, acc=0.89062
# [29/100] training 17.2% loss=0.22870, acc=0.90625
# [29/100] training 17.3% loss=0.15475, acc=0.95312
# [29/100] training 17.5% loss=0.27479, acc=0.87500
# [29/100] training 17.7% loss=0.15600, acc=0.93750
# [29/100] training 17.9% loss=0.23947, acc=0.90625
# [29/100] training 18.1% loss=0.28970, acc=0.82812
# [29/100] training 18.2% loss=0.31837, acc=0.89062
# [29/100] training 18.4% loss=0.22586, acc=0.89062
# [29/100] training 18.5% loss=0.20076, acc=0.92188
# [29/100] training 18.8% loss=0.17280, acc=0.90625
# [29/100] training 18.9% loss=0.18923, acc=0.90625
# [29/100] training 19.1% loss=0.32289, acc=0.89062
# [29/100] training 19.2% loss=0.09810, acc=0.98438
# [29/100] training 19.4% loss=0.18703, acc=0.90625
# [29/100] training 19.6% loss=0.24224, acc=0.89062
# [29/100] training 19.7% loss=0.18978, acc=0.92188
# [29/100] training 20.0% loss=0.13357, acc=0.95312
# [29/100] training 20.1% loss=0.26431, acc=0.90625
# [29/100] training 20.3% loss=0.23819, acc=0.87500
# [29/100] training 20.4% loss=0.35014, acc=0.84375
# [29/100] training 20.6% loss=0.24829, acc=0.90625
# [29/100] training 20.8% loss=0.17998, acc=0.93750
# [29/100] training 20.9% loss=0.16360, acc=0.95312
# [29/100] training 21.2% loss=0.31655, acc=0.90625
# [29/100] training 21.3% loss=0.32455, acc=0.89062
# [29/100] training 21.5% loss=0.28613, acc=0.92188
# [29/100] training 21.6% loss=0.10743, acc=0.98438
# [29/100] training 21.8% loss=0.18436, acc=0.89062
# [29/100] training 21.9% loss=0.20720, acc=0.89062
# [29/100] training 22.2% loss=0.26091, acc=0.90625
# [29/100] training 22.4% loss=0.28988, acc=0.87500
# [29/100] training 22.5% loss=0.18421, acc=0.90625
# [29/100] training 22.7% loss=0.16983, acc=0.92188
# [29/100] training 22.8% loss=0.22948, acc=0.90625
# [29/100] training 23.0% loss=0.12630, acc=0.95312
# [29/100] training 23.1% loss=0.27676, acc=0.92188
# [29/100] training 23.4% loss=0.35349, acc=0.82812
# [29/100] training 23.6% loss=0.35500, acc=0.89062
# [29/100] training 23.7% loss=0.27839, acc=0.87500
# [29/100] training 23.9% loss=0.23053, acc=0.89062
# [29/100] training 24.0% loss=0.22212, acc=0.89062
# [29/100] training 24.2% loss=0.22108, acc=0.89062
# [29/100] training 24.3% loss=0.27219, acc=0.93750
# [29/100] training 24.6% loss=0.21022, acc=0.90625
# [29/100] training 24.7% loss=0.32117, acc=0.87500
# [29/100] training 24.9% loss=0.30939, acc=0.89062
# [29/100] training 25.1% loss=0.28669, acc=0.84375
# [29/100] training 25.2% loss=0.14712, acc=0.96875
# [29/100] training 25.4% loss=0.28599, acc=0.85938
# [29/100] training 25.6% loss=0.17770, acc=0.96875
# [29/100] training 25.8% loss=0.23657, acc=0.87500
# [29/100] training 25.9% loss=0.13090, acc=0.98438
# [29/100] training 26.1% loss=0.13229, acc=0.93750
# [29/100] training 26.3% loss=0.23766, acc=0.92188
# [29/100] training 26.4% loss=0.11774, acc=0.92188
# [29/100] training 26.6% loss=0.12724, acc=0.95312
# [29/100] training 26.8% loss=0.20025, acc=0.92188
# [29/100] training 27.0% loss=0.26394, acc=0.89062
# [29/100] training 27.1% loss=0.21675, acc=0.90625
# [29/100] training 27.3% loss=0.26599, acc=0.87500
# [29/100] training 27.4% loss=0.13926, acc=0.96875
# [29/100] training 27.6% loss=0.40046, acc=0.93750
# [29/100] training 27.9% loss=0.21473, acc=0.87500
# [29/100] training 28.0% loss=0.37988, acc=0.87500
# [29/100] training 28.2% loss=0.21846, acc=0.90625
# [29/100] training 28.3% loss=0.09514, acc=0.98438
# [29/100] training 28.5% loss=0.24127, acc=0.90625
# [29/100] training 28.6% loss=0.23771, acc=0.92188
# [29/100] training 28.8% loss=0.14519, acc=0.96875
# [29/100] training 29.1% loss=0.17776, acc=0.90625
# [29/100] training 29.2% loss=0.16187, acc=0.96875
# [29/100] training 29.4% loss=0.29073, acc=0.89062
# [29/100] training 29.5% loss=0.13334, acc=0.96875
# [29/100] training 29.7% loss=0.27544, acc=0.85938
# [29/100] training 29.8% loss=0.15616, acc=0.93750
# [29/100] training 30.0% loss=0.20188, acc=0.89062
# [29/100] training 30.2% loss=0.15947, acc=0.92188
# [29/100] training 30.4% loss=0.13366, acc=0.93750
# [29/100] training 30.6% loss=0.24339, acc=0.92188
# [29/100] training 30.7% loss=0.19588, acc=0.90625
# [29/100] training 30.9% loss=0.42305, acc=0.89062
# [29/100] training 31.0% loss=0.12877, acc=0.93750
# [29/100] training 31.3% loss=0.19038, acc=0.93750
# [29/100] training 31.4% loss=0.42626, acc=0.81250
# [29/100] training 31.6% loss=0.22068, acc=0.93750
# [29/100] training 31.8% loss=0.20114, acc=0.87500
# [29/100] training 31.9% loss=0.25622, acc=0.92188
# [29/100] training 32.1% loss=0.24905, acc=0.90625
# [29/100] training 32.2% loss=0.23594, acc=0.92188
# [29/100] training 32.5% loss=0.14416, acc=0.95312
# [29/100] training 32.6% loss=0.14913, acc=0.93750
# [29/100] training 32.8% loss=0.17046, acc=0.95312
# [29/100] training 32.9% loss=0.26976, acc=0.92188
# [29/100] training 33.1% loss=0.17466, acc=0.89062
# [29/100] training 33.3% loss=0.27650, acc=0.85938
# [29/100] training 33.4% loss=0.17628, acc=0.95312
# [29/100] training 33.7% loss=0.25212, acc=0.89062
# [29/100] training 33.8% loss=0.29275, acc=0.84375
# [29/100] training 34.0% loss=0.23103, acc=0.89062
# [29/100] training 34.1% loss=0.22471, acc=0.90625
# [29/100] training 34.3% loss=0.19065, acc=0.92188
# [29/100] training 34.5% loss=0.15773, acc=0.93750
# [29/100] training 34.7% loss=0.10667, acc=0.95312
# [29/100] training 34.9% loss=0.17011, acc=0.90625
# [29/100] training 35.0% loss=0.16459, acc=0.93750
# [29/100] training 35.2% loss=0.30626, acc=0.84375
# [29/100] training 35.3% loss=0.25183, acc=0.90625
# [29/100] training 35.5% loss=0.23188, acc=0.90625
# [29/100] training 35.6% loss=0.30091, acc=0.87500
# [29/100] training 35.9% loss=0.24356, acc=0.85938
# [29/100] training 36.1% loss=0.35107, acc=0.82812
# [29/100] training 36.2% loss=0.26500, acc=0.87500
# [29/100] training 36.4% loss=0.24739, acc=0.93750
# [29/100] training 36.5% loss=0.16777, acc=0.96875
# [29/100] training 36.7% loss=0.14500, acc=0.95312
# [29/100] training 36.8% loss=0.07380, acc=0.98438
# [29/100] training 37.1% loss=0.32906, acc=0.87500
# [29/100] training 37.3% loss=0.25266, acc=0.89062
# [29/100] training 37.4% loss=0.19279, acc=0.93750
# [29/100] training 37.6% loss=0.17418, acc=0.95312
# [29/100] training 37.7% loss=0.28094, acc=0.87500
# [29/100] training 37.9% loss=0.27571, acc=0.89062
# [29/100] training 38.1% loss=0.27771, acc=0.92188
# [29/100] training 38.3% loss=0.18497, acc=0.95312
# [29/100] training 38.4% loss=0.08005, acc=0.98438
# [29/100] training 38.6% loss=0.17054, acc=0.95312
# [29/100] training 38.8% loss=0.26763, acc=0.82812
# [29/100] training 38.9% loss=0.24141, acc=0.89062
# [29/100] training 39.1% loss=0.23386, acc=0.90625
# [29/100] training 39.3% loss=0.22180, acc=0.87500
# [29/100] training 39.5% loss=0.32793, acc=0.87500
# [29/100] training 39.6% loss=0.19415, acc=0.92188
# [29/100] training 39.8% loss=0.09457, acc=0.96875
# [29/100] training 40.0% loss=0.27575, acc=0.89062
# [29/100] training 40.1% loss=0.23042, acc=0.89062
# [29/100] training 40.4% loss=0.19547, acc=0.89062
# [29/100] training 40.5% loss=0.20477, acc=0.92188
# [29/100] training 40.7% loss=0.25944, acc=0.89062
# [29/100] training 40.8% loss=0.11101, acc=0.93750
# [29/100] training 41.0% loss=0.15837, acc=0.93750
# [29/100] training 41.1% loss=0.36717, acc=0.84375
# [29/100] training 41.3% loss=0.20925, acc=0.93750
# [29/100] training 41.6% loss=0.20278, acc=0.90625
# [29/100] training 41.7% loss=0.34711, acc=0.90625
# [29/100] training 41.9% loss=0.16101, acc=0.95312
# [29/100] training 42.0% loss=0.25033, acc=0.85938
# [29/100] training 42.2% loss=0.26546, acc=0.84375
# [29/100] training 42.3% loss=0.20076, acc=0.90625
# [29/100] training 42.5% loss=0.16040, acc=0.96875
# [29/100] training 42.8% loss=0.13942, acc=0.93750
# [29/100] training 42.9% loss=0.13635, acc=0.93750
# [29/100] training 43.1% loss=0.17200, acc=0.93750
# [29/100] training 43.2% loss=0.22775, acc=0.87500
# [29/100] training 43.4% loss=0.29157, acc=0.92188
# [29/100] training 43.5% loss=0.20782, acc=0.93750
# [29/100] training 43.8% loss=0.20556, acc=0.90625
# [29/100] training 43.9% loss=0.25088, acc=0.90625
# [29/100] training 44.1% loss=0.23402, acc=0.93750
# [29/100] training 44.3% loss=0.24226, acc=0.90625
# [29/100] training 44.4% loss=0.21091, acc=0.89062
# [29/100] training 44.6% loss=0.30934, acc=0.87500
# [29/100] training 44.7% loss=0.37377, acc=0.82812
# [29/100] training 45.0% loss=0.15293, acc=0.92188
# [29/100] training 45.1% loss=0.34871, acc=0.85938
# [29/100] training 45.3% loss=0.28102, acc=0.90625
# [29/100] training 45.5% loss=0.15502, acc=0.93750
# [29/100] training 45.6% loss=0.25972, acc=0.89062
# [29/100] training 45.8% loss=0.14348, acc=0.95312
# [29/100] training 45.9% loss=0.18061, acc=0.92188
# [29/100] training 46.2% loss=0.17063, acc=0.92188
# [29/100] training 46.3% loss=0.18224, acc=0.90625
# [29/100] training 46.5% loss=0.28605, acc=0.90625
# [29/100] training 46.6% loss=0.18423, acc=0.93750
# [29/100] training 46.8% loss=0.17569, acc=0.93750
# [29/100] training 47.0% loss=0.22249, acc=0.90625
# [29/100] training 47.2% loss=0.23393, acc=0.90625
# [29/100] training 47.4% loss=0.21499, acc=0.92188
# [29/100] training 47.5% loss=0.23861, acc=0.95312
# [29/100] training 47.7% loss=0.20774, acc=0.87500
# [29/100] training 47.8% loss=0.35151, acc=0.85938
# [29/100] training 48.0% loss=0.17511, acc=0.93750
# [29/100] training 48.3% loss=0.13934, acc=0.95312
# [29/100] training 48.4% loss=0.08680, acc=0.96875
# [29/100] training 48.6% loss=0.21313, acc=0.92188
# [29/100] training 48.7% loss=0.30486, acc=0.85938
# [29/100] training 48.9% loss=0.15205, acc=0.93750
# [29/100] training 49.0% loss=0.13025, acc=0.95312
# [29/100] training 49.2% loss=0.24541, acc=0.87500
# [29/100] training 49.3% loss=0.18723, acc=0.93750
# [29/100] training 49.6% loss=0.29384, acc=0.90625
# [29/100] training 49.8% loss=0.24525, acc=0.87500
# [29/100] training 49.9% loss=0.24366, acc=0.90625
# [29/100] training 50.1% loss=0.13554, acc=0.92188
# [29/100] training 50.2% loss=0.26556, acc=0.87500
# [29/100] training 50.4% loss=0.30348, acc=0.89062
# [29/100] training 50.6% loss=0.23559, acc=0.93750
# [29/100] training 50.8% loss=0.34888, acc=0.82812
# [29/100] training 51.0% loss=0.31130, acc=0.89062
# [29/100] training 51.1% loss=0.31477, acc=0.87500
# [29/100] training 51.3% loss=0.24635, acc=0.92188
# [29/100] training 51.4% loss=0.23371, acc=0.95312
# [29/100] training 51.7% loss=0.23644, acc=0.89062
# [29/100] training 51.8% loss=0.25930, acc=0.89062
# [29/100] training 52.0% loss=0.19429, acc=0.90625
# [29/100] training 52.1% loss=0.28068, acc=0.92188
# [29/100] training 52.3% loss=0.27762, acc=0.89062
# [29/100] training 52.5% loss=0.14590, acc=0.95312
# [29/100] training 52.6% loss=0.19980, acc=0.92188
# [29/100] training 52.9% loss=0.31184, acc=0.92188
# [29/100] training 53.0% loss=0.13841, acc=0.93750
# [29/100] training 53.2% loss=0.18681, acc=0.93750
# [29/100] training 53.3% loss=0.09739, acc=0.95312
# [29/100] training 53.5% loss=0.28566, acc=0.92188
# [29/100] training 53.7% loss=0.15388, acc=0.90625
# [29/100] training 53.8% loss=0.35652, acc=0.82812
# [29/100] training 54.1% loss=0.26383, acc=0.90625
# [29/100] training 54.2% loss=0.11921, acc=0.96875
# [29/100] training 54.4% loss=0.16574, acc=0.93750
# [29/100] training 54.5% loss=0.26888, acc=0.87500
# [29/100] training 54.7% loss=0.30926, acc=0.85938
# [29/100] training 54.8% loss=0.17772, acc=0.93750
# [29/100] training 55.1% loss=0.33284, acc=0.87500
# [29/100] training 55.3% loss=0.17087, acc=0.93750
# [29/100] training 55.4% loss=0.26483, acc=0.87500
# [29/100] training 55.6% loss=0.28000, acc=0.89062
# [29/100] training 55.7% loss=0.17953, acc=0.90625
# [29/100] training 55.9% loss=0.12912, acc=0.93750
# [29/100] training 56.0% loss=0.21222, acc=0.90625
# [29/100] training 56.3% loss=0.36019, acc=0.82812
# [29/100] training 56.5% loss=0.26305, acc=0.90625
# [29/100] training 56.6% loss=0.16105, acc=0.92188
# [29/100] training 56.8% loss=0.22746, acc=0.89062
# [29/100] training 56.9% loss=0.22007, acc=0.93750
# [29/100] training 57.1% loss=0.25330, acc=0.89062
# [29/100] training 57.2% loss=0.11100, acc=0.96875
# [29/100] training 57.5% loss=0.13582, acc=0.96875
# [29/100] training 57.6% loss=0.21633, acc=0.92188
# [29/100] training 57.8% loss=0.18468, acc=0.95312
# [29/100] training 58.0% loss=0.20497, acc=0.93750
# [29/100] training 58.1% loss=0.18862, acc=0.87500
# [29/100] training 58.3% loss=0.16792, acc=0.90625
# [29/100] training 58.4% loss=0.24050, acc=0.95312
# [29/100] training 58.7% loss=0.30073, acc=0.90625
# [29/100] training 58.8% loss=0.26092, acc=0.89062
# [29/100] training 59.0% loss=0.11716, acc=0.98438
# [29/100] training 59.2% loss=0.22798, acc=0.89062
# [29/100] training 59.3% loss=0.24232, acc=0.87500
# [29/100] training 59.5% loss=0.20680, acc=0.92188
# [29/100] training 59.7% loss=0.27511, acc=0.90625
# [29/100] training 59.9% loss=0.13917, acc=0.98438
# [29/100] training 60.0% loss=0.22427, acc=0.87500
# [29/100] training 60.2% loss=0.20364, acc=0.92188
# [29/100] training 60.3% loss=0.17396, acc=0.93750
# [29/100] training 60.5% loss=0.39017, acc=0.84375
# [29/100] training 60.8% loss=0.23938, acc=0.90625
# [29/100] training 60.9% loss=0.24264, acc=0.89062
# [29/100] training 61.1% loss=0.21469, acc=0.93750
# [29/100] training 61.2% loss=0.19273, acc=0.92188
# [29/100] training 61.4% loss=0.23256, acc=0.90625
# [29/100] training 61.5% loss=0.29229, acc=0.87500
# [29/100] training 61.7% loss=0.14100, acc=0.95312
# [29/100] training 62.0% loss=0.33597, acc=0.89062
# [29/100] training 62.1% loss=0.29977, acc=0.82812
# [29/100] training 62.3% loss=0.12354, acc=0.93750
# [29/100] training 62.4% loss=0.17027, acc=0.93750
# [29/100] training 62.6% loss=0.25010, acc=0.89062
# [29/100] training 62.7% loss=0.16830, acc=0.93750
# [29/100] training 62.9% loss=0.22446, acc=0.90625
# [29/100] training 63.1% loss=0.23446, acc=0.87500
# [29/100] training 63.3% loss=0.17382, acc=0.93750
# [29/100] training 63.5% loss=0.22733, acc=0.93750
# [29/100] training 63.6% loss=0.30781, acc=0.90625
# [29/100] training 63.8% loss=0.30182, acc=0.84375
# [29/100] training 63.9% loss=0.20672, acc=0.90625
# [29/100] training 64.2% loss=0.22745, acc=0.90625
# [29/100] training 64.3% loss=0.27347, acc=0.90625
# [29/100] training 64.5% loss=0.22340, acc=0.87500
# [29/100] training 64.7% loss=0.20837, acc=0.92188
# [29/100] training 64.8% loss=0.39500, acc=0.84375
# [29/100] training 65.0% loss=0.23108, acc=0.90625
# [29/100] training 65.1% loss=0.33929, acc=0.84375
# [29/100] training 65.4% loss=0.14631, acc=0.95312
# [29/100] training 65.5% loss=0.15016, acc=0.95312
# [29/100] training 65.7% loss=0.12736, acc=0.93750
# [29/100] training 65.8% loss=0.39184, acc=0.82812
# [29/100] training 66.0% loss=0.25036, acc=0.89062
# [29/100] training 66.2% loss=0.12950, acc=0.93750
# [29/100] training 66.3% loss=0.27844, acc=0.85938
# [29/100] training 66.6% loss=0.22578, acc=0.89062
# [29/100] training 66.7% loss=0.12750, acc=0.95312
# [29/100] training 66.9% loss=0.25621, acc=0.87500
# [29/100] training 67.0% loss=0.19990, acc=0.92188
# [29/100] training 67.2% loss=0.18472, acc=0.92188
# [29/100] training 67.4% loss=0.14852, acc=0.92188
# [29/100] training 67.6% loss=0.23280, acc=0.90625
# [29/100] training 67.8% loss=0.14641, acc=0.96875
# [29/100] training 67.9% loss=0.07359, acc=0.98438
# [29/100] training 68.1% loss=0.23336, acc=0.95312
# [29/100] training 68.2% loss=0.09726, acc=0.95312
# [29/100] training 68.4% loss=0.11797, acc=0.95312
# [29/100] training 68.5% loss=0.25193, acc=0.84375
# [29/100] training 68.8% loss=0.27011, acc=0.87500
# [29/100] training 69.0% loss=0.31671, acc=0.93750
# [29/100] training 69.1% loss=0.16103, acc=0.92188
# [29/100] training 69.3% loss=0.36554, acc=0.89062
# [29/100] training 69.4% loss=0.30935, acc=0.85938
# [29/100] training 69.6% loss=0.18694, acc=0.90625
# [29/100] training 69.7% loss=0.23955, acc=0.89062
# [29/100] training 70.0% loss=0.31540, acc=0.87500
# [29/100] training 70.2% loss=0.22363, acc=0.93750
# [29/100] training 70.3% loss=0.19800, acc=0.92188
# [29/100] training 70.5% loss=0.30035, acc=0.85938
# [29/100] training 70.6% loss=0.18750, acc=0.93750
# [29/100] training 70.8% loss=0.22584, acc=0.90625
# [29/100] training 71.0% loss=0.25876, acc=0.90625
# [29/100] training 71.2% loss=0.16467, acc=0.89062
# [29/100] training 71.3% loss=0.14496, acc=0.96875
# [29/100] training 71.5% loss=0.25507, acc=0.95312
# [29/100] training 71.7% loss=0.29470, acc=0.85938
# [29/100] training 71.8% loss=0.28694, acc=0.90625
# [29/100] training 72.0% loss=0.09475, acc=0.98438
# [29/100] training 72.2% loss=0.24229, acc=0.90625
# [29/100] training 72.4% loss=0.28673, acc=0.84375
# [29/100] training 72.5% loss=0.28515, acc=0.89062
# [29/100] training 72.7% loss=0.29839, acc=0.87500
# [29/100] training 72.9% loss=0.16850, acc=0.95312
# [29/100] training 73.0% loss=0.15012, acc=0.93750
# [29/100] training 73.3% loss=0.33179, acc=0.89062
# [29/100] training 73.4% loss=0.10816, acc=0.96875
# [29/100] training 73.6% loss=0.14070, acc=0.93750
# [29/100] training 73.7% loss=0.17131, acc=0.93750
# [29/100] training 73.9% loss=0.15352, acc=0.95312
# [29/100] training 74.0% loss=0.24131, acc=0.90625
# [29/100] training 74.2% loss=0.15523, acc=0.90625
# [29/100] training 74.5% loss=0.11510, acc=0.95312
# [29/100] training 74.6% loss=0.49355, acc=0.85938
# [29/100] training 74.8% loss=0.43444, acc=0.82812
# [29/100] training 74.9% loss=0.31669, acc=0.84375
# [29/100] training 75.1% loss=0.17172, acc=0.90625
# [29/100] training 75.2% loss=0.14164, acc=0.93750
# [29/100] training 75.4% loss=0.30400, acc=0.85938
# [29/100] training 75.7% loss=0.28378, acc=0.90625
# [29/100] training 75.8% loss=0.21667, acc=0.89062
# [29/100] training 76.0% loss=0.15945, acc=0.92188
# [29/100] training 76.1% loss=0.34645, acc=0.84375
# [29/100] training 76.3% loss=0.13330, acc=0.95312
# [29/100] training 76.4% loss=0.28582, acc=0.89062
# [29/100] training 76.7% loss=0.23666, acc=0.90625
# [29/100] training 76.8% loss=0.15755, acc=0.95312
# [29/100] training 77.0% loss=0.17916, acc=0.92188
# [29/100] training 77.2% loss=0.29583, acc=0.87500
# [29/100] training 77.3% loss=0.12157, acc=0.96875
# [29/100] training 77.5% loss=0.20227, acc=0.92188
# [29/100] training 77.6% loss=0.23423, acc=0.90625
# [29/100] training 77.9% loss=0.24115, acc=0.87500
# [29/100] training 78.0% loss=0.21493, acc=0.90625
# [29/100] training 78.2% loss=0.18928, acc=0.93750
# [29/100] training 78.4% loss=0.11708, acc=0.93750
# [29/100] training 78.5% loss=0.35677, acc=0.87500
# [29/100] training 78.7% loss=0.16196, acc=0.90625
# [29/100] training 78.8% loss=0.08481, acc=0.98438
# [29/100] training 79.1% loss=0.10979, acc=0.96875
# [29/100] training 79.2% loss=0.14661, acc=0.93750
# [29/100] training 79.4% loss=0.24490, acc=0.92188
# [29/100] training 79.5% loss=0.24659, acc=0.92188
# [29/100] training 79.7% loss=0.07242, acc=0.96875
# [29/100] training 79.9% loss=0.19790, acc=0.92188
# [29/100] training 80.1% loss=0.18375, acc=0.92188
# [29/100] training 80.3% loss=0.23833, acc=0.90625
# [29/100] training 80.4% loss=0.22408, acc=0.93750
# [29/100] training 80.6% loss=0.23285, acc=0.93750
# [29/100] training 80.7% loss=0.18577, acc=0.95312
# [29/100] training 80.9% loss=0.22313, acc=0.87500
# [29/100] training 81.2% loss=0.25504, acc=0.92188
# [29/100] training 81.3% loss=0.28158, acc=0.87500
# [29/100] training 81.5% loss=0.16282, acc=0.95312
# [29/100] training 81.6% loss=0.27776, acc=0.87500
# [29/100] training 81.8% loss=0.21295, acc=0.87500
# [29/100] training 81.9% loss=0.32779, acc=0.89062
# [29/100] training 82.1% loss=0.14854, acc=0.95312
# [29/100] training 82.2% loss=0.25801, acc=0.87500
# [29/100] training 82.5% loss=0.08769, acc=0.98438
# [29/100] training 82.7% loss=0.19836, acc=0.92188
# [29/100] training 82.8% loss=0.24720, acc=0.90625
# [29/100] training 83.0% loss=0.18249, acc=0.93750
# [29/100] training 83.1% loss=0.18225, acc=0.93750
# [29/100] training 83.3% loss=0.11311, acc=0.96875
# [29/100] training 83.5% loss=0.15975, acc=0.92188
# [29/100] training 83.7% loss=0.31135, acc=0.93750
# [29/100] training 83.9% loss=0.18785, acc=0.93750
# [29/100] training 84.0% loss=0.10592, acc=0.95312
# [29/100] training 84.2% loss=0.13440, acc=0.95312
# [29/100] training 84.3% loss=0.17206, acc=0.93750
# [29/100] training 84.5% loss=0.20829, acc=0.93750
# [29/100] training 84.7% loss=0.21819, acc=0.92188
# [29/100] training 84.9% loss=0.11061, acc=0.96875
# [29/100] training 85.0% loss=0.21641, acc=0.89062
# [29/100] training 85.2% loss=0.25159, acc=0.90625
# [29/100] training 85.4% loss=0.18970, acc=0.90625
# [29/100] training 85.5% loss=0.27612, acc=0.85938
# [29/100] training 85.8% loss=0.20380, acc=0.85938
# [29/100] training 85.9% loss=0.11864, acc=0.93750
# [29/100] training 86.1% loss=0.29155, acc=0.87500
# [29/100] training 86.2% loss=0.14658, acc=0.95312
# [29/100] training 86.4% loss=0.27959, acc=0.87500
# [29/100] training 86.6% loss=0.18687, acc=0.90625
# [29/100] training 86.7% loss=0.19559, acc=0.92188
# [29/100] training 87.0% loss=0.24596, acc=0.90625
# [29/100] training 87.1% loss=0.19588, acc=0.90625
# [29/100] training 87.3% loss=0.17847, acc=0.92188
# [29/100] training 87.4% loss=0.32445, acc=0.85938
# [29/100] training 87.6% loss=0.22138, acc=0.90625
# [29/100] training 87.7% loss=0.36060, acc=0.92188
# [29/100] training 87.9% loss=0.25570, acc=0.89062
# [29/100] training 88.2% loss=0.21603, acc=0.89062
# [29/100] training 88.3% loss=0.27252, acc=0.92188
# [29/100] training 88.5% loss=0.21265, acc=0.89062
# [29/100] training 88.6% loss=0.13781, acc=0.93750
# [29/100] training 88.8% loss=0.19307, acc=0.93750
# [29/100] training 88.9% loss=0.25085, acc=0.89062
# [29/100] training 89.2% loss=0.18091, acc=0.93750
# [29/100] training 89.4% loss=0.16815, acc=0.95312
# [29/100] training 89.5% loss=0.21077, acc=0.92188
# [29/100] training 89.7% loss=0.20859, acc=0.89062
# [29/100] training 89.8% loss=0.16515, acc=0.96875
# [29/100] training 90.0% loss=0.13184, acc=0.93750
# [29/100] training 90.1% loss=0.16712, acc=0.93750
# [29/100] training 90.4% loss=0.21614, acc=0.93750
# [29/100] training 90.5% loss=0.12240, acc=0.92188
# [29/100] training 90.7% loss=0.15079, acc=0.93750
# [29/100] training 90.9% loss=0.15600, acc=0.95312
# [29/100] training 91.0% loss=0.21582, acc=0.93750
# [29/100] training 91.2% loss=0.29277, acc=0.90625
# [29/100] training 91.3% loss=0.42014, acc=0.85938
# [29/100] training 91.6% loss=0.28837, acc=0.90625
# [29/100] training 91.7% loss=0.15540, acc=0.95312
# [29/100] training 91.9% loss=0.36612, acc=0.84375
# [29/100] training 92.1% loss=0.16079, acc=0.89062
# [29/100] training 92.2% loss=0.16573, acc=0.90625
# [29/100] training 92.4% loss=0.17191, acc=0.90625
# [29/100] training 92.6% loss=0.37034, acc=0.79688
# [29/100] training 92.8% loss=0.24045, acc=0.92188
# [29/100] training 92.9% loss=0.28824, acc=0.87500
# [29/100] training 93.1% loss=0.48345, acc=0.82812
# [29/100] training 93.2% loss=0.30055, acc=0.87500
# [29/100] training 93.4% loss=0.23209, acc=0.90625
# [29/100] training 93.7% loss=0.20643, acc=0.92188
# [29/100] training 93.8% loss=0.25413, acc=0.85938
# [29/100] training 94.0% loss=0.15524, acc=0.93750
# [29/100] training 94.1% loss=0.22874, acc=0.92188
# [29/100] training 94.3% loss=0.15388, acc=0.93750
# [29/100] training 94.4% loss=0.25952, acc=0.92188
# [29/100] training 94.6% loss=0.14311, acc=0.95312
# [29/100] training 94.9% loss=0.18851, acc=0.90625
# [29/100] training 95.0% loss=0.22639, acc=0.90625
# [29/100] training 95.2% loss=0.48109, acc=0.85938
# [29/100] training 95.3% loss=0.33665, acc=0.90625
# [29/100] training 95.5% loss=0.23156, acc=0.95312
# [29/100] training 95.6% loss=0.26270, acc=0.90625
# [29/100] training 95.8% loss=0.22795, acc=0.89062
# [29/100] training 96.0% loss=0.24840, acc=0.90625
# [29/100] training 96.2% loss=0.20535, acc=0.92188
# [29/100] training 96.4% loss=0.20343, acc=0.93750
# [29/100] training 96.5% loss=0.26419, acc=0.89062
# [29/100] training 96.7% loss=0.17084, acc=0.95312
# [29/100] training 96.8% loss=0.33778, acc=0.87500
# [29/100] training 97.1% loss=0.20042, acc=0.92188
# [29/100] training 97.2% loss=0.22347, acc=0.90625
# [29/100] training 97.4% loss=0.22453, acc=0.90625
# [29/100] training 97.6% loss=0.25402, acc=0.84375
# [29/100] training 97.7% loss=0.13798, acc=0.95312
# [29/100] training 97.9% loss=0.17100, acc=0.93750
# [29/100] training 98.0% loss=0.15106, acc=0.93750
# [29/100] training 98.3% loss=0.17119, acc=0.95312
# [29/100] training 98.4% loss=0.18847, acc=0.93750
# [29/100] training 98.6% loss=0.49980, acc=0.81250
# [29/100] training 98.7% loss=0.27414, acc=0.90625
# [29/100] training 98.9% loss=0.20473, acc=0.87500
# [29/100] training 99.1% loss=0.17884, acc=0.92188
# [29/100] training 99.2% loss=0.20821, acc=0.92188
# [29/100] training 99.5% loss=0.25727, acc=0.87500
# [29/100] training 99.6% loss=0.23346, acc=0.92188
# [29/100] training 99.8% loss=0.12358, acc=0.96875
# [29/100] training 99.9% loss=0.07645, acc=0.95312
# [29/100] testing 0.9% loss=0.15940, acc=0.93750
# [29/100] testing 1.8% loss=0.28165, acc=0.90625
# [29/100] testing 2.2% loss=0.22208, acc=0.92188
# [29/100] testing 3.1% loss=0.38232, acc=0.85938
# [29/100] testing 3.5% loss=0.17272, acc=0.92188
# [29/100] testing 4.4% loss=0.26272, acc=0.89062
# [29/100] testing 4.8% loss=0.37540, acc=0.87500
# [29/100] testing 5.7% loss=0.19145, acc=0.90625
# [29/100] testing 6.6% loss=0.27682, acc=0.90625
# [29/100] testing 7.0% loss=0.14221, acc=0.93750
# [29/100] testing 7.9% loss=0.37705, acc=0.84375
# [29/100] testing 8.3% loss=0.25313, acc=0.90625
# [29/100] testing 9.2% loss=0.32367, acc=0.84375
# [29/100] testing 9.7% loss=0.20228, acc=0.90625
# [29/100] testing 10.5% loss=0.32162, acc=0.85938
# [29/100] testing 11.0% loss=0.33646, acc=0.89062
# [29/100] testing 11.8% loss=0.23418, acc=0.90625
# [29/100] testing 12.7% loss=0.47356, acc=0.82812
# [29/100] testing 13.2% loss=0.18495, acc=0.95312
# [29/100] testing 14.0% loss=0.39730, acc=0.87500
# [29/100] testing 14.5% loss=0.39774, acc=0.84375
# [29/100] testing 15.4% loss=0.37185, acc=0.87500
# [29/100] testing 15.8% loss=0.20733, acc=0.90625
# [29/100] testing 16.7% loss=0.26161, acc=0.87500
# [29/100] testing 17.5% loss=0.20782, acc=0.92188
# [29/100] testing 18.0% loss=0.28018, acc=0.87500
# [29/100] testing 18.9% loss=0.13742, acc=0.95312
# [29/100] testing 19.3% loss=0.35844, acc=0.87500
# [29/100] testing 20.2% loss=0.40096, acc=0.84375
# [29/100] testing 20.6% loss=0.31838, acc=0.85938
# [29/100] testing 21.5% loss=0.26260, acc=0.89062
# [29/100] testing 21.9% loss=0.51075, acc=0.85938
# [29/100] testing 22.8% loss=0.37864, acc=0.85938
# [29/100] testing 23.7% loss=0.42013, acc=0.84375
# [29/100] testing 24.1% loss=0.33035, acc=0.89062
# [29/100] testing 25.0% loss=0.57481, acc=0.85938
# [29/100] testing 25.4% loss=0.12770, acc=0.95312
# [29/100] testing 26.3% loss=0.28037, acc=0.90625
# [29/100] testing 26.8% loss=0.34271, acc=0.89062
# [29/100] testing 27.6% loss=0.29571, acc=0.89062
# [29/100] testing 28.5% loss=0.25296, acc=0.92188
# [29/100] testing 29.0% loss=0.17649, acc=0.93750
# [29/100] testing 29.8% loss=0.33667, acc=0.87500
# [29/100] testing 30.3% loss=0.33538, acc=0.89062
# [29/100] testing 31.1% loss=0.40088, acc=0.84375
# [29/100] testing 31.6% loss=0.20806, acc=0.90625
# [29/100] testing 32.5% loss=0.27148, acc=0.90625
# [29/100] testing 32.9% loss=0.40860, acc=0.87500
# [29/100] testing 33.8% loss=0.31501, acc=0.90625
# [29/100] testing 34.7% loss=0.35658, acc=0.89062
# [29/100] testing 35.1% loss=0.20295, acc=0.90625
# [29/100] testing 36.0% loss=0.37330, acc=0.85938
# [29/100] testing 36.4% loss=0.35615, acc=0.89062
# [29/100] testing 37.3% loss=0.31739, acc=0.92188
# [29/100] testing 37.7% loss=0.44987, acc=0.82812
# [29/100] testing 38.6% loss=0.29077, acc=0.92188
# [29/100] testing 39.5% loss=0.32388, acc=0.87500
# [29/100] testing 39.9% loss=0.33074, acc=0.90625
# [29/100] testing 40.8% loss=0.35579, acc=0.81250
# [29/100] testing 41.2% loss=0.27058, acc=0.90625
# [29/100] testing 42.1% loss=0.39222, acc=0.85938
# [29/100] testing 42.5% loss=0.26299, acc=0.87500
# [29/100] testing 43.4% loss=0.49249, acc=0.85938
# [29/100] testing 43.9% loss=0.21460, acc=0.93750
# [29/100] testing 44.7% loss=0.43830, acc=0.79688
# [29/100] testing 45.6% loss=0.36978, acc=0.92188
# [29/100] testing 46.1% loss=0.31957, acc=0.85938
# [29/100] testing 46.9% loss=0.17058, acc=0.93750
# [29/100] testing 47.4% loss=0.14728, acc=0.95312
# [29/100] testing 48.3% loss=0.42194, acc=0.85938
# [29/100] testing 48.7% loss=0.46826, acc=0.79688
# [29/100] testing 49.6% loss=0.58158, acc=0.81250
# [29/100] testing 50.4% loss=0.35541, acc=0.90625
# [29/100] testing 50.9% loss=0.29887, acc=0.85938
# [29/100] testing 51.8% loss=0.27487, acc=0.87500
# [29/100] testing 52.2% loss=0.25596, acc=0.89062
# [29/100] testing 53.1% loss=0.23345, acc=0.87500
# [29/100] testing 53.5% loss=0.24307, acc=0.90625
# [29/100] testing 54.4% loss=0.42096, acc=0.85938
# [29/100] testing 54.8% loss=0.41726, acc=0.82812
# [29/100] testing 55.7% loss=0.14813, acc=0.93750
# [29/100] testing 56.6% loss=0.36771, acc=0.85938
# [29/100] testing 57.0% loss=0.46036, acc=0.85938
# [29/100] testing 57.9% loss=0.27464, acc=0.85938
# [29/100] testing 58.3% loss=0.49400, acc=0.85938
# [29/100] testing 59.2% loss=0.26567, acc=0.89062
# [29/100] testing 59.7% loss=0.26551, acc=0.87500
# [29/100] testing 60.5% loss=0.32444, acc=0.82812
# [29/100] testing 61.4% loss=0.13335, acc=0.93750
# [29/100] testing 61.9% loss=0.28402, acc=0.89062
# [29/100] testing 62.7% loss=0.34209, acc=0.93750
# [29/100] testing 63.2% loss=0.43254, acc=0.84375
# [29/100] testing 64.0% loss=0.51975, acc=0.87500
# [29/100] testing 64.5% loss=0.24228, acc=0.93750
# [29/100] testing 65.4% loss=0.16477, acc=0.93750
# [29/100] testing 65.8% loss=0.41126, acc=0.81250
# [29/100] testing 66.7% loss=0.23244, acc=0.90625
# [29/100] testing 67.6% loss=0.44476, acc=0.87500
# [29/100] testing 68.0% loss=0.18651, acc=0.95312
# [29/100] testing 68.9% loss=0.38312, acc=0.85938
# [29/100] testing 69.3% loss=0.35096, acc=0.85938
# [29/100] testing 70.2% loss=0.44784, acc=0.87500
# [29/100] testing 70.6% loss=0.31909, acc=0.87500
# [29/100] testing 71.5% loss=0.53503, acc=0.84375
# [29/100] testing 72.4% loss=0.16416, acc=0.93750
# [29/100] testing 72.8% loss=0.23352, acc=0.90625
# [29/100] testing 73.7% loss=0.19068, acc=0.93750
# [29/100] testing 74.1% loss=0.36909, acc=0.90625
# [29/100] testing 75.0% loss=0.16504, acc=0.93750
# [29/100] testing 75.4% loss=0.34777, acc=0.87500
# [29/100] testing 76.3% loss=0.09634, acc=0.95312
# [29/100] testing 76.8% loss=0.32484, acc=0.84375
# [29/100] testing 77.6% loss=0.30891, acc=0.90625
# [29/100] testing 78.5% loss=0.51893, acc=0.84375
# [29/100] testing 79.0% loss=0.31205, acc=0.89062
# [29/100] testing 79.8% loss=0.25585, acc=0.90625
# [29/100] testing 80.3% loss=0.35954, acc=0.89062
# [29/100] testing 81.2% loss=0.43974, acc=0.84375
# [29/100] testing 81.6% loss=0.32082, acc=0.87500
# [29/100] testing 82.5% loss=0.22163, acc=0.92188
# [29/100] testing 83.3% loss=0.14534, acc=0.93750
# [29/100] testing 83.8% loss=0.16205, acc=0.92188
# [29/100] testing 84.7% loss=0.30914, acc=0.85938
# [29/100] testing 85.1% loss=0.28527, acc=0.87500
# [29/100] testing 86.0% loss=0.40964, acc=0.82812
# [29/100] testing 86.4% loss=0.43919, acc=0.84375
# [29/100] testing 87.3% loss=0.45253, acc=0.78125
# [29/100] testing 87.7% loss=0.28877, acc=0.89062
# [29/100] testing 88.6% loss=0.32715, acc=0.89062
# [29/100] testing 89.5% loss=0.54618, acc=0.78125
# [29/100] testing 89.9% loss=0.31491, acc=0.84375
# [29/100] testing 90.8% loss=0.26875, acc=0.93750
# [29/100] testing 91.2% loss=0.17158, acc=0.93750
# [29/100] testing 92.1% loss=0.38924, acc=0.84375
# [29/100] testing 92.6% loss=0.25360, acc=0.90625
# [29/100] testing 93.4% loss=0.37486, acc=0.85938
# [29/100] testing 94.3% loss=0.17805, acc=0.90625
# [29/100] testing 94.7% loss=0.24040, acc=0.89062
# [29/100] testing 95.6% loss=0.43930, acc=0.85938
# [29/100] testing 96.1% loss=0.20218, acc=0.87500
# [29/100] testing 96.9% loss=0.23663, acc=0.90625
# [29/100] testing 97.4% loss=0.14860, acc=0.96875
# [29/100] testing 98.3% loss=0.23791, acc=0.89062
# [29/100] testing 98.7% loss=0.28000, acc=0.90625
# [29/100] testing 99.6% loss=0.38340, acc=0.90625
# [30/100] training 0.2% loss=0.45665, acc=0.84375
# [30/100] training 0.4% loss=0.39525, acc=0.82812
# [30/100] training 0.5% loss=0.15505, acc=0.92188
# [30/100] training 0.8% loss=0.28109, acc=0.87500
# [30/100] training 0.9% loss=0.20530, acc=0.89062
# [30/100] training 1.1% loss=0.26255, acc=0.89062
# [30/100] training 1.2% loss=0.31264, acc=0.84375
# [30/100] training 1.4% loss=0.33666, acc=0.90625
# [30/100] training 1.6% loss=0.12129, acc=0.96875
# [30/100] training 1.8% loss=0.17322, acc=0.92188
# [30/100] training 2.0% loss=0.29347, acc=0.87500
# [30/100] training 2.1% loss=0.24671, acc=0.90625
# [30/100] training 2.3% loss=0.22922, acc=0.87500
# [30/100] training 2.4% loss=0.24166, acc=0.90625
# [30/100] training 2.6% loss=0.14797, acc=0.95312
# [30/100] training 2.7% loss=0.25818, acc=0.89062
# [30/100] training 3.0% loss=0.22845, acc=0.89062
# [30/100] training 3.2% loss=0.20142, acc=0.90625
# [30/100] training 3.3% loss=0.37438, acc=0.87500
# [30/100] training 3.5% loss=0.19561, acc=0.93750
# [30/100] training 3.6% loss=0.26535, acc=0.90625
# [30/100] training 3.8% loss=0.24363, acc=0.87500
# [30/100] training 3.9% loss=0.26242, acc=0.87500
# [30/100] training 4.2% loss=0.15906, acc=0.93750
# [30/100] training 4.4% loss=0.14240, acc=0.98438
# [30/100] training 4.5% loss=0.20745, acc=0.92188
# [30/100] training 4.7% loss=0.29995, acc=0.84375
# [30/100] training 4.8% loss=0.24767, acc=0.90625
# [30/100] training 5.0% loss=0.15181, acc=0.93750
# [30/100] training 5.2% loss=0.21239, acc=0.92188
# [30/100] training 5.4% loss=0.12101, acc=0.96875
# [30/100] training 5.5% loss=0.22823, acc=0.89062
# [30/100] training 5.7% loss=0.15858, acc=0.95312
# [30/100] training 5.9% loss=0.37185, acc=0.84375
# [30/100] training 6.0% loss=0.22265, acc=0.90625
# [30/100] training 6.3% loss=0.23502, acc=0.89062
# [30/100] training 6.4% loss=0.14362, acc=0.96875
# [30/100] training 6.6% loss=0.27609, acc=0.89062
# [30/100] training 6.7% loss=0.29222, acc=0.85938
# [30/100] training 6.9% loss=0.18485, acc=0.92188
# [30/100] training 7.1% loss=0.21642, acc=0.92188
# [30/100] training 7.2% loss=0.19773, acc=0.87500
# [30/100] training 7.5% loss=0.18702, acc=0.92188
# [30/100] training 7.6% loss=0.18032, acc=0.92188
# [30/100] training 7.8% loss=0.19620, acc=0.93750
# [30/100] training 7.9% loss=0.20832, acc=0.89062
# [30/100] training 8.1% loss=0.15189, acc=0.90625
# [30/100] training 8.2% loss=0.26124, acc=0.89062
# [30/100] training 8.4% loss=0.27511, acc=0.85938
# [30/100] training 8.7% loss=0.29841, acc=0.92188
# [30/100] training 8.8% loss=0.26720, acc=0.82812
# [30/100] training 9.0% loss=0.24289, acc=0.90625
# [30/100] training 9.1% loss=0.33601, acc=0.85938
# [30/100] training 9.3% loss=0.36651, acc=0.87500
# [30/100] training 9.4% loss=0.15007, acc=0.93750
# [30/100] training 9.7% loss=0.25762, acc=0.93750
# [30/100] training 9.9% loss=0.24601, acc=0.90625
# [30/100] training 10.0% loss=0.20295, acc=0.90625
# [30/100] training 10.2% loss=0.19891, acc=0.92188
# [30/100] training 10.3% loss=0.15236, acc=0.95312
# [30/100] training 10.5% loss=0.49435, acc=0.82812
# [30/100] training 10.6% loss=0.24989, acc=0.90625
# [30/100] training 10.9% loss=0.13091, acc=0.95312
# [30/100] training 11.0% loss=0.31074, acc=0.89062
# [30/100] training 11.2% loss=0.17947, acc=0.96875
# [30/100] training 11.4% loss=0.25548, acc=0.89062
# [30/100] training 11.5% loss=0.33113, acc=0.82812
# [30/100] training 11.7% loss=0.13191, acc=0.96875
# [30/100] training 11.8% loss=0.25233, acc=0.92188
# [30/100] training 12.1% loss=0.18740, acc=0.92188
# [30/100] training 12.2% loss=0.15625, acc=0.95312
# [30/100] training 12.4% loss=0.21248, acc=0.89062
# [30/100] training 12.6% loss=0.26493, acc=0.93750
# [30/100] training 12.7% loss=0.18323, acc=0.92188
# [30/100] training 12.9% loss=0.26907, acc=0.85938
# [30/100] training 13.0% loss=0.11711, acc=0.95312
# [30/100] training 13.3% loss=0.32226, acc=0.89062
# [30/100] training 13.4% loss=0.23246, acc=0.89062
# [30/100] training 13.6% loss=0.20994, acc=0.89062
# [30/100] training 13.7% loss=0.32169, acc=0.87500
# [30/100] training 13.9% loss=0.19275, acc=0.90625
# [30/100] training 14.1% loss=0.27644, acc=0.92188
# [30/100] training 14.3% loss=0.17620, acc=0.92188
# [30/100] training 14.5% loss=0.24639, acc=0.93750
# [30/100] training 14.6% loss=0.20900, acc=0.90625
# [30/100] training 14.8% loss=0.26255, acc=0.89062
# [30/100] training 14.9% loss=0.15970, acc=0.95312
# [30/100] training 15.1% loss=0.36637, acc=0.85938
# [30/100] training 15.4% loss=0.36084, acc=0.87500
# [30/100] training 15.5% loss=0.26100, acc=0.87500
# [30/100] training 15.7% loss=0.41931, acc=0.85938
# [30/100] training 15.8% loss=0.20170, acc=0.90625
# [30/100] training 16.0% loss=0.29839, acc=0.87500
# [30/100] training 16.1% loss=0.36157, acc=0.84375
# [30/100] training 16.3% loss=0.21928, acc=0.89062
# [30/100] training 16.4% loss=0.19079, acc=0.89062
# [30/100] training 16.7% loss=0.29215, acc=0.87500
# [30/100] training 16.9% loss=0.24533, acc=0.92188
# [30/100] training 17.0% loss=0.24043, acc=0.92188
# [30/100] training 17.2% loss=0.23143, acc=0.89062
# [30/100] training 17.3% loss=0.20724, acc=0.95312
# [30/100] training 17.5% loss=0.22906, acc=0.89062
# [30/100] training 17.7% loss=0.19041, acc=0.92188
# [30/100] training 17.9% loss=0.26578, acc=0.89062
# [30/100] training 18.1% loss=0.31113, acc=0.87500
# [30/100] training 18.2% loss=0.24762, acc=0.89062
# [30/100] training 18.4% loss=0.32272, acc=0.85938
# [30/100] training 18.5% loss=0.14871, acc=0.95312
# [30/100] training 18.8% loss=0.17194, acc=0.95312
# [30/100] training 18.9% loss=0.15142, acc=0.93750
# [30/100] training 19.1% loss=0.38979, acc=0.81250
# [30/100] training 19.2% loss=0.15528, acc=0.92188
# [30/100] training 19.4% loss=0.19510, acc=0.90625
# [30/100] training 19.6% loss=0.24860, acc=0.89062
# [30/100] training 19.7% loss=0.32763, acc=0.87500
# [30/100] training 20.0% loss=0.15754, acc=0.93750
# [30/100] training 20.1% loss=0.23368, acc=0.87500
# [30/100] training 20.3% loss=0.31329, acc=0.85938
# [30/100] training 20.4% loss=0.28185, acc=0.87500
# [30/100] training 20.6% loss=0.27086, acc=0.89062
# [30/100] training 20.8% loss=0.17220, acc=0.92188
# [30/100] training 20.9% loss=0.17040, acc=0.93750
# [30/100] training 21.2% loss=0.22989, acc=0.92188
# [30/100] training 21.3% loss=0.25455, acc=0.93750
# [30/100] training 21.5% loss=0.25932, acc=0.92188
# [30/100] training 21.6% loss=0.12023, acc=0.95312
# [30/100] training 21.8% loss=0.29076, acc=0.85938
# [30/100] training 21.9% loss=0.27206, acc=0.90625
# [30/100] training 22.2% loss=0.24527, acc=0.90625
# [30/100] training 22.4% loss=0.24767, acc=0.89062
# [30/100] training 22.5% loss=0.13435, acc=0.95312
# [30/100] training 22.7% loss=0.23690, acc=0.89062
# [30/100] training 22.8% loss=0.24114, acc=0.93750
# [30/100] training 23.0% loss=0.15143, acc=0.95312
# [30/100] training 23.1% loss=0.30371, acc=0.84375
# [30/100] training 23.4% loss=0.29586, acc=0.87500
# [30/100] training 23.6% loss=0.28254, acc=0.92188
# [30/100] training 23.7% loss=0.18485, acc=0.93750
# [30/100] training 23.9% loss=0.23996, acc=0.87500
# [30/100] training 24.0% loss=0.20521, acc=0.92188
# [30/100] training 24.2% loss=0.16965, acc=0.90625
# [30/100] training 24.3% loss=0.33422, acc=0.89062
# [30/100] training 24.6% loss=0.16913, acc=0.92188
# [30/100] training 24.7% loss=0.44061, acc=0.82812
# [30/100] training 24.9% loss=0.19260, acc=0.90625
# [30/100] training 25.1% loss=0.23859, acc=0.85938
# [30/100] training 25.2% loss=0.14113, acc=0.95312
# [30/100] training 25.4% loss=0.23525, acc=0.92188
# [30/100] training 25.6% loss=0.14551, acc=0.96875
# [30/100] training 25.8% loss=0.22837, acc=0.93750
# [30/100] training 25.9% loss=0.17160, acc=0.93750
# [30/100] training 26.1% loss=0.18669, acc=0.93750
# [30/100] training 26.3% loss=0.24302, acc=0.92188
# [30/100] training 26.4% loss=0.13008, acc=0.93750
# [30/100] training 26.6% loss=0.08494, acc=0.96875
# [30/100] training 26.8% loss=0.19281, acc=0.96875
# [30/100] training 27.0% loss=0.18533, acc=0.93750
# [30/100] training 27.1% loss=0.13153, acc=0.92188
# [30/100] training 27.3% loss=0.10354, acc=0.98438
# [30/100] training 27.4% loss=0.13550, acc=0.95312
# [30/100] training 27.6% loss=0.39390, acc=0.89062
# [30/100] training 27.9% loss=0.18108, acc=0.92188
# [30/100] training 28.0% loss=0.31523, acc=0.85938
# [30/100] training 28.2% loss=0.27054, acc=0.90625
# [30/100] training 28.3% loss=0.19982, acc=0.89062
# [30/100] training 28.5% loss=0.28552, acc=0.90625
# [30/100] training 28.6% loss=0.23170, acc=0.95312
# [30/100] training 28.8% loss=0.19196, acc=0.90625
# [30/100] training 29.1% loss=0.29718, acc=0.85938
# [30/100] training 29.2% loss=0.22902, acc=0.89062
# [30/100] training 29.4% loss=0.25106, acc=0.90625
# [30/100] training 29.5% loss=0.16895, acc=0.92188
# [30/100] training 29.7% loss=0.29410, acc=0.85938
# [30/100] training 29.8% loss=0.18863, acc=0.90625
# [30/100] training 30.0% loss=0.26719, acc=0.89062
# [30/100] training 30.2% loss=0.19059, acc=0.92188
# [30/100] training 30.4% loss=0.15287, acc=0.92188
# [30/100] training 30.6% loss=0.31086, acc=0.82812
# [30/100] training 30.7% loss=0.16056, acc=0.93750
# [30/100] training 30.9% loss=0.33388, acc=0.87500
# [30/100] training 31.0% loss=0.17868, acc=0.95312
# [30/100] training 31.3% loss=0.17301, acc=0.93750
# [30/100] training 31.4% loss=0.35257, acc=0.82812
# [30/100] training 31.6% loss=0.31961, acc=0.84375
# [30/100] training 31.8% loss=0.15668, acc=0.92188
# [30/100] training 31.9% loss=0.28091, acc=0.90625
# [30/100] training 32.1% loss=0.21400, acc=0.90625
# [30/100] training 32.2% loss=0.23411, acc=0.92188
# [30/100] training 32.5% loss=0.14544, acc=0.95312
# [30/100] training 32.6% loss=0.21033, acc=0.90625
# [30/100] training 32.8% loss=0.17269, acc=0.92188
# [30/100] training 32.9% loss=0.26561, acc=0.89062
# [30/100] training 33.1% loss=0.19890, acc=0.89062
# [30/100] training 33.3% loss=0.26719, acc=0.89062
# [30/100] training 33.4% loss=0.13625, acc=0.95312
# [30/100] training 33.7% loss=0.25215, acc=0.89062
# [30/100] training 33.8% loss=0.20111, acc=0.90625
# [30/100] training 34.0% loss=0.23996, acc=0.89062
# [30/100] training 34.1% loss=0.26832, acc=0.90625
# [30/100] training 34.3% loss=0.20067, acc=0.90625
# [30/100] training 34.5% loss=0.20927, acc=0.92188
# [30/100] training 34.7% loss=0.20123, acc=0.90625
# [30/100] training 34.9% loss=0.23005, acc=0.85938
# [30/100] training 35.0% loss=0.11536, acc=0.96875
# [30/100] training 35.2% loss=0.30760, acc=0.85938
# [30/100] training 35.3% loss=0.21500, acc=0.90625
# [30/100] training 35.5% loss=0.22357, acc=0.90625
# [30/100] training 35.6% loss=0.33220, acc=0.84375
# [30/100] training 35.9% loss=0.24451, acc=0.89062
# [30/100] training 36.1% loss=0.30127, acc=0.84375
# [30/100] training 36.2% loss=0.25375, acc=0.92188
# [30/100] training 36.4% loss=0.30590, acc=0.87500
# [30/100] training 36.5% loss=0.22579, acc=0.93750
# [30/100] training 36.7% loss=0.22448, acc=0.92188
# [30/100] training 36.8% loss=0.08983, acc=1.00000
# [30/100] training 37.1% loss=0.27573, acc=0.85938
# [30/100] training 37.3% loss=0.23137, acc=0.93750
# [30/100] training 37.4% loss=0.11484, acc=0.95312
# [30/100] training 37.6% loss=0.19455, acc=0.90625
# [30/100] training 37.7% loss=0.30232, acc=0.87500
# [30/100] training 37.9% loss=0.22907, acc=0.87500
# [30/100] training 38.1% loss=0.31319, acc=0.89062
# [30/100] training 38.3% loss=0.22817, acc=0.93750
# [30/100] training 38.4% loss=0.14736, acc=0.93750
# [30/100] training 38.6% loss=0.19132, acc=0.90625
# [30/100] training 38.8% loss=0.28522, acc=0.87500
# [30/100] training 38.9% loss=0.21428, acc=0.92188
# [30/100] training 39.1% loss=0.23502, acc=0.93750
# [30/100] training 39.3% loss=0.23526, acc=0.90625
# [30/100] training 39.5% loss=0.29923, acc=0.85938
# [30/100] training 39.6% loss=0.15116, acc=0.95312
# [30/100] training 39.8% loss=0.12941, acc=0.95312
# [30/100] training 40.0% loss=0.20520, acc=0.89062
# [30/100] training 40.1% loss=0.23647, acc=0.89062
# [30/100] training 40.4% loss=0.23207, acc=0.87500
# [30/100] training 40.5% loss=0.19028, acc=0.92188
# [30/100] training 40.7% loss=0.18417, acc=0.90625
# [30/100] training 40.8% loss=0.13397, acc=0.93750
# [30/100] training 41.0% loss=0.10969, acc=0.93750
# [30/100] training 41.1% loss=0.35287, acc=0.87500
# [30/100] training 41.3% loss=0.30002, acc=0.87500
# [30/100] training 41.6% loss=0.28976, acc=0.87500
# [30/100] training 41.7% loss=0.29828, acc=0.87500
# [30/100] training 41.9% loss=0.22204, acc=0.92188
# [30/100] training 42.0% loss=0.30069, acc=0.84375
# [30/100] training 42.2% loss=0.20066, acc=0.92188
# [30/100] training 42.3% loss=0.22229, acc=0.90625
# [30/100] training 42.5% loss=0.20057, acc=0.89062
# [30/100] training 42.8% loss=0.10960, acc=0.95312
# [30/100] training 42.9% loss=0.17975, acc=0.92188
# [30/100] training 43.1% loss=0.20928, acc=0.93750
# [30/100] training 43.2% loss=0.14924, acc=0.93750
# [30/100] training 43.4% loss=0.27385, acc=0.90625
# [30/100] training 43.5% loss=0.30689, acc=0.85938
# [30/100] training 43.8% loss=0.20159, acc=0.93750
# [30/100] training 43.9% loss=0.18243, acc=0.93750
# [30/100] training 44.1% loss=0.13796, acc=0.95312
# [30/100] training 44.3% loss=0.19746, acc=0.93750
# [30/100] training 44.4% loss=0.23898, acc=0.90625
# [30/100] training 44.6% loss=0.23285, acc=0.89062
# [30/100] training 44.7% loss=0.36456, acc=0.84375
# [30/100] training 45.0% loss=0.18168, acc=0.90625
# [30/100] training 45.1% loss=0.34893, acc=0.87500
# [30/100] training 45.3% loss=0.25626, acc=0.87500
# [30/100] training 45.5% loss=0.06057, acc=1.00000
# [30/100] training 45.6% loss=0.22437, acc=0.92188
# [30/100] training 45.8% loss=0.08415, acc=0.98438
# [30/100] training 45.9% loss=0.21007, acc=0.90625
# [30/100] training 46.2% loss=0.11155, acc=0.93750
# [30/100] training 46.3% loss=0.17185, acc=0.92188
# [30/100] training 46.5% loss=0.28565, acc=0.85938
# [30/100] training 46.6% loss=0.21697, acc=0.90625
# [30/100] training 46.8% loss=0.18351, acc=0.93750
# [30/100] training 47.0% loss=0.24613, acc=0.87500
# [30/100] training 47.2% loss=0.34517, acc=0.87500
# [30/100] training 47.4% loss=0.25954, acc=0.85938
# [30/100] training 47.5% loss=0.30318, acc=0.92188
# [30/100] training 47.7% loss=0.17539, acc=0.95312
# [30/100] training 47.8% loss=0.32470, acc=0.87500
# [30/100] training 48.0% loss=0.23581, acc=0.87500
# [30/100] training 48.3% loss=0.11949, acc=0.92188
# [30/100] training 48.4% loss=0.12378, acc=0.95312
# [30/100] training 48.6% loss=0.11370, acc=0.95312
# [30/100] training 48.7% loss=0.27689, acc=0.85938
# [30/100] training 48.9% loss=0.27273, acc=0.85938
# [30/100] training 49.0% loss=0.13615, acc=0.95312
# [30/100] training 49.2% loss=0.42255, acc=0.84375
# [30/100] training 49.3% loss=0.15954, acc=0.95312
# [30/100] training 49.6% loss=0.27963, acc=0.87500
# [30/100] training 49.8% loss=0.26517, acc=0.87500
# [30/100] training 49.9% loss=0.28512, acc=0.89062
# [30/100] training 50.1% loss=0.17051, acc=0.95312
# [30/100] training 50.2% loss=0.24076, acc=0.87500
# [30/100] training 50.4% loss=0.31724, acc=0.87500
# [30/100] training 50.6% loss=0.18051, acc=0.89062
# [30/100] training 50.8% loss=0.27708, acc=0.87500
# [30/100] training 51.0% loss=0.27885, acc=0.89062
# [30/100] training 51.1% loss=0.28128, acc=0.90625
# [30/100] training 51.3% loss=0.17956, acc=0.93750
# [30/100] training 51.4% loss=0.25629, acc=0.92188
# [30/100] training 51.7% loss=0.23776, acc=0.87500
# [30/100] training 51.8% loss=0.29212, acc=0.82812
# [30/100] training 52.0% loss=0.18161, acc=0.93750
# [30/100] training 52.1% loss=0.18254, acc=0.95312
# [30/100] training 52.3% loss=0.29536, acc=0.89062
# [30/100] training 52.5% loss=0.16835, acc=0.92188
# [30/100] training 52.6% loss=0.16048, acc=0.96875
# [30/100] training 52.9% loss=0.30204, acc=0.90625
# [30/100] training 53.0% loss=0.14272, acc=0.93750
# [30/100] training 53.2% loss=0.21733, acc=0.92188
# [30/100] training 53.3% loss=0.17974, acc=0.96875
# [30/100] training 53.5% loss=0.23666, acc=0.92188
# [30/100] training 53.7% loss=0.16617, acc=0.95312
# [30/100] training 53.8% loss=0.26199, acc=0.87500
# [30/100] training 54.1% loss=0.30766, acc=0.84375
# [30/100] training 54.2% loss=0.12075, acc=0.93750
# [30/100] training 54.4% loss=0.17104, acc=0.93750
# [30/100] training 54.5% loss=0.29556, acc=0.89062
# [30/100] training 54.7% loss=0.38656, acc=0.82812
# [30/100] training 54.8% loss=0.12173, acc=0.95312
# [30/100] training 55.1% loss=0.18692, acc=0.93750
# [30/100] training 55.3% loss=0.14049, acc=0.96875
# [30/100] training 55.4% loss=0.18099, acc=0.93750
# [30/100] training 55.6% loss=0.19610, acc=0.93750
# [30/100] training 55.7% loss=0.22670, acc=0.89062
# [30/100] training 55.9% loss=0.18988, acc=0.90625
# [30/100] training 56.0% loss=0.18555, acc=0.93750
# [30/100] training 56.3% loss=0.43400, acc=0.85938
# [30/100] training 56.5% loss=0.22145, acc=0.90625
# [30/100] training 56.6% loss=0.36820, acc=0.82812
# [30/100] training 56.8% loss=0.22975, acc=0.89062
# [30/100] training 56.9% loss=0.21186, acc=0.90625
# [30/100] training 57.1% loss=0.24883, acc=0.90625
# [30/100] training 57.2% loss=0.11771, acc=0.95312
# [30/100] training 57.5% loss=0.15706, acc=0.95312
# [30/100] training 57.6% loss=0.18476, acc=0.90625
# [30/100] training 57.8% loss=0.19868, acc=0.92188
# [30/100] training 58.0% loss=0.13606, acc=0.96875
# [30/100] training 58.1% loss=0.18220, acc=0.92188
# [30/100] training 58.3% loss=0.14542, acc=0.95312
# [30/100] training 58.4% loss=0.24420, acc=0.93750
# [30/100] training 58.7% loss=0.38223, acc=0.89062
# [30/100] training 58.8% loss=0.31272, acc=0.89062
# [30/100] training 59.0% loss=0.17766, acc=0.90625
# [30/100] training 59.2% loss=0.20361, acc=0.89062
# [30/100] training 59.3% loss=0.18053, acc=0.92188
# [30/100] training 59.5% loss=0.24061, acc=0.90625
# [30/100] training 59.7% loss=0.31628, acc=0.89062
# [30/100] training 59.9% loss=0.17218, acc=0.93750
# [30/100] training 60.0% loss=0.20031, acc=0.90625
# [30/100] training 60.2% loss=0.24482, acc=0.89062
# [30/100] training 60.3% loss=0.17741, acc=0.92188
# [30/100] training 60.5% loss=0.22971, acc=0.87500
# [30/100] training 60.8% loss=0.20614, acc=0.89062
# [30/100] training 60.9% loss=0.34416, acc=0.82812
# [30/100] training 61.1% loss=0.27307, acc=0.85938
# [30/100] training 61.2% loss=0.13340, acc=0.96875
# [30/100] training 61.4% loss=0.27654, acc=0.93750
# [30/100] training 61.5% loss=0.37052, acc=0.82812
# [30/100] training 61.7% loss=0.20396, acc=0.92188
# [30/100] training 62.0% loss=0.30456, acc=0.85938
# [30/100] training 62.1% loss=0.31771, acc=0.84375
# [30/100] training 62.3% loss=0.10031, acc=0.96875
# [30/100] training 62.4% loss=0.24621, acc=0.87500
# [30/100] training 62.6% loss=0.29388, acc=0.82812
# [30/100] training 62.7% loss=0.13040, acc=0.95312
# [30/100] training 62.9% loss=0.19013, acc=0.93750
# [30/100] training 63.1% loss=0.24094, acc=0.89062
# [30/100] training 63.3% loss=0.20947, acc=0.90625
# [30/100] training 63.5% loss=0.22822, acc=0.92188
# [30/100] training 63.6% loss=0.27062, acc=0.92188
# [30/100] training 63.8% loss=0.26515, acc=0.89062
# [30/100] training 63.9% loss=0.16699, acc=0.95312
# [30/100] training 64.2% loss=0.20484, acc=0.92188
# [30/100] training 64.3% loss=0.25171, acc=0.89062
# [30/100] training 64.5% loss=0.25412, acc=0.89062
# [30/100] training 64.7% loss=0.17475, acc=0.90625
# [30/100] training 64.8% loss=0.35929, acc=0.81250
# [30/100] training 65.0% loss=0.22673, acc=0.89062
# [30/100] training 65.1% loss=0.31696, acc=0.85938
# [30/100] training 65.4% loss=0.20837, acc=0.90625
# [30/100] training 65.5% loss=0.17171, acc=0.93750
# [30/100] training 65.7% loss=0.13943, acc=0.93750
# [30/100] training 65.8% loss=0.20067, acc=0.93750
# [30/100] training 66.0% loss=0.26549, acc=0.89062
# [30/100] training 66.2% loss=0.08325, acc=0.98438
# [30/100] training 66.3% loss=0.36435, acc=0.84375
# [30/100] training 66.6% loss=0.31018, acc=0.85938
# [30/100] training 66.7% loss=0.14970, acc=0.93750
# [30/100] training 66.9% loss=0.22206, acc=0.87500
# [30/100] training 67.0% loss=0.25433, acc=0.90625
# [30/100] training 67.2% loss=0.19888, acc=0.95312
# [30/100] training 67.4% loss=0.15996, acc=0.90625
# [30/100] training 67.6% loss=0.22923, acc=0.92188
# [30/100] training 67.8% loss=0.12955, acc=0.96875
# [30/100] training 67.9% loss=0.08117, acc=0.98438
# [30/100] training 68.1% loss=0.13376, acc=0.95312
# [30/100] training 68.2% loss=0.16280, acc=0.93750
# [30/100] training 68.4% loss=0.16538, acc=0.93750
# [30/100] training 68.5% loss=0.26343, acc=0.90625
# [30/100] training 68.8% loss=0.19229, acc=0.92188
# [30/100] training 69.0% loss=0.28184, acc=0.90625
# [30/100] training 69.1% loss=0.14285, acc=0.93750
# [30/100] training 69.3% loss=0.23431, acc=0.89062
# [30/100] training 69.4% loss=0.17854, acc=0.93750
# [30/100] training 69.6% loss=0.25359, acc=0.89062
# [30/100] training 69.7% loss=0.17644, acc=0.89062
# [30/100] training 70.0% loss=0.39384, acc=0.85938
# [30/100] training 70.2% loss=0.27778, acc=0.89062
# [30/100] training 70.3% loss=0.19337, acc=0.92188
# [30/100] training 70.5% loss=0.19944, acc=0.93750
# [30/100] training 70.6% loss=0.12786, acc=0.96875
# [30/100] training 70.8% loss=0.24716, acc=0.89062
# [30/100] training 71.0% loss=0.22203, acc=0.92188
# [30/100] training 71.2% loss=0.18372, acc=0.90625
# [30/100] training 71.3% loss=0.15441, acc=0.92188
# [30/100] training 71.5% loss=0.29407, acc=0.95312
# [30/100] training 71.7% loss=0.25975, acc=0.92188
# [30/100] training 71.8% loss=0.24004, acc=0.92188
# [30/100] training 72.0% loss=0.16993, acc=0.90625
# [30/100] training 72.2% loss=0.22380, acc=0.90625
# [30/100] training 72.4% loss=0.23267, acc=0.89062
# [30/100] training 72.5% loss=0.31375, acc=0.87500
# [30/100] training 72.7% loss=0.29647, acc=0.85938
# [30/100] training 72.9% loss=0.18081, acc=0.92188
# [30/100] training 73.0% loss=0.16002, acc=0.93750
# [30/100] training 73.3% loss=0.29963, acc=0.90625
# [30/100] training 73.4% loss=0.19632, acc=0.89062
# [30/100] training 73.6% loss=0.09445, acc=0.96875
# [30/100] training 73.7% loss=0.18197, acc=0.92188
# [30/100] training 73.9% loss=0.17379, acc=0.93750
# [30/100] training 74.0% loss=0.22364, acc=0.90625
# [30/100] training 74.2% loss=0.16870, acc=0.93750
# [30/100] training 74.5% loss=0.14801, acc=0.92188
# [30/100] training 74.6% loss=0.41708, acc=0.85938
# [30/100] training 74.8% loss=0.31085, acc=0.92188
# [30/100] training 74.9% loss=0.38130, acc=0.84375
# [30/100] training 75.1% loss=0.14880, acc=0.95312
# [30/100] training 75.2% loss=0.20602, acc=0.93750
# [30/100] training 75.4% loss=0.22365, acc=0.89062
# [30/100] training 75.7% loss=0.24923, acc=0.85938
# [30/100] training 75.8% loss=0.40035, acc=0.87500
# [30/100] training 76.0% loss=0.15368, acc=0.92188
# [30/100] training 76.1% loss=0.26185, acc=0.87500
# [30/100] training 76.3% loss=0.11634, acc=0.95312
# [30/100] training 76.4% loss=0.18557, acc=0.93750
# [30/100] training 76.7% loss=0.19928, acc=0.93750
# [30/100] training 76.8% loss=0.14439, acc=0.95312
# [30/100] training 77.0% loss=0.15098, acc=0.93750
# [30/100] training 77.2% loss=0.27756, acc=0.89062
# [30/100] training 77.3% loss=0.12649, acc=0.95312
# [30/100] training 77.5% loss=0.20884, acc=0.90625
# [30/100] training 77.6% loss=0.29876, acc=0.92188
# [30/100] training 77.9% loss=0.23986, acc=0.90625
# [30/100] training 78.0% loss=0.21523, acc=0.93750
# [30/100] training 78.2% loss=0.24627, acc=0.87500
# [30/100] training 78.4% loss=0.12374, acc=0.92188
# [30/100] training 78.5% loss=0.30345, acc=0.89062
# [30/100] training 78.7% loss=0.21513, acc=0.90625
# [30/100] training 78.8% loss=0.10861, acc=0.98438
# [30/100] training 79.1% loss=0.11117, acc=0.98438
# [30/100] training 79.2% loss=0.14579, acc=0.96875
# [30/100] training 79.4% loss=0.23615, acc=0.90625
# [30/100] training 79.5% loss=0.12381, acc=0.92188
# [30/100] training 79.7% loss=0.14123, acc=0.90625
# [30/100] training 79.9% loss=0.17515, acc=0.93750
# [30/100] training 80.1% loss=0.19407, acc=0.92188
# [30/100] training 80.3% loss=0.33713, acc=0.89062
# [30/100] training 80.4% loss=0.20918, acc=0.89062
# [30/100] training 80.6% loss=0.29400, acc=0.87500
# [30/100] training 80.7% loss=0.20720, acc=0.90625
# [30/100] training 80.9% loss=0.35440, acc=0.85938
# [30/100] training 81.2% loss=0.28834, acc=0.89062
# [30/100] training 81.3% loss=0.28134, acc=0.87500
# [30/100] training 81.5% loss=0.23782, acc=0.90625
# [30/100] training 81.6% loss=0.30686, acc=0.87500
# [30/100] training 81.8% loss=0.28410, acc=0.87500
# [30/100] training 81.9% loss=0.34014, acc=0.90625
# [30/100] training 82.1% loss=0.14176, acc=0.96875
# [30/100] training 82.2% loss=0.18340, acc=0.93750
# [30/100] training 82.5% loss=0.14725, acc=0.92188
# [30/100] training 82.7% loss=0.21867, acc=0.92188
# [30/100] training 82.8% loss=0.18799, acc=0.90625
# [30/100] training 83.0% loss=0.18254, acc=0.93750
# [30/100] training 83.1% loss=0.19570, acc=0.89062
# [30/100] training 83.3% loss=0.11453, acc=0.96875
# [30/100] training 83.5% loss=0.15841, acc=0.96875
# [30/100] training 83.7% loss=0.24196, acc=0.89062
# [30/100] training 83.9% loss=0.18966, acc=0.92188
# [30/100] training 84.0% loss=0.17966, acc=0.92188
# [30/100] training 84.2% loss=0.12675, acc=0.95312
# [30/100] training 84.3% loss=0.14284, acc=0.93750
# [30/100] training 84.5% loss=0.20035, acc=0.92188
# [30/100] training 84.7% loss=0.21365, acc=0.92188
# [30/100] training 84.9% loss=0.18255, acc=0.93750
# [30/100] training 85.0% loss=0.24753, acc=0.93750
# [30/100] training 85.2% loss=0.16725, acc=0.93750
# [30/100] training 85.4% loss=0.19807, acc=0.95312
# [30/100] training 85.5% loss=0.22695, acc=0.87500
# [30/100] training 85.8% loss=0.20116, acc=0.90625
# [30/100] training 85.9% loss=0.15334, acc=0.95312
# [30/100] training 86.1% loss=0.25579, acc=0.87500
# [30/100] training 86.2% loss=0.14814, acc=0.93750
# [30/100] training 86.4% loss=0.26767, acc=0.87500
# [30/100] training 86.6% loss=0.18148, acc=0.92188
# [30/100] training 86.7% loss=0.19422, acc=0.89062
# [30/100] training 87.0% loss=0.33009, acc=0.87500
# [30/100] training 87.1% loss=0.21567, acc=0.87500
# [30/100] training 87.3% loss=0.23171, acc=0.92188
# [30/100] training 87.4% loss=0.20358, acc=0.92188
# [30/100] training 87.6% loss=0.22512, acc=0.89062
# [30/100] training 87.7% loss=0.31353, acc=0.92188
# [30/100] training 87.9% loss=0.25546, acc=0.90625
# [30/100] training 88.2% loss=0.19826, acc=0.92188
# [30/100] training 88.3% loss=0.24985, acc=0.90625
# [30/100] training 88.5% loss=0.23198, acc=0.90625
# [30/100] training 88.6% loss=0.16315, acc=0.95312
# [30/100] training 88.8% loss=0.19658, acc=0.93750
# [30/100] training 88.9% loss=0.22826, acc=0.87500
# [30/100] training 89.2% loss=0.16950, acc=0.92188
# [30/100] training 89.4% loss=0.17191, acc=0.92188
# [30/100] training 89.5% loss=0.17094, acc=0.92188
# [30/100] training 89.7% loss=0.22047, acc=0.90625
# [30/100] training 89.8% loss=0.22802, acc=0.96875
# [30/100] training 90.0% loss=0.16847, acc=0.93750
# [30/100] training 90.1% loss=0.25927, acc=0.89062
# [30/100] training 90.4% loss=0.24888, acc=0.89062
# [30/100] training 90.5% loss=0.16616, acc=0.95312
# [30/100] training 90.7% loss=0.14312, acc=0.93750
# [30/100] training 90.9% loss=0.16902, acc=0.90625
# [30/100] training 91.0% loss=0.20435, acc=0.95312
# [30/100] training 91.2% loss=0.19561, acc=0.93750
# [30/100] training 91.3% loss=0.28017, acc=0.90625
# [30/100] training 91.6% loss=0.33533, acc=0.87500
# [30/100] training 91.7% loss=0.17776, acc=0.92188
# [30/100] training 91.9% loss=0.27078, acc=0.89062
# [30/100] training 92.1% loss=0.15963, acc=0.87500
# [30/100] training 92.2% loss=0.12260, acc=0.92188
# [30/100] training 92.4% loss=0.20727, acc=0.89062
# [30/100] training 92.6% loss=0.34575, acc=0.85938
# [30/100] training 92.8% loss=0.28854, acc=0.87500
# [30/100] training 92.9% loss=0.22844, acc=0.92188
# [30/100] training 93.1% loss=0.39388, acc=0.82812
# [30/100] training 93.2% loss=0.32500, acc=0.89062
# [30/100] training 93.4% loss=0.31097, acc=0.89062
# [30/100] training 93.7% loss=0.24354, acc=0.90625
# [30/100] training 93.8% loss=0.20962, acc=0.92188
# [30/100] training 94.0% loss=0.23682, acc=0.92188
# [30/100] training 94.1% loss=0.21115, acc=0.93750
# [30/100] training 94.3% loss=0.16653, acc=0.93750
# [30/100] training 94.4% loss=0.17072, acc=0.92188
# [30/100] training 94.6% loss=0.13175, acc=0.96875
# [30/100] training 94.9% loss=0.13413, acc=0.95312
# [30/100] training 95.0% loss=0.19154, acc=0.90625
# [30/100] training 95.2% loss=0.36597, acc=0.87500
# [30/100] training 95.3% loss=0.35443, acc=0.87500
# [30/100] training 95.5% loss=0.12973, acc=0.95312
# [30/100] training 95.6% loss=0.27314, acc=0.89062
# [30/100] training 95.8% loss=0.22909, acc=0.90625
# [30/100] training 96.0% loss=0.27165, acc=0.90625
# [30/100] training 96.2% loss=0.19076, acc=0.93750
# [30/100] training 96.4% loss=0.15609, acc=0.96875
# [30/100] training 96.5% loss=0.22854, acc=0.90625
# [30/100] training 96.7% loss=0.15931, acc=0.93750
# [30/100] training 96.8% loss=0.26293, acc=0.90625
# [30/100] training 97.1% loss=0.15363, acc=0.96875
# [30/100] training 97.2% loss=0.17871, acc=0.95312
# [30/100] training 97.4% loss=0.15900, acc=0.95312
# [30/100] training 97.6% loss=0.33750, acc=0.84375
# [30/100] training 97.7% loss=0.10891, acc=0.95312
# [30/100] training 97.9% loss=0.20143, acc=0.92188
# [30/100] training 98.0% loss=0.25088, acc=0.92188
# [30/100] training 98.3% loss=0.21264, acc=0.89062
# [30/100] training 98.4% loss=0.19620, acc=0.90625
# [30/100] training 98.6% loss=0.39180, acc=0.84375
# [30/100] training 98.7% loss=0.29093, acc=0.89062
# [30/100] training 98.9% loss=0.21601, acc=0.90625
# [30/100] training 99.1% loss=0.23678, acc=0.87500
# [30/100] training 99.2% loss=0.18308, acc=0.93750
# [30/100] training 99.5% loss=0.28139, acc=0.87500
# [30/100] training 99.6% loss=0.26427, acc=0.87500
# [30/100] training 99.8% loss=0.14474, acc=0.96875
# [30/100] training 99.9% loss=0.12134, acc=0.96875
# [30/100] testing 0.9% loss=0.16752, acc=0.89062
# [30/100] testing 1.8% loss=0.29800, acc=0.87500
# [30/100] testing 2.2% loss=0.29373, acc=0.84375
# [30/100] testing 3.1% loss=0.32668, acc=0.84375
# [30/100] testing 3.5% loss=0.18125, acc=0.90625
# [30/100] testing 4.4% loss=0.21439, acc=0.93750
# [30/100] testing 4.8% loss=0.32712, acc=0.85938
# [30/100] testing 5.7% loss=0.23677, acc=0.92188
# [30/100] testing 6.6% loss=0.18840, acc=0.90625
# [30/100] testing 7.0% loss=0.19201, acc=0.90625
# [30/100] testing 7.9% loss=0.27219, acc=0.84375
# [30/100] testing 8.3% loss=0.30627, acc=0.89062
# [30/100] testing 9.2% loss=0.29182, acc=0.85938
# [30/100] testing 9.7% loss=0.15612, acc=0.93750
# [30/100] testing 10.5% loss=0.30794, acc=0.85938
# [30/100] testing 11.0% loss=0.31867, acc=0.85938
# [30/100] testing 11.8% loss=0.33410, acc=0.85938
# [30/100] testing 12.7% loss=0.33917, acc=0.90625
# [30/100] testing 13.2% loss=0.22185, acc=0.90625
# [30/100] testing 14.0% loss=0.27737, acc=0.93750
# [30/100] testing 14.5% loss=0.32146, acc=0.87500
# [30/100] testing 15.4% loss=0.31897, acc=0.90625
# [30/100] testing 15.8% loss=0.18117, acc=0.90625
# [30/100] testing 16.7% loss=0.17721, acc=0.90625
# [30/100] testing 17.5% loss=0.14583, acc=0.95312
# [30/100] testing 18.0% loss=0.17683, acc=0.93750
# [30/100] testing 18.9% loss=0.16776, acc=0.92188
# [30/100] testing 19.3% loss=0.34048, acc=0.85938
# [30/100] testing 20.2% loss=0.25237, acc=0.93750
# [30/100] testing 20.6% loss=0.32500, acc=0.85938
# [30/100] testing 21.5% loss=0.18113, acc=0.93750
# [30/100] testing 21.9% loss=0.36480, acc=0.90625
# [30/100] testing 22.8% loss=0.35019, acc=0.82812
# [30/100] testing 23.7% loss=0.29189, acc=0.90625
# [30/100] testing 24.1% loss=0.20065, acc=0.95312
# [30/100] testing 25.0% loss=0.43108, acc=0.84375
# [30/100] testing 25.4% loss=0.17481, acc=0.95312
# [30/100] testing 26.3% loss=0.21391, acc=0.89062
# [30/100] testing 26.8% loss=0.32734, acc=0.89062
# [30/100] testing 27.6% loss=0.28175, acc=0.90625
# [30/100] testing 28.5% loss=0.22780, acc=0.93750
# [30/100] testing 29.0% loss=0.17640, acc=0.96875
# [30/100] testing 29.8% loss=0.39865, acc=0.85938
# [30/100] testing 30.3% loss=0.28759, acc=0.90625
# [30/100] testing 31.1% loss=0.27759, acc=0.87500
# [30/100] testing 31.6% loss=0.14219, acc=0.95312
# [30/100] testing 32.5% loss=0.21199, acc=0.92188
# [30/100] testing 32.9% loss=0.44783, acc=0.85938
# [30/100] testing 33.8% loss=0.24900, acc=0.90625
# [30/100] testing 34.7% loss=0.26127, acc=0.90625
# [30/100] testing 35.1% loss=0.23093, acc=0.90625
# [30/100] testing 36.0% loss=0.38859, acc=0.87500
# [30/100] testing 36.4% loss=0.28664, acc=0.90625
# [30/100] testing 37.3% loss=0.31948, acc=0.90625
# [30/100] testing 37.7% loss=0.49848, acc=0.82812
# [30/100] testing 38.6% loss=0.29000, acc=0.90625
# [30/100] testing 39.5% loss=0.27966, acc=0.93750
# [30/100] testing 39.9% loss=0.38212, acc=0.87500
# [30/100] testing 40.8% loss=0.35767, acc=0.89062
# [30/100] testing 41.2% loss=0.28661, acc=0.93750
# [30/100] testing 42.1% loss=0.27209, acc=0.90625
# [30/100] testing 42.5% loss=0.24646, acc=0.89062
# [30/100] testing 43.4% loss=0.37850, acc=0.87500
# [30/100] testing 43.9% loss=0.16743, acc=0.93750
# [30/100] testing 44.7% loss=0.43825, acc=0.81250
# [30/100] testing 45.6% loss=0.35684, acc=0.84375
# [30/100] testing 46.1% loss=0.22747, acc=0.89062
# [30/100] testing 46.9% loss=0.27890, acc=0.89062
# [30/100] testing 47.4% loss=0.12352, acc=0.95312
# [30/100] testing 48.3% loss=0.31768, acc=0.85938
# [30/100] testing 48.7% loss=0.44770, acc=0.84375
# [30/100] testing 49.6% loss=0.51344, acc=0.81250
# [30/100] testing 50.4% loss=0.19907, acc=0.90625
# [30/100] testing 50.9% loss=0.24955, acc=0.89062
# [30/100] testing 51.8% loss=0.26240, acc=0.92188
# [30/100] testing 52.2% loss=0.27161, acc=0.87500
# [30/100] testing 53.1% loss=0.22216, acc=0.89062
# [30/100] testing 53.5% loss=0.22581, acc=0.90625
# [30/100] testing 54.4% loss=0.43760, acc=0.81250
# [30/100] testing 54.8% loss=0.35201, acc=0.84375
# [30/100] testing 55.7% loss=0.19613, acc=0.90625
# [30/100] testing 56.6% loss=0.30749, acc=0.89062
# [30/100] testing 57.0% loss=0.46721, acc=0.84375
# [30/100] testing 57.9% loss=0.16795, acc=0.92188
# [30/100] testing 58.3% loss=0.35254, acc=0.85938
# [30/100] testing 59.2% loss=0.24798, acc=0.89062
# [30/100] testing 59.7% loss=0.24661, acc=0.89062
# [30/100] testing 60.5% loss=0.30015, acc=0.87500
# [30/100] testing 61.4% loss=0.11637, acc=0.95312
# [30/100] testing 61.9% loss=0.18133, acc=0.92188
# [30/100] testing 62.7% loss=0.24950, acc=0.90625
# [30/100] testing 63.2% loss=0.36313, acc=0.85938
# [30/100] testing 64.0% loss=0.52095, acc=0.79688
# [30/100] testing 64.5% loss=0.20024, acc=0.89062
# [30/100] testing 65.4% loss=0.17811, acc=0.95312
# [30/100] testing 65.8% loss=0.32625, acc=0.85938
# [30/100] testing 66.7% loss=0.25556, acc=0.90625
# [30/100] testing 67.6% loss=0.36898, acc=0.89062
# [30/100] testing 68.0% loss=0.20903, acc=0.92188
# [30/100] testing 68.9% loss=0.26833, acc=0.89062
# [30/100] testing 69.3% loss=0.34181, acc=0.85938
# [30/100] testing 70.2% loss=0.40832, acc=0.84375
# [30/100] testing 70.6% loss=0.32775, acc=0.85938
# [30/100] testing 71.5% loss=0.42868, acc=0.85938
# [30/100] testing 72.4% loss=0.10309, acc=0.95312
# [30/100] testing 72.8% loss=0.16073, acc=0.90625
# [30/100] testing 73.7% loss=0.21588, acc=0.92188
# [30/100] testing 74.1% loss=0.44323, acc=0.85938
# [30/100] testing 75.0% loss=0.20957, acc=0.89062
# [30/100] testing 75.4% loss=0.38173, acc=0.82812
# [30/100] testing 76.3% loss=0.10007, acc=0.95312
# [30/100] testing 76.8% loss=0.26623, acc=0.84375
# [30/100] testing 77.6% loss=0.24450, acc=0.87500
# [30/100] testing 78.5% loss=0.43259, acc=0.82812
# [30/100] testing 79.0% loss=0.28187, acc=0.85938
# [30/100] testing 79.8% loss=0.26426, acc=0.89062
# [30/100] testing 80.3% loss=0.24374, acc=0.90625
# [30/100] testing 81.2% loss=0.39887, acc=0.81250
# [30/100] testing 81.6% loss=0.27341, acc=0.90625
# [30/100] testing 82.5% loss=0.26026, acc=0.89062
# [30/100] testing 83.3% loss=0.19108, acc=0.95312
# [30/100] testing 83.8% loss=0.17168, acc=0.93750
# [30/100] testing 84.7% loss=0.37707, acc=0.84375
# [30/100] testing 85.1% loss=0.24610, acc=0.85938
# [30/100] testing 86.0% loss=0.32816, acc=0.85938
# [30/100] testing 86.4% loss=0.42544, acc=0.89062
# [30/100] testing 87.3% loss=0.30017, acc=0.82812
# [30/100] testing 87.7% loss=0.28540, acc=0.87500
# [30/100] testing 88.6% loss=0.34888, acc=0.85938
# [30/100] testing 89.5% loss=0.51098, acc=0.78125
# [30/100] testing 89.9% loss=0.25255, acc=0.90625
# [30/100] testing 90.8% loss=0.33069, acc=0.90625
# [30/100] testing 91.2% loss=0.17096, acc=0.92188
# [30/100] testing 92.1% loss=0.39277, acc=0.82812
# [30/100] testing 92.6% loss=0.35045, acc=0.85938
# [30/100] testing 93.4% loss=0.53411, acc=0.79688
# [30/100] testing 94.3% loss=0.13422, acc=0.93750
# [30/100] testing 94.7% loss=0.17141, acc=0.93750
# [30/100] testing 95.6% loss=0.35670, acc=0.84375
# [30/100] testing 96.1% loss=0.18473, acc=0.92188
# [30/100] testing 96.9% loss=0.23950, acc=0.89062
# [30/100] testing 97.4% loss=0.14679, acc=0.95312
# [30/100] testing 98.3% loss=0.32241, acc=0.87500
# [30/100] testing 98.7% loss=0.29679, acc=0.85938
# [30/100] testing 99.6% loss=0.28687, acc=0.92188
# [31/100] training 0.2% loss=0.40564, acc=0.79688
# [31/100] training 0.4% loss=0.36229, acc=0.84375
# [31/100] training 0.5% loss=0.16668, acc=0.89062
# [31/100] training 0.8% loss=0.18663, acc=0.95312
# [31/100] training 0.9% loss=0.15549, acc=0.95312
# [31/100] training 1.1% loss=0.23106, acc=0.93750
# [31/100] training 1.2% loss=0.18751, acc=0.92188
# [31/100] training 1.4% loss=0.35403, acc=0.90625
# [31/100] training 1.6% loss=0.12539, acc=0.93750
# [31/100] training 1.8% loss=0.23532, acc=0.90625
# [31/100] training 2.0% loss=0.29409, acc=0.84375
# [31/100] training 2.1% loss=0.28797, acc=0.87500
# [31/100] training 2.3% loss=0.22690, acc=0.90625
# [31/100] training 2.4% loss=0.24221, acc=0.93750
# [31/100] training 2.6% loss=0.20839, acc=0.93750
# [31/100] training 2.7% loss=0.22841, acc=0.93750
# [31/100] training 3.0% loss=0.17729, acc=0.92188
# [31/100] training 3.2% loss=0.12545, acc=0.96875
# [31/100] training 3.3% loss=0.38864, acc=0.87500
# [31/100] training 3.5% loss=0.24460, acc=0.85938
# [31/100] training 3.6% loss=0.29376, acc=0.84375
# [31/100] training 3.8% loss=0.24096, acc=0.87500
# [31/100] training 3.9% loss=0.17746, acc=0.93750
# [31/100] training 4.2% loss=0.18561, acc=0.92188
# [31/100] training 4.4% loss=0.15391, acc=0.96875
# [31/100] training 4.5% loss=0.17540, acc=0.87500
# [31/100] training 4.7% loss=0.24377, acc=0.90625
# [31/100] training 4.8% loss=0.27708, acc=0.92188
# [31/100] training 5.0% loss=0.15764, acc=0.93750
# [31/100] training 5.2% loss=0.24570, acc=0.90625
# [31/100] training 5.4% loss=0.11049, acc=0.98438
# [31/100] training 5.5% loss=0.15242, acc=0.95312
# [31/100] training 5.7% loss=0.12718, acc=0.95312
# [31/100] training 5.9% loss=0.20518, acc=0.89062
# [31/100] training 6.0% loss=0.16621, acc=0.90625
# [31/100] training 6.3% loss=0.25462, acc=0.87500
# [31/100] training 6.4% loss=0.22752, acc=0.89062
# [31/100] training 6.6% loss=0.24533, acc=0.92188
# [31/100] training 6.7% loss=0.34822, acc=0.81250
# [31/100] training 6.9% loss=0.24095, acc=0.90625
# [31/100] training 7.1% loss=0.14394, acc=0.95312
# [31/100] training 7.2% loss=0.23384, acc=0.92188
# [31/100] training 7.5% loss=0.22123, acc=0.90625
# [31/100] training 7.6% loss=0.21932, acc=0.92188
# [31/100] training 7.8% loss=0.25051, acc=0.90625
# [31/100] training 7.9% loss=0.28464, acc=0.84375
# [31/100] training 8.1% loss=0.16197, acc=0.92188
# [31/100] training 8.2% loss=0.23887, acc=0.93750
# [31/100] training 8.4% loss=0.22221, acc=0.87500
# [31/100] training 8.7% loss=0.25105, acc=0.92188
# [31/100] training 8.8% loss=0.21090, acc=0.90625
# [31/100] training 9.0% loss=0.21699, acc=0.89062
# [31/100] training 9.1% loss=0.28891, acc=0.89062
# [31/100] training 9.3% loss=0.41409, acc=0.84375
# [31/100] training 9.4% loss=0.20028, acc=0.87500
# [31/100] training 9.7% loss=0.22017, acc=0.90625
# [31/100] training 9.9% loss=0.25908, acc=0.89062
# [31/100] training 10.0% loss=0.21842, acc=0.92188
# [31/100] training 10.2% loss=0.23494, acc=0.90625
# [31/100] training 10.3% loss=0.18936, acc=0.92188
# [31/100] training 10.5% loss=0.40290, acc=0.89062
# [31/100] training 10.6% loss=0.27409, acc=0.92188
# [31/100] training 10.9% loss=0.16369, acc=0.92188
# [31/100] training 11.0% loss=0.30088, acc=0.87500
# [31/100] training 11.2% loss=0.17684, acc=0.95312
# [31/100] training 11.4% loss=0.23981, acc=0.85938
# [31/100] training 11.5% loss=0.34237, acc=0.85938
# [31/100] training 11.7% loss=0.10050, acc=0.96875
# [31/100] training 11.8% loss=0.19340, acc=0.90625
# [31/100] training 12.1% loss=0.23854, acc=0.89062
# [31/100] training 12.2% loss=0.15469, acc=0.95312
# [31/100] training 12.4% loss=0.25496, acc=0.92188
# [31/100] training 12.6% loss=0.24832, acc=0.93750
# [31/100] training 12.7% loss=0.19392, acc=0.90625
# [31/100] training 12.9% loss=0.32775, acc=0.89062
# [31/100] training 13.0% loss=0.18054, acc=0.93750
# [31/100] training 13.3% loss=0.13458, acc=0.95312
# [31/100] training 13.4% loss=0.21631, acc=0.89062
# [31/100] training 13.6% loss=0.21075, acc=0.95312
# [31/100] training 13.7% loss=0.33286, acc=0.85938
# [31/100] training 13.9% loss=0.18964, acc=0.90625
# [31/100] training 14.1% loss=0.28815, acc=0.90625
# [31/100] training 14.3% loss=0.18554, acc=0.92188
# [31/100] training 14.5% loss=0.20180, acc=0.93750
# [31/100] training 14.6% loss=0.17616, acc=0.93750
# [31/100] training 14.8% loss=0.26785, acc=0.87500
# [31/100] training 14.9% loss=0.15701, acc=0.95312
# [31/100] training 15.1% loss=0.31919, acc=0.87500
# [31/100] training 15.4% loss=0.24563, acc=0.92188
# [31/100] training 15.5% loss=0.26534, acc=0.89062
# [31/100] training 15.7% loss=0.34831, acc=0.89062
# [31/100] training 15.8% loss=0.20598, acc=0.89062
# [31/100] training 16.0% loss=0.28654, acc=0.90625
# [31/100] training 16.1% loss=0.44775, acc=0.82812
# [31/100] training 16.3% loss=0.30695, acc=0.85938
# [31/100] training 16.4% loss=0.19099, acc=0.89062
# [31/100] training 16.7% loss=0.28239, acc=0.89062
# [31/100] training 16.9% loss=0.19888, acc=0.93750
# [31/100] training 17.0% loss=0.22677, acc=0.87500
# [31/100] training 17.2% loss=0.22579, acc=0.92188
# [31/100] training 17.3% loss=0.18256, acc=0.93750
# [31/100] training 17.5% loss=0.20106, acc=0.90625
# [31/100] training 17.7% loss=0.22086, acc=0.89062
# [31/100] training 17.9% loss=0.31899, acc=0.90625
# [31/100] training 18.1% loss=0.29899, acc=0.89062
# [31/100] training 18.2% loss=0.25503, acc=0.90625
# [31/100] training 18.4% loss=0.35025, acc=0.82812
# [31/100] training 18.5% loss=0.21566, acc=0.92188
# [31/100] training 18.8% loss=0.18316, acc=0.90625
# [31/100] training 18.9% loss=0.14513, acc=0.93750
# [31/100] training 19.1% loss=0.31006, acc=0.85938
# [31/100] training 19.2% loss=0.12821, acc=0.96875
# [31/100] training 19.4% loss=0.18364, acc=0.92188
# [31/100] training 19.6% loss=0.22809, acc=0.89062
# [31/100] training 19.7% loss=0.28180, acc=0.87500
# [31/100] training 20.0% loss=0.14602, acc=0.93750
# [31/100] training 20.1% loss=0.25709, acc=0.93750
# [31/100] training 20.3% loss=0.29376, acc=0.87500
# [31/100] training 20.4% loss=0.29493, acc=0.87500
# [31/100] training 20.6% loss=0.29636, acc=0.87500
# [31/100] training 20.8% loss=0.19639, acc=0.92188
# [31/100] training 20.9% loss=0.18037, acc=0.92188
# [31/100] training 21.2% loss=0.20537, acc=0.95312
# [31/100] training 21.3% loss=0.24250, acc=0.92188
# [31/100] training 21.5% loss=0.29979, acc=0.90625
# [31/100] training 21.6% loss=0.08037, acc=0.95312
# [31/100] training 21.8% loss=0.14793, acc=0.93750
# [31/100] training 21.9% loss=0.21374, acc=0.92188
# [31/100] training 22.2% loss=0.22067, acc=0.87500
# [31/100] training 22.4% loss=0.37270, acc=0.89062
# [31/100] training 22.5% loss=0.16528, acc=0.95312
# [31/100] training 22.7% loss=0.22675, acc=0.92188
# [31/100] training 22.8% loss=0.24201, acc=0.89062
# [31/100] training 23.0% loss=0.13754, acc=0.93750
# [31/100] training 23.1% loss=0.23402, acc=0.92188
# [31/100] training 23.4% loss=0.30169, acc=0.87500
# [31/100] training 23.6% loss=0.29986, acc=0.92188
# [31/100] training 23.7% loss=0.21312, acc=0.93750
# [31/100] training 23.9% loss=0.24087, acc=0.89062
# [31/100] training 24.0% loss=0.19980, acc=0.93750
# [31/100] training 24.2% loss=0.22809, acc=0.92188
# [31/100] training 24.3% loss=0.28563, acc=0.89062
# [31/100] training 24.6% loss=0.22980, acc=0.89062
# [31/100] training 24.7% loss=0.28439, acc=0.87500
# [31/100] training 24.9% loss=0.17328, acc=0.93750
# [31/100] training 25.1% loss=0.28015, acc=0.87500
# [31/100] training 25.2% loss=0.16346, acc=0.96875
# [31/100] training 25.4% loss=0.21496, acc=0.93750
# [31/100] training 25.6% loss=0.14422, acc=0.95312
# [31/100] training 25.8% loss=0.24295, acc=0.87500
# [31/100] training 25.9% loss=0.16555, acc=0.93750
# [31/100] training 26.1% loss=0.13794, acc=0.95312
# [31/100] training 26.3% loss=0.11188, acc=0.96875
# [31/100] training 26.4% loss=0.28064, acc=0.89062
# [31/100] training 26.6% loss=0.08602, acc=0.96875
# [31/100] training 26.8% loss=0.15688, acc=0.93750
# [31/100] training 27.0% loss=0.23174, acc=0.89062
# [31/100] training 27.1% loss=0.22289, acc=0.89062
# [31/100] training 27.3% loss=0.18806, acc=0.89062
# [31/100] training 27.4% loss=0.13519, acc=0.95312
# [31/100] training 27.6% loss=0.29039, acc=0.93750
# [31/100] training 27.9% loss=0.17881, acc=0.92188
# [31/100] training 28.0% loss=0.33028, acc=0.89062
# [31/100] training 28.2% loss=0.14498, acc=0.93750
# [31/100] training 28.3% loss=0.11679, acc=0.95312
# [31/100] training 28.5% loss=0.22223, acc=0.89062
# [31/100] training 28.6% loss=0.18117, acc=0.92188
# [31/100] training 28.8% loss=0.12828, acc=0.96875
# [31/100] training 29.1% loss=0.16219, acc=0.92188
# [31/100] training 29.2% loss=0.23244, acc=0.92188
# [31/100] training 29.4% loss=0.28328, acc=0.81250
# [31/100] training 29.5% loss=0.16535, acc=0.92188
# [31/100] training 29.7% loss=0.24860, acc=0.87500
# [31/100] training 29.8% loss=0.15234, acc=0.93750
# [31/100] training 30.0% loss=0.28119, acc=0.85938
# [31/100] training 30.2% loss=0.19841, acc=0.89062
# [31/100] training 30.4% loss=0.11956, acc=0.93750
# [31/100] training 30.6% loss=0.33721, acc=0.87500
# [31/100] training 30.7% loss=0.17468, acc=0.93750
# [31/100] training 30.9% loss=0.35558, acc=0.87500
# [31/100] training 31.0% loss=0.16356, acc=0.90625
# [31/100] training 31.3% loss=0.22589, acc=0.90625
# [31/100] training 31.4% loss=0.46543, acc=0.79688
# [31/100] training 31.6% loss=0.22756, acc=0.93750
# [31/100] training 31.8% loss=0.17659, acc=0.89062
# [31/100] training 31.9% loss=0.23559, acc=0.89062
# [31/100] training 32.1% loss=0.25039, acc=0.92188
# [31/100] training 32.2% loss=0.26868, acc=0.92188
# [31/100] training 32.5% loss=0.18280, acc=0.90625
# [31/100] training 32.6% loss=0.28891, acc=0.85938
# [31/100] training 32.8% loss=0.15165, acc=0.95312
# [31/100] training 32.9% loss=0.22109, acc=0.90625
# [31/100] training 33.1% loss=0.21027, acc=0.89062
# [31/100] training 33.3% loss=0.29142, acc=0.84375
# [31/100] training 33.4% loss=0.14067, acc=0.92188
# [31/100] training 33.7% loss=0.22436, acc=0.90625
# [31/100] training 33.8% loss=0.21624, acc=0.89062
# [31/100] training 34.0% loss=0.23917, acc=0.90625
# [31/100] training 34.1% loss=0.24873, acc=0.92188
# [31/100] training 34.3% loss=0.14659, acc=0.96875
# [31/100] training 34.5% loss=0.23062, acc=0.90625
# [31/100] training 34.7% loss=0.11995, acc=0.93750
# [31/100] training 34.9% loss=0.17489, acc=0.90625
# [31/100] training 35.0% loss=0.20084, acc=0.93750
# [31/100] training 35.2% loss=0.36142, acc=0.84375
# [31/100] training 35.3% loss=0.22955, acc=0.89062
# [31/100] training 35.5% loss=0.21912, acc=0.95312
# [31/100] training 35.6% loss=0.30506, acc=0.87500
# [31/100] training 35.9% loss=0.20899, acc=0.89062
# [31/100] training 36.1% loss=0.41284, acc=0.82812
# [31/100] training 36.2% loss=0.27701, acc=0.89062
# [31/100] training 36.4% loss=0.28082, acc=0.93750
# [31/100] training 36.5% loss=0.20821, acc=0.95312
# [31/100] training 36.7% loss=0.25854, acc=0.85938
# [31/100] training 36.8% loss=0.15950, acc=0.98438
# [31/100] training 37.1% loss=0.22343, acc=0.92188
# [31/100] training 37.3% loss=0.20266, acc=0.93750
# [31/100] training 37.4% loss=0.19838, acc=0.93750
# [31/100] training 37.6% loss=0.12242, acc=0.96875
# [31/100] training 37.7% loss=0.20149, acc=0.89062
# [31/100] training 37.9% loss=0.26439, acc=0.90625
# [31/100] training 38.1% loss=0.23052, acc=0.90625
# [31/100] training 38.3% loss=0.17957, acc=0.90625
# [31/100] training 38.4% loss=0.09289, acc=0.96875
# [31/100] training 38.6% loss=0.18203, acc=0.90625
# [31/100] training 38.8% loss=0.29960, acc=0.89062
# [31/100] training 38.9% loss=0.18747, acc=0.92188
# [31/100] training 39.1% loss=0.21376, acc=0.92188
# [31/100] training 39.3% loss=0.21481, acc=0.90625
# [31/100] training 39.5% loss=0.24697, acc=0.87500
# [31/100] training 39.6% loss=0.20279, acc=0.93750
# [31/100] training 39.8% loss=0.12021, acc=0.96875
# [31/100] training 40.0% loss=0.24449, acc=0.90625
# [31/100] training 40.1% loss=0.33490, acc=0.82812
# [31/100] training 40.4% loss=0.10802, acc=0.96875
# [31/100] training 40.5% loss=0.19674, acc=0.90625
# [31/100] training 40.7% loss=0.18929, acc=0.89062
# [31/100] training 40.8% loss=0.13873, acc=0.93750
# [31/100] training 41.0% loss=0.12808, acc=0.92188
# [31/100] training 41.1% loss=0.36559, acc=0.85938
# [31/100] training 41.3% loss=0.24017, acc=0.89062
# [31/100] training 41.6% loss=0.22034, acc=0.89062
# [31/100] training 41.7% loss=0.39602, acc=0.85938
# [31/100] training 41.9% loss=0.22135, acc=0.90625
# [31/100] training 42.0% loss=0.20767, acc=0.90625
# [31/100] training 42.2% loss=0.24072, acc=0.89062
# [31/100] training 42.3% loss=0.22233, acc=0.95312
# [31/100] training 42.5% loss=0.12693, acc=0.95312
# [31/100] training 42.8% loss=0.15358, acc=0.92188
# [31/100] training 42.9% loss=0.21776, acc=0.90625
# [31/100] training 43.1% loss=0.21810, acc=0.90625
# [31/100] training 43.2% loss=0.17243, acc=0.92188
# [31/100] training 43.4% loss=0.25291, acc=0.92188
# [31/100] training 43.5% loss=0.27135, acc=0.85938
# [31/100] training 43.8% loss=0.24147, acc=0.92188
# [31/100] training 43.9% loss=0.23362, acc=0.92188
# [31/100] training 44.1% loss=0.19899, acc=0.95312
# [31/100] training 44.3% loss=0.20817, acc=0.90625
# [31/100] training 44.4% loss=0.33267, acc=0.90625
# [31/100] training 44.6% loss=0.21875, acc=0.89062
# [31/100] training 44.7% loss=0.29220, acc=0.85938
# [31/100] training 45.0% loss=0.14885, acc=0.90625
# [31/100] training 45.1% loss=0.31313, acc=0.89062
# [31/100] training 45.3% loss=0.27286, acc=0.87500
# [31/100] training 45.5% loss=0.08119, acc=1.00000
# [31/100] training 45.6% loss=0.25909, acc=0.90625
# [31/100] training 45.8% loss=0.16149, acc=0.96875
# [31/100] training 45.9% loss=0.16048, acc=0.92188
# [31/100] training 46.2% loss=0.12522, acc=0.93750
# [31/100] training 46.3% loss=0.11932, acc=0.95312
# [31/100] training 46.5% loss=0.29291, acc=0.90625
# [31/100] training 46.6% loss=0.24238, acc=0.93750
# [31/100] training 46.8% loss=0.19616, acc=0.92188
# [31/100] training 47.0% loss=0.27595, acc=0.89062
# [31/100] training 47.2% loss=0.27211, acc=0.87500
# [31/100] training 47.4% loss=0.13208, acc=0.95312
# [31/100] training 47.5% loss=0.30934, acc=0.89062
# [31/100] training 47.7% loss=0.19942, acc=0.92188
# [31/100] training 47.8% loss=0.26489, acc=0.90625
# [31/100] training 48.0% loss=0.21823, acc=0.90625
# [31/100] training 48.3% loss=0.13255, acc=0.98438
# [31/100] training 48.4% loss=0.10827, acc=0.98438
# [31/100] training 48.6% loss=0.17580, acc=0.95312
# [31/100] training 48.7% loss=0.19918, acc=0.89062
# [31/100] training 48.9% loss=0.20295, acc=0.90625
# [31/100] training 49.0% loss=0.16375, acc=0.95312
# [31/100] training 49.2% loss=0.25431, acc=0.84375
# [31/100] training 49.3% loss=0.18608, acc=0.93750
# [31/100] training 49.6% loss=0.20390, acc=0.90625
# [31/100] training 49.8% loss=0.20425, acc=0.93750
# [31/100] training 49.9% loss=0.27580, acc=0.89062
# [31/100] training 50.1% loss=0.12513, acc=0.96875
# [31/100] training 50.2% loss=0.29012, acc=0.85938
# [31/100] training 50.4% loss=0.32261, acc=0.92188
# [31/100] training 50.6% loss=0.18734, acc=0.93750
# [31/100] training 50.8% loss=0.31078, acc=0.87500
# [31/100] training 51.0% loss=0.22952, acc=0.89062
# [31/100] training 51.1% loss=0.23487, acc=0.85938
# [31/100] training 51.3% loss=0.22604, acc=0.90625
# [31/100] training 51.4% loss=0.25541, acc=0.92188
# [31/100] training 51.7% loss=0.18950, acc=0.92188
# [31/100] training 51.8% loss=0.24273, acc=0.87500
# [31/100] training 52.0% loss=0.21410, acc=0.89062
# [31/100] training 52.1% loss=0.17805, acc=0.92188
# [31/100] training 52.3% loss=0.30299, acc=0.87500
# [31/100] training 52.5% loss=0.09653, acc=0.98438
# [31/100] training 52.6% loss=0.21615, acc=0.92188
# [31/100] training 52.9% loss=0.32901, acc=0.89062
# [31/100] training 53.0% loss=0.14429, acc=0.92188
# [31/100] training 53.2% loss=0.21143, acc=0.90625
# [31/100] training 53.3% loss=0.14167, acc=0.95312
# [31/100] training 53.5% loss=0.21653, acc=0.90625
# [31/100] training 53.7% loss=0.10421, acc=0.95312
# [31/100] training 53.8% loss=0.28508, acc=0.87500
# [31/100] training 54.1% loss=0.28010, acc=0.89062
# [31/100] training 54.2% loss=0.13756, acc=0.95312
# [31/100] training 54.4% loss=0.23585, acc=0.92188
# [31/100] training 54.5% loss=0.27158, acc=0.89062
# [31/100] training 54.7% loss=0.25660, acc=0.90625
# [31/100] training 54.8% loss=0.10873, acc=0.95312
# [31/100] training 55.1% loss=0.14052, acc=0.95312
# [31/100] training 55.3% loss=0.09279, acc=0.98438
# [31/100] training 55.4% loss=0.16561, acc=0.92188
# [31/100] training 55.6% loss=0.21737, acc=0.89062
# [31/100] training 55.7% loss=0.18255, acc=0.92188
# [31/100] training 55.9% loss=0.12962, acc=0.95312
# [31/100] training 56.0% loss=0.16584, acc=0.90625
# [31/100] training 56.3% loss=0.45262, acc=0.84375
# [31/100] training 56.5% loss=0.27529, acc=0.89062
# [31/100] training 56.6% loss=0.21070, acc=0.90625
# [31/100] training 56.8% loss=0.25773, acc=0.89062
# [31/100] training 56.9% loss=0.23403, acc=0.92188
# [31/100] training 57.1% loss=0.27582, acc=0.90625
# [31/100] training 57.2% loss=0.12147, acc=0.96875
# [31/100] training 57.5% loss=0.20398, acc=0.90625
# [31/100] training 57.6% loss=0.23954, acc=0.90625
# [31/100] training 57.8% loss=0.13960, acc=0.95312
# [31/100] training 58.0% loss=0.18659, acc=0.89062
# [31/100] training 58.1% loss=0.19975, acc=0.92188
# [31/100] training 58.3% loss=0.23122, acc=0.92188
# [31/100] training 58.4% loss=0.27794, acc=0.90625
# [31/100] training 58.7% loss=0.25913, acc=0.90625
# [31/100] training 58.8% loss=0.21231, acc=0.92188
# [31/100] training 59.0% loss=0.18639, acc=0.89062
# [31/100] training 59.2% loss=0.17886, acc=0.92188
# [31/100] training 59.3% loss=0.19544, acc=0.90625
# [31/100] training 59.5% loss=0.21313, acc=0.92188
# [31/100] training 59.7% loss=0.21385, acc=0.95312
# [31/100] training 59.9% loss=0.15557, acc=0.95312
# [31/100] training 60.0% loss=0.18081, acc=0.93750
# [31/100] training 60.2% loss=0.16121, acc=0.93750
# [31/100] training 60.3% loss=0.32942, acc=0.90625
# [31/100] training 60.5% loss=0.25968, acc=0.89062
# [31/100] training 60.8% loss=0.21797, acc=0.89062
# [31/100] training 60.9% loss=0.33733, acc=0.90625
# [31/100] training 61.1% loss=0.29643, acc=0.89062
# [31/100] training 61.2% loss=0.32870, acc=0.85938
# [31/100] training 61.4% loss=0.17507, acc=0.95312
# [31/100] training 61.5% loss=0.28369, acc=0.84375
# [31/100] training 61.7% loss=0.19941, acc=0.93750
# [31/100] training 62.0% loss=0.25958, acc=0.90625
# [31/100] training 62.1% loss=0.34524, acc=0.87500
# [31/100] training 62.3% loss=0.11071, acc=0.95312
# [31/100] training 62.4% loss=0.16691, acc=0.93750
# [31/100] training 62.6% loss=0.22407, acc=0.90625
# [31/100] training 62.7% loss=0.14705, acc=0.93750
# [31/100] training 62.9% loss=0.23269, acc=0.90625
# [31/100] training 63.1% loss=0.16715, acc=0.93750
# [31/100] training 63.3% loss=0.10703, acc=0.96875
# [31/100] training 63.5% loss=0.28724, acc=0.90625
# [31/100] training 63.6% loss=0.27884, acc=0.89062
# [31/100] training 63.8% loss=0.41993, acc=0.79688
# [31/100] training 63.9% loss=0.19759, acc=0.93750
# [31/100] training 64.2% loss=0.17244, acc=0.92188
# [31/100] training 64.3% loss=0.32362, acc=0.87500
# [31/100] training 64.5% loss=0.22863, acc=0.90625
# [31/100] training 64.7% loss=0.22692, acc=0.89062
# [31/100] training 64.8% loss=0.29691, acc=0.87500
# [31/100] training 65.0% loss=0.19051, acc=0.93750
# [31/100] training 65.1% loss=0.27449, acc=0.85938
# [31/100] training 65.4% loss=0.17401, acc=0.96875
# [31/100] training 65.5% loss=0.13658, acc=0.95312
# [31/100] training 65.7% loss=0.15543, acc=0.90625
# [31/100] training 65.8% loss=0.21966, acc=0.90625
# [31/100] training 66.0% loss=0.21583, acc=0.95312
# [31/100] training 66.2% loss=0.12302, acc=0.96875
# [31/100] training 66.3% loss=0.32083, acc=0.87500
# [31/100] training 66.6% loss=0.20373, acc=0.90625
# [31/100] training 66.7% loss=0.13274, acc=0.93750
# [31/100] training 66.9% loss=0.20740, acc=0.92188
# [31/100] training 67.0% loss=0.34301, acc=0.85938
# [31/100] training 67.2% loss=0.16088, acc=0.92188
# [31/100] training 67.4% loss=0.22766, acc=0.90625
# [31/100] training 67.6% loss=0.20048, acc=0.90625
# [31/100] training 67.8% loss=0.17155, acc=0.93750
# [31/100] training 67.9% loss=0.17110, acc=0.92188
# [31/100] training 68.1% loss=0.18600, acc=0.92188
# [31/100] training 68.2% loss=0.17136, acc=0.93750
# [31/100] training 68.4% loss=0.09593, acc=0.93750
# [31/100] training 68.5% loss=0.32810, acc=0.89062
# [31/100] training 68.8% loss=0.26609, acc=0.87500
# [31/100] training 69.0% loss=0.32710, acc=0.92188
# [31/100] training 69.1% loss=0.14803, acc=0.92188
# [31/100] training 69.3% loss=0.24232, acc=0.92188
# [31/100] training 69.4% loss=0.12114, acc=0.96875
# [31/100] training 69.6% loss=0.14410, acc=0.96875
# [31/100] training 69.7% loss=0.15027, acc=0.93750
# [31/100] training 70.0% loss=0.33836, acc=0.89062
# [31/100] training 70.2% loss=0.26571, acc=0.92188
# [31/100] training 70.3% loss=0.20331, acc=0.92188
# [31/100] training 70.5% loss=0.22576, acc=0.92188
# [31/100] training 70.6% loss=0.16025, acc=0.95312
# [31/100] training 70.8% loss=0.21635, acc=0.93750
# [31/100] training 71.0% loss=0.25852, acc=0.89062
# [31/100] training 71.2% loss=0.14407, acc=0.93750
# [31/100] training 71.3% loss=0.10328, acc=0.96875
# [31/100] training 71.5% loss=0.28077, acc=0.93750
# [31/100] training 71.7% loss=0.28336, acc=0.87500
# [31/100] training 71.8% loss=0.23555, acc=0.92188
# [31/100] training 72.0% loss=0.15267, acc=0.95312
# [31/100] training 72.2% loss=0.26392, acc=0.87500
# [31/100] training 72.4% loss=0.30228, acc=0.85938
# [31/100] training 72.5% loss=0.23693, acc=0.85938
# [31/100] training 72.7% loss=0.35640, acc=0.85938
# [31/100] training 72.9% loss=0.19962, acc=0.95312
# [31/100] training 73.0% loss=0.12211, acc=0.93750
# [31/100] training 73.3% loss=0.22847, acc=0.92188
# [31/100] training 73.4% loss=0.13820, acc=0.95312
# [31/100] training 73.6% loss=0.15912, acc=0.96875
# [31/100] training 73.7% loss=0.21648, acc=0.89062
# [31/100] training 73.9% loss=0.14229, acc=0.95312
# [31/100] training 74.0% loss=0.25444, acc=0.87500
# [31/100] training 74.2% loss=0.19635, acc=0.93750
# [31/100] training 74.5% loss=0.19507, acc=0.92188
# [31/100] training 74.6% loss=0.30483, acc=0.82812
# [31/100] training 74.8% loss=0.35368, acc=0.85938
# [31/100] training 74.9% loss=0.31760, acc=0.87500
# [31/100] training 75.1% loss=0.16115, acc=0.93750
# [31/100] training 75.2% loss=0.11951, acc=0.93750
# [31/100] training 75.4% loss=0.16704, acc=0.90625
# [31/100] training 75.7% loss=0.24983, acc=0.87500
# [31/100] training 75.8% loss=0.29092, acc=0.87500
# [31/100] training 76.0% loss=0.14810, acc=0.95312
# [31/100] training 76.1% loss=0.20033, acc=0.92188
# [31/100] training 76.3% loss=0.12374, acc=0.95312
# [31/100] training 76.4% loss=0.19731, acc=0.93750
# [31/100] training 76.7% loss=0.13058, acc=0.95312
# [31/100] training 76.8% loss=0.13687, acc=0.92188
# [31/100] training 77.0% loss=0.19537, acc=0.92188
# [31/100] training 77.2% loss=0.20950, acc=0.90625
# [31/100] training 77.3% loss=0.11273, acc=0.95312
# [31/100] training 77.5% loss=0.18869, acc=0.92188
# [31/100] training 77.6% loss=0.29706, acc=0.85938
# [31/100] training 77.9% loss=0.16523, acc=0.93750
# [31/100] training 78.0% loss=0.18157, acc=0.90625
# [31/100] training 78.2% loss=0.29180, acc=0.87500
# [31/100] training 78.4% loss=0.11421, acc=0.93750
# [31/100] training 78.5% loss=0.28827, acc=0.87500
# [31/100] training 78.7% loss=0.16457, acc=0.90625
# [31/100] training 78.8% loss=0.08426, acc=0.98438
# [31/100] training 79.1% loss=0.15133, acc=0.90625
# [31/100] training 79.2% loss=0.19890, acc=0.96875
# [31/100] training 79.4% loss=0.18697, acc=0.93750
# [31/100] training 79.5% loss=0.20312, acc=0.90625
# [31/100] training 79.7% loss=0.09752, acc=0.96875
# [31/100] training 79.9% loss=0.13731, acc=0.96875
# [31/100] training 80.1% loss=0.14789, acc=0.93750
# [31/100] training 80.3% loss=0.25241, acc=0.87500
# [31/100] training 80.4% loss=0.16623, acc=0.93750
# [31/100] training 80.6% loss=0.27763, acc=0.87500
# [31/100] training 80.7% loss=0.17138, acc=0.93750
# [31/100] training 80.9% loss=0.20183, acc=0.92188
# [31/100] training 81.2% loss=0.30386, acc=0.90625
# [31/100] training 81.3% loss=0.23909, acc=0.89062
# [31/100] training 81.5% loss=0.19448, acc=0.93750
# [31/100] training 81.6% loss=0.30066, acc=0.87500
# [31/100] training 81.8% loss=0.18502, acc=0.92188
# [31/100] training 81.9% loss=0.40735, acc=0.89062
# [31/100] training 82.1% loss=0.16701, acc=0.92188
# [31/100] training 82.2% loss=0.22134, acc=0.90625
# [31/100] training 82.5% loss=0.15263, acc=0.96875
# [31/100] training 82.7% loss=0.18866, acc=0.90625
# [31/100] training 82.8% loss=0.24749, acc=0.89062
# [31/100] training 83.0% loss=0.13968, acc=0.96875
# [31/100] training 83.1% loss=0.20649, acc=0.93750
# [31/100] training 83.3% loss=0.19807, acc=0.93750
# [31/100] training 83.5% loss=0.15941, acc=0.95312
# [31/100] training 83.7% loss=0.32329, acc=0.89062
# [31/100] training 83.9% loss=0.16288, acc=0.95312
# [31/100] training 84.0% loss=0.16401, acc=0.92188
# [31/100] training 84.2% loss=0.13084, acc=0.95312
# [31/100] training 84.3% loss=0.27044, acc=0.87500
# [31/100] training 84.5% loss=0.20059, acc=0.89062
# [31/100] training 84.7% loss=0.28709, acc=0.85938
# [31/100] training 84.9% loss=0.14603, acc=0.93750
# [31/100] training 85.0% loss=0.23625, acc=0.92188
# [31/100] training 85.2% loss=0.22032, acc=0.90625
# [31/100] training 85.4% loss=0.19118, acc=0.90625
# [31/100] training 85.5% loss=0.24254, acc=0.90625
# [31/100] training 85.8% loss=0.20667, acc=0.87500
# [31/100] training 85.9% loss=0.15878, acc=0.95312
# [31/100] training 86.1% loss=0.19510, acc=0.90625
# [31/100] training 86.2% loss=0.11929, acc=0.95312
# [31/100] training 86.4% loss=0.32095, acc=0.85938
# [31/100] training 86.6% loss=0.18229, acc=0.93750
# [31/100] training 86.7% loss=0.20900, acc=0.92188
# [31/100] training 87.0% loss=0.30957, acc=0.82812
# [31/100] training 87.1% loss=0.20906, acc=0.90625
# [31/100] training 87.3% loss=0.18178, acc=0.93750
# [31/100] training 87.4% loss=0.23056, acc=0.89062
# [31/100] training 87.6% loss=0.24891, acc=0.92188
# [31/100] training 87.7% loss=0.30495, acc=0.90625
# [31/100] training 87.9% loss=0.24495, acc=0.92188
# [31/100] training 88.2% loss=0.12743, acc=0.95312
# [31/100] training 88.3% loss=0.23047, acc=0.95312
# [31/100] training 88.5% loss=0.23897, acc=0.92188
# [31/100] training 88.6% loss=0.17316, acc=0.93750
# [31/100] training 88.8% loss=0.15318, acc=0.93750
# [31/100] training 88.9% loss=0.24448, acc=0.87500
# [31/100] training 89.2% loss=0.17363, acc=0.93750
# [31/100] training 89.4% loss=0.21540, acc=0.89062
# [31/100] training 89.5% loss=0.25621, acc=0.90625
# [31/100] training 89.7% loss=0.17608, acc=0.90625
# [31/100] training 89.8% loss=0.14619, acc=0.93750
# [31/100] training 90.0% loss=0.15704, acc=0.89062
# [31/100] training 90.1% loss=0.16714, acc=0.95312
# [31/100] training 90.4% loss=0.17261, acc=0.95312
# [31/100] training 90.5% loss=0.18687, acc=0.92188
# [31/100] training 90.7% loss=0.17433, acc=0.93750
# [31/100] training 90.9% loss=0.17805, acc=0.93750
# [31/100] training 91.0% loss=0.15423, acc=0.95312
# [31/100] training 91.2% loss=0.24246, acc=0.92188
# [31/100] training 91.3% loss=0.43607, acc=0.82812
# [31/100] training 91.6% loss=0.31405, acc=0.89062
# [31/100] training 91.7% loss=0.15542, acc=0.93750
# [31/100] training 91.9% loss=0.29758, acc=0.84375
# [31/100] training 92.1% loss=0.17080, acc=0.92188
# [31/100] training 92.2% loss=0.13587, acc=0.93750
# [31/100] training 92.4% loss=0.25681, acc=0.89062
# [31/100] training 92.6% loss=0.30345, acc=0.84375
# [31/100] training 92.8% loss=0.26007, acc=0.90625
# [31/100] training 92.9% loss=0.22996, acc=0.92188
# [31/100] training 93.1% loss=0.38601, acc=0.82812
# [31/100] training 93.2% loss=0.21970, acc=0.92188
# [31/100] training 93.4% loss=0.25347, acc=0.90625
# [31/100] training 93.7% loss=0.22001, acc=0.89062
# [31/100] training 93.8% loss=0.24096, acc=0.85938
# [31/100] training 94.0% loss=0.18520, acc=0.92188
# [31/100] training 94.1% loss=0.18809, acc=0.92188
# [31/100] training 94.3% loss=0.13647, acc=0.95312
# [31/100] training 94.4% loss=0.18179, acc=0.93750
# [31/100] training 94.6% loss=0.12299, acc=0.96875
# [31/100] training 94.9% loss=0.13307, acc=0.93750
# [31/100] training 95.0% loss=0.28713, acc=0.90625
# [31/100] training 95.2% loss=0.44431, acc=0.89062
# [31/100] training 95.3% loss=0.28249, acc=0.92188
# [31/100] training 95.5% loss=0.14142, acc=0.93750
# [31/100] training 95.6% loss=0.28467, acc=0.92188
# [31/100] training 95.8% loss=0.17827, acc=0.93750
# [31/100] training 96.0% loss=0.21849, acc=0.92188
# [31/100] training 96.2% loss=0.10428, acc=0.98438
# [31/100] training 96.4% loss=0.18060, acc=0.95312
# [31/100] training 96.5% loss=0.26632, acc=0.85938
# [31/100] training 96.7% loss=0.14020, acc=0.96875
# [31/100] training 96.8% loss=0.27288, acc=0.89062
# [31/100] training 97.1% loss=0.20828, acc=0.92188
# [31/100] training 97.2% loss=0.17953, acc=0.95312
# [31/100] training 97.4% loss=0.27833, acc=0.87500
# [31/100] training 97.6% loss=0.26296, acc=0.85938
# [31/100] training 97.7% loss=0.14655, acc=0.93750
# [31/100] training 97.9% loss=0.12692, acc=0.93750
# [31/100] training 98.0% loss=0.12778, acc=0.95312
# [31/100] training 98.3% loss=0.19104, acc=0.90625
# [31/100] training 98.4% loss=0.21247, acc=0.90625
# [31/100] training 98.6% loss=0.38712, acc=0.84375
# [31/100] training 98.7% loss=0.32471, acc=0.89062
# [31/100] training 98.9% loss=0.13358, acc=0.95312
# [31/100] training 99.1% loss=0.14439, acc=0.93750
# [31/100] training 99.2% loss=0.18595, acc=0.92188
# [31/100] training 99.5% loss=0.27944, acc=0.90625
# [31/100] training 99.6% loss=0.18136, acc=0.96875
# [31/100] training 99.8% loss=0.18144, acc=0.93750
# [31/100] training 99.9% loss=0.09353, acc=0.98438
# [31/100] testing 0.9% loss=0.16292, acc=0.90625
# [31/100] testing 1.8% loss=0.43259, acc=0.84375
# [31/100] testing 2.2% loss=0.31797, acc=0.85938
# [31/100] testing 3.1% loss=0.34655, acc=0.85938
# [31/100] testing 3.5% loss=0.16440, acc=0.93750
# [31/100] testing 4.4% loss=0.21328, acc=0.93750
# [31/100] testing 4.8% loss=0.30347, acc=0.85938
# [31/100] testing 5.7% loss=0.19381, acc=0.92188
# [31/100] testing 6.6% loss=0.15183, acc=0.95312
# [31/100] testing 7.0% loss=0.12864, acc=0.95312
# [31/100] testing 7.9% loss=0.28015, acc=0.87500
# [31/100] testing 8.3% loss=0.37336, acc=0.89062
# [31/100] testing 9.2% loss=0.36379, acc=0.87500
# [31/100] testing 9.7% loss=0.12658, acc=0.93750
# [31/100] testing 10.5% loss=0.24536, acc=0.85938
# [31/100] testing 11.0% loss=0.36023, acc=0.89062
# [31/100] testing 11.8% loss=0.20211, acc=0.90625
# [31/100] testing 12.7% loss=0.33178, acc=0.84375
# [31/100] testing 13.2% loss=0.24723, acc=0.90625
# [31/100] testing 14.0% loss=0.29184, acc=0.90625
# [31/100] testing 14.5% loss=0.38105, acc=0.84375
# [31/100] testing 15.4% loss=0.33066, acc=0.90625
# [31/100] testing 15.8% loss=0.22951, acc=0.90625
# [31/100] testing 16.7% loss=0.24681, acc=0.85938
# [31/100] testing 17.5% loss=0.19879, acc=0.90625
# [31/100] testing 18.0% loss=0.24118, acc=0.87500
# [31/100] testing 18.9% loss=0.12832, acc=0.93750
# [31/100] testing 19.3% loss=0.33425, acc=0.85938
# [31/100] testing 20.2% loss=0.32264, acc=0.93750
# [31/100] testing 20.6% loss=0.33059, acc=0.89062
# [31/100] testing 21.5% loss=0.18108, acc=0.93750
# [31/100] testing 21.9% loss=0.46371, acc=0.79688
# [31/100] testing 22.8% loss=0.40665, acc=0.84375
# [31/100] testing 23.7% loss=0.31070, acc=0.84375
# [31/100] testing 24.1% loss=0.21221, acc=0.90625
# [31/100] testing 25.0% loss=0.31769, acc=0.92188
# [31/100] testing 25.4% loss=0.13046, acc=0.95312
# [31/100] testing 26.3% loss=0.31286, acc=0.85938
# [31/100] testing 26.8% loss=0.22187, acc=0.95312
# [31/100] testing 27.6% loss=0.29286, acc=0.89062
# [31/100] testing 28.5% loss=0.25171, acc=0.89062
# [31/100] testing 29.0% loss=0.20446, acc=0.92188
# [31/100] testing 29.8% loss=0.42440, acc=0.84375
# [31/100] testing 30.3% loss=0.32552, acc=0.84375
# [31/100] testing 31.1% loss=0.30370, acc=0.87500
# [31/100] testing 31.6% loss=0.22783, acc=0.89062
# [31/100] testing 32.5% loss=0.22592, acc=0.92188
# [31/100] testing 32.9% loss=0.36855, acc=0.90625
# [31/100] testing 33.8% loss=0.36832, acc=0.89062
# [31/100] testing 34.7% loss=0.29744, acc=0.89062
# [31/100] testing 35.1% loss=0.18820, acc=0.90625
# [31/100] testing 36.0% loss=0.27682, acc=0.90625
# [31/100] testing 36.4% loss=0.22420, acc=0.89062
# [31/100] testing 37.3% loss=0.26775, acc=0.90625
# [31/100] testing 37.7% loss=0.40020, acc=0.82812
# [31/100] testing 38.6% loss=0.30205, acc=0.89062
# [31/100] testing 39.5% loss=0.32883, acc=0.89062
# [31/100] testing 39.9% loss=0.28210, acc=0.90625
# [31/100] testing 40.8% loss=0.34574, acc=0.87500
# [31/100] testing 41.2% loss=0.24981, acc=0.92188
# [31/100] testing 42.1% loss=0.27910, acc=0.89062
# [31/100] testing 42.5% loss=0.20192, acc=0.90625
# [31/100] testing 43.4% loss=0.35871, acc=0.84375
# [31/100] testing 43.9% loss=0.15536, acc=0.92188
# [31/100] testing 44.7% loss=0.39333, acc=0.84375
# [31/100] testing 45.6% loss=0.36814, acc=0.85938
# [31/100] testing 46.1% loss=0.27908, acc=0.87500
# [31/100] testing 46.9% loss=0.30448, acc=0.89062
# [31/100] testing 47.4% loss=0.12780, acc=0.95312
# [31/100] testing 48.3% loss=0.33332, acc=0.89062
# [31/100] testing 48.7% loss=0.29794, acc=0.87500
# [31/100] testing 49.6% loss=0.47794, acc=0.81250
# [31/100] testing 50.4% loss=0.16958, acc=0.92188
# [31/100] testing 50.9% loss=0.31944, acc=0.89062
# [31/100] testing 51.8% loss=0.23166, acc=0.92188
# [31/100] testing 52.2% loss=0.18472, acc=0.92188
# [31/100] testing 53.1% loss=0.18815, acc=0.92188
# [31/100] testing 53.5% loss=0.29558, acc=0.89062
# [31/100] testing 54.4% loss=0.37259, acc=0.85938
# [31/100] testing 54.8% loss=0.36904, acc=0.85938
# [31/100] testing 55.7% loss=0.21586, acc=0.90625
# [31/100] testing 56.6% loss=0.34284, acc=0.85938
# [31/100] testing 57.0% loss=0.25840, acc=0.89062
# [31/100] testing 57.9% loss=0.24686, acc=0.89062
# [31/100] testing 58.3% loss=0.30218, acc=0.90625
# [31/100] testing 59.2% loss=0.28908, acc=0.85938
# [31/100] testing 59.7% loss=0.21459, acc=0.89062
# [31/100] testing 60.5% loss=0.33752, acc=0.85938
# [31/100] testing 61.4% loss=0.18195, acc=0.96875
# [31/100] testing 61.9% loss=0.23589, acc=0.92188
# [31/100] testing 62.7% loss=0.21618, acc=0.90625
# [31/100] testing 63.2% loss=0.45178, acc=0.85938
# [31/100] testing 64.0% loss=0.49902, acc=0.87500
# [31/100] testing 64.5% loss=0.24288, acc=0.89062
# [31/100] testing 65.4% loss=0.15272, acc=0.93750
# [31/100] testing 65.8% loss=0.39015, acc=0.87500
# [31/100] testing 66.7% loss=0.20332, acc=0.92188
# [31/100] testing 67.6% loss=0.33019, acc=0.90625
# [31/100] testing 68.0% loss=0.10464, acc=0.96875
# [31/100] testing 68.9% loss=0.28079, acc=0.90625
# [31/100] testing 69.3% loss=0.36001, acc=0.84375
# [31/100] testing 70.2% loss=0.44717, acc=0.82812
# [31/100] testing 70.6% loss=0.45380, acc=0.81250
# [31/100] testing 71.5% loss=0.42184, acc=0.85938
# [31/100] testing 72.4% loss=0.13556, acc=0.96875
# [31/100] testing 72.8% loss=0.19691, acc=0.89062
# [31/100] testing 73.7% loss=0.17961, acc=0.95312
# [31/100] testing 74.1% loss=0.37711, acc=0.89062
# [31/100] testing 75.0% loss=0.21750, acc=0.90625
# [31/100] testing 75.4% loss=0.42205, acc=0.87500
# [31/100] testing 76.3% loss=0.08737, acc=0.98438
# [31/100] testing 76.8% loss=0.26007, acc=0.89062
# [31/100] testing 77.6% loss=0.27484, acc=0.89062
# [31/100] testing 78.5% loss=0.48621, acc=0.82812
# [31/100] testing 79.0% loss=0.27370, acc=0.87500
# [31/100] testing 79.8% loss=0.30194, acc=0.84375
# [31/100] testing 80.3% loss=0.31171, acc=0.85938
# [31/100] testing 81.2% loss=0.38059, acc=0.82812
# [31/100] testing 81.6% loss=0.30623, acc=0.87500
# [31/100] testing 82.5% loss=0.25385, acc=0.90625
# [31/100] testing 83.3% loss=0.21992, acc=0.92188
# [31/100] testing 83.8% loss=0.13436, acc=0.95312
# [31/100] testing 84.7% loss=0.39639, acc=0.82812
# [31/100] testing 85.1% loss=0.33919, acc=0.84375
# [31/100] testing 86.0% loss=0.26622, acc=0.87500
# [31/100] testing 86.4% loss=0.44785, acc=0.84375
# [31/100] testing 87.3% loss=0.28175, acc=0.87500
# [31/100] testing 87.7% loss=0.17923, acc=0.93750
# [31/100] testing 88.6% loss=0.33356, acc=0.87500
# [31/100] testing 89.5% loss=0.50777, acc=0.78125
# [31/100] testing 89.9% loss=0.23756, acc=0.90625
# [31/100] testing 90.8% loss=0.34189, acc=0.89062
# [31/100] testing 91.2% loss=0.15447, acc=0.92188
# [31/100] testing 92.1% loss=0.37041, acc=0.87500
# [31/100] testing 92.6% loss=0.29838, acc=0.89062
# [31/100] testing 93.4% loss=0.38289, acc=0.84375
# [31/100] testing 94.3% loss=0.08751, acc=0.95312
# [31/100] testing 94.7% loss=0.16181, acc=0.92188
# [31/100] testing 95.6% loss=0.43587, acc=0.81250
# [31/100] testing 96.1% loss=0.18217, acc=0.92188
# [31/100] testing 96.9% loss=0.29560, acc=0.90625
# [31/100] testing 97.4% loss=0.16777, acc=0.96875
# [31/100] testing 98.3% loss=0.24940, acc=0.89062
# [31/100] testing 98.7% loss=0.21958, acc=0.92188
# [31/100] testing 99.6% loss=0.30316, acc=0.89062
# [32/100] training 0.2% loss=0.32634, acc=0.89062
# [32/100] training 0.4% loss=0.36695, acc=0.85938
# [32/100] training 0.5% loss=0.13901, acc=0.93750
# [32/100] training 0.8% loss=0.14671, acc=0.93750
# [32/100] training 0.9% loss=0.21232, acc=0.90625
# [32/100] training 1.1% loss=0.23585, acc=0.93750
# [32/100] training 1.2% loss=0.26638, acc=0.90625
# [32/100] training 1.4% loss=0.19111, acc=0.92188
# [32/100] training 1.6% loss=0.12913, acc=0.95312
# [32/100] training 1.8% loss=0.20766, acc=0.93750
# [32/100] training 2.0% loss=0.24390, acc=0.90625
# [32/100] training 2.1% loss=0.28203, acc=0.84375
# [32/100] training 2.3% loss=0.20786, acc=0.92188
# [32/100] training 2.4% loss=0.23446, acc=0.90625
# [32/100] training 2.6% loss=0.14327, acc=0.96875
# [32/100] training 2.7% loss=0.27841, acc=0.92188
# [32/100] training 3.0% loss=0.27399, acc=0.93750
# [32/100] training 3.2% loss=0.12415, acc=0.96875
# [32/100] training 3.3% loss=0.28511, acc=0.92188
# [32/100] training 3.5% loss=0.23185, acc=0.89062
# [32/100] training 3.6% loss=0.24484, acc=0.89062
# [32/100] training 3.8% loss=0.26394, acc=0.87500
# [32/100] training 3.9% loss=0.17426, acc=0.93750
# [32/100] training 4.2% loss=0.10682, acc=0.98438
# [32/100] training 4.4% loss=0.10593, acc=0.98438
# [32/100] training 4.5% loss=0.19636, acc=0.89062
# [32/100] training 4.7% loss=0.27191, acc=0.90625
# [32/100] training 4.8% loss=0.25607, acc=0.90625
# [32/100] training 5.0% loss=0.14126, acc=0.93750
# [32/100] training 5.2% loss=0.25197, acc=0.85938
# [32/100] training 5.4% loss=0.12714, acc=0.95312
# [32/100] training 5.5% loss=0.22133, acc=0.90625
# [32/100] training 5.7% loss=0.15862, acc=0.95312
# [32/100] training 5.9% loss=0.21050, acc=0.96875
# [32/100] training 6.0% loss=0.21120, acc=0.89062
# [32/100] training 6.3% loss=0.24124, acc=0.92188
# [32/100] training 6.4% loss=0.21744, acc=0.89062
# [32/100] training 6.6% loss=0.21627, acc=0.89062
# [32/100] training 6.7% loss=0.22165, acc=0.89062
# [32/100] training 6.9% loss=0.18717, acc=0.90625
# [32/100] training 7.1% loss=0.20954, acc=0.92188
# [32/100] training 7.2% loss=0.27828, acc=0.89062
# [32/100] training 7.5% loss=0.17811, acc=0.92188
# [32/100] training 7.6% loss=0.24855, acc=0.90625
# [32/100] training 7.8% loss=0.24799, acc=0.93750
# [32/100] training 7.9% loss=0.22803, acc=0.92188
# [32/100] training 8.1% loss=0.15605, acc=0.96875
# [32/100] training 8.2% loss=0.30081, acc=0.89062
# [32/100] training 8.4% loss=0.19928, acc=0.93750
# [32/100] training 8.7% loss=0.20140, acc=0.95312
# [32/100] training 8.8% loss=0.22147, acc=0.89062
# [32/100] training 9.0% loss=0.20011, acc=0.92188
# [32/100] training 9.1% loss=0.20037, acc=0.95312
# [32/100] training 9.3% loss=0.33490, acc=0.87500
# [32/100] training 9.4% loss=0.14164, acc=0.95312
# [32/100] training 9.7% loss=0.22594, acc=0.92188
# [32/100] training 9.9% loss=0.19297, acc=0.92188
# [32/100] training 10.0% loss=0.32483, acc=0.89062
# [32/100] training 10.2% loss=0.20260, acc=0.90625
# [32/100] training 10.3% loss=0.25264, acc=0.87500
# [32/100] training 10.5% loss=0.33736, acc=0.89062
# [32/100] training 10.6% loss=0.21086, acc=0.93750
# [32/100] training 10.9% loss=0.18447, acc=0.92188
# [32/100] training 11.0% loss=0.25945, acc=0.85938
# [32/100] training 11.2% loss=0.12678, acc=0.95312
# [32/100] training 11.4% loss=0.27298, acc=0.89062
# [32/100] training 11.5% loss=0.47039, acc=0.81250
# [32/100] training 11.7% loss=0.11886, acc=0.96875
# [32/100] training 11.8% loss=0.17293, acc=0.89062
# [32/100] training 12.1% loss=0.18265, acc=0.93750
# [32/100] training 12.2% loss=0.13622, acc=0.95312
# [32/100] training 12.4% loss=0.27824, acc=0.89062
# [32/100] training 12.6% loss=0.27672, acc=0.92188
# [32/100] training 12.7% loss=0.22956, acc=0.89062
# [32/100] training 12.9% loss=0.25367, acc=0.90625
# [32/100] training 13.0% loss=0.16723, acc=0.95312
# [32/100] training 13.3% loss=0.20986, acc=0.92188
# [32/100] training 13.4% loss=0.21155, acc=0.92188
# [32/100] training 13.6% loss=0.18261, acc=0.92188
# [32/100] training 13.7% loss=0.26133, acc=0.93750
# [32/100] training 13.9% loss=0.23262, acc=0.92188
# [32/100] training 14.1% loss=0.31096, acc=0.93750
# [32/100] training 14.3% loss=0.10717, acc=0.95312
# [32/100] training 14.5% loss=0.22299, acc=0.95312
# [32/100] training 14.6% loss=0.17140, acc=0.93750
# [32/100] training 14.8% loss=0.25143, acc=0.89062
# [32/100] training 14.9% loss=0.12642, acc=0.92188
# [32/100] training 15.1% loss=0.41474, acc=0.82812
# [32/100] training 15.4% loss=0.19210, acc=0.92188
# [32/100] training 15.5% loss=0.24603, acc=0.90625
# [32/100] training 15.7% loss=0.41316, acc=0.89062
# [32/100] training 15.8% loss=0.14225, acc=0.93750
# [32/100] training 16.0% loss=0.28249, acc=0.84375
# [32/100] training 16.1% loss=0.37010, acc=0.89062
# [32/100] training 16.3% loss=0.27290, acc=0.87500
# [32/100] training 16.4% loss=0.23719, acc=0.89062
# [32/100] training 16.7% loss=0.30912, acc=0.87500
# [32/100] training 16.9% loss=0.22315, acc=0.90625
# [32/100] training 17.0% loss=0.23770, acc=0.90625
# [32/100] training 17.2% loss=0.19335, acc=0.89062
# [32/100] training 17.3% loss=0.18338, acc=0.92188
# [32/100] training 17.5% loss=0.20081, acc=0.93750
# [32/100] training 17.7% loss=0.22112, acc=0.89062
# [32/100] training 17.9% loss=0.27972, acc=0.92188
# [32/100] training 18.1% loss=0.36230, acc=0.84375
# [32/100] training 18.2% loss=0.27811, acc=0.89062
# [32/100] training 18.4% loss=0.26524, acc=0.90625
# [32/100] training 18.5% loss=0.22122, acc=0.90625
# [32/100] training 18.8% loss=0.21718, acc=0.87500
# [32/100] training 18.9% loss=0.11986, acc=0.95312
# [32/100] training 19.1% loss=0.20561, acc=0.92188
# [32/100] training 19.2% loss=0.14450, acc=0.96875
# [32/100] training 19.4% loss=0.21373, acc=0.92188
# [32/100] training 19.6% loss=0.26239, acc=0.89062
# [32/100] training 19.7% loss=0.24700, acc=0.90625
# [32/100] training 20.0% loss=0.11916, acc=0.95312
# [32/100] training 20.1% loss=0.18935, acc=0.93750
# [32/100] training 20.3% loss=0.29548, acc=0.90625
# [32/100] training 20.4% loss=0.22120, acc=0.87500
# [32/100] training 20.6% loss=0.29766, acc=0.89062
# [32/100] training 20.8% loss=0.16510, acc=0.92188
# [32/100] training 20.9% loss=0.21795, acc=0.90625
# [32/100] training 21.2% loss=0.25748, acc=0.90625
# [32/100] training 21.3% loss=0.29561, acc=0.90625
# [32/100] training 21.5% loss=0.24716, acc=0.92188
# [32/100] training 21.6% loss=0.09777, acc=0.98438
# [32/100] training 21.8% loss=0.15673, acc=0.95312
# [32/100] training 21.9% loss=0.20491, acc=0.93750
# [32/100] training 22.2% loss=0.26700, acc=0.87500
# [32/100] training 22.4% loss=0.21625, acc=0.89062
# [32/100] training 22.5% loss=0.18005, acc=0.93750
# [32/100] training 22.7% loss=0.25435, acc=0.87500
# [32/100] training 22.8% loss=0.17112, acc=0.93750
# [32/100] training 23.0% loss=0.13511, acc=0.92188
# [32/100] training 23.1% loss=0.27441, acc=0.90625
# [32/100] training 23.4% loss=0.22711, acc=0.89062
# [32/100] training 23.6% loss=0.30415, acc=0.89062
# [32/100] training 23.7% loss=0.22346, acc=0.87500
# [32/100] training 23.9% loss=0.23140, acc=0.92188
# [32/100] training 24.0% loss=0.18380, acc=0.93750
# [32/100] training 24.2% loss=0.22229, acc=0.89062
# [32/100] training 24.3% loss=0.25588, acc=0.90625
# [32/100] training 24.6% loss=0.20620, acc=0.90625
# [32/100] training 24.7% loss=0.29503, acc=0.85938
# [32/100] training 24.9% loss=0.25492, acc=0.87500
# [32/100] training 25.1% loss=0.23331, acc=0.92188
# [32/100] training 25.2% loss=0.10337, acc=0.98438
# [32/100] training 25.4% loss=0.26982, acc=0.89062
# [32/100] training 25.6% loss=0.15950, acc=0.95312
# [32/100] training 25.8% loss=0.19878, acc=0.90625
# [32/100] training 25.9% loss=0.17942, acc=0.93750
# [32/100] training 26.1% loss=0.13668, acc=0.96875
# [32/100] training 26.3% loss=0.10129, acc=0.96875
# [32/100] training 26.4% loss=0.10016, acc=0.96875
# [32/100] training 26.6% loss=0.07118, acc=0.96875
# [32/100] training 26.8% loss=0.26823, acc=0.89062
# [32/100] training 27.0% loss=0.15211, acc=0.93750
# [32/100] training 27.1% loss=0.20781, acc=0.92188
# [32/100] training 27.3% loss=0.16717, acc=0.92188
# [32/100] training 27.4% loss=0.14387, acc=0.92188
# [32/100] training 27.6% loss=0.33263, acc=0.92188
# [32/100] training 27.9% loss=0.33584, acc=0.89062
# [32/100] training 28.0% loss=0.25760, acc=0.87500
# [32/100] training 28.2% loss=0.20104, acc=0.92188
# [32/100] training 28.3% loss=0.11581, acc=0.93750
# [32/100] training 28.5% loss=0.28368, acc=0.84375
# [32/100] training 28.6% loss=0.18247, acc=0.93750
# [32/100] training 28.8% loss=0.10921, acc=0.96875
# [32/100] training 29.1% loss=0.15648, acc=0.93750
# [32/100] training 29.2% loss=0.14866, acc=0.93750
# [32/100] training 29.4% loss=0.27127, acc=0.89062
# [32/100] training 29.5% loss=0.06185, acc=1.00000
# [32/100] training 29.7% loss=0.31768, acc=0.90625
# [32/100] training 29.8% loss=0.19607, acc=0.93750
# [32/100] training 30.0% loss=0.21089, acc=0.92188
# [32/100] training 30.2% loss=0.11931, acc=0.98438
# [32/100] training 30.4% loss=0.09700, acc=0.96875
# [32/100] training 30.6% loss=0.31735, acc=0.90625
# [32/100] training 30.7% loss=0.17043, acc=0.90625
# [32/100] training 30.9% loss=0.34227, acc=0.90625
# [32/100] training 31.0% loss=0.13283, acc=0.96875
# [32/100] training 31.3% loss=0.17724, acc=0.92188
# [32/100] training 31.4% loss=0.31450, acc=0.89062
# [32/100] training 31.6% loss=0.17076, acc=0.93750
# [32/100] training 31.8% loss=0.19819, acc=0.89062
# [32/100] training 31.9% loss=0.30637, acc=0.90625
# [32/100] training 32.1% loss=0.20841, acc=0.92188
# [32/100] training 32.2% loss=0.22331, acc=0.92188
# [32/100] training 32.5% loss=0.14657, acc=0.93750
# [32/100] training 32.6% loss=0.20154, acc=0.90625
# [32/100] training 32.8% loss=0.19631, acc=0.90625
# [32/100] training 32.9% loss=0.22872, acc=0.89062
# [32/100] training 33.1% loss=0.22817, acc=0.89062
# [32/100] training 33.3% loss=0.28160, acc=0.89062
# [32/100] training 33.4% loss=0.17861, acc=0.93750
# [32/100] training 33.7% loss=0.18889, acc=0.93750
# [32/100] training 33.8% loss=0.26088, acc=0.87500
# [32/100] training 34.0% loss=0.26111, acc=0.90625
# [32/100] training 34.1% loss=0.12999, acc=0.95312
# [32/100] training 34.3% loss=0.22653, acc=0.89062
# [32/100] training 34.5% loss=0.26858, acc=0.85938
# [32/100] training 34.7% loss=0.06909, acc=0.98438
# [32/100] training 34.9% loss=0.14666, acc=0.95312
# [32/100] training 35.0% loss=0.22431, acc=0.92188
# [32/100] training 35.2% loss=0.23376, acc=0.90625
# [32/100] training 35.3% loss=0.28384, acc=0.90625
# [32/100] training 35.5% loss=0.26387, acc=0.90625
# [32/100] training 35.6% loss=0.43091, acc=0.84375
# [32/100] training 35.9% loss=0.23817, acc=0.87500
# [32/100] training 36.1% loss=0.30462, acc=0.85938
# [32/100] training 36.2% loss=0.20320, acc=0.90625
# [32/100] training 36.4% loss=0.23107, acc=0.92188
# [32/100] training 36.5% loss=0.21789, acc=0.93750
# [32/100] training 36.7% loss=0.20692, acc=0.92188
# [32/100] training 36.8% loss=0.07135, acc=1.00000
# [32/100] training 37.1% loss=0.21268, acc=0.90625
# [32/100] training 37.3% loss=0.24300, acc=0.89062
# [32/100] training 37.4% loss=0.24256, acc=0.93750
# [32/100] training 37.6% loss=0.14268, acc=0.96875
# [32/100] training 37.7% loss=0.25329, acc=0.92188
# [32/100] training 37.9% loss=0.17397, acc=0.93750
# [32/100] training 38.1% loss=0.20261, acc=0.90625
# [32/100] training 38.3% loss=0.14788, acc=0.90625
# [32/100] training 38.4% loss=0.07867, acc=0.96875
# [32/100] training 38.6% loss=0.17229, acc=0.89062
# [32/100] training 38.8% loss=0.29515, acc=0.90625
# [32/100] training 38.9% loss=0.16144, acc=0.90625
# [32/100] training 39.1% loss=0.18128, acc=0.93750
# [32/100] training 39.3% loss=0.24393, acc=0.87500
# [32/100] training 39.5% loss=0.28156, acc=0.89062
# [32/100] training 39.6% loss=0.21253, acc=0.92188
# [32/100] training 39.8% loss=0.12797, acc=0.95312
# [32/100] training 40.0% loss=0.24132, acc=0.92188
# [32/100] training 40.1% loss=0.21988, acc=0.90625
# [32/100] training 40.4% loss=0.14117, acc=0.93750
# [32/100] training 40.5% loss=0.18166, acc=0.92188
# [32/100] training 40.7% loss=0.25614, acc=0.89062
# [32/100] training 40.8% loss=0.11888, acc=0.95312
# [32/100] training 41.0% loss=0.17563, acc=0.90625
# [32/100] training 41.1% loss=0.37066, acc=0.84375
# [32/100] training 41.3% loss=0.22451, acc=0.92188
# [32/100] training 41.6% loss=0.24494, acc=0.87500
# [32/100] training 41.7% loss=0.30910, acc=0.85938
# [32/100] training 41.9% loss=0.17634, acc=0.93750
# [32/100] training 42.0% loss=0.17596, acc=0.92188
# [32/100] training 42.2% loss=0.27154, acc=0.87500
# [32/100] training 42.3% loss=0.13759, acc=0.93750
# [32/100] training 42.5% loss=0.16865, acc=0.96875
# [32/100] training 42.8% loss=0.09716, acc=0.98438
# [32/100] training 42.9% loss=0.15119, acc=0.93750
# [32/100] training 43.1% loss=0.17138, acc=0.93750
# [32/100] training 43.2% loss=0.11831, acc=0.93750
# [32/100] training 43.4% loss=0.33313, acc=0.93750
# [32/100] training 43.5% loss=0.29236, acc=0.87500
# [32/100] training 43.8% loss=0.26118, acc=0.87500
# [32/100] training 43.9% loss=0.21719, acc=0.93750
# [32/100] training 44.1% loss=0.20639, acc=0.92188
# [32/100] training 44.3% loss=0.18233, acc=0.95312
# [32/100] training 44.4% loss=0.25610, acc=0.92188
# [32/100] training 44.6% loss=0.23063, acc=0.89062
# [32/100] training 44.7% loss=0.27823, acc=0.87500
# [32/100] training 45.0% loss=0.17528, acc=0.93750
# [32/100] training 45.1% loss=0.32280, acc=0.89062
# [32/100] training 45.3% loss=0.26955, acc=0.87500
# [32/100] training 45.5% loss=0.08786, acc=0.98438
# [32/100] training 45.6% loss=0.24994, acc=0.89062
# [32/100] training 45.8% loss=0.14937, acc=0.92188
# [32/100] training 45.9% loss=0.14522, acc=0.95312
# [32/100] training 46.2% loss=0.09622, acc=0.96875
# [32/100] training 46.3% loss=0.12803, acc=0.93750
# [32/100] training 46.5% loss=0.37400, acc=0.82812
# [32/100] training 46.6% loss=0.19805, acc=0.92188
# [32/100] training 46.8% loss=0.19111, acc=0.95312
# [32/100] training 47.0% loss=0.30949, acc=0.90625
# [32/100] training 47.2% loss=0.14968, acc=0.96875
# [32/100] training 47.4% loss=0.13592, acc=0.96875
# [32/100] training 47.5% loss=0.26052, acc=0.92188
# [32/100] training 47.7% loss=0.23083, acc=0.89062
# [32/100] training 47.8% loss=0.28715, acc=0.87500
# [32/100] training 48.0% loss=0.28336, acc=0.89062
# [32/100] training 48.3% loss=0.08861, acc=0.98438
# [32/100] training 48.4% loss=0.12244, acc=0.96875
# [32/100] training 48.6% loss=0.17754, acc=0.92188
# [32/100] training 48.7% loss=0.27955, acc=0.84375
# [32/100] training 48.9% loss=0.20425, acc=0.90625
# [32/100] training 49.0% loss=0.19892, acc=0.92188
# [32/100] training 49.2% loss=0.26004, acc=0.87500
# [32/100] training 49.3% loss=0.17556, acc=0.93750
# [32/100] training 49.6% loss=0.23919, acc=0.87500
# [32/100] training 49.8% loss=0.20091, acc=0.93750
# [32/100] training 49.9% loss=0.22299, acc=0.85938
# [32/100] training 50.1% loss=0.16817, acc=0.92188
# [32/100] training 50.2% loss=0.29228, acc=0.84375
# [32/100] training 50.4% loss=0.36037, acc=0.87500
# [32/100] training 50.6% loss=0.19998, acc=0.93750
# [32/100] training 50.8% loss=0.21212, acc=0.92188
# [32/100] training 51.0% loss=0.28711, acc=0.90625
# [32/100] training 51.1% loss=0.25451, acc=0.85938
# [32/100] training 51.3% loss=0.24548, acc=0.87500
# [32/100] training 51.4% loss=0.23007, acc=0.93750
# [32/100] training 51.7% loss=0.23624, acc=0.87500
# [32/100] training 51.8% loss=0.25305, acc=0.89062
# [32/100] training 52.0% loss=0.18662, acc=0.89062
# [32/100] training 52.1% loss=0.16252, acc=0.93750
# [32/100] training 52.3% loss=0.23609, acc=0.92188
# [32/100] training 52.5% loss=0.14040, acc=0.93750
# [32/100] training 52.6% loss=0.19735, acc=0.89062
# [32/100] training 52.9% loss=0.38286, acc=0.85938
# [32/100] training 53.0% loss=0.16914, acc=0.93750
# [32/100] training 53.2% loss=0.26985, acc=0.92188
# [32/100] training 53.3% loss=0.18273, acc=0.92188
# [32/100] training 53.5% loss=0.24921, acc=0.92188
# [32/100] training 53.7% loss=0.12632, acc=0.96875
# [32/100] training 53.8% loss=0.28357, acc=0.85938
# [32/100] training 54.1% loss=0.39191, acc=0.87500
# [32/100] training 54.2% loss=0.14842, acc=0.93750
# [32/100] training 54.4% loss=0.21736, acc=0.92188
# [32/100] training 54.5% loss=0.27153, acc=0.89062
# [32/100] training 54.7% loss=0.31042, acc=0.84375
# [32/100] training 54.8% loss=0.16617, acc=0.92188
# [32/100] training 55.1% loss=0.19673, acc=0.92188
# [32/100] training 55.3% loss=0.15066, acc=0.92188
# [32/100] training 55.4% loss=0.19255, acc=0.93750
# [32/100] training 55.6% loss=0.17951, acc=0.93750
# [32/100] training 55.7% loss=0.15085, acc=0.93750
# [32/100] training 55.9% loss=0.18336, acc=0.90625
# [32/100] training 56.0% loss=0.15107, acc=0.90625
# [32/100] training 56.3% loss=0.45171, acc=0.85938
# [32/100] training 56.5% loss=0.24817, acc=0.93750
# [32/100] training 56.6% loss=0.24575, acc=0.90625
# [32/100] training 56.8% loss=0.19166, acc=0.95312
# [32/100] training 56.9% loss=0.22790, acc=0.95312
# [32/100] training 57.1% loss=0.31272, acc=0.85938
# [32/100] training 57.2% loss=0.11776, acc=0.93750
# [32/100] training 57.5% loss=0.22197, acc=0.90625
# [32/100] training 57.6% loss=0.28516, acc=0.84375
# [32/100] training 57.8% loss=0.14657, acc=0.95312
# [32/100] training 58.0% loss=0.13803, acc=0.95312
# [32/100] training 58.1% loss=0.19462, acc=0.90625
# [32/100] training 58.3% loss=0.16132, acc=0.93750
# [32/100] training 58.4% loss=0.17988, acc=0.93750
# [32/100] training 58.7% loss=0.26520, acc=0.92188
# [32/100] training 58.8% loss=0.29069, acc=0.87500
# [32/100] training 59.0% loss=0.13441, acc=0.95312
# [32/100] training 59.2% loss=0.17395, acc=0.92188
# [32/100] training 59.3% loss=0.16390, acc=0.95312
# [32/100] training 59.5% loss=0.17988, acc=0.92188
# [32/100] training 59.7% loss=0.19047, acc=0.92188
# [32/100] training 59.9% loss=0.19311, acc=0.95312
# [32/100] training 60.0% loss=0.20720, acc=0.92188
# [32/100] training 60.2% loss=0.14767, acc=0.93750
# [32/100] training 60.3% loss=0.18320, acc=0.90625
# [32/100] training 60.5% loss=0.23525, acc=0.85938
# [32/100] training 60.8% loss=0.21462, acc=0.92188
# [32/100] training 60.9% loss=0.38388, acc=0.84375
# [32/100] training 61.1% loss=0.27936, acc=0.87500
# [32/100] training 61.2% loss=0.23435, acc=0.90625
# [32/100] training 61.4% loss=0.31858, acc=0.79688
# [32/100] training 61.5% loss=0.39117, acc=0.81250
# [32/100] training 61.7% loss=0.21071, acc=0.92188
# [32/100] training 62.0% loss=0.27894, acc=0.87500
# [32/100] training 62.1% loss=0.28598, acc=0.89062
# [32/100] training 62.3% loss=0.18570, acc=0.92188
# [32/100] training 62.4% loss=0.19151, acc=0.89062
# [32/100] training 62.6% loss=0.28385, acc=0.82812
# [32/100] training 62.7% loss=0.09708, acc=0.98438
# [32/100] training 62.9% loss=0.20766, acc=0.92188
# [32/100] training 63.1% loss=0.16070, acc=0.90625
# [32/100] training 63.3% loss=0.22137, acc=0.90625
# [32/100] training 63.5% loss=0.22787, acc=0.93750
# [32/100] training 63.6% loss=0.23433, acc=0.90625
# [32/100] training 63.8% loss=0.37745, acc=0.85938
# [32/100] training 63.9% loss=0.22224, acc=0.93750
# [32/100] training 64.2% loss=0.16838, acc=0.92188
# [32/100] training 64.3% loss=0.18150, acc=0.93750
# [32/100] training 64.5% loss=0.17636, acc=0.95312
# [32/100] training 64.7% loss=0.21962, acc=0.85938
# [32/100] training 64.8% loss=0.34434, acc=0.87500
# [32/100] training 65.0% loss=0.26887, acc=0.87500
# [32/100] training 65.1% loss=0.28975, acc=0.85938
# [32/100] training 65.4% loss=0.21193, acc=0.92188
# [32/100] training 65.5% loss=0.14721, acc=0.93750
# [32/100] training 65.7% loss=0.15034, acc=0.92188
# [32/100] training 65.8% loss=0.19592, acc=0.92188
# [32/100] training 66.0% loss=0.18105, acc=0.95312
# [32/100] training 66.2% loss=0.11065, acc=0.93750
# [32/100] training 66.3% loss=0.32163, acc=0.87500
# [32/100] training 66.6% loss=0.26542, acc=0.92188
# [32/100] training 66.7% loss=0.16900, acc=0.92188
# [32/100] training 66.9% loss=0.20123, acc=0.90625
# [32/100] training 67.0% loss=0.24863, acc=0.87500
# [32/100] training 67.2% loss=0.18202, acc=0.93750
# [32/100] training 67.4% loss=0.21319, acc=0.89062
# [32/100] training 67.6% loss=0.25060, acc=0.87500
# [32/100] training 67.8% loss=0.13717, acc=0.96875
# [32/100] training 67.9% loss=0.12978, acc=0.96875
# [32/100] training 68.1% loss=0.17334, acc=0.93750
# [32/100] training 68.2% loss=0.10136, acc=0.98438
# [32/100] training 68.4% loss=0.18150, acc=0.95312
# [32/100] training 68.5% loss=0.28575, acc=0.87500
# [32/100] training 68.8% loss=0.29294, acc=0.87500
# [32/100] training 69.0% loss=0.31648, acc=0.93750
# [32/100] training 69.1% loss=0.15933, acc=0.93750
# [32/100] training 69.3% loss=0.24364, acc=0.87500
# [32/100] training 69.4% loss=0.27373, acc=0.93750
# [32/100] training 69.6% loss=0.41182, acc=0.82812
# [32/100] training 69.7% loss=0.17806, acc=0.90625
# [32/100] training 70.0% loss=0.23815, acc=0.92188
# [32/100] training 70.2% loss=0.24346, acc=0.90625
# [32/100] training 70.3% loss=0.14280, acc=0.98438
# [32/100] training 70.5% loss=0.20165, acc=0.92188
# [32/100] training 70.6% loss=0.07660, acc=1.00000
# [32/100] training 70.8% loss=0.26040, acc=0.87500
# [32/100] training 71.0% loss=0.22045, acc=0.92188
# [32/100] training 71.2% loss=0.18325, acc=0.90625
# [32/100] training 71.3% loss=0.15751, acc=0.93750
# [32/100] training 71.5% loss=0.24811, acc=0.95312
# [32/100] training 71.7% loss=0.23388, acc=0.89062
# [32/100] training 71.8% loss=0.18191, acc=0.92188
# [32/100] training 72.0% loss=0.15923, acc=0.95312
# [32/100] training 72.2% loss=0.18137, acc=0.92188
# [32/100] training 72.4% loss=0.32250, acc=0.87500
# [32/100] training 72.5% loss=0.36946, acc=0.89062
# [32/100] training 72.7% loss=0.32312, acc=0.87500
# [32/100] training 72.9% loss=0.24959, acc=0.92188
# [32/100] training 73.0% loss=0.17585, acc=0.92188
# [32/100] training 73.3% loss=0.34547, acc=0.90625
# [32/100] training 73.4% loss=0.12223, acc=0.95312
# [32/100] training 73.6% loss=0.15160, acc=0.95312
# [32/100] training 73.7% loss=0.19197, acc=0.95312
# [32/100] training 73.9% loss=0.20094, acc=0.92188
# [32/100] training 74.0% loss=0.17378, acc=0.93750
# [32/100] training 74.2% loss=0.16899, acc=0.93750
# [32/100] training 74.5% loss=0.13881, acc=0.95312
# [32/100] training 74.6% loss=0.36727, acc=0.84375
# [32/100] training 74.8% loss=0.34474, acc=0.92188
# [32/100] training 74.9% loss=0.33205, acc=0.90625
# [32/100] training 75.1% loss=0.29059, acc=0.87500
# [32/100] training 75.2% loss=0.12698, acc=0.96875
# [32/100] training 75.4% loss=0.20568, acc=0.90625
# [32/100] training 75.7% loss=0.26705, acc=0.87500
# [32/100] training 75.8% loss=0.27667, acc=0.89062
# [32/100] training 76.0% loss=0.17898, acc=0.93750
# [32/100] training 76.1% loss=0.25825, acc=0.89062
# [32/100] training 76.3% loss=0.15500, acc=0.95312
# [32/100] training 76.4% loss=0.18119, acc=0.89062
# [32/100] training 76.7% loss=0.15257, acc=0.95312
# [32/100] training 76.8% loss=0.18775, acc=0.92188
# [32/100] training 77.0% loss=0.12012, acc=0.96875
# [32/100] training 77.2% loss=0.20616, acc=0.93750
# [32/100] training 77.3% loss=0.16341, acc=0.93750
# [32/100] training 77.5% loss=0.36417, acc=0.85938
# [32/100] training 77.6% loss=0.23508, acc=0.87500
# [32/100] training 77.9% loss=0.19578, acc=0.92188
# [32/100] training 78.0% loss=0.18987, acc=0.92188
# [32/100] training 78.2% loss=0.19977, acc=0.92188
# [32/100] training 78.4% loss=0.13589, acc=0.93750
# [32/100] training 78.5% loss=0.22889, acc=0.90625
# [32/100] training 78.7% loss=0.28079, acc=0.89062
# [32/100] training 78.8% loss=0.14342, acc=0.96875
# [32/100] training 79.1% loss=0.08624, acc=0.98438
# [32/100] training 79.2% loss=0.14247, acc=0.95312
# [32/100] training 79.4% loss=0.13644, acc=0.93750
# [32/100] training 79.5% loss=0.14991, acc=0.95312
# [32/100] training 79.7% loss=0.08342, acc=0.96875
# [32/100] training 79.9% loss=0.12927, acc=0.95312
# [32/100] training 80.1% loss=0.09396, acc=0.96875
# [32/100] training 80.3% loss=0.25808, acc=0.90625
# [32/100] training 80.4% loss=0.24800, acc=0.90625
# [32/100] training 80.6% loss=0.27352, acc=0.82812
# [32/100] training 80.7% loss=0.18770, acc=0.92188
# [32/100] training 80.9% loss=0.18070, acc=0.89062
# [32/100] training 81.2% loss=0.21104, acc=0.90625
# [32/100] training 81.3% loss=0.24928, acc=0.84375
# [32/100] training 81.5% loss=0.17276, acc=0.90625
# [32/100] training 81.6% loss=0.36848, acc=0.84375
# [32/100] training 81.8% loss=0.30209, acc=0.87500
# [32/100] training 81.9% loss=0.31141, acc=0.90625
# [32/100] training 82.1% loss=0.16814, acc=0.92188
# [32/100] training 82.2% loss=0.19386, acc=0.92188
# [32/100] training 82.5% loss=0.13615, acc=0.95312
# [32/100] training 82.7% loss=0.20496, acc=0.92188
# [32/100] training 82.8% loss=0.22662, acc=0.92188
# [32/100] training 83.0% loss=0.24156, acc=0.87500
# [32/100] training 83.1% loss=0.17424, acc=0.93750
# [32/100] training 83.3% loss=0.18173, acc=0.92188
# [32/100] training 83.5% loss=0.16704, acc=0.95312
# [32/100] training 83.7% loss=0.33797, acc=0.89062
# [32/100] training 83.9% loss=0.20901, acc=0.87500
# [32/100] training 84.0% loss=0.15229, acc=0.95312
# [32/100] training 84.2% loss=0.12959, acc=0.95312
# [32/100] training 84.3% loss=0.18686, acc=0.92188
# [32/100] training 84.5% loss=0.16992, acc=0.89062
# [32/100] training 84.7% loss=0.09992, acc=0.95312
# [32/100] training 84.9% loss=0.12869, acc=0.95312
# [32/100] training 85.0% loss=0.28657, acc=0.90625
# [32/100] training 85.2% loss=0.21832, acc=0.90625
# [32/100] training 85.4% loss=0.15104, acc=0.95312
# [32/100] training 85.5% loss=0.15670, acc=0.93750
# [32/100] training 85.8% loss=0.19038, acc=0.93750
# [32/100] training 85.9% loss=0.14554, acc=0.93750
# [32/100] training 86.1% loss=0.18427, acc=0.93750
# [32/100] training 86.2% loss=0.16484, acc=0.95312
# [32/100] training 86.4% loss=0.30054, acc=0.89062
# [32/100] training 86.6% loss=0.28099, acc=0.87500
# [32/100] training 86.7% loss=0.20321, acc=0.96875
# [32/100] training 87.0% loss=0.23757, acc=0.93750
# [32/100] training 87.1% loss=0.21287, acc=0.90625
# [32/100] training 87.3% loss=0.22155, acc=0.90625
# [32/100] training 87.4% loss=0.19341, acc=0.93750
# [32/100] training 87.6% loss=0.17080, acc=0.90625
# [32/100] training 87.7% loss=0.34144, acc=0.95312
# [32/100] training 87.9% loss=0.20305, acc=0.90625
# [32/100] training 88.2% loss=0.18037, acc=0.93750
# [32/100] training 88.3% loss=0.27617, acc=0.92188
# [32/100] training 88.5% loss=0.22990, acc=0.90625
# [32/100] training 88.6% loss=0.13416, acc=0.93750
# [32/100] training 88.8% loss=0.21071, acc=0.93750
# [32/100] training 88.9% loss=0.29220, acc=0.90625
# [32/100] training 89.2% loss=0.19836, acc=0.89062
# [32/100] training 89.4% loss=0.20232, acc=0.92188
# [32/100] training 89.5% loss=0.20610, acc=0.93750
# [32/100] training 89.7% loss=0.16309, acc=0.93750
# [32/100] training 89.8% loss=0.18847, acc=0.95312
# [32/100] training 90.0% loss=0.13049, acc=0.93750
# [32/100] training 90.1% loss=0.24109, acc=0.89062
# [32/100] training 90.4% loss=0.21700, acc=0.89062
# [32/100] training 90.5% loss=0.16257, acc=0.92188
# [32/100] training 90.7% loss=0.19217, acc=0.90625
# [32/100] training 90.9% loss=0.14308, acc=0.95312
# [32/100] training 91.0% loss=0.14574, acc=0.95312
# [32/100] training 91.2% loss=0.25315, acc=0.89062
# [32/100] training 91.3% loss=0.33235, acc=0.90625
# [32/100] training 91.6% loss=0.34184, acc=0.85938
# [32/100] training 91.7% loss=0.19019, acc=0.92188
# [32/100] training 91.9% loss=0.30090, acc=0.90625
# [32/100] training 92.1% loss=0.12978, acc=0.93750
# [32/100] training 92.2% loss=0.10414, acc=0.95312
# [32/100] training 92.4% loss=0.16488, acc=0.92188
# [32/100] training 92.6% loss=0.18581, acc=0.93750
# [32/100] training 92.8% loss=0.26798, acc=0.85938
# [32/100] training 92.9% loss=0.15382, acc=0.96875
# [32/100] training 93.1% loss=0.30671, acc=0.89062
# [32/100] training 93.2% loss=0.33969, acc=0.90625
# [32/100] training 93.4% loss=0.36611, acc=0.87500
# [32/100] training 93.7% loss=0.09845, acc=0.96875
# [32/100] training 93.8% loss=0.22752, acc=0.84375
# [32/100] training 94.0% loss=0.21552, acc=0.92188
# [32/100] training 94.1% loss=0.20515, acc=0.89062
# [32/100] training 94.3% loss=0.19170, acc=0.92188
# [32/100] training 94.4% loss=0.21456, acc=0.92188
# [32/100] training 94.6% loss=0.17675, acc=0.96875
# [32/100] training 94.9% loss=0.17569, acc=0.92188
# [32/100] training 95.0% loss=0.28015, acc=0.87500
# [32/100] training 95.2% loss=0.45234, acc=0.87500
# [32/100] training 95.3% loss=0.31485, acc=0.89062
# [32/100] training 95.5% loss=0.13274, acc=0.96875
# [32/100] training 95.6% loss=0.31907, acc=0.89062
# [32/100] training 95.8% loss=0.23102, acc=0.87500
# [32/100] training 96.0% loss=0.24388, acc=0.90625
# [32/100] training 96.2% loss=0.15213, acc=0.96875
# [32/100] training 96.4% loss=0.14662, acc=0.96875
# [32/100] training 96.5% loss=0.22526, acc=0.90625
# [32/100] training 96.7% loss=0.15764, acc=0.95312
# [32/100] training 96.8% loss=0.26743, acc=0.87500
# [32/100] training 97.1% loss=0.14791, acc=0.92188
# [32/100] training 97.2% loss=0.22685, acc=0.89062
# [32/100] training 97.4% loss=0.24503, acc=0.92188
# [32/100] training 97.6% loss=0.21623, acc=0.92188
# [32/100] training 97.7% loss=0.17073, acc=0.92188
# [32/100] training 97.9% loss=0.16766, acc=0.92188
# [32/100] training 98.0% loss=0.17977, acc=0.92188
# [32/100] training 98.3% loss=0.21762, acc=0.92188
# [32/100] training 98.4% loss=0.16858, acc=0.95312
# [32/100] training 98.6% loss=0.35153, acc=0.84375
# [32/100] training 98.7% loss=0.28913, acc=0.90625
# [32/100] training 98.9% loss=0.11914, acc=0.93750
# [32/100] training 99.1% loss=0.24132, acc=0.89062
# [32/100] training 99.2% loss=0.17919, acc=0.93750
# [32/100] training 99.5% loss=0.38806, acc=0.87500
# [32/100] training 99.6% loss=0.22890, acc=0.89062
# [32/100] training 99.8% loss=0.20730, acc=0.93750
# [32/100] training 99.9% loss=0.08260, acc=0.95312
# [32/100] testing 0.9% loss=0.17072, acc=0.93750
# [32/100] testing 1.8% loss=0.36301, acc=0.85938
# [32/100] testing 2.2% loss=0.30261, acc=0.90625
# [32/100] testing 3.1% loss=0.32446, acc=0.87500
# [32/100] testing 3.5% loss=0.16101, acc=0.93750
# [32/100] testing 4.4% loss=0.18239, acc=0.90625
# [32/100] testing 4.8% loss=0.40952, acc=0.82812
# [32/100] testing 5.7% loss=0.21028, acc=0.95312
# [32/100] testing 6.6% loss=0.23622, acc=0.90625
# [32/100] testing 7.0% loss=0.15548, acc=0.93750
# [32/100] testing 7.9% loss=0.33012, acc=0.85938
# [32/100] testing 8.3% loss=0.23440, acc=0.90625
# [32/100] testing 9.2% loss=0.32532, acc=0.89062
# [32/100] testing 9.7% loss=0.12086, acc=0.93750
# [32/100] testing 10.5% loss=0.23479, acc=0.87500
# [32/100] testing 11.0% loss=0.23055, acc=0.89062
# [32/100] testing 11.8% loss=0.15974, acc=0.92188
# [32/100] testing 12.7% loss=0.35774, acc=0.87500
# [32/100] testing 13.2% loss=0.22126, acc=0.85938
# [32/100] testing 14.0% loss=0.36466, acc=0.92188
# [32/100] testing 14.5% loss=0.32551, acc=0.85938
# [32/100] testing 15.4% loss=0.28602, acc=0.87500
# [32/100] testing 15.8% loss=0.13909, acc=0.95312
# [32/100] testing 16.7% loss=0.31783, acc=0.84375
# [32/100] testing 17.5% loss=0.17798, acc=0.92188
# [32/100] testing 18.0% loss=0.15255, acc=0.95312
# [32/100] testing 18.9% loss=0.12008, acc=0.98438
# [32/100] testing 19.3% loss=0.30675, acc=0.90625
# [32/100] testing 20.2% loss=0.29944, acc=0.85938
# [32/100] testing 20.6% loss=0.29405, acc=0.90625
# [32/100] testing 21.5% loss=0.23014, acc=0.89062
# [32/100] testing 21.9% loss=0.36959, acc=0.82812
# [32/100] testing 22.8% loss=0.32432, acc=0.87500
# [32/100] testing 23.7% loss=0.35583, acc=0.85938
# [32/100] testing 24.1% loss=0.19554, acc=0.89062
# [32/100] testing 25.0% loss=0.34218, acc=0.87500
# [32/100] testing 25.4% loss=0.09358, acc=0.96875
# [32/100] testing 26.3% loss=0.33918, acc=0.84375
# [32/100] testing 26.8% loss=0.33586, acc=0.89062
# [32/100] testing 27.6% loss=0.21766, acc=0.90625
# [32/100] testing 28.5% loss=0.27395, acc=0.87500
# [32/100] testing 29.0% loss=0.15502, acc=0.96875
# [32/100] testing 29.8% loss=0.41514, acc=0.81250
# [32/100] testing 30.3% loss=0.28404, acc=0.89062
# [32/100] testing 31.1% loss=0.29803, acc=0.82812
# [32/100] testing 31.6% loss=0.18764, acc=0.90625
# [32/100] testing 32.5% loss=0.26239, acc=0.90625
# [32/100] testing 32.9% loss=0.47042, acc=0.85938
# [32/100] testing 33.8% loss=0.30347, acc=0.85938
# [32/100] testing 34.7% loss=0.31452, acc=0.89062
# [32/100] testing 35.1% loss=0.13991, acc=0.95312
# [32/100] testing 36.0% loss=0.30301, acc=0.85938
# [32/100] testing 36.4% loss=0.30978, acc=0.87500
# [32/100] testing 37.3% loss=0.22989, acc=0.90625
# [32/100] testing 37.7% loss=0.39415, acc=0.85938
# [32/100] testing 38.6% loss=0.21832, acc=0.93750
# [32/100] testing 39.5% loss=0.29032, acc=0.93750
# [32/100] testing 39.9% loss=0.28223, acc=0.92188
# [32/100] testing 40.8% loss=0.29064, acc=0.92188
# [32/100] testing 41.2% loss=0.20889, acc=0.93750
# [32/100] testing 42.1% loss=0.25337, acc=0.92188
# [32/100] testing 42.5% loss=0.20243, acc=0.92188
# [32/100] testing 43.4% loss=0.28181, acc=0.85938
# [32/100] testing 43.9% loss=0.11111, acc=0.96875
# [32/100] testing 44.7% loss=0.23018, acc=0.90625
# [32/100] testing 45.6% loss=0.27305, acc=0.90625
# [32/100] testing 46.1% loss=0.26640, acc=0.89062
# [32/100] testing 46.9% loss=0.21665, acc=0.92188
# [32/100] testing 47.4% loss=0.15666, acc=0.92188
# [32/100] testing 48.3% loss=0.44383, acc=0.84375
# [32/100] testing 48.7% loss=0.36798, acc=0.84375
# [32/100] testing 49.6% loss=0.51218, acc=0.81250
# [32/100] testing 50.4% loss=0.18984, acc=0.92188
# [32/100] testing 50.9% loss=0.27865, acc=0.87500
# [32/100] testing 51.8% loss=0.22414, acc=0.89062
# [32/100] testing 52.2% loss=0.23542, acc=0.87500
# [32/100] testing 53.1% loss=0.22480, acc=0.92188
# [32/100] testing 53.5% loss=0.28236, acc=0.89062
# [32/100] testing 54.4% loss=0.42395, acc=0.82812
# [32/100] testing 54.8% loss=0.35918, acc=0.85938
# [32/100] testing 55.7% loss=0.11945, acc=0.95312
# [32/100] testing 56.6% loss=0.28623, acc=0.89062
# [32/100] testing 57.0% loss=0.33075, acc=0.89062
# [32/100] testing 57.9% loss=0.23243, acc=0.92188
# [32/100] testing 58.3% loss=0.35649, acc=0.82812
# [32/100] testing 59.2% loss=0.30433, acc=0.90625
# [32/100] testing 59.7% loss=0.25370, acc=0.90625
# [32/100] testing 60.5% loss=0.36701, acc=0.84375
# [32/100] testing 61.4% loss=0.15458, acc=0.93750
# [32/100] testing 61.9% loss=0.20767, acc=0.92188
# [32/100] testing 62.7% loss=0.20311, acc=0.92188
# [32/100] testing 63.2% loss=0.34189, acc=0.85938
# [32/100] testing 64.0% loss=0.42621, acc=0.85938
# [32/100] testing 64.5% loss=0.21718, acc=0.89062
# [32/100] testing 65.4% loss=0.22296, acc=0.93750
# [32/100] testing 65.8% loss=0.39032, acc=0.82812
# [32/100] testing 66.7% loss=0.26763, acc=0.87500
# [32/100] testing 67.6% loss=0.32465, acc=0.89062
# [32/100] testing 68.0% loss=0.11872, acc=0.95312
# [32/100] testing 68.9% loss=0.28599, acc=0.90625
# [32/100] testing 69.3% loss=0.29337, acc=0.87500
# [32/100] testing 70.2% loss=0.44386, acc=0.84375
# [32/100] testing 70.6% loss=0.34402, acc=0.84375
# [32/100] testing 71.5% loss=0.39191, acc=0.85938
# [32/100] testing 72.4% loss=0.11252, acc=0.96875
# [32/100] testing 72.8% loss=0.15892, acc=0.92188
# [32/100] testing 73.7% loss=0.18985, acc=0.93750
# [32/100] testing 74.1% loss=0.30293, acc=0.89062
# [32/100] testing 75.0% loss=0.18815, acc=0.93750
# [32/100] testing 75.4% loss=0.41065, acc=0.85938
# [32/100] testing 76.3% loss=0.09443, acc=0.98438
# [32/100] testing 76.8% loss=0.26040, acc=0.89062
# [32/100] testing 77.6% loss=0.21448, acc=0.92188
# [32/100] testing 78.5% loss=0.47823, acc=0.84375
# [32/100] testing 79.0% loss=0.18815, acc=0.92188
# [32/100] testing 79.8% loss=0.28680, acc=0.87500
# [32/100] testing 80.3% loss=0.26782, acc=0.90625
# [32/100] testing 81.2% loss=0.36039, acc=0.85938
# [32/100] testing 81.6% loss=0.25259, acc=0.90625
# [32/100] testing 82.5% loss=0.21183, acc=0.90625
# [32/100] testing 83.3% loss=0.25103, acc=0.93750
# [32/100] testing 83.8% loss=0.16637, acc=0.93750
# [32/100] testing 84.7% loss=0.28457, acc=0.85938
# [32/100] testing 85.1% loss=0.23284, acc=0.92188
# [32/100] testing 86.0% loss=0.22717, acc=0.90625
# [32/100] testing 86.4% loss=0.29557, acc=0.85938
# [32/100] testing 87.3% loss=0.27656, acc=0.87500
# [32/100] testing 87.7% loss=0.23293, acc=0.93750
# [32/100] testing 88.6% loss=0.31869, acc=0.90625
# [32/100] testing 89.5% loss=0.51951, acc=0.76562
# [32/100] testing 89.9% loss=0.17044, acc=0.95312
# [32/100] testing 90.8% loss=0.29967, acc=0.92188
# [32/100] testing 91.2% loss=0.13613, acc=0.93750
# [32/100] testing 92.1% loss=0.32669, acc=0.90625
# [32/100] testing 92.6% loss=0.27059, acc=0.87500
# [32/100] testing 93.4% loss=0.28206, acc=0.87500
# [32/100] testing 94.3% loss=0.15494, acc=0.95312
# [32/100] testing 94.7% loss=0.21431, acc=0.89062
# [32/100] testing 95.6% loss=0.40827, acc=0.81250
# [32/100] testing 96.1% loss=0.23614, acc=0.87500
# [32/100] testing 96.9% loss=0.27974, acc=0.89062
# [32/100] testing 97.4% loss=0.11723, acc=0.98438
# [32/100] testing 98.3% loss=0.23316, acc=0.87500
# [32/100] testing 98.7% loss=0.22842, acc=0.89062
# [32/100] testing 99.6% loss=0.26892, acc=0.90625
# [33/100] training 0.2% loss=0.35434, acc=0.85938
# [33/100] training 0.4% loss=0.42465, acc=0.85938
# [33/100] training 0.5% loss=0.15226, acc=0.93750
# [33/100] training 0.8% loss=0.20126, acc=0.93750
# [33/100] training 0.9% loss=0.15636, acc=0.90625
# [33/100] training 1.1% loss=0.19991, acc=0.95312
# [33/100] training 1.2% loss=0.21017, acc=0.93750
# [33/100] training 1.4% loss=0.17654, acc=0.95312
# [33/100] training 1.6% loss=0.13933, acc=0.92188
# [33/100] training 1.8% loss=0.23040, acc=0.90625
# [33/100] training 2.0% loss=0.36320, acc=0.84375
# [33/100] training 2.1% loss=0.26112, acc=0.84375
# [33/100] training 2.3% loss=0.17672, acc=0.92188
# [33/100] training 2.4% loss=0.29531, acc=0.89062
# [33/100] training 2.6% loss=0.15746, acc=0.95312
# [33/100] training 2.7% loss=0.26262, acc=0.89062
# [33/100] training 3.0% loss=0.21801, acc=0.92188
# [33/100] training 3.2% loss=0.18770, acc=0.90625
# [33/100] training 3.3% loss=0.33482, acc=0.89062
# [33/100] training 3.5% loss=0.20452, acc=0.90625
# [33/100] training 3.6% loss=0.33765, acc=0.85938
# [33/100] training 3.8% loss=0.28980, acc=0.87500
# [33/100] training 3.9% loss=0.19948, acc=0.92188
# [33/100] training 4.2% loss=0.13673, acc=0.93750
# [33/100] training 4.4% loss=0.14358, acc=0.95312
# [33/100] training 4.5% loss=0.10135, acc=0.95312
# [33/100] training 4.7% loss=0.26553, acc=0.93750
# [33/100] training 4.8% loss=0.23824, acc=0.92188
# [33/100] training 5.0% loss=0.21510, acc=0.92188
# [33/100] training 5.2% loss=0.20896, acc=0.89062
# [33/100] training 5.4% loss=0.08863, acc=0.98438
# [33/100] training 5.5% loss=0.23723, acc=0.90625
# [33/100] training 5.7% loss=0.21179, acc=0.93750
# [33/100] training 5.9% loss=0.22779, acc=0.93750
# [33/100] training 6.0% loss=0.12400, acc=0.95312
# [33/100] training 6.3% loss=0.17582, acc=0.93750
# [33/100] training 6.4% loss=0.16201, acc=0.90625
# [33/100] training 6.6% loss=0.23898, acc=0.89062
# [33/100] training 6.7% loss=0.37721, acc=0.82812
# [33/100] training 6.9% loss=0.20391, acc=0.93750
# [33/100] training 7.1% loss=0.19257, acc=0.87500
# [33/100] training 7.2% loss=0.36540, acc=0.87500
# [33/100] training 7.5% loss=0.18940, acc=0.90625
# [33/100] training 7.6% loss=0.23084, acc=0.89062
# [33/100] training 7.8% loss=0.25710, acc=0.87500
# [33/100] training 7.9% loss=0.30854, acc=0.84375
# [33/100] training 8.1% loss=0.14066, acc=0.98438
# [33/100] training 8.2% loss=0.23649, acc=0.89062
# [33/100] training 8.4% loss=0.20996, acc=0.95312
# [33/100] training 8.7% loss=0.16095, acc=0.96875
# [33/100] training 8.8% loss=0.19331, acc=0.89062
# [33/100] training 9.0% loss=0.21014, acc=0.92188
# [33/100] training 9.1% loss=0.23523, acc=0.90625
# [33/100] training 9.3% loss=0.30088, acc=0.90625
# [33/100] training 9.4% loss=0.13444, acc=0.96875
# [33/100] training 9.7% loss=0.21677, acc=0.90625
# [33/100] training 9.9% loss=0.35412, acc=0.81250
# [33/100] training 10.0% loss=0.23348, acc=0.85938
# [33/100] training 10.2% loss=0.17923, acc=0.90625
# [33/100] training 10.3% loss=0.19632, acc=0.90625
# [33/100] training 10.5% loss=0.38891, acc=0.90625
# [33/100] training 10.6% loss=0.26804, acc=0.92188
# [33/100] training 10.9% loss=0.12086, acc=0.95312
# [33/100] training 11.0% loss=0.29671, acc=0.87500
# [33/100] training 11.2% loss=0.14870, acc=0.95312
# [33/100] training 11.4% loss=0.23869, acc=0.90625
# [33/100] training 11.5% loss=0.33988, acc=0.85938
# [33/100] training 11.7% loss=0.09891, acc=0.96875
# [33/100] training 11.8% loss=0.20424, acc=0.93750
# [33/100] training 12.1% loss=0.14719, acc=0.95312
# [33/100] training 12.2% loss=0.19093, acc=0.93750
# [33/100] training 12.4% loss=0.22843, acc=0.93750
# [33/100] training 12.6% loss=0.20925, acc=0.95312
# [33/100] training 12.7% loss=0.27587, acc=0.87500
# [33/100] training 12.9% loss=0.25831, acc=0.89062
# [33/100] training 13.0% loss=0.19290, acc=0.93750
# [33/100] training 13.3% loss=0.19253, acc=0.90625
# [33/100] training 13.4% loss=0.24106, acc=0.87500
# [33/100] training 13.6% loss=0.18986, acc=0.90625
# [33/100] training 13.7% loss=0.32371, acc=0.84375
# [33/100] training 13.9% loss=0.28197, acc=0.89062
# [33/100] training 14.1% loss=0.31252, acc=0.87500
# [33/100] training 14.3% loss=0.12064, acc=0.95312
# [33/100] training 14.5% loss=0.22911, acc=0.92188
# [33/100] training 14.6% loss=0.17198, acc=0.95312
# [33/100] training 14.8% loss=0.35739, acc=0.85938
# [33/100] training 14.9% loss=0.13140, acc=0.95312
# [33/100] training 15.1% loss=0.39254, acc=0.79688
# [33/100] training 15.4% loss=0.34005, acc=0.87500
# [33/100] training 15.5% loss=0.24775, acc=0.92188
# [33/100] training 15.7% loss=0.31332, acc=0.89062
# [33/100] training 15.8% loss=0.15461, acc=0.92188
# [33/100] training 16.0% loss=0.32210, acc=0.89062
# [33/100] training 16.1% loss=0.38747, acc=0.84375
# [33/100] training 16.3% loss=0.23472, acc=0.92188
# [33/100] training 16.4% loss=0.18815, acc=0.95312
# [33/100] training 16.7% loss=0.29893, acc=0.85938
# [33/100] training 16.9% loss=0.20348, acc=0.92188
# [33/100] training 17.0% loss=0.23726, acc=0.87500
# [33/100] training 17.2% loss=0.15582, acc=0.92188
# [33/100] training 17.3% loss=0.17360, acc=0.92188
# [33/100] training 17.5% loss=0.20168, acc=0.92188
# [33/100] training 17.7% loss=0.21204, acc=0.93750
# [33/100] training 17.9% loss=0.23401, acc=0.90625
# [33/100] training 18.1% loss=0.27719, acc=0.87500
# [33/100] training 18.2% loss=0.32243, acc=0.85938
# [33/100] training 18.4% loss=0.31169, acc=0.85938
# [33/100] training 18.5% loss=0.16231, acc=0.93750
# [33/100] training 18.8% loss=0.19181, acc=0.93750
# [33/100] training 18.9% loss=0.17253, acc=0.95312
# [33/100] training 19.1% loss=0.23732, acc=0.87500
# [33/100] training 19.2% loss=0.18428, acc=0.90625
# [33/100] training 19.4% loss=0.15332, acc=0.95312
# [33/100] training 19.6% loss=0.28057, acc=0.87500
# [33/100] training 19.7% loss=0.33513, acc=0.85938
# [33/100] training 20.0% loss=0.13534, acc=0.93750
# [33/100] training 20.1% loss=0.22449, acc=0.90625
# [33/100] training 20.3% loss=0.21659, acc=0.89062
# [33/100] training 20.4% loss=0.31016, acc=0.85938
# [33/100] training 20.6% loss=0.24293, acc=0.89062
# [33/100] training 20.8% loss=0.13619, acc=0.93750
# [33/100] training 20.9% loss=0.21903, acc=0.90625
# [33/100] training 21.2% loss=0.29086, acc=0.93750
# [33/100] training 21.3% loss=0.26533, acc=0.85938
# [33/100] training 21.5% loss=0.27001, acc=0.90625
# [33/100] training 21.6% loss=0.10268, acc=0.95312
# [33/100] training 21.8% loss=0.13662, acc=0.96875
# [33/100] training 21.9% loss=0.27041, acc=0.89062
# [33/100] training 22.2% loss=0.24328, acc=0.89062
# [33/100] training 22.4% loss=0.29746, acc=0.85938
# [33/100] training 22.5% loss=0.17264, acc=0.95312
# [33/100] training 22.7% loss=0.20837, acc=0.90625
# [33/100] training 22.8% loss=0.23133, acc=0.93750
# [33/100] training 23.0% loss=0.16814, acc=0.92188
# [33/100] training 23.1% loss=0.23580, acc=0.92188
# [33/100] training 23.4% loss=0.29874, acc=0.87500
# [33/100] training 23.6% loss=0.31659, acc=0.92188
# [33/100] training 23.7% loss=0.19246, acc=0.93750
# [33/100] training 23.9% loss=0.18602, acc=0.92188
# [33/100] training 24.0% loss=0.15634, acc=0.92188
# [33/100] training 24.2% loss=0.11314, acc=0.95312
# [33/100] training 24.3% loss=0.30866, acc=0.89062
# [33/100] training 24.6% loss=0.26310, acc=0.87500
# [33/100] training 24.7% loss=0.33315, acc=0.90625
# [33/100] training 24.9% loss=0.24592, acc=0.89062
# [33/100] training 25.1% loss=0.29156, acc=0.85938
# [33/100] training 25.2% loss=0.16004, acc=0.93750
# [33/100] training 25.4% loss=0.20711, acc=0.93750
# [33/100] training 25.6% loss=0.20093, acc=0.93750
# [33/100] training 25.8% loss=0.25219, acc=0.90625
# [33/100] training 25.9% loss=0.22055, acc=0.90625
# [33/100] training 26.1% loss=0.14670, acc=0.93750
# [33/100] training 26.3% loss=0.13242, acc=0.93750
# [33/100] training 26.4% loss=0.13173, acc=0.95312
# [33/100] training 26.6% loss=0.10601, acc=0.96875
# [33/100] training 26.8% loss=0.23619, acc=0.92188
# [33/100] training 27.0% loss=0.27093, acc=0.87500
# [33/100] training 27.1% loss=0.09404, acc=0.95312
# [33/100] training 27.3% loss=0.22253, acc=0.89062
# [33/100] training 27.4% loss=0.10369, acc=0.93750
# [33/100] training 27.6% loss=0.29341, acc=0.90625
# [33/100] training 27.9% loss=0.13573, acc=0.92188
# [33/100] training 28.0% loss=0.21707, acc=0.90625
# [33/100] training 28.2% loss=0.25221, acc=0.89062
# [33/100] training 28.3% loss=0.10568, acc=0.96875
# [33/100] training 28.5% loss=0.22095, acc=0.90625
# [33/100] training 28.6% loss=0.21022, acc=0.93750
# [33/100] training 28.8% loss=0.10417, acc=0.95312
# [33/100] training 29.1% loss=0.18404, acc=0.93750
# [33/100] training 29.2% loss=0.13763, acc=0.93750
# [33/100] training 29.4% loss=0.32064, acc=0.85938
# [33/100] training 29.5% loss=0.13244, acc=0.93750
# [33/100] training 29.7% loss=0.20535, acc=0.92188
# [33/100] training 29.8% loss=0.22177, acc=0.90625
# [33/100] training 30.0% loss=0.27939, acc=0.90625
# [33/100] training 30.2% loss=0.17998, acc=0.95312
# [33/100] training 30.4% loss=0.14625, acc=0.93750
# [33/100] training 30.6% loss=0.19710, acc=0.90625
# [33/100] training 30.7% loss=0.23787, acc=0.89062
# [33/100] training 30.9% loss=0.33793, acc=0.90625
# [33/100] training 31.0% loss=0.14866, acc=0.95312
# [33/100] training 31.3% loss=0.18264, acc=0.95312
# [33/100] training 31.4% loss=0.30898, acc=0.85938
# [33/100] training 31.6% loss=0.23029, acc=0.93750
# [33/100] training 31.8% loss=0.28560, acc=0.87500
# [33/100] training 31.9% loss=0.21722, acc=0.92188
# [33/100] training 32.1% loss=0.20986, acc=0.95312
# [33/100] training 32.2% loss=0.27418, acc=0.90625
# [33/100] training 32.5% loss=0.18753, acc=0.92188
# [33/100] training 32.6% loss=0.19665, acc=0.90625
# [33/100] training 32.8% loss=0.19132, acc=0.93750
# [33/100] training 32.9% loss=0.25121, acc=0.90625
# [33/100] training 33.1% loss=0.12676, acc=0.95312
# [33/100] training 33.3% loss=0.23438, acc=0.90625
# [33/100] training 33.4% loss=0.15604, acc=0.93750
# [33/100] training 33.7% loss=0.24002, acc=0.90625
# [33/100] training 33.8% loss=0.16421, acc=0.93750
# [33/100] training 34.0% loss=0.23545, acc=0.89062
# [33/100] training 34.1% loss=0.19472, acc=0.90625
# [33/100] training 34.3% loss=0.19907, acc=0.92188
# [33/100] training 34.5% loss=0.20177, acc=0.93750
# [33/100] training 34.7% loss=0.17474, acc=0.90625
# [33/100] training 34.9% loss=0.13493, acc=0.92188
# [33/100] training 35.0% loss=0.17026, acc=0.92188
# [33/100] training 35.2% loss=0.28195, acc=0.87500
# [33/100] training 35.3% loss=0.26895, acc=0.87500
# [33/100] training 35.5% loss=0.11095, acc=0.93750
# [33/100] training 35.6% loss=0.33892, acc=0.89062
# [33/100] training 35.9% loss=0.18947, acc=0.92188
# [33/100] training 36.1% loss=0.25562, acc=0.90625
# [33/100] training 36.2% loss=0.22812, acc=0.87500
# [33/100] training 36.4% loss=0.28298, acc=0.89062
# [33/100] training 36.5% loss=0.22129, acc=0.92188
# [33/100] training 36.7% loss=0.19597, acc=0.93750
# [33/100] training 36.8% loss=0.10277, acc=0.98438
# [33/100] training 37.1% loss=0.28515, acc=0.85938
# [33/100] training 37.3% loss=0.20591, acc=0.93750
# [33/100] training 37.4% loss=0.19062, acc=0.92188
# [33/100] training 37.6% loss=0.18434, acc=0.95312
# [33/100] training 37.7% loss=0.30827, acc=0.89062
# [33/100] training 37.9% loss=0.20124, acc=0.92188
# [33/100] training 38.1% loss=0.25373, acc=0.89062
# [33/100] training 38.3% loss=0.21212, acc=0.90625
# [33/100] training 38.4% loss=0.07139, acc=0.96875
# [33/100] training 38.6% loss=0.16704, acc=0.93750
# [33/100] training 38.8% loss=0.31368, acc=0.85938
# [33/100] training 38.9% loss=0.24298, acc=0.89062
# [33/100] training 39.1% loss=0.21720, acc=0.93750
# [33/100] training 39.3% loss=0.19037, acc=0.90625
# [33/100] training 39.5% loss=0.25064, acc=0.90625
# [33/100] training 39.6% loss=0.21672, acc=0.92188
# [33/100] training 39.8% loss=0.18010, acc=0.93750
# [33/100] training 40.0% loss=0.23389, acc=0.92188
# [33/100] training 40.1% loss=0.27363, acc=0.85938
# [33/100] training 40.4% loss=0.20763, acc=0.90625
# [33/100] training 40.5% loss=0.23414, acc=0.90625
# [33/100] training 40.7% loss=0.23170, acc=0.89062
# [33/100] training 40.8% loss=0.14405, acc=0.95312
# [33/100] training 41.0% loss=0.19798, acc=0.92188
# [33/100] training 41.1% loss=0.28107, acc=0.84375
# [33/100] training 41.3% loss=0.30493, acc=0.84375
# [33/100] training 41.6% loss=0.22749, acc=0.89062
# [33/100] training 41.7% loss=0.28638, acc=0.90625
# [33/100] training 41.9% loss=0.22710, acc=0.87500
# [33/100] training 42.0% loss=0.19178, acc=0.93750
# [33/100] training 42.2% loss=0.24825, acc=0.89062
# [33/100] training 42.3% loss=0.21755, acc=0.90625
# [33/100] training 42.5% loss=0.19804, acc=0.93750
# [33/100] training 42.8% loss=0.08811, acc=0.96875
# [33/100] training 42.9% loss=0.13479, acc=0.93750
# [33/100] training 43.1% loss=0.18662, acc=0.93750
# [33/100] training 43.2% loss=0.08319, acc=0.96875
# [33/100] training 43.4% loss=0.22612, acc=0.93750
# [33/100] training 43.5% loss=0.16044, acc=0.92188
# [33/100] training 43.8% loss=0.19721, acc=0.89062
# [33/100] training 43.9% loss=0.19538, acc=0.93750
# [33/100] training 44.1% loss=0.15675, acc=0.95312
# [33/100] training 44.3% loss=0.13696, acc=0.95312
# [33/100] training 44.4% loss=0.29590, acc=0.84375
# [33/100] training 44.6% loss=0.23957, acc=0.92188
# [33/100] training 44.7% loss=0.34662, acc=0.87500
# [33/100] training 45.0% loss=0.08767, acc=0.96875
# [33/100] training 45.1% loss=0.39105, acc=0.89062
# [33/100] training 45.3% loss=0.20869, acc=0.89062
# [33/100] training 45.5% loss=0.07291, acc=0.98438
# [33/100] training 45.6% loss=0.24473, acc=0.90625
# [33/100] training 45.8% loss=0.10215, acc=0.96875
# [33/100] training 45.9% loss=0.16778, acc=0.92188
# [33/100] training 46.2% loss=0.16628, acc=0.93750
# [33/100] training 46.3% loss=0.13649, acc=0.93750
# [33/100] training 46.5% loss=0.27999, acc=0.89062
# [33/100] training 46.6% loss=0.17647, acc=0.93750
# [33/100] training 46.8% loss=0.15394, acc=0.93750
# [33/100] training 47.0% loss=0.21282, acc=0.87500
# [33/100] training 47.2% loss=0.19845, acc=0.92188
# [33/100] training 47.4% loss=0.16361, acc=0.95312
# [33/100] training 47.5% loss=0.31911, acc=0.89062
# [33/100] training 47.7% loss=0.19723, acc=0.90625
# [33/100] training 47.8% loss=0.22057, acc=0.90625
# [33/100] training 48.0% loss=0.25768, acc=0.92188
# [33/100] training 48.3% loss=0.09965, acc=0.95312
# [33/100] training 48.4% loss=0.11711, acc=0.98438
# [33/100] training 48.6% loss=0.15020, acc=0.95312
# [33/100] training 48.7% loss=0.30231, acc=0.84375
# [33/100] training 48.9% loss=0.17332, acc=0.95312
# [33/100] training 49.0% loss=0.15030, acc=0.92188
# [33/100] training 49.2% loss=0.22238, acc=0.92188
# [33/100] training 49.3% loss=0.20012, acc=0.93750
# [33/100] training 49.6% loss=0.27560, acc=0.92188
# [33/100] training 49.8% loss=0.22575, acc=0.92188
# [33/100] training 49.9% loss=0.25286, acc=0.90625
# [33/100] training 50.1% loss=0.19167, acc=0.93750
# [33/100] training 50.2% loss=0.29192, acc=0.89062
# [33/100] training 50.4% loss=0.37679, acc=0.87500
# [33/100] training 50.6% loss=0.30830, acc=0.87500
# [33/100] training 50.8% loss=0.32203, acc=0.89062
# [33/100] training 51.0% loss=0.34247, acc=0.84375
# [33/100] training 51.1% loss=0.22207, acc=0.90625
# [33/100] training 51.3% loss=0.24022, acc=0.89062
# [33/100] training 51.4% loss=0.22264, acc=0.93750
# [33/100] training 51.7% loss=0.17513, acc=0.90625
# [33/100] training 51.8% loss=0.22889, acc=0.87500
# [33/100] training 52.0% loss=0.19940, acc=0.90625
# [33/100] training 52.1% loss=0.20843, acc=0.90625
# [33/100] training 52.3% loss=0.25842, acc=0.90625
# [33/100] training 52.5% loss=0.14689, acc=0.93750
# [33/100] training 52.6% loss=0.19253, acc=0.89062
# [33/100] training 52.9% loss=0.34779, acc=0.90625
# [33/100] training 53.0% loss=0.13220, acc=0.95312
# [33/100] training 53.2% loss=0.16488, acc=0.96875
# [33/100] training 53.3% loss=0.12437, acc=0.96875
# [33/100] training 53.5% loss=0.17758, acc=0.92188
# [33/100] training 53.7% loss=0.10256, acc=0.96875
# [33/100] training 53.8% loss=0.32665, acc=0.87500
# [33/100] training 54.1% loss=0.30613, acc=0.87500
# [33/100] training 54.2% loss=0.15776, acc=0.93750
# [33/100] training 54.4% loss=0.21391, acc=0.89062
# [33/100] training 54.5% loss=0.31685, acc=0.89062
# [33/100] training 54.7% loss=0.33952, acc=0.90625
# [33/100] training 54.8% loss=0.16443, acc=0.93750
# [33/100] training 55.1% loss=0.26101, acc=0.89062
# [33/100] training 55.3% loss=0.19014, acc=0.90625
# [33/100] training 55.4% loss=0.19973, acc=0.93750
# [33/100] training 55.6% loss=0.16102, acc=0.92188
# [33/100] training 55.7% loss=0.15366, acc=0.92188
# [33/100] training 55.9% loss=0.14054, acc=0.92188
# [33/100] training 56.0% loss=0.11732, acc=0.95312
# [33/100] training 56.3% loss=0.40127, acc=0.87500
# [33/100] training 56.5% loss=0.12795, acc=0.95312
# [33/100] training 56.6% loss=0.20930, acc=0.89062
# [33/100] training 56.8% loss=0.20888, acc=0.89062
# [33/100] training 56.9% loss=0.16837, acc=0.92188
# [33/100] training 57.1% loss=0.23067, acc=0.90625
# [33/100] training 57.2% loss=0.08848, acc=0.95312
# [33/100] training 57.5% loss=0.19011, acc=0.90625
# [33/100] training 57.6% loss=0.23280, acc=0.89062
# [33/100] training 57.8% loss=0.14334, acc=0.96875
# [33/100] training 58.0% loss=0.16606, acc=0.93750
# [33/100] training 58.1% loss=0.21099, acc=0.92188
# [33/100] training 58.3% loss=0.14623, acc=0.92188
# [33/100] training 58.4% loss=0.21485, acc=0.92188
# [33/100] training 58.7% loss=0.37464, acc=0.90625
# [33/100] training 58.8% loss=0.31034, acc=0.84375
# [33/100] training 59.0% loss=0.19326, acc=0.93750
# [33/100] training 59.2% loss=0.14397, acc=0.93750
# [33/100] training 59.3% loss=0.14867, acc=0.95312
# [33/100] training 59.5% loss=0.25625, acc=0.89062
# [33/100] training 59.7% loss=0.28784, acc=0.87500
# [33/100] training 59.9% loss=0.20641, acc=0.90625
# [33/100] training 60.0% loss=0.23012, acc=0.87500
# [33/100] training 60.2% loss=0.22861, acc=0.87500
# [33/100] training 60.3% loss=0.16035, acc=0.95312
# [33/100] training 60.5% loss=0.25070, acc=0.87500
# [33/100] training 60.8% loss=0.20262, acc=0.89062
# [33/100] training 60.9% loss=0.33896, acc=0.85938
# [33/100] training 61.1% loss=0.24742, acc=0.89062
# [33/100] training 61.2% loss=0.20869, acc=0.87500
# [33/100] training 61.4% loss=0.27964, acc=0.87500
# [33/100] training 61.5% loss=0.42106, acc=0.79688
# [33/100] training 61.7% loss=0.28489, acc=0.85938
# [33/100] training 62.0% loss=0.28408, acc=0.87500
# [33/100] training 62.1% loss=0.27171, acc=0.87500
# [33/100] training 62.3% loss=0.12797, acc=0.95312
# [33/100] training 62.4% loss=0.16648, acc=0.93750
# [33/100] training 62.6% loss=0.39464, acc=0.81250
# [33/100] training 62.7% loss=0.18503, acc=0.92188
# [33/100] training 62.9% loss=0.32931, acc=0.85938
# [33/100] training 63.1% loss=0.13839, acc=0.96875
# [33/100] training 63.3% loss=0.18741, acc=0.89062
# [33/100] training 63.5% loss=0.29028, acc=0.85938
# [33/100] training 63.6% loss=0.21631, acc=0.93750
# [33/100] training 63.8% loss=0.38763, acc=0.87500
# [33/100] training 63.9% loss=0.22053, acc=0.93750
# [33/100] training 64.2% loss=0.14376, acc=0.93750
# [33/100] training 64.3% loss=0.31115, acc=0.85938
# [33/100] training 64.5% loss=0.29008, acc=0.87500
# [33/100] training 64.7% loss=0.31944, acc=0.84375
# [33/100] training 64.8% loss=0.32491, acc=0.85938
# [33/100] training 65.0% loss=0.23003, acc=0.90625
# [33/100] training 65.1% loss=0.23514, acc=0.92188
# [33/100] training 65.4% loss=0.14694, acc=0.95312
# [33/100] training 65.5% loss=0.15731, acc=0.96875
# [33/100] training 65.7% loss=0.13407, acc=0.93750
# [33/100] training 65.8% loss=0.23962, acc=0.89062
# [33/100] training 66.0% loss=0.26907, acc=0.89062
# [33/100] training 66.2% loss=0.09966, acc=0.96875
# [33/100] training 66.3% loss=0.25066, acc=0.87500
# [33/100] training 66.6% loss=0.27243, acc=0.89062
# [33/100] training 66.7% loss=0.13435, acc=0.93750
# [33/100] training 66.9% loss=0.12009, acc=0.95312
# [33/100] training 67.0% loss=0.18263, acc=0.93750
# [33/100] training 67.2% loss=0.16749, acc=0.92188
# [33/100] training 67.4% loss=0.25052, acc=0.90625
# [33/100] training 67.6% loss=0.23904, acc=0.90625
# [33/100] training 67.8% loss=0.22644, acc=0.89062
# [33/100] training 67.9% loss=0.13864, acc=0.92188
# [33/100] training 68.1% loss=0.15828, acc=0.95312
# [33/100] training 68.2% loss=0.11294, acc=0.95312
# [33/100] training 68.4% loss=0.09248, acc=0.95312
# [33/100] training 68.5% loss=0.29408, acc=0.87500
# [33/100] training 68.8% loss=0.15331, acc=0.90625
# [33/100] training 69.0% loss=0.25276, acc=0.93750
# [33/100] training 69.1% loss=0.15100, acc=0.92188
# [33/100] training 69.3% loss=0.19818, acc=0.90625
# [33/100] training 69.4% loss=0.24958, acc=0.87500
# [33/100] training 69.6% loss=0.23522, acc=0.90625
# [33/100] training 69.7% loss=0.20631, acc=0.89062
# [33/100] training 70.0% loss=0.27013, acc=0.90625
# [33/100] training 70.2% loss=0.38047, acc=0.87500
# [33/100] training 70.3% loss=0.17796, acc=0.93750
# [33/100] training 70.5% loss=0.20413, acc=0.92188
# [33/100] training 70.6% loss=0.17982, acc=0.90625
# [33/100] training 70.8% loss=0.35603, acc=0.82812
# [33/100] training 71.0% loss=0.20986, acc=0.89062
# [33/100] training 71.2% loss=0.18400, acc=0.92188
# [33/100] training 71.3% loss=0.16731, acc=0.93750
# [33/100] training 71.5% loss=0.30157, acc=0.89062
# [33/100] training 71.7% loss=0.29569, acc=0.84375
# [33/100] training 71.8% loss=0.29123, acc=0.87500
# [33/100] training 72.0% loss=0.16302, acc=0.95312
# [33/100] training 72.2% loss=0.22625, acc=0.90625
# [33/100] training 72.4% loss=0.23684, acc=0.90625
# [33/100] training 72.5% loss=0.26162, acc=0.90625
# [33/100] training 72.7% loss=0.25493, acc=0.92188
# [33/100] training 72.9% loss=0.19473, acc=0.93750
# [33/100] training 73.0% loss=0.13715, acc=0.93750
# [33/100] training 73.3% loss=0.32714, acc=0.87500
# [33/100] training 73.4% loss=0.12308, acc=0.95312
# [33/100] training 73.6% loss=0.13220, acc=0.95312
# [33/100] training 73.7% loss=0.17104, acc=0.96875
# [33/100] training 73.9% loss=0.15104, acc=0.96875
# [33/100] training 74.0% loss=0.27244, acc=0.87500
# [33/100] training 74.2% loss=0.15683, acc=0.92188
# [33/100] training 74.5% loss=0.15264, acc=0.90625
# [33/100] training 74.6% loss=0.46655, acc=0.82812
# [33/100] training 74.8% loss=0.38548, acc=0.81250
# [33/100] training 74.9% loss=0.30075, acc=0.85938
# [33/100] training 75.1% loss=0.19319, acc=0.92188
# [33/100] training 75.2% loss=0.17246, acc=0.92188
# [33/100] training 75.4% loss=0.23350, acc=0.90625
# [33/100] training 75.7% loss=0.21942, acc=0.90625
# [33/100] training 75.8% loss=0.27119, acc=0.90625
# [33/100] training 76.0% loss=0.19132, acc=0.90625
# [33/100] training 76.1% loss=0.29752, acc=0.87500
# [33/100] training 76.3% loss=0.13412, acc=0.93750
# [33/100] training 76.4% loss=0.22476, acc=0.95312
# [33/100] training 76.7% loss=0.21009, acc=0.92188
# [33/100] training 76.8% loss=0.17471, acc=0.93750
# [33/100] training 77.0% loss=0.11331, acc=0.98438
# [33/100] training 77.2% loss=0.25238, acc=0.90625
# [33/100] training 77.3% loss=0.12323, acc=0.98438
# [33/100] training 77.5% loss=0.21543, acc=0.90625
# [33/100] training 77.6% loss=0.19906, acc=0.92188
# [33/100] training 77.9% loss=0.24204, acc=0.90625
# [33/100] training 78.0% loss=0.21826, acc=0.92188
# [33/100] training 78.2% loss=0.15776, acc=0.93750
# [33/100] training 78.4% loss=0.10001, acc=0.98438
# [33/100] training 78.5% loss=0.36842, acc=0.89062
# [33/100] training 78.7% loss=0.24737, acc=0.89062
# [33/100] training 78.8% loss=0.06907, acc=1.00000
# [33/100] training 79.1% loss=0.07359, acc=1.00000
# [33/100] training 79.2% loss=0.14958, acc=0.95312
# [33/100] training 79.4% loss=0.17166, acc=0.92188
# [33/100] training 79.5% loss=0.22309, acc=0.92188
# [33/100] training 79.7% loss=0.12414, acc=0.93750
# [33/100] training 79.9% loss=0.20758, acc=0.90625
# [33/100] training 80.1% loss=0.12450, acc=0.96875
# [33/100] training 80.3% loss=0.22522, acc=0.92188
# [33/100] training 80.4% loss=0.14377, acc=0.95312
# [33/100] training 80.6% loss=0.23559, acc=0.87500
# [33/100] training 80.7% loss=0.15781, acc=0.95312
# [33/100] training 80.9% loss=0.18924, acc=0.92188
# [33/100] training 81.2% loss=0.23465, acc=0.90625
# [33/100] training 81.3% loss=0.25828, acc=0.89062
# [33/100] training 81.5% loss=0.23259, acc=0.90625
# [33/100] training 81.6% loss=0.28874, acc=0.87500
# [33/100] training 81.8% loss=0.24658, acc=0.87500
# [33/100] training 81.9% loss=0.26332, acc=0.90625
# [33/100] training 82.1% loss=0.18081, acc=0.92188
# [33/100] training 82.2% loss=0.19191, acc=0.92188
# [33/100] training 82.5% loss=0.14541, acc=0.93750
# [33/100] training 82.7% loss=0.24000, acc=0.92188
# [33/100] training 82.8% loss=0.15514, acc=0.93750
# [33/100] training 83.0% loss=0.12393, acc=0.96875
# [33/100] training 83.1% loss=0.18746, acc=0.93750
# [33/100] training 83.3% loss=0.13490, acc=0.92188
# [33/100] training 83.5% loss=0.13896, acc=0.93750
# [33/100] training 83.7% loss=0.21519, acc=0.90625
# [33/100] training 83.9% loss=0.19959, acc=0.89062
# [33/100] training 84.0% loss=0.17404, acc=0.95312
# [33/100] training 84.2% loss=0.12232, acc=0.95312
# [33/100] training 84.3% loss=0.16722, acc=0.90625
# [33/100] training 84.5% loss=0.24270, acc=0.90625
# [33/100] training 84.7% loss=0.21156, acc=0.87500
# [33/100] training 84.9% loss=0.17236, acc=0.92188
# [33/100] training 85.0% loss=0.17322, acc=0.93750
# [33/100] training 85.2% loss=0.12906, acc=0.95312
# [33/100] training 85.4% loss=0.23725, acc=0.92188
# [33/100] training 85.5% loss=0.18168, acc=0.90625
# [33/100] training 85.8% loss=0.24963, acc=0.90625
# [33/100] training 85.9% loss=0.15590, acc=0.93750
# [33/100] training 86.1% loss=0.14515, acc=0.93750
# [33/100] training 86.2% loss=0.12325, acc=0.95312
# [33/100] training 86.4% loss=0.33467, acc=0.89062
# [33/100] training 86.6% loss=0.24923, acc=0.92188
# [33/100] training 86.7% loss=0.13911, acc=0.96875
# [33/100] training 87.0% loss=0.21776, acc=0.90625
# [33/100] training 87.1% loss=0.29077, acc=0.87500
# [33/100] training 87.3% loss=0.27876, acc=0.87500
# [33/100] training 87.4% loss=0.26018, acc=0.85938
# [33/100] training 87.6% loss=0.16953, acc=0.92188
# [33/100] training 87.7% loss=0.32026, acc=0.90625
# [33/100] training 87.9% loss=0.17730, acc=0.89062
# [33/100] training 88.2% loss=0.14381, acc=0.95312
# [33/100] training 88.3% loss=0.23100, acc=0.93750
# [33/100] training 88.5% loss=0.20278, acc=0.89062
# [33/100] training 88.6% loss=0.14477, acc=0.95312
# [33/100] training 88.8% loss=0.15749, acc=0.95312
# [33/100] training 88.9% loss=0.28250, acc=0.90625
# [33/100] training 89.2% loss=0.20279, acc=0.90625
# [33/100] training 89.4% loss=0.18875, acc=0.96875
# [33/100] training 89.5% loss=0.24822, acc=0.87500
# [33/100] training 89.7% loss=0.23652, acc=0.89062
# [33/100] training 89.8% loss=0.17952, acc=0.93750
# [33/100] training 90.0% loss=0.13298, acc=0.95312
# [33/100] training 90.1% loss=0.23376, acc=0.92188
# [33/100] training 90.4% loss=0.26169, acc=0.89062
# [33/100] training 90.5% loss=0.11917, acc=0.96875
# [33/100] training 90.7% loss=0.19594, acc=0.95312
# [33/100] training 90.9% loss=0.11812, acc=0.95312
# [33/100] training 91.0% loss=0.22361, acc=0.93750
# [33/100] training 91.2% loss=0.19046, acc=0.90625
# [33/100] training 91.3% loss=0.24137, acc=0.92188
# [33/100] training 91.6% loss=0.18843, acc=0.92188
# [33/100] training 91.7% loss=0.15061, acc=0.93750
# [33/100] training 91.9% loss=0.28577, acc=0.89062
# [33/100] training 92.1% loss=0.16034, acc=0.90625
# [33/100] training 92.2% loss=0.13335, acc=0.95312
# [33/100] training 92.4% loss=0.15665, acc=0.93750
# [33/100] training 92.6% loss=0.42637, acc=0.82812
# [33/100] training 92.8% loss=0.35826, acc=0.85938
# [33/100] training 92.9% loss=0.24180, acc=0.90625
# [33/100] training 93.1% loss=0.38853, acc=0.84375
# [33/100] training 93.2% loss=0.22516, acc=0.92188
# [33/100] training 93.4% loss=0.30982, acc=0.90625
# [33/100] training 93.7% loss=0.17633, acc=0.93750
# [33/100] training 93.8% loss=0.19612, acc=0.92188
# [33/100] training 94.0% loss=0.17413, acc=0.96875
# [33/100] training 94.1% loss=0.20969, acc=0.92188
# [33/100] training 94.3% loss=0.11449, acc=0.95312
# [33/100] training 94.4% loss=0.21497, acc=0.95312
# [33/100] training 94.6% loss=0.10443, acc=0.96875
# [33/100] training 94.9% loss=0.16819, acc=0.90625
# [33/100] training 95.0% loss=0.30602, acc=0.87500
# [33/100] training 95.2% loss=0.43267, acc=0.85938
# [33/100] training 95.3% loss=0.39137, acc=0.90625
# [33/100] training 95.5% loss=0.15731, acc=0.96875
# [33/100] training 95.6% loss=0.26535, acc=0.89062
# [33/100] training 95.8% loss=0.30561, acc=0.81250
# [33/100] training 96.0% loss=0.24834, acc=0.89062
# [33/100] training 96.2% loss=0.17231, acc=0.92188
# [33/100] training 96.4% loss=0.17462, acc=0.95312
# [33/100] training 96.5% loss=0.22392, acc=0.92188
# [33/100] training 96.7% loss=0.12528, acc=0.96875
# [33/100] training 96.8% loss=0.24583, acc=0.87500
# [33/100] training 97.1% loss=0.12440, acc=0.93750
# [33/100] training 97.2% loss=0.18972, acc=0.93750
# [33/100] training 97.4% loss=0.29858, acc=0.95312
# [33/100] training 97.6% loss=0.18630, acc=0.93750
# [33/100] training 97.7% loss=0.20146, acc=0.95312
# [33/100] training 97.9% loss=0.21411, acc=0.90625
# [33/100] training 98.0% loss=0.21329, acc=0.89062
# [33/100] training 98.3% loss=0.29587, acc=0.85938
# [33/100] training 98.4% loss=0.15044, acc=0.93750
# [33/100] training 98.6% loss=0.38439, acc=0.82812
# [33/100] training 98.7% loss=0.27580, acc=0.90625
# [33/100] training 98.9% loss=0.20820, acc=0.92188
# [33/100] training 99.1% loss=0.20333, acc=0.92188
# [33/100] training 99.2% loss=0.18183, acc=0.90625
# [33/100] training 99.5% loss=0.28963, acc=0.90625
# [33/100] training 99.6% loss=0.19226, acc=0.93750
# [33/100] training 99.8% loss=0.14730, acc=0.95312
# [33/100] training 99.9% loss=0.09070, acc=0.95312
# [33/100] testing 0.9% loss=0.14515, acc=0.92188
# [33/100] testing 1.8% loss=0.41662, acc=0.84375
# [33/100] testing 2.2% loss=0.24920, acc=0.89062
# [33/100] testing 3.1% loss=0.38496, acc=0.82812
# [33/100] testing 3.5% loss=0.16799, acc=0.93750
# [33/100] testing 4.4% loss=0.23832, acc=0.89062
# [33/100] testing 4.8% loss=0.33410, acc=0.85938
# [33/100] testing 5.7% loss=0.20869, acc=0.89062
# [33/100] testing 6.6% loss=0.20427, acc=0.90625
# [33/100] testing 7.0% loss=0.16114, acc=0.89062
# [33/100] testing 7.9% loss=0.22753, acc=0.87500
# [33/100] testing 8.3% loss=0.28619, acc=0.89062
# [33/100] testing 9.2% loss=0.30790, acc=0.87500
# [33/100] testing 9.7% loss=0.11063, acc=0.96875
# [33/100] testing 10.5% loss=0.27132, acc=0.90625
# [33/100] testing 11.0% loss=0.22228, acc=0.89062
# [33/100] testing 11.8% loss=0.19113, acc=0.92188
# [33/100] testing 12.7% loss=0.35562, acc=0.90625
# [33/100] testing 13.2% loss=0.21595, acc=0.89062
# [33/100] testing 14.0% loss=0.32520, acc=0.92188
# [33/100] testing 14.5% loss=0.28417, acc=0.89062
# [33/100] testing 15.4% loss=0.32882, acc=0.89062
# [33/100] testing 15.8% loss=0.24564, acc=0.87500
# [33/100] testing 16.7% loss=0.29421, acc=0.84375
# [33/100] testing 17.5% loss=0.24341, acc=0.87500
# [33/100] testing 18.0% loss=0.26266, acc=0.84375
# [33/100] testing 18.9% loss=0.16927, acc=0.92188
# [33/100] testing 19.3% loss=0.29936, acc=0.89062
# [33/100] testing 20.2% loss=0.20714, acc=0.89062
# [33/100] testing 20.6% loss=0.34751, acc=0.89062
# [33/100] testing 21.5% loss=0.22453, acc=0.92188
# [33/100] testing 21.9% loss=0.34195, acc=0.87500
# [33/100] testing 22.8% loss=0.34373, acc=0.82812
# [33/100] testing 23.7% loss=0.29408, acc=0.89062
# [33/100] testing 24.1% loss=0.19937, acc=0.93750
# [33/100] testing 25.0% loss=0.33458, acc=0.87500
# [33/100] testing 25.4% loss=0.14383, acc=0.92188
# [33/100] testing 26.3% loss=0.35345, acc=0.85938
# [33/100] testing 26.8% loss=0.32759, acc=0.87500
# [33/100] testing 27.6% loss=0.21126, acc=0.92188
# [33/100] testing 28.5% loss=0.28243, acc=0.92188
# [33/100] testing 29.0% loss=0.18071, acc=0.95312
# [33/100] testing 29.8% loss=0.44523, acc=0.84375
# [33/100] testing 30.3% loss=0.27150, acc=0.89062
# [33/100] testing 31.1% loss=0.26375, acc=0.89062
# [33/100] testing 31.6% loss=0.21551, acc=0.92188
# [33/100] testing 32.5% loss=0.23834, acc=0.92188
# [33/100] testing 32.9% loss=0.52991, acc=0.81250
# [33/100] testing 33.8% loss=0.30407, acc=0.87500
# [33/100] testing 34.7% loss=0.42698, acc=0.84375
# [33/100] testing 35.1% loss=0.14834, acc=0.93750
# [33/100] testing 36.0% loss=0.28490, acc=0.87500
# [33/100] testing 36.4% loss=0.25693, acc=0.87500
# [33/100] testing 37.3% loss=0.22958, acc=0.96875
# [33/100] testing 37.7% loss=0.43982, acc=0.81250
# [33/100] testing 38.6% loss=0.19938, acc=0.90625
# [33/100] testing 39.5% loss=0.26929, acc=0.93750
# [33/100] testing 39.9% loss=0.28587, acc=0.87500
# [33/100] testing 40.8% loss=0.26384, acc=0.95312
# [33/100] testing 41.2% loss=0.29180, acc=0.92188
# [33/100] testing 42.1% loss=0.20102, acc=0.90625
# [33/100] testing 42.5% loss=0.16181, acc=0.90625
# [33/100] testing 43.4% loss=0.32280, acc=0.87500
# [33/100] testing 43.9% loss=0.14347, acc=0.93750
# [33/100] testing 44.7% loss=0.28843, acc=0.87500
# [33/100] testing 45.6% loss=0.20796, acc=0.90625
# [33/100] testing 46.1% loss=0.33277, acc=0.87500
# [33/100] testing 46.9% loss=0.23244, acc=0.92188
# [33/100] testing 47.4% loss=0.11852, acc=0.93750
# [33/100] testing 48.3% loss=0.38044, acc=0.84375
# [33/100] testing 48.7% loss=0.34744, acc=0.87500
# [33/100] testing 49.6% loss=0.46149, acc=0.81250
# [33/100] testing 50.4% loss=0.24840, acc=0.90625
# [33/100] testing 50.9% loss=0.30886, acc=0.87500
# [33/100] testing 51.8% loss=0.18291, acc=0.92188
# [33/100] testing 52.2% loss=0.25594, acc=0.87500
# [33/100] testing 53.1% loss=0.27061, acc=0.89062
# [33/100] testing 53.5% loss=0.21453, acc=0.90625
# [33/100] testing 54.4% loss=0.40409, acc=0.84375
# [33/100] testing 54.8% loss=0.30767, acc=0.89062
# [33/100] testing 55.7% loss=0.15615, acc=0.93750
# [33/100] testing 56.6% loss=0.34296, acc=0.85938
# [33/100] testing 57.0% loss=0.41058, acc=0.82812
# [33/100] testing 57.9% loss=0.25208, acc=0.89062
# [33/100] testing 58.3% loss=0.35493, acc=0.87500
# [33/100] testing 59.2% loss=0.27872, acc=0.85938
# [33/100] testing 59.7% loss=0.32052, acc=0.87500
# [33/100] testing 60.5% loss=0.45017, acc=0.82812
# [33/100] testing 61.4% loss=0.14093, acc=0.93750
# [33/100] testing 61.9% loss=0.21553, acc=0.90625
# [33/100] testing 62.7% loss=0.19268, acc=0.92188
# [33/100] testing 63.2% loss=0.42245, acc=0.81250
# [33/100] testing 64.0% loss=0.41835, acc=0.84375
# [33/100] testing 64.5% loss=0.18263, acc=0.93750
# [33/100] testing 65.4% loss=0.18679, acc=0.90625
# [33/100] testing 65.8% loss=0.33151, acc=0.85938
# [33/100] testing 66.7% loss=0.23623, acc=0.90625
# [33/100] testing 67.6% loss=0.35510, acc=0.89062
# [33/100] testing 68.0% loss=0.12100, acc=0.93750
# [33/100] testing 68.9% loss=0.28357, acc=0.92188
# [33/100] testing 69.3% loss=0.30010, acc=0.79688
# [33/100] testing 70.2% loss=0.31710, acc=0.87500
# [33/100] testing 70.6% loss=0.25625, acc=0.89062
# [33/100] testing 71.5% loss=0.31996, acc=0.87500
# [33/100] testing 72.4% loss=0.13190, acc=0.95312
# [33/100] testing 72.8% loss=0.17596, acc=0.93750
# [33/100] testing 73.7% loss=0.19908, acc=0.95312
# [33/100] testing 74.1% loss=0.37148, acc=0.92188
# [33/100] testing 75.0% loss=0.27330, acc=0.87500
# [33/100] testing 75.4% loss=0.43405, acc=0.84375
# [33/100] testing 76.3% loss=0.11966, acc=0.95312
# [33/100] testing 76.8% loss=0.28892, acc=0.89062
# [33/100] testing 77.6% loss=0.24202, acc=0.90625
# [33/100] testing 78.5% loss=0.41935, acc=0.84375
# [33/100] testing 79.0% loss=0.20064, acc=0.90625
# [33/100] testing 79.8% loss=0.32198, acc=0.82812
# [33/100] testing 80.3% loss=0.28276, acc=0.85938
# [33/100] testing 81.2% loss=0.38835, acc=0.87500
# [33/100] testing 81.6% loss=0.27955, acc=0.89062
# [33/100] testing 82.5% loss=0.15813, acc=0.93750
# [33/100] testing 83.3% loss=0.23809, acc=0.93750
# [33/100] testing 83.8% loss=0.16753, acc=0.93750
# [33/100] testing 84.7% loss=0.37019, acc=0.82812
# [33/100] testing 85.1% loss=0.25219, acc=0.92188
# [33/100] testing 86.0% loss=0.34445, acc=0.85938
# [33/100] testing 86.4% loss=0.37836, acc=0.84375
# [33/100] testing 87.3% loss=0.32778, acc=0.85938
# [33/100] testing 87.7% loss=0.21486, acc=0.89062
# [33/100] testing 88.6% loss=0.29996, acc=0.87500
# [33/100] testing 89.5% loss=0.57243, acc=0.79688
# [33/100] testing 89.9% loss=0.21809, acc=0.90625
# [33/100] testing 90.8% loss=0.29475, acc=0.90625
# [33/100] testing 91.2% loss=0.09326, acc=0.95312
# [33/100] testing 92.1% loss=0.38819, acc=0.87500
# [33/100] testing 92.6% loss=0.29883, acc=0.90625
# [33/100] testing 93.4% loss=0.31977, acc=0.85938
# [33/100] testing 94.3% loss=0.15978, acc=0.93750
# [33/100] testing 94.7% loss=0.22880, acc=0.89062
# [33/100] testing 95.6% loss=0.31595, acc=0.82812
# [33/100] testing 96.1% loss=0.18338, acc=0.90625
# [33/100] testing 96.9% loss=0.28083, acc=0.85938
# [33/100] testing 97.4% loss=0.23182, acc=0.92188
# [33/100] testing 98.3% loss=0.23891, acc=0.87500
# [33/100] testing 98.7% loss=0.32082, acc=0.89062
# [33/100] testing 99.6% loss=0.30637, acc=0.90625
# [34/100] training 0.2% loss=0.32616, acc=0.87500
# [34/100] training 0.4% loss=0.34566, acc=0.85938
# [34/100] training 0.5% loss=0.12711, acc=0.96875
# [34/100] training 0.8% loss=0.17017, acc=0.92188
# [34/100] training 0.9% loss=0.18523, acc=0.92188
# [34/100] training 1.1% loss=0.24131, acc=0.95312
# [34/100] training 1.2% loss=0.25090, acc=0.90625
# [34/100] training 1.4% loss=0.17514, acc=0.92188
# [34/100] training 1.6% loss=0.11056, acc=0.96875
# [34/100] training 1.8% loss=0.20540, acc=0.90625
# [34/100] training 2.0% loss=0.25351, acc=0.90625
# [34/100] training 2.1% loss=0.21366, acc=0.90625
# [34/100] training 2.3% loss=0.21137, acc=0.92188
# [34/100] training 2.4% loss=0.29909, acc=0.87500
# [34/100] training 2.6% loss=0.10210, acc=0.98438
# [34/100] training 2.7% loss=0.26919, acc=0.92188
# [34/100] training 3.0% loss=0.28897, acc=0.87500
# [34/100] training 3.2% loss=0.16214, acc=0.95312
# [34/100] training 3.3% loss=0.30851, acc=0.90625
# [34/100] training 3.5% loss=0.15353, acc=0.95312
# [34/100] training 3.6% loss=0.24744, acc=0.89062
# [34/100] training 3.8% loss=0.13845, acc=0.90625
# [34/100] training 3.9% loss=0.15704, acc=0.93750
# [34/100] training 4.2% loss=0.11355, acc=0.95312
# [34/100] training 4.4% loss=0.10315, acc=0.96875
# [34/100] training 4.5% loss=0.19729, acc=0.90625
# [34/100] training 4.7% loss=0.40691, acc=0.85938
# [34/100] training 4.8% loss=0.38363, acc=0.85938
# [34/100] training 5.0% loss=0.07626, acc=0.98438
# [34/100] training 5.2% loss=0.25123, acc=0.89062
# [34/100] training 5.4% loss=0.10728, acc=0.95312
# [34/100] training 5.5% loss=0.22338, acc=0.87500
# [34/100] training 5.7% loss=0.26091, acc=0.87500
# [34/100] training 5.9% loss=0.19608, acc=0.92188
# [34/100] training 6.0% loss=0.19164, acc=0.92188
# [34/100] training 6.3% loss=0.21164, acc=0.92188
# [34/100] training 6.4% loss=0.15223, acc=0.93750
# [34/100] training 6.6% loss=0.22247, acc=0.89062
# [34/100] training 6.7% loss=0.30610, acc=0.84375
# [34/100] training 6.9% loss=0.25637, acc=0.92188
# [34/100] training 7.1% loss=0.14027, acc=0.93750
# [34/100] training 7.2% loss=0.16687, acc=0.90625
# [34/100] training 7.5% loss=0.19135, acc=0.90625
# [34/100] training 7.6% loss=0.29778, acc=0.89062
# [34/100] training 7.8% loss=0.21535, acc=0.92188
# [34/100] training 7.9% loss=0.21720, acc=0.90625
# [34/100] training 8.1% loss=0.16234, acc=0.92188
# [34/100] training 8.2% loss=0.25788, acc=0.90625
# [34/100] training 8.4% loss=0.20423, acc=0.90625
# [34/100] training 8.7% loss=0.25425, acc=0.90625
# [34/100] training 8.8% loss=0.24211, acc=0.87500
# [34/100] training 9.0% loss=0.23299, acc=0.90625
# [34/100] training 9.1% loss=0.26484, acc=0.92188
# [34/100] training 9.3% loss=0.33794, acc=0.85938
# [34/100] training 9.4% loss=0.09987, acc=0.96875
# [34/100] training 9.7% loss=0.24174, acc=0.84375
# [34/100] training 9.9% loss=0.28479, acc=0.85938
# [34/100] training 10.0% loss=0.21462, acc=0.89062
# [34/100] training 10.2% loss=0.18872, acc=0.93750
# [34/100] training 10.3% loss=0.20136, acc=0.90625
# [34/100] training 10.5% loss=0.35161, acc=0.85938
# [34/100] training 10.6% loss=0.23468, acc=0.92188
# [34/100] training 10.9% loss=0.16949, acc=0.92188
# [34/100] training 11.0% loss=0.38223, acc=0.87500
# [34/100] training 11.2% loss=0.09732, acc=0.95312
# [34/100] training 11.4% loss=0.31157, acc=0.84375
# [34/100] training 11.5% loss=0.37312, acc=0.89062
# [34/100] training 11.7% loss=0.11916, acc=0.96875
# [34/100] training 11.8% loss=0.12454, acc=0.93750
# [34/100] training 12.1% loss=0.17505, acc=0.90625
# [34/100] training 12.2% loss=0.14810, acc=0.93750
# [34/100] training 12.4% loss=0.27941, acc=0.87500
# [34/100] training 12.6% loss=0.21804, acc=0.90625
# [34/100] training 12.7% loss=0.26237, acc=0.92188
# [34/100] training 12.9% loss=0.33191, acc=0.90625
# [34/100] training 13.0% loss=0.12983, acc=0.96875
# [34/100] training 13.3% loss=0.12405, acc=0.96875
# [34/100] training 13.4% loss=0.20625, acc=0.90625
# [34/100] training 13.6% loss=0.19780, acc=0.93750
# [34/100] training 13.7% loss=0.33932, acc=0.82812
# [34/100] training 13.9% loss=0.15473, acc=0.95312
# [34/100] training 14.1% loss=0.21942, acc=0.90625
# [34/100] training 14.3% loss=0.16235, acc=0.92188
# [34/100] training 14.5% loss=0.18685, acc=0.96875
# [34/100] training 14.6% loss=0.17848, acc=0.93750
# [34/100] training 14.8% loss=0.35068, acc=0.87500
# [34/100] training 14.9% loss=0.16282, acc=0.92188
# [34/100] training 15.1% loss=0.33169, acc=0.82812
# [34/100] training 15.4% loss=0.19687, acc=0.93750
# [34/100] training 15.5% loss=0.24422, acc=0.93750
# [34/100] training 15.7% loss=0.28871, acc=0.89062
# [34/100] training 15.8% loss=0.19547, acc=0.89062
# [34/100] training 16.0% loss=0.18077, acc=0.92188
# [34/100] training 16.1% loss=0.43950, acc=0.84375
# [34/100] training 16.3% loss=0.23926, acc=0.89062
# [34/100] training 16.4% loss=0.16205, acc=0.92188
# [34/100] training 16.7% loss=0.21757, acc=0.89062
# [34/100] training 16.9% loss=0.28215, acc=0.85938
# [34/100] training 17.0% loss=0.26208, acc=0.87500
# [34/100] training 17.2% loss=0.19831, acc=0.92188
# [34/100] training 17.3% loss=0.14471, acc=0.92188
# [34/100] training 17.5% loss=0.18987, acc=0.95312
# [34/100] training 17.7% loss=0.17555, acc=0.92188
# [34/100] training 17.9% loss=0.30039, acc=0.90625
# [34/100] training 18.1% loss=0.31943, acc=0.84375
# [34/100] training 18.2% loss=0.32580, acc=0.85938
# [34/100] training 18.4% loss=0.31654, acc=0.90625
# [34/100] training 18.5% loss=0.20331, acc=0.92188
# [34/100] training 18.8% loss=0.15724, acc=0.92188
# [34/100] training 18.9% loss=0.09874, acc=0.98438
# [34/100] training 19.1% loss=0.27866, acc=0.90625
# [34/100] training 19.2% loss=0.12223, acc=0.96875
# [34/100] training 19.4% loss=0.15803, acc=0.95312
# [34/100] training 19.6% loss=0.35198, acc=0.85938
# [34/100] training 19.7% loss=0.28113, acc=0.87500
# [34/100] training 20.0% loss=0.15446, acc=0.95312
# [34/100] training 20.1% loss=0.19578, acc=0.95312
# [34/100] training 20.3% loss=0.17742, acc=0.92188
# [34/100] training 20.4% loss=0.19401, acc=0.87500
# [34/100] training 20.6% loss=0.26913, acc=0.87500
# [34/100] training 20.8% loss=0.20372, acc=0.90625
# [34/100] training 20.9% loss=0.12913, acc=0.93750
# [34/100] training 21.2% loss=0.21309, acc=0.95312
# [34/100] training 21.3% loss=0.23925, acc=0.90625
# [34/100] training 21.5% loss=0.31319, acc=0.89062
# [34/100] training 21.6% loss=0.14724, acc=0.95312
# [34/100] training 21.8% loss=0.19208, acc=0.92188
# [34/100] training 21.9% loss=0.19133, acc=0.92188
# [34/100] training 22.2% loss=0.29039, acc=0.85938
# [34/100] training 22.4% loss=0.19613, acc=0.93750
# [34/100] training 22.5% loss=0.21487, acc=0.93750
# [34/100] training 22.7% loss=0.21268, acc=0.93750
# [34/100] training 22.8% loss=0.21479, acc=0.92188
# [34/100] training 23.0% loss=0.14200, acc=0.92188
# [34/100] training 23.1% loss=0.28307, acc=0.89062
# [34/100] training 23.4% loss=0.22846, acc=0.87500
# [34/100] training 23.6% loss=0.32203, acc=0.89062
# [34/100] training 23.7% loss=0.18517, acc=0.92188
# [34/100] training 23.9% loss=0.16321, acc=0.93750
# [34/100] training 24.0% loss=0.16601, acc=0.92188
# [34/100] training 24.2% loss=0.22688, acc=0.89062
# [34/100] training 24.3% loss=0.25591, acc=0.87500
# [34/100] training 24.6% loss=0.22725, acc=0.90625
# [34/100] training 24.7% loss=0.34829, acc=0.89062
# [34/100] training 24.9% loss=0.24766, acc=0.90625
# [34/100] training 25.1% loss=0.25064, acc=0.87500
# [34/100] training 25.2% loss=0.12385, acc=0.95312
# [34/100] training 25.4% loss=0.28003, acc=0.90625
# [34/100] training 25.6% loss=0.21269, acc=0.95312
# [34/100] training 25.8% loss=0.20960, acc=0.92188
# [34/100] training 25.9% loss=0.15626, acc=0.93750
# [34/100] training 26.1% loss=0.21191, acc=0.95312
# [34/100] training 26.3% loss=0.13179, acc=0.95312
# [34/100] training 26.4% loss=0.16723, acc=0.93750
# [34/100] training 26.6% loss=0.08220, acc=0.96875
# [34/100] training 26.8% loss=0.19298, acc=0.93750
# [34/100] training 27.0% loss=0.12960, acc=0.96875
# [34/100] training 27.1% loss=0.15171, acc=0.93750
# [34/100] training 27.3% loss=0.19709, acc=0.89062
# [34/100] training 27.4% loss=0.17582, acc=0.93750
# [34/100] training 27.6% loss=0.34771, acc=0.92188
# [34/100] training 27.9% loss=0.15193, acc=0.95312
# [34/100] training 28.0% loss=0.24360, acc=0.90625
# [34/100] training 28.2% loss=0.20420, acc=0.93750
# [34/100] training 28.3% loss=0.07627, acc=0.98438
# [34/100] training 28.5% loss=0.23215, acc=0.89062
# [34/100] training 28.6% loss=0.19411, acc=0.95312
# [34/100] training 28.8% loss=0.12896, acc=0.96875
# [34/100] training 29.1% loss=0.26722, acc=0.87500
# [34/100] training 29.2% loss=0.13864, acc=0.95312
# [34/100] training 29.4% loss=0.24152, acc=0.90625
# [34/100] training 29.5% loss=0.15162, acc=0.95312
# [34/100] training 29.7% loss=0.25593, acc=0.87500
# [34/100] training 29.8% loss=0.13955, acc=0.93750
# [34/100] training 30.0% loss=0.19725, acc=0.92188
# [34/100] training 30.2% loss=0.16050, acc=0.95312
# [34/100] training 30.4% loss=0.12815, acc=0.90625
# [34/100] training 30.6% loss=0.29938, acc=0.89062
# [34/100] training 30.7% loss=0.20598, acc=0.93750
# [34/100] training 30.9% loss=0.32761, acc=0.89062
# [34/100] training 31.0% loss=0.10777, acc=0.96875
# [34/100] training 31.3% loss=0.16219, acc=0.95312
# [34/100] training 31.4% loss=0.35965, acc=0.79688
# [34/100] training 31.6% loss=0.25769, acc=0.92188
# [34/100] training 31.8% loss=0.27385, acc=0.85938
# [34/100] training 31.9% loss=0.28571, acc=0.93750
# [34/100] training 32.1% loss=0.17264, acc=0.92188
# [34/100] training 32.2% loss=0.29236, acc=0.89062
# [34/100] training 32.5% loss=0.12496, acc=0.95312
# [34/100] training 32.6% loss=0.21682, acc=0.90625
# [34/100] training 32.8% loss=0.15249, acc=0.96875
# [34/100] training 32.9% loss=0.28623, acc=0.89062
# [34/100] training 33.1% loss=0.20078, acc=0.90625
# [34/100] training 33.3% loss=0.25851, acc=0.89062
# [34/100] training 33.4% loss=0.12468, acc=0.93750
# [34/100] training 33.7% loss=0.14620, acc=0.95312
# [34/100] training 33.8% loss=0.18572, acc=0.90625
# [34/100] training 34.0% loss=0.24436, acc=0.89062
# [34/100] training 34.1% loss=0.21244, acc=0.92188
# [34/100] training 34.3% loss=0.17674, acc=0.93750
# [34/100] training 34.5% loss=0.19320, acc=0.89062
# [34/100] training 34.7% loss=0.08374, acc=0.95312
# [34/100] training 34.9% loss=0.16141, acc=0.95312
# [34/100] training 35.0% loss=0.13582, acc=0.96875
# [34/100] training 35.2% loss=0.28470, acc=0.84375
# [34/100] training 35.3% loss=0.19409, acc=0.93750
# [34/100] training 35.5% loss=0.23212, acc=0.89062
# [34/100] training 35.6% loss=0.40496, acc=0.89062
# [34/100] training 35.9% loss=0.18833, acc=0.92188
# [34/100] training 36.1% loss=0.32059, acc=0.87500
# [34/100] training 36.2% loss=0.26127, acc=0.92188
# [34/100] training 36.4% loss=0.20495, acc=0.93750
# [34/100] training 36.5% loss=0.27449, acc=0.89062
# [34/100] training 36.7% loss=0.17536, acc=0.92188
# [34/100] training 36.8% loss=0.12409, acc=0.95312
# [34/100] training 37.1% loss=0.26460, acc=0.89062
# [34/100] training 37.3% loss=0.26617, acc=0.93750
# [34/100] training 37.4% loss=0.23891, acc=0.90625
# [34/100] training 37.6% loss=0.19943, acc=0.89062
# [34/100] training 37.7% loss=0.23015, acc=0.90625
# [34/100] training 37.9% loss=0.14769, acc=0.93750
# [34/100] training 38.1% loss=0.28651, acc=0.89062
# [34/100] training 38.3% loss=0.22182, acc=0.89062
# [34/100] training 38.4% loss=0.06724, acc=0.98438
# [34/100] training 38.6% loss=0.18843, acc=0.93750
# [34/100] training 38.8% loss=0.29478, acc=0.85938
# [34/100] training 38.9% loss=0.25518, acc=0.87500
# [34/100] training 39.1% loss=0.22566, acc=0.90625
# [34/100] training 39.3% loss=0.21516, acc=0.92188
# [34/100] training 39.5% loss=0.35385, acc=0.87500
# [34/100] training 39.6% loss=0.22508, acc=0.95312
# [34/100] training 39.8% loss=0.09199, acc=0.98438
# [34/100] training 40.0% loss=0.23370, acc=0.87500
# [34/100] training 40.1% loss=0.26863, acc=0.87500
# [34/100] training 40.4% loss=0.17607, acc=0.92188
# [34/100] training 40.5% loss=0.31082, acc=0.89062
# [34/100] training 40.7% loss=0.20424, acc=0.89062
# [34/100] training 40.8% loss=0.14203, acc=0.93750
# [34/100] training 41.0% loss=0.12019, acc=0.95312
# [34/100] training 41.1% loss=0.19018, acc=0.90625
# [34/100] training 41.3% loss=0.24044, acc=0.89062
# [34/100] training 41.6% loss=0.23489, acc=0.90625
# [34/100] training 41.7% loss=0.29027, acc=0.85938
# [34/100] training 41.9% loss=0.18292, acc=0.93750
# [34/100] training 42.0% loss=0.19037, acc=0.90625
# [34/100] training 42.2% loss=0.27724, acc=0.89062
# [34/100] training 42.3% loss=0.14393, acc=0.95312
# [34/100] training 42.5% loss=0.12078, acc=0.96875
# [34/100] training 42.8% loss=0.13147, acc=0.96875
# [34/100] training 42.9% loss=0.17355, acc=0.93750
# [34/100] training 43.1% loss=0.19013, acc=0.93750
# [34/100] training 43.2% loss=0.12933, acc=0.92188
# [34/100] training 43.4% loss=0.24446, acc=0.90625
# [34/100] training 43.5% loss=0.12907, acc=0.95312
# [34/100] training 43.8% loss=0.22005, acc=0.93750
# [34/100] training 43.9% loss=0.16033, acc=0.95312
# [34/100] training 44.1% loss=0.19133, acc=0.95312
# [34/100] training 44.3% loss=0.15873, acc=0.93750
# [34/100] training 44.4% loss=0.34151, acc=0.87500
# [34/100] training 44.6% loss=0.32327, acc=0.87500
# [34/100] training 44.7% loss=0.28728, acc=0.89062
# [34/100] training 45.0% loss=0.24690, acc=0.89062
# [34/100] training 45.1% loss=0.33864, acc=0.92188
# [34/100] training 45.3% loss=0.23669, acc=0.85938
# [34/100] training 45.5% loss=0.12371, acc=0.95312
# [34/100] training 45.6% loss=0.25341, acc=0.89062
# [34/100] training 45.8% loss=0.12641, acc=0.95312
# [34/100] training 45.9% loss=0.13747, acc=0.93750
# [34/100] training 46.2% loss=0.15118, acc=0.92188
# [34/100] training 46.3% loss=0.13733, acc=0.90625
# [34/100] training 46.5% loss=0.31135, acc=0.87500
# [34/100] training 46.6% loss=0.22486, acc=0.90625
# [34/100] training 46.8% loss=0.14832, acc=0.93750
# [34/100] training 47.0% loss=0.27272, acc=0.90625
# [34/100] training 47.2% loss=0.24515, acc=0.92188
# [34/100] training 47.4% loss=0.14685, acc=0.93750
# [34/100] training 47.5% loss=0.34141, acc=0.87500
# [34/100] training 47.7% loss=0.22903, acc=0.90625
# [34/100] training 47.8% loss=0.27588, acc=0.89062
# [34/100] training 48.0% loss=0.22193, acc=0.92188
# [34/100] training 48.3% loss=0.09278, acc=0.96875
# [34/100] training 48.4% loss=0.10038, acc=0.95312
# [34/100] training 48.6% loss=0.12614, acc=0.95312
# [34/100] training 48.7% loss=0.23124, acc=0.87500
# [34/100] training 48.9% loss=0.19184, acc=0.90625
# [34/100] training 49.0% loss=0.15135, acc=0.95312
# [34/100] training 49.2% loss=0.25480, acc=0.84375
# [34/100] training 49.3% loss=0.17910, acc=0.93750
# [34/100] training 49.6% loss=0.18581, acc=0.93750
# [34/100] training 49.8% loss=0.20206, acc=0.92188
# [34/100] training 49.9% loss=0.19798, acc=0.92188
# [34/100] training 50.1% loss=0.23507, acc=0.92188
# [34/100] training 50.2% loss=0.21820, acc=0.90625
# [34/100] training 50.4% loss=0.36189, acc=0.87500
# [34/100] training 50.6% loss=0.28191, acc=0.92188
# [34/100] training 50.8% loss=0.23282, acc=0.90625
# [34/100] training 51.0% loss=0.29688, acc=0.89062
# [34/100] training 51.1% loss=0.26845, acc=0.90625
# [34/100] training 51.3% loss=0.26767, acc=0.87500
# [34/100] training 51.4% loss=0.20655, acc=0.93750
# [34/100] training 51.7% loss=0.21481, acc=0.90625
# [34/100] training 51.8% loss=0.26452, acc=0.87500
# [34/100] training 52.0% loss=0.15660, acc=0.93750
# [34/100] training 52.1% loss=0.21065, acc=0.90625
# [34/100] training 52.3% loss=0.25668, acc=0.90625
# [34/100] training 52.5% loss=0.08795, acc=0.98438
# [34/100] training 52.6% loss=0.18247, acc=0.90625
# [34/100] training 52.9% loss=0.35495, acc=0.85938
# [34/100] training 53.0% loss=0.13443, acc=0.93750
# [34/100] training 53.2% loss=0.29867, acc=0.90625
# [34/100] training 53.3% loss=0.12265, acc=0.95312
# [34/100] training 53.5% loss=0.18124, acc=0.92188
# [34/100] training 53.7% loss=0.10337, acc=0.95312
# [34/100] training 53.8% loss=0.32403, acc=0.84375
# [34/100] training 54.1% loss=0.29965, acc=0.87500
# [34/100] training 54.2% loss=0.13431, acc=0.95312
# [34/100] training 54.4% loss=0.26207, acc=0.85938
# [34/100] training 54.5% loss=0.29092, acc=0.90625
# [34/100] training 54.7% loss=0.31571, acc=0.82812
# [34/100] training 54.8% loss=0.12082, acc=0.95312
# [34/100] training 55.1% loss=0.17942, acc=0.92188
# [34/100] training 55.3% loss=0.21867, acc=0.90625
# [34/100] training 55.4% loss=0.18590, acc=0.92188
# [34/100] training 55.6% loss=0.18050, acc=0.92188
# [34/100] training 55.7% loss=0.13214, acc=0.95312
# [34/100] training 55.9% loss=0.14053, acc=0.93750
# [34/100] training 56.0% loss=0.08964, acc=0.95312
# [34/100] training 56.3% loss=0.36387, acc=0.84375
# [34/100] training 56.5% loss=0.22054, acc=0.93750
# [34/100] training 56.6% loss=0.17698, acc=0.95312
# [34/100] training 56.8% loss=0.22838, acc=0.89062
# [34/100] training 56.9% loss=0.16613, acc=0.92188
# [34/100] training 57.1% loss=0.19289, acc=0.90625
# [34/100] training 57.2% loss=0.15063, acc=0.92188
# [34/100] training 57.5% loss=0.19309, acc=0.90625
# [34/100] training 57.6% loss=0.24344, acc=0.89062
# [34/100] training 57.8% loss=0.11757, acc=0.95312
# [34/100] training 58.0% loss=0.16892, acc=0.90625
# [34/100] training 58.1% loss=0.18384, acc=0.92188
# [34/100] training 58.3% loss=0.15337, acc=0.90625
# [34/100] training 58.4% loss=0.18504, acc=0.96875
# [34/100] training 58.7% loss=0.31199, acc=0.90625
# [34/100] training 58.8% loss=0.27727, acc=0.89062
# [34/100] training 59.0% loss=0.20177, acc=0.87500
# [34/100] training 59.2% loss=0.16115, acc=0.89062
# [34/100] training 59.3% loss=0.17447, acc=0.93750
# [34/100] training 59.5% loss=0.18477, acc=0.92188
# [34/100] training 59.7% loss=0.22204, acc=0.90625
# [34/100] training 59.9% loss=0.20017, acc=0.95312
# [34/100] training 60.0% loss=0.19186, acc=0.92188
# [34/100] training 60.2% loss=0.29386, acc=0.85938
# [34/100] training 60.3% loss=0.19835, acc=0.92188
# [34/100] training 60.5% loss=0.23479, acc=0.90625
# [34/100] training 60.8% loss=0.19984, acc=0.90625
# [34/100] training 60.9% loss=0.30527, acc=0.89062
# [34/100] training 61.1% loss=0.30507, acc=0.92188
# [34/100] training 61.2% loss=0.17578, acc=0.93750
# [34/100] training 61.4% loss=0.27092, acc=0.85938
# [34/100] training 61.5% loss=0.32171, acc=0.82812
# [34/100] training 61.7% loss=0.22328, acc=0.92188
# [34/100] training 62.0% loss=0.28088, acc=0.87500
# [34/100] training 62.1% loss=0.22987, acc=0.90625
# [34/100] training 62.3% loss=0.13876, acc=0.95312
# [34/100] training 62.4% loss=0.21062, acc=0.85938
# [34/100] training 62.6% loss=0.24607, acc=0.90625
# [34/100] training 62.7% loss=0.12999, acc=0.96875
# [34/100] training 62.9% loss=0.20383, acc=0.95312
# [34/100] training 63.1% loss=0.18558, acc=0.92188
# [34/100] training 63.3% loss=0.17709, acc=0.90625
# [34/100] training 63.5% loss=0.22186, acc=0.92188
# [34/100] training 63.6% loss=0.31606, acc=0.93750
# [34/100] training 63.8% loss=0.35410, acc=0.85938
# [34/100] training 63.9% loss=0.16149, acc=0.95312
# [34/100] training 64.2% loss=0.17998, acc=0.92188
# [34/100] training 64.3% loss=0.24007, acc=0.90625
# [34/100] training 64.5% loss=0.26240, acc=0.89062
# [34/100] training 64.7% loss=0.20764, acc=0.87500
# [34/100] training 64.8% loss=0.33028, acc=0.85938
# [34/100] training 65.0% loss=0.25972, acc=0.89062
# [34/100] training 65.1% loss=0.31766, acc=0.87500
# [34/100] training 65.4% loss=0.13648, acc=0.96875
# [34/100] training 65.5% loss=0.14669, acc=0.95312
# [34/100] training 65.7% loss=0.22008, acc=0.89062
# [34/100] training 65.8% loss=0.22638, acc=0.89062
# [34/100] training 66.0% loss=0.15601, acc=0.93750
# [34/100] training 66.2% loss=0.17775, acc=0.93750
# [34/100] training 66.3% loss=0.26385, acc=0.89062
# [34/100] training 66.6% loss=0.20923, acc=0.93750
# [34/100] training 66.7% loss=0.15467, acc=0.93750
# [34/100] training 66.9% loss=0.23308, acc=0.87500
# [34/100] training 67.0% loss=0.26083, acc=0.87500
# [34/100] training 67.2% loss=0.12333, acc=0.96875
# [34/100] training 67.4% loss=0.19924, acc=0.93750
# [34/100] training 67.6% loss=0.21841, acc=0.90625
# [34/100] training 67.8% loss=0.14190, acc=0.95312
# [34/100] training 67.9% loss=0.15262, acc=0.93750
# [34/100] training 68.1% loss=0.23363, acc=0.90625
# [34/100] training 68.2% loss=0.12184, acc=0.95312
# [34/100] training 68.4% loss=0.13955, acc=0.96875
# [34/100] training 68.5% loss=0.17332, acc=0.92188
# [34/100] training 68.8% loss=0.23606, acc=0.87500
# [34/100] training 69.0% loss=0.22150, acc=0.93750
# [34/100] training 69.1% loss=0.13532, acc=0.93750
# [34/100] training 69.3% loss=0.25575, acc=0.90625
# [34/100] training 69.4% loss=0.29569, acc=0.89062
# [34/100] training 69.6% loss=0.33462, acc=0.87500
# [34/100] training 69.7% loss=0.14393, acc=0.93750
# [34/100] training 70.0% loss=0.28349, acc=0.84375
# [34/100] training 70.2% loss=0.24706, acc=0.92188
# [34/100] training 70.3% loss=0.21549, acc=0.89062
# [34/100] training 70.5% loss=0.25531, acc=0.87500
# [34/100] training 70.6% loss=0.20650, acc=0.90625
# [34/100] training 70.8% loss=0.26773, acc=0.84375
# [34/100] training 71.0% loss=0.30565, acc=0.90625
# [34/100] training 71.2% loss=0.17389, acc=0.92188
# [34/100] training 71.3% loss=0.10811, acc=0.96875
# [34/100] training 71.5% loss=0.25801, acc=0.90625
# [34/100] training 71.7% loss=0.22571, acc=0.90625
# [34/100] training 71.8% loss=0.21624, acc=0.90625
# [34/100] training 72.0% loss=0.08700, acc=0.95312
# [34/100] training 72.2% loss=0.17654, acc=0.93750
# [34/100] training 72.4% loss=0.20601, acc=0.90625
# [34/100] training 72.5% loss=0.21101, acc=0.90625
# [34/100] training 72.7% loss=0.22385, acc=0.89062
# [34/100] training 72.9% loss=0.12286, acc=0.95312
# [34/100] training 73.0% loss=0.14740, acc=0.93750
# [34/100] training 73.3% loss=0.28991, acc=0.90625
# [34/100] training 73.4% loss=0.16149, acc=0.93750
# [34/100] training 73.6% loss=0.13570, acc=0.93750
# [34/100] training 73.7% loss=0.18270, acc=0.90625
# [34/100] training 73.9% loss=0.13021, acc=0.95312
# [34/100] training 74.0% loss=0.20825, acc=0.89062
# [34/100] training 74.2% loss=0.14151, acc=0.93750
# [34/100] training 74.5% loss=0.18566, acc=0.92188
# [34/100] training 74.6% loss=0.45265, acc=0.84375
# [34/100] training 74.8% loss=0.44585, acc=0.89062
# [34/100] training 74.9% loss=0.26045, acc=0.82812
# [34/100] training 75.1% loss=0.13724, acc=0.95312
# [34/100] training 75.2% loss=0.14639, acc=0.96875
# [34/100] training 75.4% loss=0.16939, acc=0.93750
# [34/100] training 75.7% loss=0.20308, acc=0.92188
# [34/100] training 75.8% loss=0.21618, acc=0.87500
# [34/100] training 76.0% loss=0.15750, acc=0.93750
# [34/100] training 76.1% loss=0.21159, acc=0.90625
# [34/100] training 76.3% loss=0.13894, acc=0.93750
# [34/100] training 76.4% loss=0.25579, acc=0.90625
# [34/100] training 76.7% loss=0.22095, acc=0.92188
# [34/100] training 76.8% loss=0.11964, acc=0.95312
# [34/100] training 77.0% loss=0.10628, acc=0.95312
# [34/100] training 77.2% loss=0.28830, acc=0.90625
# [34/100] training 77.3% loss=0.10766, acc=0.95312
# [34/100] training 77.5% loss=0.27563, acc=0.92188
# [34/100] training 77.6% loss=0.28142, acc=0.89062
# [34/100] training 77.9% loss=0.18536, acc=0.92188
# [34/100] training 78.0% loss=0.20615, acc=0.90625
# [34/100] training 78.2% loss=0.22268, acc=0.89062
# [34/100] training 78.4% loss=0.12448, acc=0.95312
# [34/100] training 78.5% loss=0.26267, acc=0.92188
# [34/100] training 78.7% loss=0.22381, acc=0.90625
# [34/100] training 78.8% loss=0.14780, acc=0.95312
# [34/100] training 79.1% loss=0.09394, acc=0.96875
# [34/100] training 79.2% loss=0.13385, acc=0.95312
# [34/100] training 79.4% loss=0.18374, acc=0.95312
# [34/100] training 79.5% loss=0.20446, acc=0.93750
# [34/100] training 79.7% loss=0.10528, acc=0.93750
# [34/100] training 79.9% loss=0.20984, acc=0.89062
# [34/100] training 80.1% loss=0.12168, acc=0.93750
# [34/100] training 80.3% loss=0.35910, acc=0.84375
# [34/100] training 80.4% loss=0.29868, acc=0.87500
# [34/100] training 80.6% loss=0.22655, acc=0.84375
# [34/100] training 80.7% loss=0.27564, acc=0.85938
# [34/100] training 80.9% loss=0.22380, acc=0.89062
# [34/100] training 81.2% loss=0.28477, acc=0.87500
# [34/100] training 81.3% loss=0.20646, acc=0.93750
# [34/100] training 81.5% loss=0.16504, acc=0.92188
# [34/100] training 81.6% loss=0.36253, acc=0.84375
# [34/100] training 81.8% loss=0.24834, acc=0.90625
# [34/100] training 81.9% loss=0.27619, acc=0.87500
# [34/100] training 82.1% loss=0.16215, acc=0.92188
# [34/100] training 82.2% loss=0.17739, acc=0.92188
# [34/100] training 82.5% loss=0.20602, acc=0.89062
# [34/100] training 82.7% loss=0.19414, acc=0.90625
# [34/100] training 82.8% loss=0.12850, acc=0.92188
# [34/100] training 83.0% loss=0.11440, acc=0.98438
# [34/100] training 83.1% loss=0.20620, acc=0.93750
# [34/100] training 83.3% loss=0.11655, acc=0.95312
# [34/100] training 83.5% loss=0.13993, acc=0.95312
# [34/100] training 83.7% loss=0.30100, acc=0.92188
# [34/100] training 83.9% loss=0.21755, acc=0.89062
# [34/100] training 84.0% loss=0.12918, acc=0.92188
# [34/100] training 84.2% loss=0.14006, acc=0.93750
# [34/100] training 84.3% loss=0.21149, acc=0.90625
# [34/100] training 84.5% loss=0.20158, acc=0.95312
# [34/100] training 84.7% loss=0.16879, acc=0.93750
# [34/100] training 84.9% loss=0.18814, acc=0.93750
# [34/100] training 85.0% loss=0.18651, acc=0.92188
# [34/100] training 85.2% loss=0.22818, acc=0.90625
# [34/100] training 85.4% loss=0.24090, acc=0.92188
# [34/100] training 85.5% loss=0.20358, acc=0.89062
# [34/100] training 85.8% loss=0.15600, acc=0.93750
# [34/100] training 85.9% loss=0.20054, acc=0.90625
# [34/100] training 86.1% loss=0.27001, acc=0.87500
# [34/100] training 86.2% loss=0.15971, acc=0.95312
# [34/100] training 86.4% loss=0.30097, acc=0.89062
# [34/100] training 86.6% loss=0.31003, acc=0.84375
# [34/100] training 86.7% loss=0.22065, acc=0.92188
# [34/100] training 87.0% loss=0.30434, acc=0.87500
# [34/100] training 87.1% loss=0.23549, acc=0.85938
# [34/100] training 87.3% loss=0.17038, acc=0.95312
# [34/100] training 87.4% loss=0.26652, acc=0.87500
# [34/100] training 87.6% loss=0.18532, acc=0.90625
# [34/100] training 87.7% loss=0.31563, acc=0.87500
# [34/100] training 87.9% loss=0.26553, acc=0.90625
# [34/100] training 88.2% loss=0.13684, acc=0.95312
# [34/100] training 88.3% loss=0.19962, acc=0.93750
# [34/100] training 88.5% loss=0.23568, acc=0.87500
# [34/100] training 88.6% loss=0.09547, acc=0.96875
# [34/100] training 88.8% loss=0.23665, acc=0.92188
# [34/100] training 88.9% loss=0.33490, acc=0.89062
# [34/100] training 89.2% loss=0.21406, acc=0.95312
# [34/100] training 89.4% loss=0.15400, acc=0.93750
# [34/100] training 89.5% loss=0.22118, acc=0.92188
# [34/100] training 89.7% loss=0.20297, acc=0.92188
# [34/100] training 89.8% loss=0.22494, acc=0.92188
# [34/100] training 90.0% loss=0.13668, acc=0.93750
# [34/100] training 90.1% loss=0.23279, acc=0.89062
# [34/100] training 90.4% loss=0.21216, acc=0.92188
# [34/100] training 90.5% loss=0.17487, acc=0.90625
# [34/100] training 90.7% loss=0.14336, acc=0.95312
# [34/100] training 90.9% loss=0.17293, acc=0.93750
# [34/100] training 91.0% loss=0.21755, acc=0.92188
# [34/100] training 91.2% loss=0.18530, acc=0.92188
# [34/100] training 91.3% loss=0.24359, acc=0.92188
# [34/100] training 91.6% loss=0.36278, acc=0.85938
# [34/100] training 91.7% loss=0.17261, acc=0.95312
# [34/100] training 91.9% loss=0.25120, acc=0.92188
# [34/100] training 92.1% loss=0.18940, acc=0.92188
# [34/100] training 92.2% loss=0.12545, acc=0.98438
# [34/100] training 92.4% loss=0.10946, acc=0.96875
# [34/100] training 92.6% loss=0.21006, acc=0.92188
# [34/100] training 92.8% loss=0.23569, acc=0.87500
# [34/100] training 92.9% loss=0.20408, acc=0.92188
# [34/100] training 93.1% loss=0.42501, acc=0.84375
# [34/100] training 93.2% loss=0.30279, acc=0.90625
# [34/100] training 93.4% loss=0.30376, acc=0.90625
# [34/100] training 93.7% loss=0.21675, acc=0.87500
# [34/100] training 93.8% loss=0.16311, acc=0.93750
# [34/100] training 94.0% loss=0.17151, acc=0.95312
# [34/100] training 94.1% loss=0.21356, acc=0.90625
# [34/100] training 94.3% loss=0.18163, acc=0.90625
# [34/100] training 94.4% loss=0.28729, acc=0.92188
# [34/100] training 94.6% loss=0.06460, acc=0.98438
# [34/100] training 94.9% loss=0.18221, acc=0.90625
# [34/100] training 95.0% loss=0.17754, acc=0.95312
# [34/100] training 95.2% loss=0.49509, acc=0.84375
# [34/100] training 95.3% loss=0.29418, acc=0.89062
# [34/100] training 95.5% loss=0.15765, acc=0.96875
# [34/100] training 95.6% loss=0.42033, acc=0.85938
# [34/100] training 95.8% loss=0.30012, acc=0.84375
# [34/100] training 96.0% loss=0.25166, acc=0.89062
# [34/100] training 96.2% loss=0.19573, acc=0.96875
# [34/100] training 96.4% loss=0.17791, acc=0.98438
# [34/100] training 96.5% loss=0.22725, acc=0.89062
# [34/100] training 96.7% loss=0.14528, acc=0.95312
# [34/100] training 96.8% loss=0.19527, acc=0.92188
# [34/100] training 97.1% loss=0.16714, acc=0.92188
# [34/100] training 97.2% loss=0.16400, acc=0.95312
# [34/100] training 97.4% loss=0.23159, acc=0.93750
# [34/100] training 97.6% loss=0.22390, acc=0.89062
# [34/100] training 97.7% loss=0.15757, acc=0.93750
# [34/100] training 97.9% loss=0.18222, acc=0.90625
# [34/100] training 98.0% loss=0.15331, acc=0.96875
# [34/100] training 98.3% loss=0.20412, acc=0.89062
# [34/100] training 98.4% loss=0.22964, acc=0.90625
# [34/100] training 98.6% loss=0.36448, acc=0.85938
# [34/100] training 98.7% loss=0.31319, acc=0.87500
# [34/100] training 98.9% loss=0.10981, acc=0.96875
# [34/100] training 99.1% loss=0.15906, acc=0.92188
# [34/100] training 99.2% loss=0.12800, acc=0.93750
# [34/100] training 99.5% loss=0.27387, acc=0.89062
# [34/100] training 99.6% loss=0.18665, acc=0.92188
# [34/100] training 99.8% loss=0.14767, acc=0.93750
# [34/100] training 99.9% loss=0.06874, acc=0.96875
# [34/100] testing 0.9% loss=0.15181, acc=0.93750
# [34/100] testing 1.8% loss=0.25551, acc=0.89062
# [34/100] testing 2.2% loss=0.29108, acc=0.87500
# [34/100] testing 3.1% loss=0.33889, acc=0.89062
# [34/100] testing 3.5% loss=0.23186, acc=0.92188
# [34/100] testing 4.4% loss=0.21809, acc=0.85938
# [34/100] testing 4.8% loss=0.42397, acc=0.82812
# [34/100] testing 5.7% loss=0.20708, acc=0.89062
# [34/100] testing 6.6% loss=0.22111, acc=0.89062
# [34/100] testing 7.0% loss=0.12122, acc=0.98438
# [34/100] testing 7.9% loss=0.24717, acc=0.90625
# [34/100] testing 8.3% loss=0.29092, acc=0.87500
# [34/100] testing 9.2% loss=0.25906, acc=0.90625
# [34/100] testing 9.7% loss=0.13643, acc=0.93750
# [34/100] testing 10.5% loss=0.25817, acc=0.87500
# [34/100] testing 11.0% loss=0.33359, acc=0.87500
# [34/100] testing 11.8% loss=0.17320, acc=0.92188
# [34/100] testing 12.7% loss=0.52964, acc=0.87500
# [34/100] testing 13.2% loss=0.20691, acc=0.89062
# [34/100] testing 14.0% loss=0.43467, acc=0.85938
# [34/100] testing 14.5% loss=0.24656, acc=0.87500
# [34/100] testing 15.4% loss=0.39142, acc=0.90625
# [34/100] testing 15.8% loss=0.22486, acc=0.89062
# [34/100] testing 16.7% loss=0.24162, acc=0.92188
# [34/100] testing 17.5% loss=0.17447, acc=0.95312
# [34/100] testing 18.0% loss=0.19510, acc=0.90625
# [34/100] testing 18.9% loss=0.12845, acc=0.93750
# [34/100] testing 19.3% loss=0.39083, acc=0.87500
# [34/100] testing 20.2% loss=0.23591, acc=0.92188
# [34/100] testing 20.6% loss=0.34823, acc=0.87500
# [34/100] testing 21.5% loss=0.18670, acc=0.93750
# [34/100] testing 21.9% loss=0.54208, acc=0.85938
# [34/100] testing 22.8% loss=0.29780, acc=0.87500
# [34/100] testing 23.7% loss=0.46218, acc=0.85938
# [34/100] testing 24.1% loss=0.17786, acc=0.90625
# [34/100] testing 25.0% loss=0.35804, acc=0.87500
# [34/100] testing 25.4% loss=0.09197, acc=0.96875
# [34/100] testing 26.3% loss=0.33473, acc=0.85938
# [34/100] testing 26.8% loss=0.28143, acc=0.92188
# [34/100] testing 27.6% loss=0.24587, acc=0.92188
# [34/100] testing 28.5% loss=0.23077, acc=0.92188
# [34/100] testing 29.0% loss=0.21093, acc=0.93750
# [34/100] testing 29.8% loss=0.38543, acc=0.85938
# [34/100] testing 30.3% loss=0.26791, acc=0.89062
# [34/100] testing 31.1% loss=0.25398, acc=0.89062
# [34/100] testing 31.6% loss=0.21243, acc=0.92188
# [34/100] testing 32.5% loss=0.31008, acc=0.85938
# [34/100] testing 32.9% loss=0.50149, acc=0.85938
# [34/100] testing 33.8% loss=0.31991, acc=0.87500
# [34/100] testing 34.7% loss=0.40096, acc=0.87500
# [34/100] testing 35.1% loss=0.23281, acc=0.92188
# [34/100] testing 36.0% loss=0.31539, acc=0.90625
# [34/100] testing 36.4% loss=0.27870, acc=0.89062
# [34/100] testing 37.3% loss=0.29940, acc=0.93750
# [34/100] testing 37.7% loss=0.50603, acc=0.81250
# [34/100] testing 38.6% loss=0.17867, acc=0.92188
# [34/100] testing 39.5% loss=0.20171, acc=0.95312
# [34/100] testing 39.9% loss=0.37547, acc=0.85938
# [34/100] testing 40.8% loss=0.35091, acc=0.90625
# [34/100] testing 41.2% loss=0.25909, acc=0.92188
# [34/100] testing 42.1% loss=0.26858, acc=0.90625
# [34/100] testing 42.5% loss=0.13801, acc=0.93750
# [34/100] testing 43.4% loss=0.35538, acc=0.85938
# [34/100] testing 43.9% loss=0.14020, acc=0.96875
# [34/100] testing 44.7% loss=0.33592, acc=0.90625
# [34/100] testing 45.6% loss=0.20029, acc=0.92188
# [34/100] testing 46.1% loss=0.29552, acc=0.84375
# [34/100] testing 46.9% loss=0.19278, acc=0.93750
# [34/100] testing 47.4% loss=0.15664, acc=0.92188
# [34/100] testing 48.3% loss=0.45093, acc=0.89062
# [34/100] testing 48.7% loss=0.47168, acc=0.78125
# [34/100] testing 49.6% loss=0.52208, acc=0.81250
# [34/100] testing 50.4% loss=0.14696, acc=0.95312
# [34/100] testing 50.9% loss=0.27292, acc=0.87500
# [34/100] testing 51.8% loss=0.23087, acc=0.87500
# [34/100] testing 52.2% loss=0.32251, acc=0.90625
# [34/100] testing 53.1% loss=0.22983, acc=0.90625
# [34/100] testing 53.5% loss=0.21490, acc=0.92188
# [34/100] testing 54.4% loss=0.45723, acc=0.82812
# [34/100] testing 54.8% loss=0.37378, acc=0.84375
# [34/100] testing 55.7% loss=0.14337, acc=0.95312
# [34/100] testing 56.6% loss=0.38263, acc=0.85938
# [34/100] testing 57.0% loss=0.48533, acc=0.84375
# [34/100] testing 57.9% loss=0.33059, acc=0.84375
# [34/100] testing 58.3% loss=0.40583, acc=0.85938
# [34/100] testing 59.2% loss=0.27020, acc=0.87500
# [34/100] testing 59.7% loss=0.25766, acc=0.85938
# [34/100] testing 60.5% loss=0.38577, acc=0.82812
# [34/100] testing 61.4% loss=0.15598, acc=0.93750
# [34/100] testing 61.9% loss=0.18672, acc=0.92188
# [34/100] testing 62.7% loss=0.29018, acc=0.90625
# [34/100] testing 63.2% loss=0.40442, acc=0.89062
# [34/100] testing 64.0% loss=0.36813, acc=0.82812
# [34/100] testing 64.5% loss=0.17294, acc=0.92188
# [34/100] testing 65.4% loss=0.17740, acc=0.93750
# [34/100] testing 65.8% loss=0.31830, acc=0.90625
# [34/100] testing 66.7% loss=0.25712, acc=0.89062
# [34/100] testing 67.6% loss=0.41017, acc=0.90625
# [34/100] testing 68.0% loss=0.15186, acc=0.93750
# [34/100] testing 68.9% loss=0.26827, acc=0.92188
# [34/100] testing 69.3% loss=0.29612, acc=0.87500
# [34/100] testing 70.2% loss=0.39133, acc=0.87500
# [34/100] testing 70.6% loss=0.31490, acc=0.85938
# [34/100] testing 71.5% loss=0.39551, acc=0.84375
# [34/100] testing 72.4% loss=0.13462, acc=0.93750
# [34/100] testing 72.8% loss=0.25315, acc=0.92188
# [34/100] testing 73.7% loss=0.14721, acc=0.95312
# [34/100] testing 74.1% loss=0.40080, acc=0.85938
# [34/100] testing 75.0% loss=0.20108, acc=0.92188
# [34/100] testing 75.4% loss=0.38836, acc=0.82812
# [34/100] testing 76.3% loss=0.07915, acc=0.96875
# [34/100] testing 76.8% loss=0.30954, acc=0.85938
# [34/100] testing 77.6% loss=0.23790, acc=0.90625
# [34/100] testing 78.5% loss=0.38080, acc=0.85938
# [34/100] testing 79.0% loss=0.20756, acc=0.89062
# [34/100] testing 79.8% loss=0.31796, acc=0.85938
# [34/100] testing 80.3% loss=0.36814, acc=0.85938
# [34/100] testing 81.2% loss=0.48180, acc=0.84375
# [34/100] testing 81.6% loss=0.28800, acc=0.93750
# [34/100] testing 82.5% loss=0.17648, acc=0.92188
# [34/100] testing 83.3% loss=0.26965, acc=0.93750
# [34/100] testing 83.8% loss=0.09132, acc=0.96875
# [34/100] testing 84.7% loss=0.29989, acc=0.85938
# [34/100] testing 85.1% loss=0.27726, acc=0.89062
# [34/100] testing 86.0% loss=0.36748, acc=0.89062
# [34/100] testing 86.4% loss=0.33865, acc=0.85938
# [34/100] testing 87.3% loss=0.23396, acc=0.87500
# [34/100] testing 87.7% loss=0.21014, acc=0.93750
# [34/100] testing 88.6% loss=0.31598, acc=0.89062
# [34/100] testing 89.5% loss=0.62070, acc=0.76562
# [34/100] testing 89.9% loss=0.20281, acc=0.90625
# [34/100] testing 90.8% loss=0.26909, acc=0.92188
# [34/100] testing 91.2% loss=0.14923, acc=0.92188
# [34/100] testing 92.1% loss=0.24721, acc=0.89062
# [34/100] testing 92.6% loss=0.36675, acc=0.84375
# [34/100] testing 93.4% loss=0.35020, acc=0.84375
# [34/100] testing 94.3% loss=0.21591, acc=0.95312
# [34/100] testing 94.7% loss=0.20070, acc=0.93750
# [34/100] testing 95.6% loss=0.39392, acc=0.85938
# [34/100] testing 96.1% loss=0.20884, acc=0.90625
# [34/100] testing 96.9% loss=0.35422, acc=0.84375
# [34/100] testing 97.4% loss=0.16716, acc=0.96875
# [34/100] testing 98.3% loss=0.17083, acc=0.90625
# [34/100] testing 98.7% loss=0.25775, acc=0.92188
# [34/100] testing 99.6% loss=0.29255, acc=0.89062
# [35/100] training 0.2% loss=0.36009, acc=0.87500
# [35/100] training 0.4% loss=0.34698, acc=0.85938
# [35/100] training 0.5% loss=0.08970, acc=0.96875
# [35/100] training 0.8% loss=0.10903, acc=0.93750
# [35/100] training 0.9% loss=0.21817, acc=0.87500
# [35/100] training 1.1% loss=0.22408, acc=0.95312
# [35/100] training 1.2% loss=0.29689, acc=0.87500
# [35/100] training 1.4% loss=0.21938, acc=0.90625
# [35/100] training 1.6% loss=0.14799, acc=0.93750
# [35/100] training 1.8% loss=0.19140, acc=0.95312
# [35/100] training 2.0% loss=0.24708, acc=0.92188
# [35/100] training 2.1% loss=0.22099, acc=0.87500
# [35/100] training 2.3% loss=0.15800, acc=0.95312
# [35/100] training 2.4% loss=0.31832, acc=0.85938
# [35/100] training 2.6% loss=0.10569, acc=0.96875
# [35/100] training 2.7% loss=0.21732, acc=0.87500
# [35/100] training 3.0% loss=0.20352, acc=0.93750
# [35/100] training 3.2% loss=0.14456, acc=0.93750
# [35/100] training 3.3% loss=0.25788, acc=0.92188
# [35/100] training 3.5% loss=0.14822, acc=0.93750
# [35/100] training 3.6% loss=0.23670, acc=0.90625
# [35/100] training 3.8% loss=0.20951, acc=0.90625
# [35/100] training 3.9% loss=0.17916, acc=0.95312
# [35/100] training 4.2% loss=0.12529, acc=0.95312
# [35/100] training 4.4% loss=0.13982, acc=0.96875
# [35/100] training 4.5% loss=0.20116, acc=0.90625
# [35/100] training 4.7% loss=0.36782, acc=0.87500
# [35/100] training 4.8% loss=0.29675, acc=0.90625
# [35/100] training 5.0% loss=0.06727, acc=1.00000
# [35/100] training 5.2% loss=0.19808, acc=0.92188
# [35/100] training 5.4% loss=0.15477, acc=0.93750
# [35/100] training 5.5% loss=0.31169, acc=0.85938
# [35/100] training 5.7% loss=0.12175, acc=0.95312
# [35/100] training 5.9% loss=0.25031, acc=0.90625
# [35/100] training 6.0% loss=0.25007, acc=0.85938
# [35/100] training 6.3% loss=0.20653, acc=0.92188
# [35/100] training 6.4% loss=0.14894, acc=0.95312
# [35/100] training 6.6% loss=0.24478, acc=0.89062
# [35/100] training 6.7% loss=0.33306, acc=0.82812
# [35/100] training 6.9% loss=0.19306, acc=0.92188
# [35/100] training 7.1% loss=0.17036, acc=0.93750
# [35/100] training 7.2% loss=0.33781, acc=0.85938
# [35/100] training 7.5% loss=0.18710, acc=0.93750
# [35/100] training 7.6% loss=0.22323, acc=0.90625
# [35/100] training 7.8% loss=0.18959, acc=0.93750
# [35/100] training 7.9% loss=0.27776, acc=0.87500
# [35/100] training 8.1% loss=0.14061, acc=0.95312
# [35/100] training 8.2% loss=0.19262, acc=0.93750
# [35/100] training 8.4% loss=0.21078, acc=0.90625
# [35/100] training 8.7% loss=0.27498, acc=0.93750
# [35/100] training 8.8% loss=0.25779, acc=0.92188
# [35/100] training 9.0% loss=0.21898, acc=0.89062
# [35/100] training 9.1% loss=0.21331, acc=0.90625
# [35/100] training 9.3% loss=0.31992, acc=0.90625
# [35/100] training 9.4% loss=0.13762, acc=0.93750
# [35/100] training 9.7% loss=0.19415, acc=0.92188
# [35/100] training 9.9% loss=0.25536, acc=0.89062
# [35/100] training 10.0% loss=0.24115, acc=0.89062
# [35/100] training 10.2% loss=0.15255, acc=0.93750
# [35/100] training 10.3% loss=0.14986, acc=0.93750
# [35/100] training 10.5% loss=0.29060, acc=0.92188
# [35/100] training 10.6% loss=0.17311, acc=0.93750
# [35/100] training 10.9% loss=0.14599, acc=0.93750
# [35/100] training 11.0% loss=0.32931, acc=0.89062
# [35/100] training 11.2% loss=0.13434, acc=0.96875
# [35/100] training 11.4% loss=0.27491, acc=0.87500
# [35/100] training 11.5% loss=0.32523, acc=0.85938
# [35/100] training 11.7% loss=0.11392, acc=0.98438
# [35/100] training 11.8% loss=0.13870, acc=0.95312
# [35/100] training 12.1% loss=0.12509, acc=0.95312
# [35/100] training 12.2% loss=0.16282, acc=0.93750
# [35/100] training 12.4% loss=0.19400, acc=0.90625
# [35/100] training 12.6% loss=0.26162, acc=0.92188
# [35/100] training 12.7% loss=0.18187, acc=0.93750
# [35/100] training 12.9% loss=0.24085, acc=0.89062
# [35/100] training 13.0% loss=0.12210, acc=0.98438
# [35/100] training 13.3% loss=0.15989, acc=0.93750
# [35/100] training 13.4% loss=0.17117, acc=0.89062
# [35/100] training 13.6% loss=0.15124, acc=0.96875
# [35/100] training 13.7% loss=0.41665, acc=0.85938
# [35/100] training 13.9% loss=0.27869, acc=0.92188
# [35/100] training 14.1% loss=0.23566, acc=0.93750
# [35/100] training 14.3% loss=0.17162, acc=0.90625
# [35/100] training 14.5% loss=0.19991, acc=0.95312
# [35/100] training 14.6% loss=0.19662, acc=0.92188
# [35/100] training 14.8% loss=0.27569, acc=0.89062
# [35/100] training 14.9% loss=0.17494, acc=0.90625
# [35/100] training 15.1% loss=0.24503, acc=0.89062
# [35/100] training 15.4% loss=0.32493, acc=0.87500
# [35/100] training 15.5% loss=0.20445, acc=0.90625
# [35/100] training 15.7% loss=0.32473, acc=0.90625
# [35/100] training 15.8% loss=0.10271, acc=0.96875
# [35/100] training 16.0% loss=0.25839, acc=0.90625
# [35/100] training 16.1% loss=0.41907, acc=0.84375
# [35/100] training 16.3% loss=0.29772, acc=0.87500
# [35/100] training 16.4% loss=0.19418, acc=0.92188
# [35/100] training 16.7% loss=0.28357, acc=0.89062
# [35/100] training 16.9% loss=0.23785, acc=0.89062
# [35/100] training 17.0% loss=0.29126, acc=0.85938
# [35/100] training 17.2% loss=0.23209, acc=0.89062
# [35/100] training 17.3% loss=0.20058, acc=0.90625
# [35/100] training 17.5% loss=0.19556, acc=0.92188
# [35/100] training 17.7% loss=0.22714, acc=0.90625
# [35/100] training 17.9% loss=0.21468, acc=0.90625
# [35/100] training 18.1% loss=0.29678, acc=0.89062
# [35/100] training 18.2% loss=0.25676, acc=0.90625
# [35/100] training 18.4% loss=0.28189, acc=0.89062
# [35/100] training 18.5% loss=0.27731, acc=0.87500
# [35/100] training 18.8% loss=0.17884, acc=0.90625
# [35/100] training 18.9% loss=0.17122, acc=0.90625
# [35/100] training 19.1% loss=0.26750, acc=0.87500
# [35/100] training 19.2% loss=0.11031, acc=0.96875
# [35/100] training 19.4% loss=0.16803, acc=0.92188
# [35/100] training 19.6% loss=0.20763, acc=0.89062
# [35/100] training 19.7% loss=0.35970, acc=0.87500
# [35/100] training 20.0% loss=0.10525, acc=0.95312
# [35/100] training 20.1% loss=0.19352, acc=0.93750
# [35/100] training 20.3% loss=0.23349, acc=0.90625
# [35/100] training 20.4% loss=0.31990, acc=0.84375
# [35/100] training 20.6% loss=0.22575, acc=0.93750
# [35/100] training 20.8% loss=0.20869, acc=0.90625
# [35/100] training 20.9% loss=0.18537, acc=0.92188
# [35/100] training 21.2% loss=0.28981, acc=0.89062
# [35/100] training 21.3% loss=0.22624, acc=0.90625
# [35/100] training 21.5% loss=0.32472, acc=0.90625
# [35/100] training 21.6% loss=0.10706, acc=0.96875
# [35/100] training 21.8% loss=0.21825, acc=0.92188
# [35/100] training 21.9% loss=0.19048, acc=0.92188
# [35/100] training 22.2% loss=0.29567, acc=0.87500
# [35/100] training 22.4% loss=0.22834, acc=0.92188
# [35/100] training 22.5% loss=0.14671, acc=0.93750
# [35/100] training 22.7% loss=0.17599, acc=0.95312
# [35/100] training 22.8% loss=0.21555, acc=0.89062
# [35/100] training 23.0% loss=0.11854, acc=0.93750
# [35/100] training 23.1% loss=0.21645, acc=0.90625
# [35/100] training 23.4% loss=0.32428, acc=0.89062
# [35/100] training 23.6% loss=0.32240, acc=0.87500
# [35/100] training 23.7% loss=0.16844, acc=0.93750
# [35/100] training 23.9% loss=0.21169, acc=0.90625
# [35/100] training 24.0% loss=0.17711, acc=0.93750
# [35/100] training 24.2% loss=0.16025, acc=0.93750
# [35/100] training 24.3% loss=0.17115, acc=0.93750
# [35/100] training 24.6% loss=0.25737, acc=0.85938
# [35/100] training 24.7% loss=0.28218, acc=0.89062
# [35/100] training 24.9% loss=0.20375, acc=0.92188
# [35/100] training 25.1% loss=0.26714, acc=0.89062
# [35/100] training 25.2% loss=0.14472, acc=0.95312
# [35/100] training 25.4% loss=0.25129, acc=0.89062
# [35/100] training 25.6% loss=0.13093, acc=0.93750
# [35/100] training 25.8% loss=0.23155, acc=0.89062
# [35/100] training 25.9% loss=0.16004, acc=0.93750
# [35/100] training 26.1% loss=0.26172, acc=0.90625
# [35/100] training 26.3% loss=0.18901, acc=0.90625
# [35/100] training 26.4% loss=0.06821, acc=0.96875
# [35/100] training 26.6% loss=0.10162, acc=0.95312
# [35/100] training 26.8% loss=0.19568, acc=0.93750
# [35/100] training 27.0% loss=0.15522, acc=0.95312
# [35/100] training 27.1% loss=0.16479, acc=0.93750
# [35/100] training 27.3% loss=0.19310, acc=0.92188
# [35/100] training 27.4% loss=0.10790, acc=0.95312
# [35/100] training 27.6% loss=0.26612, acc=0.92188
# [35/100] training 27.9% loss=0.13361, acc=0.93750
# [35/100] training 28.0% loss=0.23260, acc=0.93750
# [35/100] training 28.2% loss=0.17825, acc=0.93750
# [35/100] training 28.3% loss=0.06376, acc=0.98438
# [35/100] training 28.5% loss=0.23649, acc=0.89062
# [35/100] training 28.6% loss=0.20941, acc=0.92188
# [35/100] training 28.8% loss=0.11276, acc=0.93750
# [35/100] training 29.1% loss=0.19514, acc=0.89062
# [35/100] training 29.2% loss=0.10986, acc=0.93750
# [35/100] training 29.4% loss=0.28972, acc=0.89062
# [35/100] training 29.5% loss=0.12056, acc=0.96875
# [35/100] training 29.7% loss=0.29002, acc=0.87500
# [35/100] training 29.8% loss=0.18046, acc=0.93750
# [35/100] training 30.0% loss=0.17063, acc=0.93750
# [35/100] training 30.2% loss=0.20701, acc=0.90625
# [35/100] training 30.4% loss=0.13908, acc=0.95312
# [35/100] training 30.6% loss=0.30764, acc=0.90625
# [35/100] training 30.7% loss=0.16842, acc=0.92188
# [35/100] training 30.9% loss=0.33769, acc=0.92188
# [35/100] training 31.0% loss=0.09182, acc=0.95312
# [35/100] training 31.3% loss=0.16365, acc=0.93750
# [35/100] training 31.4% loss=0.25948, acc=0.90625
# [35/100] training 31.6% loss=0.34135, acc=0.92188
# [35/100] training 31.8% loss=0.17142, acc=0.92188
# [35/100] training 31.9% loss=0.22870, acc=0.90625
# [35/100] training 32.1% loss=0.26363, acc=0.85938
# [35/100] training 32.2% loss=0.24592, acc=0.92188
# [35/100] training 32.5% loss=0.11325, acc=0.98438
# [35/100] training 32.6% loss=0.30365, acc=0.87500
# [35/100] training 32.8% loss=0.12407, acc=0.95312
# [35/100] training 32.9% loss=0.25329, acc=0.92188
# [35/100] training 33.1% loss=0.18191, acc=0.90625
# [35/100] training 33.3% loss=0.28974, acc=0.89062
# [35/100] training 33.4% loss=0.16122, acc=0.92188
# [35/100] training 33.7% loss=0.18672, acc=0.90625
# [35/100] training 33.8% loss=0.20009, acc=0.90625
# [35/100] training 34.0% loss=0.18871, acc=0.90625
# [35/100] training 34.1% loss=0.17945, acc=0.93750
# [35/100] training 34.3% loss=0.16805, acc=0.93750
# [35/100] training 34.5% loss=0.19264, acc=0.93750
# [35/100] training 34.7% loss=0.10248, acc=0.96875
# [35/100] training 34.9% loss=0.16355, acc=0.93750
# [35/100] training 35.0% loss=0.11866, acc=0.95312
# [35/100] training 35.2% loss=0.20447, acc=0.90625
# [35/100] training 35.3% loss=0.20431, acc=0.92188
# [35/100] training 35.5% loss=0.14505, acc=0.92188
# [35/100] training 35.6% loss=0.23954, acc=0.90625
# [35/100] training 35.9% loss=0.27141, acc=0.87500
# [35/100] training 36.1% loss=0.27288, acc=0.87500
# [35/100] training 36.2% loss=0.27340, acc=0.87500
# [35/100] training 36.4% loss=0.31605, acc=0.87500
# [35/100] training 36.5% loss=0.23534, acc=0.95312
# [35/100] training 36.7% loss=0.21224, acc=0.92188
# [35/100] training 36.8% loss=0.15160, acc=0.92188
# [35/100] training 37.1% loss=0.24688, acc=0.89062
# [35/100] training 37.3% loss=0.22485, acc=0.92188
# [35/100] training 37.4% loss=0.22009, acc=0.93750
# [35/100] training 37.6% loss=0.14370, acc=0.93750
# [35/100] training 37.7% loss=0.25800, acc=0.89062
# [35/100] training 37.9% loss=0.24761, acc=0.90625
# [35/100] training 38.1% loss=0.31034, acc=0.87500
# [35/100] training 38.3% loss=0.18525, acc=0.89062
# [35/100] training 38.4% loss=0.09098, acc=0.96875
# [35/100] training 38.6% loss=0.14426, acc=0.93750
# [35/100] training 38.8% loss=0.33107, acc=0.84375
# [35/100] training 38.9% loss=0.15955, acc=0.90625
# [35/100] training 39.1% loss=0.14176, acc=0.93750
# [35/100] training 39.3% loss=0.20689, acc=0.90625
# [35/100] training 39.5% loss=0.30474, acc=0.89062
# [35/100] training 39.6% loss=0.19076, acc=0.93750
# [35/100] training 39.8% loss=0.07527, acc=0.98438
# [35/100] training 40.0% loss=0.17242, acc=0.92188
# [35/100] training 40.1% loss=0.24551, acc=0.92188
# [35/100] training 40.4% loss=0.15074, acc=0.92188
# [35/100] training 40.5% loss=0.23861, acc=0.93750
# [35/100] training 40.7% loss=0.28840, acc=0.90625
# [35/100] training 40.8% loss=0.14772, acc=0.93750
# [35/100] training 41.0% loss=0.15183, acc=0.92188
# [35/100] training 41.1% loss=0.31448, acc=0.85938
# [35/100] training 41.3% loss=0.31549, acc=0.87500
# [35/100] training 41.6% loss=0.23769, acc=0.89062
# [35/100] training 41.7% loss=0.37285, acc=0.84375
# [35/100] training 41.9% loss=0.18385, acc=0.92188
# [35/100] training 42.0% loss=0.19382, acc=0.92188
# [35/100] training 42.2% loss=0.21429, acc=0.92188
# [35/100] training 42.3% loss=0.20436, acc=0.90625
# [35/100] training 42.5% loss=0.15801, acc=0.92188
# [35/100] training 42.8% loss=0.11123, acc=0.95312
# [35/100] training 42.9% loss=0.16502, acc=0.92188
# [35/100] training 43.1% loss=0.11287, acc=0.95312
# [35/100] training 43.2% loss=0.10191, acc=0.95312
# [35/100] training 43.4% loss=0.19491, acc=0.95312
# [35/100] training 43.5% loss=0.25457, acc=0.90625
# [35/100] training 43.8% loss=0.27045, acc=0.89062
# [35/100] training 43.9% loss=0.19047, acc=0.95312
# [35/100] training 44.1% loss=0.11619, acc=0.95312
# [35/100] training 44.3% loss=0.19386, acc=0.90625
# [35/100] training 44.4% loss=0.24150, acc=0.90625
# [35/100] training 44.6% loss=0.28348, acc=0.93750
# [35/100] training 44.7% loss=0.27745, acc=0.87500
# [35/100] training 45.0% loss=0.11884, acc=0.93750
# [35/100] training 45.1% loss=0.37554, acc=0.89062
# [35/100] training 45.3% loss=0.23930, acc=0.90625
# [35/100] training 45.5% loss=0.11068, acc=0.98438
# [35/100] training 45.6% loss=0.21864, acc=0.87500
# [35/100] training 45.8% loss=0.18064, acc=0.93750
# [35/100] training 45.9% loss=0.15694, acc=0.90625
# [35/100] training 46.2% loss=0.14100, acc=0.95312
# [35/100] training 46.3% loss=0.09165, acc=0.95312
# [35/100] training 46.5% loss=0.37603, acc=0.87500
# [35/100] training 46.6% loss=0.19032, acc=0.93750
# [35/100] training 46.8% loss=0.19169, acc=0.95312
# [35/100] training 47.0% loss=0.18471, acc=0.92188
# [35/100] training 47.2% loss=0.28695, acc=0.90625
# [35/100] training 47.4% loss=0.25622, acc=0.89062
# [35/100] training 47.5% loss=0.36275, acc=0.81250
# [35/100] training 47.7% loss=0.22128, acc=0.89062
# [35/100] training 47.8% loss=0.24192, acc=0.89062
# [35/100] training 48.0% loss=0.23796, acc=0.85938
# [35/100] training 48.3% loss=0.08436, acc=0.98438
# [35/100] training 48.4% loss=0.12230, acc=0.98438
# [35/100] training 48.6% loss=0.18734, acc=0.93750
# [35/100] training 48.7% loss=0.27329, acc=0.85938
# [35/100] training 48.9% loss=0.20569, acc=0.89062
# [35/100] training 49.0% loss=0.13711, acc=0.93750
# [35/100] training 49.2% loss=0.34856, acc=0.81250
# [35/100] training 49.3% loss=0.20716, acc=0.89062
# [35/100] training 49.6% loss=0.25487, acc=0.92188
# [35/100] training 49.8% loss=0.24237, acc=0.92188
# [35/100] training 49.9% loss=0.26979, acc=0.87500
# [35/100] training 50.1% loss=0.14794, acc=0.95312
# [35/100] training 50.2% loss=0.28471, acc=0.89062
# [35/100] training 50.4% loss=0.36434, acc=0.87500
# [35/100] training 50.6% loss=0.22031, acc=0.89062
# [35/100] training 50.8% loss=0.22537, acc=0.92188
# [35/100] training 51.0% loss=0.29976, acc=0.84375
# [35/100] training 51.1% loss=0.30829, acc=0.90625
# [35/100] training 51.3% loss=0.26713, acc=0.90625
# [35/100] training 51.4% loss=0.17315, acc=0.93750
# [35/100] training 51.7% loss=0.24108, acc=0.92188
# [35/100] training 51.8% loss=0.28333, acc=0.89062
# [35/100] training 52.0% loss=0.24407, acc=0.93750
# [35/100] training 52.1% loss=0.14973, acc=0.93750
# [35/100] training 52.3% loss=0.34763, acc=0.89062
# [35/100] training 52.5% loss=0.07968, acc=0.98438
# [35/100] training 52.6% loss=0.18530, acc=0.90625
# [35/100] training 52.9% loss=0.39613, acc=0.85938
# [35/100] training 53.0% loss=0.14030, acc=0.93750
# [35/100] training 53.2% loss=0.22188, acc=0.93750
# [35/100] training 53.3% loss=0.13299, acc=0.96875
# [35/100] training 53.5% loss=0.21513, acc=0.89062
# [35/100] training 53.7% loss=0.12403, acc=0.95312
# [35/100] training 53.8% loss=0.27326, acc=0.85938
# [35/100] training 54.1% loss=0.29666, acc=0.87500
# [35/100] training 54.2% loss=0.16212, acc=0.92188
# [35/100] training 54.4% loss=0.27273, acc=0.89062
# [35/100] training 54.5% loss=0.25024, acc=0.90625
# [35/100] training 54.7% loss=0.40586, acc=0.89062
# [35/100] training 54.8% loss=0.12324, acc=0.93750
# [35/100] training 55.1% loss=0.16751, acc=0.93750
# [35/100] training 55.3% loss=0.13601, acc=0.95312
# [35/100] training 55.4% loss=0.18303, acc=0.90625
# [35/100] training 55.6% loss=0.16530, acc=0.92188
# [35/100] training 55.7% loss=0.11202, acc=0.96875
# [35/100] training 55.9% loss=0.10991, acc=0.93750
# [35/100] training 56.0% loss=0.16402, acc=0.93750
# [35/100] training 56.3% loss=0.48152, acc=0.79688
# [35/100] training 56.5% loss=0.23947, acc=0.93750
# [35/100] training 56.6% loss=0.21865, acc=0.92188
# [35/100] training 56.8% loss=0.20954, acc=0.92188
# [35/100] training 56.9% loss=0.18541, acc=0.89062
# [35/100] training 57.1% loss=0.19251, acc=0.93750
# [35/100] training 57.2% loss=0.14874, acc=0.95312
# [35/100] training 57.5% loss=0.16956, acc=0.90625
# [35/100] training 57.6% loss=0.18094, acc=0.89062
# [35/100] training 57.8% loss=0.18508, acc=0.92188
# [35/100] training 58.0% loss=0.17149, acc=0.93750
# [35/100] training 58.1% loss=0.10145, acc=0.95312
# [35/100] training 58.3% loss=0.13049, acc=0.95312
# [35/100] training 58.4% loss=0.34951, acc=0.90625
# [35/100] training 58.7% loss=0.29243, acc=0.89062
# [35/100] training 58.8% loss=0.21415, acc=0.92188
# [35/100] training 59.0% loss=0.16408, acc=0.92188
# [35/100] training 59.2% loss=0.20978, acc=0.90625
# [35/100] training 59.3% loss=0.18335, acc=0.93750
# [35/100] training 59.5% loss=0.23058, acc=0.90625
# [35/100] training 59.7% loss=0.15548, acc=0.92188
# [35/100] training 59.9% loss=0.17769, acc=0.98438
# [35/100] training 60.0% loss=0.20845, acc=0.93750
# [35/100] training 60.2% loss=0.22819, acc=0.90625
# [35/100] training 60.3% loss=0.20460, acc=0.90625
# [35/100] training 60.5% loss=0.23150, acc=0.90625
# [35/100] training 60.8% loss=0.17266, acc=0.90625
# [35/100] training 60.9% loss=0.26213, acc=0.93750
# [35/100] training 61.1% loss=0.33210, acc=0.84375
# [35/100] training 61.2% loss=0.16529, acc=0.92188
# [35/100] training 61.4% loss=0.20817, acc=0.92188
# [35/100] training 61.5% loss=0.34505, acc=0.84375
# [35/100] training 61.7% loss=0.15793, acc=0.95312
# [35/100] training 62.0% loss=0.32374, acc=0.89062
# [35/100] training 62.1% loss=0.25086, acc=0.85938
# [35/100] training 62.3% loss=0.13754, acc=0.95312
# [35/100] training 62.4% loss=0.18124, acc=0.93750
# [35/100] training 62.6% loss=0.25185, acc=0.89062
# [35/100] training 62.7% loss=0.15581, acc=0.95312
# [35/100] training 62.9% loss=0.18802, acc=0.90625
# [35/100] training 63.1% loss=0.13924, acc=0.95312
# [35/100] training 63.3% loss=0.17581, acc=0.96875
# [35/100] training 63.5% loss=0.27317, acc=0.89062
# [35/100] training 63.6% loss=0.29820, acc=0.82812
# [35/100] training 63.8% loss=0.31837, acc=0.85938
# [35/100] training 63.9% loss=0.13631, acc=0.95312
# [35/100] training 64.2% loss=0.17389, acc=0.89062
# [35/100] training 64.3% loss=0.27688, acc=0.90625
# [35/100] training 64.5% loss=0.22004, acc=0.89062
# [35/100] training 64.7% loss=0.23763, acc=0.89062
# [35/100] training 64.8% loss=0.39712, acc=0.79688
# [35/100] training 65.0% loss=0.22766, acc=0.90625
# [35/100] training 65.1% loss=0.23963, acc=0.89062
# [35/100] training 65.4% loss=0.17135, acc=0.93750
# [35/100] training 65.5% loss=0.10505, acc=0.96875
# [35/100] training 65.7% loss=0.12550, acc=0.92188
# [35/100] training 65.8% loss=0.22628, acc=0.87500
# [35/100] training 66.0% loss=0.17482, acc=0.93750
# [35/100] training 66.2% loss=0.09560, acc=0.96875
# [35/100] training 66.3% loss=0.25196, acc=0.92188
# [35/100] training 66.6% loss=0.15912, acc=0.93750
# [35/100] training 66.7% loss=0.13055, acc=0.95312
# [35/100] training 66.9% loss=0.18364, acc=0.89062
# [35/100] training 67.0% loss=0.18772, acc=0.90625
# [35/100] training 67.2% loss=0.21759, acc=0.92188
# [35/100] training 67.4% loss=0.16072, acc=0.92188
# [35/100] training 67.6% loss=0.17989, acc=0.93750
# [35/100] training 67.8% loss=0.13788, acc=0.96875
# [35/100] training 67.9% loss=0.12804, acc=0.96875
# [35/100] training 68.1% loss=0.16998, acc=0.93750
# [35/100] training 68.2% loss=0.14497, acc=0.95312
# [35/100] training 68.4% loss=0.09655, acc=0.96875
# [35/100] training 68.5% loss=0.13780, acc=0.92188
# [35/100] training 68.8% loss=0.17562, acc=0.93750
# [35/100] training 69.0% loss=0.25229, acc=0.95312
# [35/100] training 69.1% loss=0.14326, acc=0.93750
# [35/100] training 69.3% loss=0.19586, acc=0.89062
# [35/100] training 69.4% loss=0.19643, acc=0.96875
# [35/100] training 69.6% loss=0.13752, acc=0.93750
# [35/100] training 69.7% loss=0.23118, acc=0.89062
# [35/100] training 70.0% loss=0.46502, acc=0.81250
# [35/100] training 70.2% loss=0.24849, acc=0.89062
# [35/100] training 70.3% loss=0.20298, acc=0.93750
# [35/100] training 70.5% loss=0.21923, acc=0.92188
# [35/100] training 70.6% loss=0.16366, acc=0.92188
# [35/100] training 70.8% loss=0.26183, acc=0.87500
# [35/100] training 71.0% loss=0.29677, acc=0.92188
# [35/100] training 71.2% loss=0.15860, acc=0.92188
# [35/100] training 71.3% loss=0.12335, acc=0.98438
# [35/100] training 71.5% loss=0.36626, acc=0.92188
# [35/100] training 71.7% loss=0.23542, acc=0.85938
# [35/100] training 71.8% loss=0.16961, acc=0.92188
# [35/100] training 72.0% loss=0.13450, acc=0.96875
# [35/100] training 72.2% loss=0.31891, acc=0.85938
# [35/100] training 72.4% loss=0.18279, acc=0.89062
# [35/100] training 72.5% loss=0.21054, acc=0.90625
# [35/100] training 72.7% loss=0.25676, acc=0.90625
# [35/100] training 72.9% loss=0.16189, acc=0.93750
# [35/100] training 73.0% loss=0.09990, acc=0.95312
# [35/100] training 73.3% loss=0.22108, acc=0.92188
# [35/100] training 73.4% loss=0.09334, acc=0.96875
# [35/100] training 73.6% loss=0.13464, acc=0.98438
# [35/100] training 73.7% loss=0.20520, acc=0.93750
# [35/100] training 73.9% loss=0.10884, acc=0.96875
# [35/100] training 74.0% loss=0.20855, acc=0.87500
# [35/100] training 74.2% loss=0.09241, acc=0.96875
# [35/100] training 74.5% loss=0.14836, acc=0.92188
# [35/100] training 74.6% loss=0.39466, acc=0.87500
# [35/100] training 74.8% loss=0.34823, acc=0.89062
# [35/100] training 74.9% loss=0.30310, acc=0.87500
# [35/100] training 75.1% loss=0.21323, acc=0.92188
# [35/100] training 75.2% loss=0.09498, acc=0.96875
# [35/100] training 75.4% loss=0.19709, acc=0.90625
# [35/100] training 75.7% loss=0.27458, acc=0.87500
# [35/100] training 75.8% loss=0.33750, acc=0.89062
# [35/100] training 76.0% loss=0.15252, acc=0.90625
# [35/100] training 76.1% loss=0.22335, acc=0.89062
# [35/100] training 76.3% loss=0.09449, acc=0.95312
# [35/100] training 76.4% loss=0.21066, acc=0.92188
# [35/100] training 76.7% loss=0.25672, acc=0.90625
# [35/100] training 76.8% loss=0.14310, acc=0.95312
# [35/100] training 77.0% loss=0.12465, acc=0.96875
# [35/100] training 77.2% loss=0.29186, acc=0.92188
# [35/100] training 77.3% loss=0.09781, acc=0.98438
# [35/100] training 77.5% loss=0.31709, acc=0.89062
# [35/100] training 77.6% loss=0.23325, acc=0.87500
# [35/100] training 77.9% loss=0.21171, acc=0.90625
# [35/100] training 78.0% loss=0.19362, acc=0.93750
# [35/100] training 78.2% loss=0.26377, acc=0.89062
# [35/100] training 78.4% loss=0.15412, acc=0.95312
# [35/100] training 78.5% loss=0.28733, acc=0.87500
# [35/100] training 78.7% loss=0.24126, acc=0.87500
# [35/100] training 78.8% loss=0.10867, acc=0.96875
# [35/100] training 79.1% loss=0.13711, acc=0.95312
# [35/100] training 79.2% loss=0.13465, acc=0.96875
# [35/100] training 79.4% loss=0.22847, acc=0.92188
# [35/100] training 79.5% loss=0.17503, acc=0.93750
# [35/100] training 79.7% loss=0.09363, acc=0.98438
# [35/100] training 79.9% loss=0.09838, acc=0.96875
# [35/100] training 80.1% loss=0.17293, acc=0.95312
# [35/100] training 80.3% loss=0.25804, acc=0.85938
# [35/100] training 80.4% loss=0.20445, acc=0.95312
# [35/100] training 80.6% loss=0.21367, acc=0.93750
# [35/100] training 80.7% loss=0.13552, acc=0.95312
# [35/100] training 80.9% loss=0.21319, acc=0.90625
# [35/100] training 81.2% loss=0.23942, acc=0.90625
# [35/100] training 81.3% loss=0.26661, acc=0.90625
# [35/100] training 81.5% loss=0.13611, acc=0.95312
# [35/100] training 81.6% loss=0.26447, acc=0.89062
# [35/100] training 81.8% loss=0.33102, acc=0.87500
# [35/100] training 81.9% loss=0.37837, acc=0.92188
# [35/100] training 82.1% loss=0.19717, acc=0.93750
# [35/100] training 82.2% loss=0.18738, acc=0.93750
# [35/100] training 82.5% loss=0.21363, acc=0.92188
# [35/100] training 82.7% loss=0.29162, acc=0.89062
# [35/100] training 82.8% loss=0.22450, acc=0.90625
# [35/100] training 83.0% loss=0.21005, acc=0.92188
# [35/100] training 83.1% loss=0.18956, acc=0.87500
# [35/100] training 83.3% loss=0.19968, acc=0.92188
# [35/100] training 83.5% loss=0.14454, acc=0.95312
# [35/100] training 83.7% loss=0.35338, acc=0.85938
# [35/100] training 83.9% loss=0.16166, acc=0.93750
# [35/100] training 84.0% loss=0.11122, acc=0.95312
# [35/100] training 84.2% loss=0.13602, acc=0.93750
# [35/100] training 84.3% loss=0.12119, acc=0.93750
# [35/100] training 84.5% loss=0.17523, acc=0.96875
# [35/100] training 84.7% loss=0.16453, acc=0.92188
# [35/100] training 84.9% loss=0.19740, acc=0.93750
# [35/100] training 85.0% loss=0.14964, acc=0.92188
# [35/100] training 85.2% loss=0.10461, acc=0.96875
# [35/100] training 85.4% loss=0.14021, acc=0.96875
# [35/100] training 85.5% loss=0.23214, acc=0.87500
# [35/100] training 85.8% loss=0.13520, acc=0.93750
# [35/100] training 85.9% loss=0.13240, acc=0.92188
# [35/100] training 86.1% loss=0.18734, acc=0.92188
# [35/100] training 86.2% loss=0.13301, acc=0.95312
# [35/100] training 86.4% loss=0.32338, acc=0.89062
# [35/100] training 86.6% loss=0.16315, acc=0.93750
# [35/100] training 86.7% loss=0.15319, acc=0.93750
# [35/100] training 87.0% loss=0.23558, acc=0.92188
# [35/100] training 87.1% loss=0.20986, acc=0.87500
# [35/100] training 87.3% loss=0.17464, acc=0.90625
# [35/100] training 87.4% loss=0.20191, acc=0.92188
# [35/100] training 87.6% loss=0.15923, acc=0.95312
# [35/100] training 87.7% loss=0.38162, acc=0.92188
# [35/100] training 87.9% loss=0.19595, acc=0.95312
# [35/100] training 88.2% loss=0.13216, acc=0.96875
# [35/100] training 88.3% loss=0.23540, acc=0.92188
# [35/100] training 88.5% loss=0.18354, acc=0.89062
# [35/100] training 88.6% loss=0.09148, acc=1.00000
# [35/100] training 88.8% loss=0.12421, acc=0.95312
# [35/100] training 88.9% loss=0.28152, acc=0.89062
# [35/100] training 89.2% loss=0.12654, acc=0.93750
# [35/100] training 89.4% loss=0.14387, acc=0.92188
# [35/100] training 89.5% loss=0.27714, acc=0.90625
# [35/100] training 89.7% loss=0.22836, acc=0.89062
# [35/100] training 89.8% loss=0.19208, acc=0.93750
# [35/100] training 90.0% loss=0.14258, acc=0.95312
# [35/100] training 90.1% loss=0.23918, acc=0.92188
# [35/100] training 90.4% loss=0.26423, acc=0.89062
# [35/100] training 90.5% loss=0.12937, acc=0.92188
# [35/100] training 90.7% loss=0.11227, acc=0.98438
# [35/100] training 90.9% loss=0.11438, acc=0.93750
# [35/100] training 91.0% loss=0.12236, acc=0.93750
# [35/100] training 91.2% loss=0.20172, acc=0.92188
# [35/100] training 91.3% loss=0.25267, acc=0.90625
# [35/100] training 91.6% loss=0.30311, acc=0.89062
# [35/100] training 91.7% loss=0.18815, acc=0.90625
# [35/100] training 91.9% loss=0.36304, acc=0.84375
# [35/100] training 92.1% loss=0.27892, acc=0.90625
# [35/100] training 92.2% loss=0.10145, acc=0.93750
# [35/100] training 92.4% loss=0.13807, acc=0.93750
# [35/100] training 92.6% loss=0.27112, acc=0.92188
# [35/100] training 92.8% loss=0.31890, acc=0.90625
# [35/100] training 92.9% loss=0.21773, acc=0.90625
# [35/100] training 93.1% loss=0.31667, acc=0.87500
# [35/100] training 93.2% loss=0.26033, acc=0.89062
# [35/100] training 93.4% loss=0.21020, acc=0.93750
# [35/100] training 93.7% loss=0.18276, acc=0.95312
# [35/100] training 93.8% loss=0.17808, acc=0.96875
# [35/100] training 94.0% loss=0.16145, acc=0.96875
# [35/100] training 94.1% loss=0.19797, acc=0.93750
# [35/100] training 94.3% loss=0.12762, acc=0.93750
# [35/100] training 94.4% loss=0.25998, acc=0.90625
# [35/100] training 94.6% loss=0.07193, acc=0.98438
# [35/100] training 94.9% loss=0.12671, acc=0.95312
# [35/100] training 95.0% loss=0.27827, acc=0.89062
# [35/100] training 95.2% loss=0.31570, acc=0.85938
# [35/100] training 95.3% loss=0.34230, acc=0.87500
# [35/100] training 95.5% loss=0.20729, acc=0.92188
# [35/100] training 95.6% loss=0.42719, acc=0.85938
# [35/100] training 95.8% loss=0.24389, acc=0.90625
# [35/100] training 96.0% loss=0.29838, acc=0.89062
# [35/100] training 96.2% loss=0.19643, acc=0.95312
# [35/100] training 96.4% loss=0.19753, acc=0.95312
# [35/100] training 96.5% loss=0.24844, acc=0.90625
# [35/100] training 96.7% loss=0.12706, acc=0.93750
# [35/100] training 96.8% loss=0.22933, acc=0.92188
# [35/100] training 97.1% loss=0.11738, acc=0.95312
# [35/100] training 97.2% loss=0.21237, acc=0.90625
# [35/100] training 97.4% loss=0.22856, acc=0.92188
# [35/100] training 97.6% loss=0.20242, acc=0.90625
# [35/100] training 97.7% loss=0.15218, acc=0.90625
# [35/100] training 97.9% loss=0.10627, acc=0.93750
# [35/100] training 98.0% loss=0.18559, acc=0.92188
# [35/100] training 98.3% loss=0.21425, acc=0.90625
# [35/100] training 98.4% loss=0.16509, acc=0.92188
# [35/100] training 98.6% loss=0.38413, acc=0.87500
# [35/100] training 98.7% loss=0.35463, acc=0.90625
# [35/100] training 98.9% loss=0.15730, acc=0.93750
# [35/100] training 99.1% loss=0.16725, acc=0.93750
# [35/100] training 99.2% loss=0.19365, acc=0.93750
# [35/100] training 99.5% loss=0.23672, acc=0.89062
# [35/100] training 99.6% loss=0.17816, acc=0.95312
# [35/100] training 99.8% loss=0.14984, acc=0.93750
# [35/100] training 99.9% loss=0.12469, acc=0.95312
# [35/100] testing 0.9% loss=0.19203, acc=0.90625
# [35/100] testing 1.8% loss=0.28465, acc=0.87500
# [35/100] testing 2.2% loss=0.26722, acc=0.92188
# [35/100] testing 3.1% loss=0.35479, acc=0.84375
# [35/100] testing 3.5% loss=0.12983, acc=0.95312
# [35/100] testing 4.4% loss=0.27566, acc=0.84375
# [35/100] testing 4.8% loss=0.27675, acc=0.87500
# [35/100] testing 5.7% loss=0.28221, acc=0.85938
# [35/100] testing 6.6% loss=0.17922, acc=0.92188
# [35/100] testing 7.0% loss=0.13623, acc=0.95312
# [35/100] testing 7.9% loss=0.25867, acc=0.87500
# [35/100] testing 8.3% loss=0.32203, acc=0.87500
# [35/100] testing 9.2% loss=0.28458, acc=0.95312
# [35/100] testing 9.7% loss=0.13017, acc=0.95312
# [35/100] testing 10.5% loss=0.24630, acc=0.84375
# [35/100] testing 11.0% loss=0.32424, acc=0.82812
# [35/100] testing 11.8% loss=0.16361, acc=0.90625
# [35/100] testing 12.7% loss=0.36829, acc=0.84375
# [35/100] testing 13.2% loss=0.23564, acc=0.90625
# [35/100] testing 14.0% loss=0.38266, acc=0.89062
# [35/100] testing 14.5% loss=0.29251, acc=0.87500
# [35/100] testing 15.4% loss=0.34913, acc=0.85938
# [35/100] testing 15.8% loss=0.20147, acc=0.92188
# [35/100] testing 16.7% loss=0.26352, acc=0.87500
# [35/100] testing 17.5% loss=0.18446, acc=0.90625
# [35/100] testing 18.0% loss=0.30836, acc=0.90625
# [35/100] testing 18.9% loss=0.21399, acc=0.85938
# [35/100] testing 19.3% loss=0.29645, acc=0.92188
# [35/100] testing 20.2% loss=0.25588, acc=0.89062
# [35/100] testing 20.6% loss=0.37594, acc=0.87500
# [35/100] testing 21.5% loss=0.22349, acc=0.90625
# [35/100] testing 21.9% loss=0.34405, acc=0.87500
# [35/100] testing 22.8% loss=0.34425, acc=0.87500
# [35/100] testing 23.7% loss=0.42151, acc=0.85938
# [35/100] testing 24.1% loss=0.22097, acc=0.92188
# [35/100] testing 25.0% loss=0.35674, acc=0.87500
# [35/100] testing 25.4% loss=0.13021, acc=0.95312
# [35/100] testing 26.3% loss=0.37752, acc=0.84375
# [35/100] testing 26.8% loss=0.28994, acc=0.89062
# [35/100] testing 27.6% loss=0.26557, acc=0.87500
# [35/100] testing 28.5% loss=0.21946, acc=0.93750
# [35/100] testing 29.0% loss=0.16347, acc=0.93750
# [35/100] testing 29.8% loss=0.32056, acc=0.85938
# [35/100] testing 30.3% loss=0.24248, acc=0.92188
# [35/100] testing 31.1% loss=0.29084, acc=0.85938
# [35/100] testing 31.6% loss=0.30390, acc=0.92188
# [35/100] testing 32.5% loss=0.24118, acc=0.92188
# [35/100] testing 32.9% loss=0.43280, acc=0.90625
# [35/100] testing 33.8% loss=0.33493, acc=0.87500
# [35/100] testing 34.7% loss=0.32472, acc=0.92188
# [35/100] testing 35.1% loss=0.15742, acc=0.92188
# [35/100] testing 36.0% loss=0.19438, acc=0.95312
# [35/100] testing 36.4% loss=0.25491, acc=0.90625
# [35/100] testing 37.3% loss=0.29085, acc=0.93750
# [35/100] testing 37.7% loss=0.52538, acc=0.82812
# [35/100] testing 38.6% loss=0.19454, acc=0.92188
# [35/100] testing 39.5% loss=0.31089, acc=0.95312
# [35/100] testing 39.9% loss=0.29441, acc=0.93750
# [35/100] testing 40.8% loss=0.28841, acc=0.90625
# [35/100] testing 41.2% loss=0.23360, acc=0.96875
# [35/100] testing 42.1% loss=0.31792, acc=0.89062
# [35/100] testing 42.5% loss=0.20673, acc=0.92188
# [35/100] testing 43.4% loss=0.33519, acc=0.89062
# [35/100] testing 43.9% loss=0.11952, acc=0.96875
# [35/100] testing 44.7% loss=0.37371, acc=0.85938
# [35/100] testing 45.6% loss=0.26422, acc=0.89062
# [35/100] testing 46.1% loss=0.25483, acc=0.87500
# [35/100] testing 46.9% loss=0.22921, acc=0.90625
# [35/100] testing 47.4% loss=0.09613, acc=0.95312
# [35/100] testing 48.3% loss=0.38767, acc=0.87500
# [35/100] testing 48.7% loss=0.31258, acc=0.87500
# [35/100] testing 49.6% loss=0.42020, acc=0.82812
# [35/100] testing 50.4% loss=0.19851, acc=0.90625
# [35/100] testing 50.9% loss=0.33214, acc=0.90625
# [35/100] testing 51.8% loss=0.30554, acc=0.87500
# [35/100] testing 52.2% loss=0.29326, acc=0.87500
# [35/100] testing 53.1% loss=0.29187, acc=0.89062
# [35/100] testing 53.5% loss=0.23249, acc=0.93750
# [35/100] testing 54.4% loss=0.24891, acc=0.90625
# [35/100] testing 54.8% loss=0.32350, acc=0.82812
# [35/100] testing 55.7% loss=0.19018, acc=0.93750
# [35/100] testing 56.6% loss=0.30853, acc=0.89062
# [35/100] testing 57.0% loss=0.37579, acc=0.87500
# [35/100] testing 57.9% loss=0.25999, acc=0.85938
# [35/100] testing 58.3% loss=0.36150, acc=0.85938
# [35/100] testing 59.2% loss=0.27377, acc=0.89062
# [35/100] testing 59.7% loss=0.36897, acc=0.85938
# [35/100] testing 60.5% loss=0.36318, acc=0.85938
# [35/100] testing 61.4% loss=0.16288, acc=0.92188
# [35/100] testing 61.9% loss=0.25722, acc=0.89062
# [35/100] testing 62.7% loss=0.23605, acc=0.92188
# [35/100] testing 63.2% loss=0.42700, acc=0.82812
# [35/100] testing 64.0% loss=0.46512, acc=0.87500
# [35/100] testing 64.5% loss=0.12504, acc=0.95312
# [35/100] testing 65.4% loss=0.24799, acc=0.92188
# [35/100] testing 65.8% loss=0.33935, acc=0.84375
# [35/100] testing 66.7% loss=0.18130, acc=0.89062
# [35/100] testing 67.6% loss=0.33514, acc=0.87500
# [35/100] testing 68.0% loss=0.15458, acc=0.95312
# [35/100] testing 68.9% loss=0.24045, acc=0.92188
# [35/100] testing 69.3% loss=0.36616, acc=0.85938
# [35/100] testing 70.2% loss=0.40112, acc=0.87500
# [35/100] testing 70.6% loss=0.33820, acc=0.87500
# [35/100] testing 71.5% loss=0.38475, acc=0.85938
# [35/100] testing 72.4% loss=0.19611, acc=0.92188
# [35/100] testing 72.8% loss=0.17914, acc=0.93750
# [35/100] testing 73.7% loss=0.21791, acc=0.93750
# [35/100] testing 74.1% loss=0.36336, acc=0.85938
# [35/100] testing 75.0% loss=0.22012, acc=0.90625
# [35/100] testing 75.4% loss=0.52659, acc=0.84375
# [35/100] testing 76.3% loss=0.11201, acc=0.96875
# [35/100] testing 76.8% loss=0.31296, acc=0.85938
# [35/100] testing 77.6% loss=0.27577, acc=0.89062
# [35/100] testing 78.5% loss=0.38579, acc=0.81250
# [35/100] testing 79.0% loss=0.28390, acc=0.90625
# [35/100] testing 79.8% loss=0.24200, acc=0.90625
# [35/100] testing 80.3% loss=0.25939, acc=0.92188
# [35/100] testing 81.2% loss=0.43759, acc=0.84375
# [35/100] testing 81.6% loss=0.25015, acc=0.93750
# [35/100] testing 82.5% loss=0.20894, acc=0.90625
# [35/100] testing 83.3% loss=0.25027, acc=0.93750
# [35/100] testing 83.8% loss=0.16135, acc=0.93750
# [35/100] testing 84.7% loss=0.36328, acc=0.87500
# [35/100] testing 85.1% loss=0.29258, acc=0.87500
# [35/100] testing 86.0% loss=0.39932, acc=0.82812
# [35/100] testing 86.4% loss=0.33207, acc=0.84375
# [35/100] testing 87.3% loss=0.32174, acc=0.82812
# [35/100] testing 87.7% loss=0.21017, acc=0.89062
# [35/100] testing 88.6% loss=0.29736, acc=0.85938
# [35/100] testing 89.5% loss=0.49795, acc=0.78125
# [35/100] testing 89.9% loss=0.25980, acc=0.87500
# [35/100] testing 90.8% loss=0.33253, acc=0.92188
# [35/100] testing 91.2% loss=0.15196, acc=0.92188
# [35/100] testing 92.1% loss=0.33949, acc=0.89062
# [35/100] testing 92.6% loss=0.34166, acc=0.89062
# [35/100] testing 93.4% loss=0.43988, acc=0.84375
# [35/100] testing 94.3% loss=0.11755, acc=0.93750
# [35/100] testing 94.7% loss=0.21215, acc=0.89062
# [35/100] testing 95.6% loss=0.28741, acc=0.84375
# [35/100] testing 96.1% loss=0.23958, acc=0.84375
# [35/100] testing 96.9% loss=0.28643, acc=0.89062
# [35/100] testing 97.4% loss=0.18834, acc=0.93750
# [35/100] testing 98.3% loss=0.31030, acc=0.85938
# [35/100] testing 98.7% loss=0.26223, acc=0.89062
# [35/100] testing 99.6% loss=0.31780, acc=0.90625
# [36/100] training 0.2% loss=0.37730, acc=0.82812
# [36/100] training 0.4% loss=0.32474, acc=0.87500
# [36/100] training 0.5% loss=0.17661, acc=0.95312
# [36/100] training 0.8% loss=0.18675, acc=0.92188
# [36/100] training 0.9% loss=0.19590, acc=0.90625
# [36/100] training 1.1% loss=0.28045, acc=0.93750
# [36/100] training 1.2% loss=0.24660, acc=0.85938
# [36/100] training 1.4% loss=0.17078, acc=0.93750
# [36/100] training 1.6% loss=0.15671, acc=0.92188
# [36/100] training 1.8% loss=0.20290, acc=0.92188
# [36/100] training 2.0% loss=0.30586, acc=0.84375
# [36/100] training 2.1% loss=0.18079, acc=0.90625
# [36/100] training 2.3% loss=0.20249, acc=0.90625
# [36/100] training 2.4% loss=0.17893, acc=0.90625
# [36/100] training 2.6% loss=0.11125, acc=0.96875
# [36/100] training 2.7% loss=0.24069, acc=0.92188
# [36/100] training 3.0% loss=0.23316, acc=0.93750
# [36/100] training 3.2% loss=0.23140, acc=0.87500
# [36/100] training 3.3% loss=0.27564, acc=0.89062
# [36/100] training 3.5% loss=0.29121, acc=0.85938
# [36/100] training 3.6% loss=0.24344, acc=0.89062
# [36/100] training 3.8% loss=0.21982, acc=0.90625
# [36/100] training 3.9% loss=0.12530, acc=0.96875
# [36/100] training 4.2% loss=0.15022, acc=0.95312
# [36/100] training 4.4% loss=0.16505, acc=0.96875
# [36/100] training 4.5% loss=0.16964, acc=0.90625
# [36/100] training 4.7% loss=0.36723, acc=0.85938
# [36/100] training 4.8% loss=0.33181, acc=0.90625
# [36/100] training 5.0% loss=0.11936, acc=0.93750
# [36/100] training 5.2% loss=0.21601, acc=0.90625
# [36/100] training 5.4% loss=0.11582, acc=0.96875
# [36/100] training 5.5% loss=0.20740, acc=0.90625
# [36/100] training 5.7% loss=0.16033, acc=0.95312
# [36/100] training 5.9% loss=0.23149, acc=0.90625
# [36/100] training 6.0% loss=0.18337, acc=0.92188
# [36/100] training 6.3% loss=0.30561, acc=0.85938
# [36/100] training 6.4% loss=0.19919, acc=0.93750
# [36/100] training 6.6% loss=0.20618, acc=0.90625
# [36/100] training 6.7% loss=0.33776, acc=0.87500
# [36/100] training 6.9% loss=0.15029, acc=0.93750
# [36/100] training 7.1% loss=0.23719, acc=0.89062
# [36/100] training 7.2% loss=0.34696, acc=0.84375
# [36/100] training 7.5% loss=0.26648, acc=0.90625
# [36/100] training 7.6% loss=0.28235, acc=0.87500
# [36/100] training 7.8% loss=0.20197, acc=0.90625
# [36/100] training 7.9% loss=0.24686, acc=0.87500
# [36/100] training 8.1% loss=0.14957, acc=0.95312
# [36/100] training 8.2% loss=0.22645, acc=0.90625
# [36/100] training 8.4% loss=0.24001, acc=0.90625
# [36/100] training 8.7% loss=0.23140, acc=0.87500
# [36/100] training 8.8% loss=0.22391, acc=0.89062
# [36/100] training 9.0% loss=0.18559, acc=0.87500
# [36/100] training 9.1% loss=0.27433, acc=0.92188
# [36/100] training 9.3% loss=0.45703, acc=0.81250
# [36/100] training 9.4% loss=0.14173, acc=0.92188
# [36/100] training 9.7% loss=0.21679, acc=0.90625
# [36/100] training 9.9% loss=0.22120, acc=0.90625
# [36/100] training 10.0% loss=0.19301, acc=0.92188
# [36/100] training 10.2% loss=0.17280, acc=0.95312
# [36/100] training 10.3% loss=0.19970, acc=0.92188
# [36/100] training 10.5% loss=0.31329, acc=0.90625
# [36/100] training 10.6% loss=0.18257, acc=0.93750
# [36/100] training 10.9% loss=0.14619, acc=0.93750
# [36/100] training 11.0% loss=0.24808, acc=0.92188
# [36/100] training 11.2% loss=0.11402, acc=0.96875
# [36/100] training 11.4% loss=0.21706, acc=0.92188
# [36/100] training 11.5% loss=0.34055, acc=0.85938
# [36/100] training 11.7% loss=0.05609, acc=0.96875
# [36/100] training 11.8% loss=0.20788, acc=0.95312
# [36/100] training 12.1% loss=0.24076, acc=0.93750
# [36/100] training 12.2% loss=0.12873, acc=0.92188
# [36/100] training 12.4% loss=0.17567, acc=0.90625
# [36/100] training 12.6% loss=0.23608, acc=0.92188
# [36/100] training 12.7% loss=0.23163, acc=0.89062
# [36/100] training 12.9% loss=0.27495, acc=0.92188
# [36/100] training 13.0% loss=0.14293, acc=0.96875
# [36/100] training 13.3% loss=0.19703, acc=0.95312
# [36/100] training 13.4% loss=0.28014, acc=0.87500
# [36/100] training 13.6% loss=0.19404, acc=0.93750
# [36/100] training 13.7% loss=0.26199, acc=0.89062
# [36/100] training 13.9% loss=0.20218, acc=0.93750
# [36/100] training 14.1% loss=0.34464, acc=0.92188
# [36/100] training 14.3% loss=0.14052, acc=0.95312
# [36/100] training 14.5% loss=0.18486, acc=0.93750
# [36/100] training 14.6% loss=0.13173, acc=0.95312
# [36/100] training 14.8% loss=0.22640, acc=0.92188
# [36/100] training 14.9% loss=0.16224, acc=0.95312
# [36/100] training 15.1% loss=0.20363, acc=0.92188
# [36/100] training 15.4% loss=0.23633, acc=0.87500
# [36/100] training 15.5% loss=0.22907, acc=0.90625
# [36/100] training 15.7% loss=0.30634, acc=0.90625
# [36/100] training 15.8% loss=0.11266, acc=0.90625
# [36/100] training 16.0% loss=0.27036, acc=0.87500
# [36/100] training 16.1% loss=0.40802, acc=0.82812
# [36/100] training 16.3% loss=0.24797, acc=0.92188
# [36/100] training 16.4% loss=0.16792, acc=0.93750
# [36/100] training 16.7% loss=0.30055, acc=0.84375
# [36/100] training 16.9% loss=0.23790, acc=0.93750
# [36/100] training 17.0% loss=0.21584, acc=0.89062
# [36/100] training 17.2% loss=0.15905, acc=0.90625
# [36/100] training 17.3% loss=0.17479, acc=0.89062
# [36/100] training 17.5% loss=0.25108, acc=0.92188
# [36/100] training 17.7% loss=0.17846, acc=0.92188
# [36/100] training 17.9% loss=0.23840, acc=0.90625
# [36/100] training 18.1% loss=0.30479, acc=0.87500
# [36/100] training 18.2% loss=0.19981, acc=0.93750
# [36/100] training 18.4% loss=0.27308, acc=0.84375
# [36/100] training 18.5% loss=0.16166, acc=0.95312
# [36/100] training 18.8% loss=0.17747, acc=0.93750
# [36/100] training 18.9% loss=0.13577, acc=0.92188
# [36/100] training 19.1% loss=0.30957, acc=0.84375
# [36/100] training 19.2% loss=0.12771, acc=0.93750
# [36/100] training 19.4% loss=0.14687, acc=0.92188
# [36/100] training 19.6% loss=0.23779, acc=0.92188
# [36/100] training 19.7% loss=0.25266, acc=0.90625
# [36/100] training 20.0% loss=0.15682, acc=0.92188
# [36/100] training 20.1% loss=0.21878, acc=0.89062
# [36/100] training 20.3% loss=0.24099, acc=0.92188
# [36/100] training 20.4% loss=0.21819, acc=0.89062
# [36/100] training 20.6% loss=0.33697, acc=0.89062
# [36/100] training 20.8% loss=0.19473, acc=0.93750
# [36/100] training 20.9% loss=0.20958, acc=0.93750
# [36/100] training 21.2% loss=0.33041, acc=0.92188
# [36/100] training 21.3% loss=0.22366, acc=0.89062
# [36/100] training 21.5% loss=0.41319, acc=0.87500
# [36/100] training 21.6% loss=0.13953, acc=0.96875
# [36/100] training 21.8% loss=0.14414, acc=0.93750
# [36/100] training 21.9% loss=0.24254, acc=0.89062
# [36/100] training 22.2% loss=0.21709, acc=0.93750
# [36/100] training 22.4% loss=0.23719, acc=0.89062
# [36/100] training 22.5% loss=0.17141, acc=0.93750
# [36/100] training 22.7% loss=0.25980, acc=0.92188
# [36/100] training 22.8% loss=0.21531, acc=0.89062
# [36/100] training 23.0% loss=0.12405, acc=0.93750
# [36/100] training 23.1% loss=0.23976, acc=0.92188
# [36/100] training 23.4% loss=0.38701, acc=0.85938
# [36/100] training 23.6% loss=0.31375, acc=0.92188
# [36/100] training 23.7% loss=0.22478, acc=0.90625
# [36/100] training 23.9% loss=0.17184, acc=0.89062
# [36/100] training 24.0% loss=0.21763, acc=0.90625
# [36/100] training 24.2% loss=0.13538, acc=0.95312
# [36/100] training 24.3% loss=0.20494, acc=0.92188
# [36/100] training 24.6% loss=0.28325, acc=0.87500
# [36/100] training 24.7% loss=0.30067, acc=0.89062
# [36/100] training 24.9% loss=0.22944, acc=0.89062
# [36/100] training 25.1% loss=0.24682, acc=0.87500
# [36/100] training 25.2% loss=0.15353, acc=0.96875
# [36/100] training 25.4% loss=0.24685, acc=0.92188
# [36/100] training 25.6% loss=0.12758, acc=0.93750
# [36/100] training 25.8% loss=0.20029, acc=0.90625
# [36/100] training 25.9% loss=0.15955, acc=0.93750
# [36/100] training 26.1% loss=0.23631, acc=0.90625
# [36/100] training 26.3% loss=0.16924, acc=0.90625
# [36/100] training 26.4% loss=0.14311, acc=0.93750
# [36/100] training 26.6% loss=0.12938, acc=0.93750
# [36/100] training 26.8% loss=0.22055, acc=0.85938
# [36/100] training 27.0% loss=0.20859, acc=0.92188
# [36/100] training 27.1% loss=0.10923, acc=0.95312
# [36/100] training 27.3% loss=0.18842, acc=0.92188
# [36/100] training 27.4% loss=0.10835, acc=0.93750
# [36/100] training 27.6% loss=0.23187, acc=0.95312
# [36/100] training 27.9% loss=0.20271, acc=0.89062
# [36/100] training 28.0% loss=0.27276, acc=0.92188
# [36/100] training 28.2% loss=0.19127, acc=0.92188
# [36/100] training 28.3% loss=0.09250, acc=0.96875
# [36/100] training 28.5% loss=0.24630, acc=0.93750
# [36/100] training 28.6% loss=0.17332, acc=0.96875
# [36/100] training 28.8% loss=0.10082, acc=0.95312
# [36/100] training 29.1% loss=0.10400, acc=0.98438
# [36/100] training 29.2% loss=0.17509, acc=0.89062
# [36/100] training 29.4% loss=0.27519, acc=0.84375
# [36/100] training 29.5% loss=0.12157, acc=0.92188
# [36/100] training 29.7% loss=0.23560, acc=0.89062
# [36/100] training 29.8% loss=0.16383, acc=0.95312
# [36/100] training 30.0% loss=0.25101, acc=0.85938
# [36/100] training 30.2% loss=0.11047, acc=0.96875
# [36/100] training 30.4% loss=0.17285, acc=0.92188
# [36/100] training 30.6% loss=0.22487, acc=0.87500
# [36/100] training 30.7% loss=0.15792, acc=0.95312
# [36/100] training 30.9% loss=0.43926, acc=0.89062
# [36/100] training 31.0% loss=0.11965, acc=0.92188
# [36/100] training 31.3% loss=0.18075, acc=0.95312
# [36/100] training 31.4% loss=0.38209, acc=0.87500
# [36/100] training 31.6% loss=0.34401, acc=0.89062
# [36/100] training 31.8% loss=0.19808, acc=0.90625
# [36/100] training 31.9% loss=0.22923, acc=0.90625
# [36/100] training 32.1% loss=0.18923, acc=0.92188
# [36/100] training 32.2% loss=0.24770, acc=0.90625
# [36/100] training 32.5% loss=0.13979, acc=0.95312
# [36/100] training 32.6% loss=0.23561, acc=0.92188
# [36/100] training 32.8% loss=0.17262, acc=0.93750
# [36/100] training 32.9% loss=0.25119, acc=0.89062
# [36/100] training 33.1% loss=0.23136, acc=0.90625
# [36/100] training 33.3% loss=0.25849, acc=0.90625
# [36/100] training 33.4% loss=0.12555, acc=0.93750
# [36/100] training 33.7% loss=0.16319, acc=0.95312
# [36/100] training 33.8% loss=0.13896, acc=0.95312
# [36/100] training 34.0% loss=0.22301, acc=0.90625
# [36/100] training 34.1% loss=0.21976, acc=0.93750
# [36/100] training 34.3% loss=0.19806, acc=0.92188
# [36/100] training 34.5% loss=0.27161, acc=0.90625
# [36/100] training 34.7% loss=0.15651, acc=0.95312
# [36/100] training 34.9% loss=0.13742, acc=0.92188
# [36/100] training 35.0% loss=0.12753, acc=0.96875
# [36/100] training 35.2% loss=0.26330, acc=0.87500
# [36/100] training 35.3% loss=0.19206, acc=0.92188
# [36/100] training 35.5% loss=0.17470, acc=0.90625
# [36/100] training 35.6% loss=0.23923, acc=0.90625
# [36/100] training 35.9% loss=0.21561, acc=0.89062
# [36/100] training 36.1% loss=0.23296, acc=0.89062
# [36/100] training 36.2% loss=0.15722, acc=0.93750
# [36/100] training 36.4% loss=0.26006, acc=0.92188
# [36/100] training 36.5% loss=0.28218, acc=0.92188
# [36/100] training 36.7% loss=0.24411, acc=0.89062
# [36/100] training 36.8% loss=0.09267, acc=0.96875
# [36/100] training 37.1% loss=0.24900, acc=0.87500
# [36/100] training 37.3% loss=0.22822, acc=0.90625
# [36/100] training 37.4% loss=0.17963, acc=0.90625
# [36/100] training 37.6% loss=0.26633, acc=0.85938
# [36/100] training 37.7% loss=0.26327, acc=0.90625
# [36/100] training 37.9% loss=0.19314, acc=0.93750
# [36/100] training 38.1% loss=0.27837, acc=0.92188
# [36/100] training 38.3% loss=0.19633, acc=0.92188
# [36/100] training 38.4% loss=0.07760, acc=0.95312
# [36/100] training 38.6% loss=0.17039, acc=0.92188
# [36/100] training 38.8% loss=0.27949, acc=0.89062
# [36/100] training 38.9% loss=0.13791, acc=0.95312
# [36/100] training 39.1% loss=0.22201, acc=0.89062
# [36/100] training 39.3% loss=0.23121, acc=0.89062
# [36/100] training 39.5% loss=0.21273, acc=0.92188
# [36/100] training 39.6% loss=0.20962, acc=0.93750
# [36/100] training 39.8% loss=0.16186, acc=0.92188
# [36/100] training 40.0% loss=0.15746, acc=0.90625
# [36/100] training 40.1% loss=0.27384, acc=0.89062
# [36/100] training 40.4% loss=0.21418, acc=0.89062
# [36/100] training 40.5% loss=0.32077, acc=0.85938
# [36/100] training 40.7% loss=0.24716, acc=0.92188
# [36/100] training 40.8% loss=0.12058, acc=0.95312
# [36/100] training 41.0% loss=0.16513, acc=0.92188
# [36/100] training 41.1% loss=0.27371, acc=0.90625
# [36/100] training 41.3% loss=0.23415, acc=0.89062
# [36/100] training 41.6% loss=0.20043, acc=0.92188
# [36/100] training 41.7% loss=0.25698, acc=0.89062
# [36/100] training 41.9% loss=0.15324, acc=0.95312
# [36/100] training 42.0% loss=0.27169, acc=0.87500
# [36/100] training 42.2% loss=0.19915, acc=0.95312
# [36/100] training 42.3% loss=0.19573, acc=0.93750
# [36/100] training 42.5% loss=0.11897, acc=0.95312
# [36/100] training 42.8% loss=0.07387, acc=1.00000
# [36/100] training 42.9% loss=0.13094, acc=0.93750
# [36/100] training 43.1% loss=0.18305, acc=0.95312
# [36/100] training 43.2% loss=0.16297, acc=0.92188
# [36/100] training 43.4% loss=0.21848, acc=0.95312
# [36/100] training 43.5% loss=0.22278, acc=0.89062
# [36/100] training 43.8% loss=0.19807, acc=0.93750
# [36/100] training 43.9% loss=0.15667, acc=0.95312
# [36/100] training 44.1% loss=0.11392, acc=0.96875
# [36/100] training 44.3% loss=0.13216, acc=0.95312
# [36/100] training 44.4% loss=0.30984, acc=0.85938
# [36/100] training 44.6% loss=0.24446, acc=0.85938
# [36/100] training 44.7% loss=0.31250, acc=0.89062
# [36/100] training 45.0% loss=0.11209, acc=0.93750
# [36/100] training 45.1% loss=0.34828, acc=0.87500
# [36/100] training 45.3% loss=0.26475, acc=0.92188
# [36/100] training 45.5% loss=0.08827, acc=0.98438
# [36/100] training 45.6% loss=0.24366, acc=0.90625
# [36/100] training 45.8% loss=0.09749, acc=0.98438
# [36/100] training 45.9% loss=0.17483, acc=0.92188
# [36/100] training 46.2% loss=0.12130, acc=0.95312
# [36/100] training 46.3% loss=0.10299, acc=0.96875
# [36/100] training 46.5% loss=0.32775, acc=0.87500
# [36/100] training 46.6% loss=0.18971, acc=0.90625
# [36/100] training 46.8% loss=0.17583, acc=0.92188
# [36/100] training 47.0% loss=0.19336, acc=0.90625
# [36/100] training 47.2% loss=0.17665, acc=0.90625
# [36/100] training 47.4% loss=0.14273, acc=0.93750
# [36/100] training 47.5% loss=0.28004, acc=0.92188
# [36/100] training 47.7% loss=0.15070, acc=0.93750
# [36/100] training 47.8% loss=0.37050, acc=0.89062
# [36/100] training 48.0% loss=0.17833, acc=0.90625
# [36/100] training 48.3% loss=0.12532, acc=0.93750
# [36/100] training 48.4% loss=0.08435, acc=1.00000
# [36/100] training 48.6% loss=0.13075, acc=0.95312
# [36/100] training 48.7% loss=0.21433, acc=0.93750
# [36/100] training 48.9% loss=0.14549, acc=0.92188
# [36/100] training 49.0% loss=0.11010, acc=0.93750
# [36/100] training 49.2% loss=0.29595, acc=0.87500
# [36/100] training 49.3% loss=0.18334, acc=0.93750
# [36/100] training 49.6% loss=0.19531, acc=0.93750
# [36/100] training 49.8% loss=0.24239, acc=0.90625
# [36/100] training 49.9% loss=0.30180, acc=0.89062
# [36/100] training 50.1% loss=0.14751, acc=0.92188
# [36/100] training 50.2% loss=0.20208, acc=0.90625
# [36/100] training 50.4% loss=0.29431, acc=0.89062
# [36/100] training 50.6% loss=0.16646, acc=0.93750
# [36/100] training 50.8% loss=0.23220, acc=0.85938
# [36/100] training 51.0% loss=0.26935, acc=0.87500
# [36/100] training 51.1% loss=0.18445, acc=0.93750
# [36/100] training 51.3% loss=0.18739, acc=0.89062
# [36/100] training 51.4% loss=0.25286, acc=0.93750
# [36/100] training 51.7% loss=0.24884, acc=0.90625
# [36/100] training 51.8% loss=0.23218, acc=0.90625
# [36/100] training 52.0% loss=0.30034, acc=0.89062
# [36/100] training 52.1% loss=0.19268, acc=0.95312
# [36/100] training 52.3% loss=0.25880, acc=0.89062
# [36/100] training 52.5% loss=0.07510, acc=1.00000
# [36/100] training 52.6% loss=0.16823, acc=0.92188
# [36/100] training 52.9% loss=0.32346, acc=0.87500
# [36/100] training 53.0% loss=0.17995, acc=0.93750
# [36/100] training 53.2% loss=0.19202, acc=0.95312
# [36/100] training 53.3% loss=0.18355, acc=0.93750
# [36/100] training 53.5% loss=0.15263, acc=0.92188
# [36/100] training 53.7% loss=0.07499, acc=0.96875
# [36/100] training 53.8% loss=0.24219, acc=0.90625
# [36/100] training 54.1% loss=0.24556, acc=0.89062
# [36/100] training 54.2% loss=0.08167, acc=0.98438
# [36/100] training 54.4% loss=0.15908, acc=0.93750
# [36/100] training 54.5% loss=0.20520, acc=0.92188
# [36/100] training 54.7% loss=0.24422, acc=0.89062
# [36/100] training 54.8% loss=0.12519, acc=0.96875
# [36/100] training 55.1% loss=0.17260, acc=0.92188
# [36/100] training 55.3% loss=0.18208, acc=0.90625
# [36/100] training 55.4% loss=0.12684, acc=0.95312
# [36/100] training 55.6% loss=0.19072, acc=0.95312
# [36/100] training 55.7% loss=0.14233, acc=0.92188
# [36/100] training 55.9% loss=0.17879, acc=0.93750
# [36/100] training 56.0% loss=0.09841, acc=0.95312
# [36/100] training 56.3% loss=0.36347, acc=0.89062
# [36/100] training 56.5% loss=0.19223, acc=0.92188
# [36/100] training 56.6% loss=0.28670, acc=0.85938
# [36/100] training 56.8% loss=0.21545, acc=0.89062
# [36/100] training 56.9% loss=0.26364, acc=0.87500
# [36/100] training 57.1% loss=0.24223, acc=0.92188
# [36/100] training 57.2% loss=0.20475, acc=0.92188
# [36/100] training 57.5% loss=0.23060, acc=0.92188
# [36/100] training 57.6% loss=0.23333, acc=0.85938
# [36/100] training 57.8% loss=0.16090, acc=0.95312
# [36/100] training 58.0% loss=0.10483, acc=0.96875
# [36/100] training 58.1% loss=0.16518, acc=0.93750
# [36/100] training 58.3% loss=0.12550, acc=0.93750
# [36/100] training 58.4% loss=0.23408, acc=0.93750
# [36/100] training 58.7% loss=0.27459, acc=0.93750
# [36/100] training 58.8% loss=0.23434, acc=0.93750
# [36/100] training 59.0% loss=0.19802, acc=0.89062
# [36/100] training 59.2% loss=0.17146, acc=0.92188
# [36/100] training 59.3% loss=0.19901, acc=0.90625
# [36/100] training 59.5% loss=0.23936, acc=0.89062
# [36/100] training 59.7% loss=0.23162, acc=0.92188
# [36/100] training 59.9% loss=0.14189, acc=0.93750
# [36/100] training 60.0% loss=0.22242, acc=0.92188
# [36/100] training 60.2% loss=0.26428, acc=0.89062
# [36/100] training 60.3% loss=0.17727, acc=0.95312
# [36/100] training 60.5% loss=0.30254, acc=0.87500
# [36/100] training 60.8% loss=0.21790, acc=0.87500
# [36/100] training 60.9% loss=0.25631, acc=0.89062
# [36/100] training 61.1% loss=0.22714, acc=0.89062
# [36/100] training 61.2% loss=0.20075, acc=0.89062
# [36/100] training 61.4% loss=0.24725, acc=0.87500
# [36/100] training 61.5% loss=0.37057, acc=0.85938
# [36/100] training 61.7% loss=0.17657, acc=0.95312
# [36/100] training 62.0% loss=0.25388, acc=0.85938
# [36/100] training 62.1% loss=0.23551, acc=0.87500
# [36/100] training 62.3% loss=0.07501, acc=0.98438
# [36/100] training 62.4% loss=0.23534, acc=0.87500
# [36/100] training 62.6% loss=0.25872, acc=0.93750
# [36/100] training 62.7% loss=0.15951, acc=0.96875
# [36/100] training 62.9% loss=0.16996, acc=0.92188
# [36/100] training 63.1% loss=0.28000, acc=0.89062
# [36/100] training 63.3% loss=0.12435, acc=0.96875
# [36/100] training 63.5% loss=0.19898, acc=0.92188
# [36/100] training 63.6% loss=0.25847, acc=0.92188
# [36/100] training 63.8% loss=0.34464, acc=0.89062
# [36/100] training 63.9% loss=0.11835, acc=0.95312
# [36/100] training 64.2% loss=0.23180, acc=0.87500
# [36/100] training 64.3% loss=0.27886, acc=0.90625
# [36/100] training 64.5% loss=0.16827, acc=0.92188
# [36/100] training 64.7% loss=0.20740, acc=0.92188
# [36/100] training 64.8% loss=0.33389, acc=0.87500
# [36/100] training 65.0% loss=0.19004, acc=0.92188
# [36/100] training 65.1% loss=0.31909, acc=0.90625
# [36/100] training 65.4% loss=0.18085, acc=0.93750
# [36/100] training 65.5% loss=0.13531, acc=0.95312
# [36/100] training 65.7% loss=0.16383, acc=0.93750
# [36/100] training 65.8% loss=0.27586, acc=0.81250
# [36/100] training 66.0% loss=0.13629, acc=0.95312
# [36/100] training 66.2% loss=0.14568, acc=0.96875
# [36/100] training 66.3% loss=0.20544, acc=0.87500
# [36/100] training 66.6% loss=0.17701, acc=0.95312
# [36/100] training 66.7% loss=0.16530, acc=0.93750
# [36/100] training 66.9% loss=0.18321, acc=0.90625
# [36/100] training 67.0% loss=0.28640, acc=0.82812
# [36/100] training 67.2% loss=0.12833, acc=0.93750
# [36/100] training 67.4% loss=0.17215, acc=0.93750
# [36/100] training 67.6% loss=0.22098, acc=0.92188
# [36/100] training 67.8% loss=0.11797, acc=0.95312
# [36/100] training 67.9% loss=0.08425, acc=0.96875
# [36/100] training 68.1% loss=0.31241, acc=0.92188
# [36/100] training 68.2% loss=0.06130, acc=0.98438
# [36/100] training 68.4% loss=0.11259, acc=0.95312
# [36/100] training 68.5% loss=0.27226, acc=0.89062
# [36/100] training 68.8% loss=0.22915, acc=0.92188
# [36/100] training 69.0% loss=0.27845, acc=0.90625
# [36/100] training 69.1% loss=0.14452, acc=0.90625
# [36/100] training 69.3% loss=0.20801, acc=0.90625
# [36/100] training 69.4% loss=0.20434, acc=0.87500
# [36/100] training 69.6% loss=0.24804, acc=0.87500
# [36/100] training 69.7% loss=0.19564, acc=0.90625
# [36/100] training 70.0% loss=0.26810, acc=0.84375
# [36/100] training 70.2% loss=0.21069, acc=0.92188
# [36/100] training 70.3% loss=0.16966, acc=0.92188
# [36/100] training 70.5% loss=0.21163, acc=0.89062
# [36/100] training 70.6% loss=0.16046, acc=0.93750
# [36/100] training 70.8% loss=0.21317, acc=0.87500
# [36/100] training 71.0% loss=0.31722, acc=0.90625
# [36/100] training 71.2% loss=0.19121, acc=0.89062
# [36/100] training 71.3% loss=0.10397, acc=0.96875
# [36/100] training 71.5% loss=0.19192, acc=0.93750
# [36/100] training 71.7% loss=0.23906, acc=0.92188
# [36/100] training 71.8% loss=0.23345, acc=0.92188
# [36/100] training 72.0% loss=0.09294, acc=0.98438
# [36/100] training 72.2% loss=0.22813, acc=0.90625
# [36/100] training 72.4% loss=0.20487, acc=0.87500
# [36/100] training 72.5% loss=0.21374, acc=0.92188
# [36/100] training 72.7% loss=0.32969, acc=0.85938
# [36/100] training 72.9% loss=0.16279, acc=0.93750
# [36/100] training 73.0% loss=0.16081, acc=0.92188
# [36/100] training 73.3% loss=0.36379, acc=0.84375
# [36/100] training 73.4% loss=0.11766, acc=0.93750
# [36/100] training 73.6% loss=0.21080, acc=0.92188
# [36/100] training 73.7% loss=0.20413, acc=0.93750
# [36/100] training 73.9% loss=0.13066, acc=0.96875
# [36/100] training 74.0% loss=0.22283, acc=0.89062
# [36/100] training 74.2% loss=0.10989, acc=0.96875
# [36/100] training 74.5% loss=0.16111, acc=0.93750
# [36/100] training 74.6% loss=0.31209, acc=0.87500
# [36/100] training 74.8% loss=0.29912, acc=0.89062
# [36/100] training 74.9% loss=0.29887, acc=0.85938
# [36/100] training 75.1% loss=0.17038, acc=0.95312
# [36/100] training 75.2% loss=0.12998, acc=0.92188
# [36/100] training 75.4% loss=0.26972, acc=0.90625
# [36/100] training 75.7% loss=0.23605, acc=0.89062
# [36/100] training 75.8% loss=0.24289, acc=0.93750
# [36/100] training 76.0% loss=0.19090, acc=0.92188
# [36/100] training 76.1% loss=0.24053, acc=0.90625
# [36/100] training 76.3% loss=0.08573, acc=0.98438
# [36/100] training 76.4% loss=0.16439, acc=0.93750
# [36/100] training 76.7% loss=0.21576, acc=0.89062
# [36/100] training 76.8% loss=0.13277, acc=0.95312
# [36/100] training 77.0% loss=0.11124, acc=0.95312
# [36/100] training 77.2% loss=0.24058, acc=0.92188
# [36/100] training 77.3% loss=0.09191, acc=0.96875
# [36/100] training 77.5% loss=0.31728, acc=0.87500
# [36/100] training 77.6% loss=0.23383, acc=0.87500
# [36/100] training 77.9% loss=0.16017, acc=0.92188
# [36/100] training 78.0% loss=0.20065, acc=0.90625
# [36/100] training 78.2% loss=0.28646, acc=0.90625
# [36/100] training 78.4% loss=0.11790, acc=0.95312
# [36/100] training 78.5% loss=0.28297, acc=0.89062
# [36/100] training 78.7% loss=0.24465, acc=0.85938
# [36/100] training 78.8% loss=0.07971, acc=1.00000
# [36/100] training 79.1% loss=0.07271, acc=0.98438
# [36/100] training 79.2% loss=0.10422, acc=0.96875
# [36/100] training 79.4% loss=0.19661, acc=0.92188
# [36/100] training 79.5% loss=0.13771, acc=0.92188
# [36/100] training 79.7% loss=0.06429, acc=0.98438
# [36/100] training 79.9% loss=0.16345, acc=0.90625
# [36/100] training 80.1% loss=0.11203, acc=0.96875
# [36/100] training 80.3% loss=0.23851, acc=0.90625
# [36/100] training 80.4% loss=0.15481, acc=0.93750
# [36/100] training 80.6% loss=0.18655, acc=0.90625
# [36/100] training 80.7% loss=0.18037, acc=0.95312
# [36/100] training 80.9% loss=0.16443, acc=0.92188
# [36/100] training 81.2% loss=0.24923, acc=0.89062
# [36/100] training 81.3% loss=0.25885, acc=0.85938
# [36/100] training 81.5% loss=0.16581, acc=0.92188
# [36/100] training 81.6% loss=0.32566, acc=0.84375
# [36/100] training 81.8% loss=0.21430, acc=0.93750
# [36/100] training 81.9% loss=0.55287, acc=0.90625
# [36/100] training 82.1% loss=0.14479, acc=0.98438
# [36/100] training 82.2% loss=0.24990, acc=0.90625
# [36/100] training 82.5% loss=0.09940, acc=0.96875
# [36/100] training 82.7% loss=0.31278, acc=0.87500
# [36/100] training 82.8% loss=0.17815, acc=0.89062
# [36/100] training 83.0% loss=0.17169, acc=0.92188
# [36/100] training 83.1% loss=0.15714, acc=0.96875
# [36/100] training 83.3% loss=0.14982, acc=0.95312
# [36/100] training 83.5% loss=0.12117, acc=0.96875
# [36/100] training 83.7% loss=0.27969, acc=0.87500
# [36/100] training 83.9% loss=0.24321, acc=0.89062
# [36/100] training 84.0% loss=0.11204, acc=0.96875
# [36/100] training 84.2% loss=0.10196, acc=0.95312
# [36/100] training 84.3% loss=0.16729, acc=0.90625
# [36/100] training 84.5% loss=0.14239, acc=0.95312
# [36/100] training 84.7% loss=0.12378, acc=0.95312
# [36/100] training 84.9% loss=0.14620, acc=0.95312
# [36/100] training 85.0% loss=0.13731, acc=0.93750
# [36/100] training 85.2% loss=0.11566, acc=0.95312
# [36/100] training 85.4% loss=0.21900, acc=0.92188
# [36/100] training 85.5% loss=0.21533, acc=0.92188
# [36/100] training 85.8% loss=0.23106, acc=0.90625
# [36/100] training 85.9% loss=0.22074, acc=0.85938
# [36/100] training 86.1% loss=0.22541, acc=0.89062
# [36/100] training 86.2% loss=0.12286, acc=0.95312
# [36/100] training 86.4% loss=0.33312, acc=0.89062
# [36/100] training 86.6% loss=0.27583, acc=0.85938
# [36/100] training 86.7% loss=0.23545, acc=0.92188
# [36/100] training 87.0% loss=0.24145, acc=0.92188
# [36/100] training 87.1% loss=0.22732, acc=0.89062
# [36/100] training 87.3% loss=0.20537, acc=0.90625
# [36/100] training 87.4% loss=0.27005, acc=0.90625
# [36/100] training 87.6% loss=0.21026, acc=0.87500
# [36/100] training 87.7% loss=0.38935, acc=0.87500
# [36/100] training 87.9% loss=0.27166, acc=0.90625
# [36/100] training 88.2% loss=0.19669, acc=0.93750
# [36/100] training 88.3% loss=0.26555, acc=0.90625
# [36/100] training 88.5% loss=0.17716, acc=0.92188
# [36/100] training 88.6% loss=0.11301, acc=0.96875
# [36/100] training 88.8% loss=0.12051, acc=0.93750
# [36/100] training 88.9% loss=0.23911, acc=0.90625
# [36/100] training 89.2% loss=0.14989, acc=0.93750
# [36/100] training 89.4% loss=0.14429, acc=0.96875
# [36/100] training 89.5% loss=0.27063, acc=0.92188
# [36/100] training 89.7% loss=0.21434, acc=0.90625
# [36/100] training 89.8% loss=0.19176, acc=0.93750
# [36/100] training 90.0% loss=0.11346, acc=0.96875
# [36/100] training 90.1% loss=0.18654, acc=0.90625
# [36/100] training 90.4% loss=0.21996, acc=0.93750
# [36/100] training 90.5% loss=0.08322, acc=0.98438
# [36/100] training 90.7% loss=0.13874, acc=0.93750
# [36/100] training 90.9% loss=0.21495, acc=0.90625
# [36/100] training 91.0% loss=0.12778, acc=0.95312
# [36/100] training 91.2% loss=0.17282, acc=0.93750
# [36/100] training 91.3% loss=0.34715, acc=0.87500
# [36/100] training 91.6% loss=0.26568, acc=0.92188
# [36/100] training 91.7% loss=0.12219, acc=0.93750
# [36/100] training 91.9% loss=0.22951, acc=0.93750
# [36/100] training 92.1% loss=0.20840, acc=0.90625
# [36/100] training 92.2% loss=0.11500, acc=0.95312
# [36/100] training 92.4% loss=0.11451, acc=0.92188
# [36/100] training 92.6% loss=0.30077, acc=0.84375
# [36/100] training 92.8% loss=0.29590, acc=0.89062
# [36/100] training 92.9% loss=0.25182, acc=0.93750
# [36/100] training 93.1% loss=0.35501, acc=0.89062
# [36/100] training 93.2% loss=0.20838, acc=0.92188
# [36/100] training 93.4% loss=0.23151, acc=0.92188
# [36/100] training 93.7% loss=0.13049, acc=0.96875
# [36/100] training 93.8% loss=0.15689, acc=0.93750
# [36/100] training 94.0% loss=0.18915, acc=0.92188
# [36/100] training 94.1% loss=0.20862, acc=0.89062
# [36/100] training 94.3% loss=0.08493, acc=0.96875
# [36/100] training 94.4% loss=0.23076, acc=0.92188
# [36/100] training 94.6% loss=0.07547, acc=0.98438
# [36/100] training 94.9% loss=0.10628, acc=0.95312
# [36/100] training 95.0% loss=0.13239, acc=0.93750
# [36/100] training 95.2% loss=0.37969, acc=0.87500
# [36/100] training 95.3% loss=0.19821, acc=0.93750
# [36/100] training 95.5% loss=0.13622, acc=0.92188
# [36/100] training 95.6% loss=0.36054, acc=0.89062
# [36/100] training 95.8% loss=0.19585, acc=0.90625
# [36/100] training 96.0% loss=0.31935, acc=0.90625
# [36/100] training 96.2% loss=0.13297, acc=0.95312
# [36/100] training 96.4% loss=0.15985, acc=0.96875
# [36/100] training 96.5% loss=0.25932, acc=0.89062
# [36/100] training 96.7% loss=0.18867, acc=0.92188
# [36/100] training 96.8% loss=0.25326, acc=0.90625
# [36/100] training 97.1% loss=0.17705, acc=0.93750
# [36/100] training 97.2% loss=0.20444, acc=0.93750
# [36/100] training 97.4% loss=0.27890, acc=0.89062
# [36/100] training 97.6% loss=0.25876, acc=0.87500
# [36/100] training 97.7% loss=0.16446, acc=0.92188
# [36/100] training 97.9% loss=0.14713, acc=0.95312
# [36/100] training 98.0% loss=0.12186, acc=0.96875
# [36/100] training 98.3% loss=0.23478, acc=0.90625
# [36/100] training 98.4% loss=0.21925, acc=0.89062
# [36/100] training 98.6% loss=0.36508, acc=0.87500
# [36/100] training 98.7% loss=0.27032, acc=0.90625
# [36/100] training 98.9% loss=0.15158, acc=0.93750
# [36/100] training 99.1% loss=0.12959, acc=0.96875
# [36/100] training 99.2% loss=0.17998, acc=0.92188
# [36/100] training 99.5% loss=0.31655, acc=0.85938
# [36/100] training 99.6% loss=0.22344, acc=0.92188
# [36/100] training 99.8% loss=0.19745, acc=0.90625
# [36/100] training 99.9% loss=0.10154, acc=0.95312
# [36/100] testing 0.9% loss=0.16622, acc=0.93750
# [36/100] testing 1.8% loss=0.41276, acc=0.85938
# [36/100] testing 2.2% loss=0.26266, acc=0.89062
# [36/100] testing 3.1% loss=0.39226, acc=0.84375
# [36/100] testing 3.5% loss=0.13794, acc=0.93750
# [36/100] testing 4.4% loss=0.17909, acc=0.92188
# [36/100] testing 4.8% loss=0.28489, acc=0.85938
# [36/100] testing 5.7% loss=0.25528, acc=0.85938
# [36/100] testing 6.6% loss=0.13893, acc=0.95312
# [36/100] testing 7.0% loss=0.11602, acc=0.95312
# [36/100] testing 7.9% loss=0.34474, acc=0.82812
# [36/100] testing 8.3% loss=0.35763, acc=0.84375
# [36/100] testing 9.2% loss=0.38347, acc=0.87500
# [36/100] testing 9.7% loss=0.08618, acc=0.96875
# [36/100] testing 10.5% loss=0.22071, acc=0.90625
# [36/100] testing 11.0% loss=0.27355, acc=0.89062
# [36/100] testing 11.8% loss=0.17476, acc=0.90625
# [36/100] testing 12.7% loss=0.32317, acc=0.90625
# [36/100] testing 13.2% loss=0.20034, acc=0.90625
# [36/100] testing 14.0% loss=0.36407, acc=0.90625
# [36/100] testing 14.5% loss=0.31598, acc=0.85938
# [36/100] testing 15.4% loss=0.28356, acc=0.87500
# [36/100] testing 15.8% loss=0.26864, acc=0.89062
# [36/100] testing 16.7% loss=0.23481, acc=0.90625
# [36/100] testing 17.5% loss=0.17581, acc=0.95312
# [36/100] testing 18.0% loss=0.27950, acc=0.89062
# [36/100] testing 18.9% loss=0.12654, acc=0.95312
# [36/100] testing 19.3% loss=0.38855, acc=0.87500
# [36/100] testing 20.2% loss=0.30996, acc=0.89062
# [36/100] testing 20.6% loss=0.27820, acc=0.89062
# [36/100] testing 21.5% loss=0.19653, acc=0.93750
# [36/100] testing 21.9% loss=0.36448, acc=0.81250
# [36/100] testing 22.8% loss=0.33204, acc=0.84375
# [36/100] testing 23.7% loss=0.38138, acc=0.85938
# [36/100] testing 24.1% loss=0.17078, acc=0.92188
# [36/100] testing 25.0% loss=0.30151, acc=0.90625
# [36/100] testing 25.4% loss=0.12516, acc=0.95312
# [36/100] testing 26.3% loss=0.30823, acc=0.85938
# [36/100] testing 26.8% loss=0.24719, acc=0.90625
# [36/100] testing 27.6% loss=0.20521, acc=0.92188
# [36/100] testing 28.5% loss=0.23279, acc=0.93750
# [36/100] testing 29.0% loss=0.21054, acc=0.90625
# [36/100] testing 29.8% loss=0.30192, acc=0.89062
# [36/100] testing 30.3% loss=0.27073, acc=0.92188
# [36/100] testing 31.1% loss=0.27737, acc=0.89062
# [36/100] testing 31.6% loss=0.25789, acc=0.89062
# [36/100] testing 32.5% loss=0.27464, acc=0.89062
# [36/100] testing 32.9% loss=0.39159, acc=0.89062
# [36/100] testing 33.8% loss=0.40448, acc=0.82812
# [36/100] testing 34.7% loss=0.31467, acc=0.90625
# [36/100] testing 35.1% loss=0.18291, acc=0.92188
# [36/100] testing 36.0% loss=0.21633, acc=0.92188
# [36/100] testing 36.4% loss=0.30267, acc=0.89062
# [36/100] testing 37.3% loss=0.25888, acc=0.92188
# [36/100] testing 37.7% loss=0.45714, acc=0.82812
# [36/100] testing 38.6% loss=0.21384, acc=0.93750
# [36/100] testing 39.5% loss=0.25830, acc=0.92188
# [36/100] testing 39.9% loss=0.31895, acc=0.85938
# [36/100] testing 40.8% loss=0.30046, acc=0.90625
# [36/100] testing 41.2% loss=0.25739, acc=0.93750
# [36/100] testing 42.1% loss=0.23820, acc=0.89062
# [36/100] testing 42.5% loss=0.13929, acc=0.96875
# [36/100] testing 43.4% loss=0.19962, acc=0.90625
# [36/100] testing 43.9% loss=0.15759, acc=0.95312
# [36/100] testing 44.7% loss=0.35645, acc=0.85938
# [36/100] testing 45.6% loss=0.25061, acc=0.89062
# [36/100] testing 46.1% loss=0.22154, acc=0.90625
# [36/100] testing 46.9% loss=0.32340, acc=0.87500
# [36/100] testing 47.4% loss=0.10142, acc=0.95312
# [36/100] testing 48.3% loss=0.32839, acc=0.90625
# [36/100] testing 48.7% loss=0.31561, acc=0.85938
# [36/100] testing 49.6% loss=0.48246, acc=0.81250
# [36/100] testing 50.4% loss=0.18886, acc=0.92188
# [36/100] testing 50.9% loss=0.27474, acc=0.89062
# [36/100] testing 51.8% loss=0.17574, acc=0.95312
# [36/100] testing 52.2% loss=0.25030, acc=0.89062
# [36/100] testing 53.1% loss=0.29663, acc=0.87500
# [36/100] testing 53.5% loss=0.29243, acc=0.93750
# [36/100] testing 54.4% loss=0.34207, acc=0.85938
# [36/100] testing 54.8% loss=0.29753, acc=0.84375
# [36/100] testing 55.7% loss=0.17957, acc=0.92188
# [36/100] testing 56.6% loss=0.31908, acc=0.89062
# [36/100] testing 57.0% loss=0.41401, acc=0.81250
# [36/100] testing 57.9% loss=0.24193, acc=0.87500
# [36/100] testing 58.3% loss=0.30054, acc=0.90625
# [36/100] testing 59.2% loss=0.24739, acc=0.90625
# [36/100] testing 59.7% loss=0.22683, acc=0.87500
# [36/100] testing 60.5% loss=0.41116, acc=0.82812
# [36/100] testing 61.4% loss=0.18232, acc=0.90625
# [36/100] testing 61.9% loss=0.20471, acc=0.90625
# [36/100] testing 62.7% loss=0.21017, acc=0.89062
# [36/100] testing 63.2% loss=0.39334, acc=0.85938
# [36/100] testing 64.0% loss=0.37129, acc=0.84375
# [36/100] testing 64.5% loss=0.21001, acc=0.92188
# [36/100] testing 65.4% loss=0.17304, acc=0.92188
# [36/100] testing 65.8% loss=0.29354, acc=0.89062
# [36/100] testing 66.7% loss=0.21263, acc=0.90625
# [36/100] testing 67.6% loss=0.40387, acc=0.92188
# [36/100] testing 68.0% loss=0.09650, acc=0.95312
# [36/100] testing 68.9% loss=0.24118, acc=0.90625
# [36/100] testing 69.3% loss=0.32336, acc=0.85938
# [36/100] testing 70.2% loss=0.41587, acc=0.85938
# [36/100] testing 70.6% loss=0.33534, acc=0.90625
# [36/100] testing 71.5% loss=0.22440, acc=0.90625
# [36/100] testing 72.4% loss=0.18107, acc=0.92188
# [36/100] testing 72.8% loss=0.16137, acc=0.93750
# [36/100] testing 73.7% loss=0.11571, acc=0.96875
# [36/100] testing 74.1% loss=0.34998, acc=0.90625
# [36/100] testing 75.0% loss=0.23855, acc=0.89062
# [36/100] testing 75.4% loss=0.36057, acc=0.90625
# [36/100] testing 76.3% loss=0.12632, acc=0.93750
# [36/100] testing 76.8% loss=0.32220, acc=0.82812
# [36/100] testing 77.6% loss=0.18544, acc=0.90625
# [36/100] testing 78.5% loss=0.36727, acc=0.87500
# [36/100] testing 79.0% loss=0.32457, acc=0.84375
# [36/100] testing 79.8% loss=0.35437, acc=0.85938
# [36/100] testing 80.3% loss=0.27082, acc=0.90625
# [36/100] testing 81.2% loss=0.48255, acc=0.84375
# [36/100] testing 81.6% loss=0.17210, acc=0.93750
# [36/100] testing 82.5% loss=0.21468, acc=0.90625
# [36/100] testing 83.3% loss=0.20966, acc=0.93750
# [36/100] testing 83.8% loss=0.12217, acc=0.96875
# [36/100] testing 84.7% loss=0.40760, acc=0.85938
# [36/100] testing 85.1% loss=0.25780, acc=0.89062
# [36/100] testing 86.0% loss=0.27511, acc=0.89062
# [36/100] testing 86.4% loss=0.42036, acc=0.82812
# [36/100] testing 87.3% loss=0.25015, acc=0.85938
# [36/100] testing 87.7% loss=0.23052, acc=0.92188
# [36/100] testing 88.6% loss=0.24551, acc=0.87500
# [36/100] testing 89.5% loss=0.51776, acc=0.82812
# [36/100] testing 89.9% loss=0.25487, acc=0.87500
# [36/100] testing 90.8% loss=0.30195, acc=0.90625
# [36/100] testing 91.2% loss=0.13811, acc=0.96875
# [36/100] testing 92.1% loss=0.31793, acc=0.90625
# [36/100] testing 92.6% loss=0.41821, acc=0.89062
# [36/100] testing 93.4% loss=0.39928, acc=0.81250
# [36/100] testing 94.3% loss=0.12752, acc=0.95312
# [36/100] testing 94.7% loss=0.17420, acc=0.90625
# [36/100] testing 95.6% loss=0.43647, acc=0.81250
# [36/100] testing 96.1% loss=0.21147, acc=0.90625
# [36/100] testing 96.9% loss=0.35500, acc=0.85938
# [36/100] testing 97.4% loss=0.20095, acc=0.90625
# [36/100] testing 98.3% loss=0.20653, acc=0.85938
# [36/100] testing 98.7% loss=0.14568, acc=0.95312
# [36/100] testing 99.6% loss=0.34515, acc=0.87500
# [37/100] training 0.2% loss=0.35850, acc=0.82812
# [37/100] training 0.4% loss=0.39585, acc=0.82812
# [37/100] training 0.5% loss=0.18241, acc=0.95312
# [37/100] training 0.8% loss=0.15270, acc=0.93750
# [37/100] training 0.9% loss=0.16567, acc=0.92188
# [37/100] training 1.1% loss=0.22549, acc=0.92188
# [37/100] training 1.2% loss=0.26107, acc=0.89062
# [37/100] training 1.4% loss=0.17801, acc=0.93750
# [37/100] training 1.6% loss=0.12885, acc=0.96875
# [37/100] training 1.8% loss=0.19586, acc=0.95312
# [37/100] training 2.0% loss=0.26724, acc=0.87500
# [37/100] training 2.1% loss=0.28235, acc=0.90625
# [37/100] training 2.3% loss=0.15505, acc=0.93750
# [37/100] training 2.4% loss=0.22541, acc=0.87500
# [37/100] training 2.6% loss=0.09105, acc=0.98438
# [37/100] training 2.7% loss=0.26361, acc=0.89062
# [37/100] training 3.0% loss=0.16242, acc=0.90625
# [37/100] training 3.2% loss=0.12884, acc=0.95312
# [37/100] training 3.3% loss=0.33929, acc=0.92188
# [37/100] training 3.5% loss=0.16751, acc=0.95312
# [37/100] training 3.6% loss=0.35277, acc=0.85938
# [37/100] training 3.8% loss=0.22416, acc=0.90625
# [37/100] training 3.9% loss=0.19835, acc=0.92188
# [37/100] training 4.2% loss=0.10880, acc=0.96875
# [37/100] training 4.4% loss=0.18041, acc=0.90625
# [37/100] training 4.5% loss=0.11689, acc=0.96875
# [37/100] training 4.7% loss=0.26562, acc=0.92188
# [37/100] training 4.8% loss=0.23091, acc=0.92188
# [37/100] training 5.0% loss=0.12677, acc=0.95312
# [37/100] training 5.2% loss=0.20422, acc=0.92188
# [37/100] training 5.4% loss=0.10523, acc=0.96875
# [37/100] training 5.5% loss=0.22827, acc=0.90625
# [37/100] training 5.7% loss=0.20778, acc=0.89062
# [37/100] training 5.9% loss=0.21814, acc=0.93750
# [37/100] training 6.0% loss=0.24306, acc=0.89062
# [37/100] training 6.3% loss=0.18926, acc=0.92188
# [37/100] training 6.4% loss=0.19171, acc=0.92188
# [37/100] training 6.6% loss=0.17628, acc=0.93750
# [37/100] training 6.7% loss=0.32079, acc=0.82812
# [37/100] training 6.9% loss=0.20483, acc=0.92188
# [37/100] training 7.1% loss=0.19709, acc=0.90625
# [37/100] training 7.2% loss=0.27602, acc=0.89062
# [37/100] training 7.5% loss=0.22762, acc=0.92188
# [37/100] training 7.6% loss=0.26700, acc=0.89062
# [37/100] training 7.8% loss=0.20548, acc=0.93750
# [37/100] training 7.9% loss=0.20842, acc=0.90625
# [37/100] training 8.1% loss=0.12381, acc=0.93750
# [37/100] training 8.2% loss=0.18350, acc=0.93750
# [37/100] training 8.4% loss=0.25540, acc=0.85938
# [37/100] training 8.7% loss=0.19117, acc=0.93750
# [37/100] training 8.8% loss=0.15777, acc=0.90625
# [37/100] training 9.0% loss=0.21033, acc=0.89062
# [37/100] training 9.1% loss=0.22318, acc=0.93750
# [37/100] training 9.3% loss=0.34575, acc=0.87500
# [37/100] training 9.4% loss=0.12606, acc=0.93750
# [37/100] training 9.7% loss=0.21531, acc=0.92188
# [37/100] training 9.9% loss=0.26543, acc=0.92188
# [37/100] training 10.0% loss=0.24346, acc=0.89062
# [37/100] training 10.2% loss=0.19804, acc=0.92188
# [37/100] training 10.3% loss=0.15935, acc=0.95312
# [37/100] training 10.5% loss=0.32256, acc=0.90625
# [37/100] training 10.6% loss=0.23223, acc=0.87500
# [37/100] training 10.9% loss=0.13766, acc=0.95312
# [37/100] training 11.0% loss=0.22553, acc=0.93750
# [37/100] training 11.2% loss=0.10982, acc=0.95312
# [37/100] training 11.4% loss=0.19133, acc=0.95312
# [37/100] training 11.5% loss=0.31711, acc=0.90625
# [37/100] training 11.7% loss=0.09015, acc=0.98438
# [37/100] training 11.8% loss=0.19605, acc=0.90625
# [37/100] training 12.1% loss=0.12197, acc=0.93750
# [37/100] training 12.2% loss=0.12423, acc=0.93750
# [37/100] training 12.4% loss=0.11261, acc=0.95312
# [37/100] training 12.6% loss=0.33902, acc=0.89062
# [37/100] training 12.7% loss=0.22660, acc=0.90625
# [37/100] training 12.9% loss=0.34452, acc=0.92188
# [37/100] training 13.0% loss=0.17618, acc=0.96875
# [37/100] training 13.3% loss=0.20269, acc=0.92188
# [37/100] training 13.4% loss=0.25380, acc=0.89062
# [37/100] training 13.6% loss=0.17201, acc=0.95312
# [37/100] training 13.7% loss=0.27691, acc=0.87500
# [37/100] training 13.9% loss=0.19275, acc=0.89062
# [37/100] training 14.1% loss=0.29365, acc=0.90625
# [37/100] training 14.3% loss=0.14100, acc=0.93750
# [37/100] training 14.5% loss=0.23082, acc=0.92188
# [37/100] training 14.6% loss=0.15558, acc=0.90625
# [37/100] training 14.8% loss=0.23068, acc=0.93750
# [37/100] training 14.9% loss=0.14608, acc=0.93750
# [37/100] training 15.1% loss=0.22623, acc=0.90625
# [37/100] training 15.4% loss=0.21021, acc=0.89062
# [37/100] training 15.5% loss=0.15406, acc=0.93750
# [37/100] training 15.7% loss=0.24881, acc=0.89062
# [37/100] training 15.8% loss=0.15746, acc=0.92188
# [37/100] training 16.0% loss=0.23774, acc=0.92188
# [37/100] training 16.1% loss=0.49040, acc=0.81250
# [37/100] training 16.3% loss=0.16883, acc=0.92188
# [37/100] training 16.4% loss=0.17769, acc=0.92188
# [37/100] training 16.7% loss=0.25800, acc=0.89062
# [37/100] training 16.9% loss=0.25231, acc=0.89062
# [37/100] training 17.0% loss=0.21846, acc=0.90625
# [37/100] training 17.2% loss=0.17390, acc=0.90625
# [37/100] training 17.3% loss=0.15366, acc=0.93750
# [37/100] training 17.5% loss=0.20402, acc=0.92188
# [37/100] training 17.7% loss=0.21275, acc=0.89062
# [37/100] training 17.9% loss=0.24447, acc=0.90625
# [37/100] training 18.1% loss=0.23748, acc=0.85938
# [37/100] training 18.2% loss=0.22836, acc=0.92188
# [37/100] training 18.4% loss=0.33594, acc=0.84375
# [37/100] training 18.5% loss=0.15595, acc=0.95312
# [37/100] training 18.8% loss=0.19274, acc=0.93750
# [37/100] training 18.9% loss=0.12672, acc=0.92188
# [37/100] training 19.1% loss=0.31128, acc=0.93750
# [37/100] training 19.2% loss=0.08985, acc=0.96875
# [37/100] training 19.4% loss=0.14575, acc=0.93750
# [37/100] training 19.6% loss=0.31315, acc=0.87500
# [37/100] training 19.7% loss=0.35994, acc=0.87500
# [37/100] training 20.0% loss=0.13309, acc=0.92188
# [37/100] training 20.1% loss=0.23098, acc=0.92188
# [37/100] training 20.3% loss=0.22393, acc=0.90625
# [37/100] training 20.4% loss=0.20386, acc=0.92188
# [37/100] training 20.6% loss=0.19436, acc=0.93750
# [37/100] training 20.8% loss=0.15329, acc=0.93750
# [37/100] training 20.9% loss=0.13946, acc=0.93750
# [37/100] training 21.2% loss=0.25429, acc=0.89062
# [37/100] training 21.3% loss=0.19717, acc=0.95312
# [37/100] training 21.5% loss=0.23754, acc=0.92188
# [37/100] training 21.6% loss=0.10984, acc=0.93750
# [37/100] training 21.8% loss=0.22740, acc=0.92188
# [37/100] training 21.9% loss=0.23290, acc=0.93750
# [37/100] training 22.2% loss=0.19464, acc=0.93750
# [37/100] training 22.4% loss=0.22128, acc=0.93750
# [37/100] training 22.5% loss=0.19271, acc=0.93750
# [37/100] training 22.7% loss=0.21106, acc=0.92188
# [37/100] training 22.8% loss=0.20625, acc=0.92188
# [37/100] training 23.0% loss=0.12531, acc=0.95312
# [37/100] training 23.1% loss=0.39477, acc=0.87500
# [37/100] training 23.4% loss=0.23593, acc=0.89062
# [37/100] training 23.6% loss=0.39611, acc=0.89062
# [37/100] training 23.7% loss=0.17273, acc=0.93750
# [37/100] training 23.9% loss=0.12702, acc=0.95312
# [37/100] training 24.0% loss=0.18948, acc=0.93750
# [37/100] training 24.2% loss=0.19485, acc=0.92188
# [37/100] training 24.3% loss=0.25582, acc=0.90625
# [37/100] training 24.6% loss=0.20436, acc=0.87500
# [37/100] training 24.7% loss=0.28098, acc=0.89062
# [37/100] training 24.9% loss=0.15873, acc=0.89062
# [37/100] training 25.1% loss=0.26770, acc=0.92188
# [37/100] training 25.2% loss=0.19352, acc=0.95312
# [37/100] training 25.4% loss=0.26571, acc=0.92188
# [37/100] training 25.6% loss=0.19161, acc=0.90625
# [37/100] training 25.8% loss=0.24324, acc=0.89062
# [37/100] training 25.9% loss=0.18282, acc=0.93750
# [37/100] training 26.1% loss=0.22166, acc=0.90625
# [37/100] training 26.3% loss=0.15177, acc=0.93750
# [37/100] training 26.4% loss=0.17526, acc=0.92188
# [37/100] training 26.6% loss=0.05022, acc=1.00000
# [37/100] training 26.8% loss=0.17347, acc=0.93750
# [37/100] training 27.0% loss=0.15363, acc=0.93750
# [37/100] training 27.1% loss=0.13178, acc=0.93750
# [37/100] training 27.3% loss=0.19847, acc=0.89062
# [37/100] training 27.4% loss=0.14561, acc=0.96875
# [37/100] training 27.6% loss=0.34863, acc=0.89062
# [37/100] training 27.9% loss=0.17139, acc=0.92188
# [37/100] training 28.0% loss=0.29279, acc=0.87500
# [37/100] training 28.2% loss=0.21077, acc=0.95312
# [37/100] training 28.3% loss=0.06988, acc=0.98438
# [37/100] training 28.5% loss=0.25880, acc=0.89062
# [37/100] training 28.6% loss=0.19479, acc=0.93750
# [37/100] training 28.8% loss=0.10775, acc=0.96875
# [37/100] training 29.1% loss=0.16322, acc=0.93750
# [37/100] training 29.2% loss=0.11629, acc=0.98438
# [37/100] training 29.4% loss=0.26644, acc=0.85938
# [37/100] training 29.5% loss=0.15921, acc=0.90625
# [37/100] training 29.7% loss=0.18640, acc=0.90625
# [37/100] training 29.8% loss=0.17223, acc=0.93750
# [37/100] training 30.0% loss=0.20154, acc=0.89062
# [37/100] training 30.2% loss=0.09591, acc=0.98438
# [37/100] training 30.4% loss=0.05849, acc=0.98438
# [37/100] training 30.6% loss=0.34837, acc=0.89062
# [37/100] training 30.7% loss=0.23855, acc=0.89062
# [37/100] training 30.9% loss=0.28464, acc=0.95312
# [37/100] training 31.0% loss=0.13988, acc=0.93750
# [37/100] training 31.3% loss=0.15134, acc=0.92188
# [37/100] training 31.4% loss=0.31729, acc=0.84375
# [37/100] training 31.6% loss=0.21738, acc=0.89062
# [37/100] training 31.8% loss=0.13617, acc=0.92188
# [37/100] training 31.9% loss=0.26704, acc=0.87500
# [37/100] training 32.1% loss=0.23117, acc=0.93750
# [37/100] training 32.2% loss=0.24786, acc=0.89062
# [37/100] training 32.5% loss=0.13416, acc=0.95312
# [37/100] training 32.6% loss=0.26420, acc=0.89062
# [37/100] training 32.8% loss=0.17243, acc=0.93750
# [37/100] training 32.9% loss=0.21533, acc=0.89062
# [37/100] training 33.1% loss=0.17242, acc=0.93750
# [37/100] training 33.3% loss=0.21250, acc=0.90625
# [37/100] training 33.4% loss=0.19974, acc=0.95312
# [37/100] training 33.7% loss=0.19108, acc=0.92188
# [37/100] training 33.8% loss=0.18507, acc=0.90625
# [37/100] training 34.0% loss=0.28878, acc=0.92188
# [37/100] training 34.1% loss=0.23855, acc=0.90625
# [37/100] training 34.3% loss=0.17695, acc=0.95312
# [37/100] training 34.5% loss=0.18552, acc=0.92188
# [37/100] training 34.7% loss=0.14347, acc=0.93750
# [37/100] training 34.9% loss=0.15853, acc=0.90625
# [37/100] training 35.0% loss=0.06701, acc=1.00000
# [37/100] training 35.2% loss=0.19684, acc=0.90625
# [37/100] training 35.3% loss=0.27534, acc=0.89062
# [37/100] training 35.5% loss=0.21459, acc=0.90625
# [37/100] training 35.6% loss=0.28037, acc=0.87500
# [37/100] training 35.9% loss=0.34521, acc=0.85938
# [37/100] training 36.1% loss=0.18284, acc=0.90625
# [37/100] training 36.2% loss=0.21909, acc=0.90625
# [37/100] training 36.4% loss=0.21893, acc=0.90625
# [37/100] training 36.5% loss=0.22077, acc=0.90625
# [37/100] training 36.7% loss=0.17222, acc=0.93750
# [37/100] training 36.8% loss=0.07991, acc=0.98438
# [37/100] training 37.1% loss=0.25950, acc=0.84375
# [37/100] training 37.3% loss=0.30309, acc=0.89062
# [37/100] training 37.4% loss=0.35918, acc=0.82812
# [37/100] training 37.6% loss=0.09664, acc=0.95312
# [37/100] training 37.7% loss=0.26661, acc=0.93750
# [37/100] training 37.9% loss=0.16645, acc=0.90625
# [37/100] training 38.1% loss=0.29677, acc=0.89062
# [37/100] training 38.3% loss=0.17952, acc=0.90625
# [37/100] training 38.4% loss=0.12622, acc=0.95312
# [37/100] training 38.6% loss=0.16480, acc=0.95312
# [37/100] training 38.8% loss=0.24519, acc=0.89062
# [37/100] training 38.9% loss=0.22725, acc=0.85938
# [37/100] training 39.1% loss=0.20092, acc=0.92188
# [37/100] training 39.3% loss=0.24022, acc=0.90625
# [37/100] training 39.5% loss=0.26195, acc=0.89062
# [37/100] training 39.6% loss=0.20394, acc=0.93750
# [37/100] training 39.8% loss=0.13730, acc=0.93750
# [37/100] training 40.0% loss=0.17284, acc=0.93750
# [37/100] training 40.1% loss=0.19984, acc=0.93750
# [37/100] training 40.4% loss=0.13051, acc=0.93750
# [37/100] training 40.5% loss=0.22744, acc=0.92188
# [37/100] training 40.7% loss=0.17668, acc=0.93750
# [37/100] training 40.8% loss=0.14061, acc=0.93750
# [37/100] training 41.0% loss=0.14335, acc=0.93750
# [37/100] training 41.1% loss=0.29982, acc=0.92188
# [37/100] training 41.3% loss=0.27658, acc=0.87500
# [37/100] training 41.6% loss=0.22110, acc=0.92188
# [37/100] training 41.7% loss=0.35161, acc=0.92188
# [37/100] training 41.9% loss=0.20053, acc=0.90625
# [37/100] training 42.0% loss=0.27365, acc=0.89062
# [37/100] training 42.2% loss=0.25034, acc=0.87500
# [37/100] training 42.3% loss=0.16363, acc=0.93750
# [37/100] training 42.5% loss=0.22050, acc=0.93750
# [37/100] training 42.8% loss=0.19576, acc=0.89062
# [37/100] training 42.9% loss=0.18624, acc=0.92188
# [37/100] training 43.1% loss=0.19732, acc=0.93750
# [37/100] training 43.2% loss=0.15272, acc=0.93750
# [37/100] training 43.4% loss=0.19808, acc=0.92188
# [37/100] training 43.5% loss=0.21064, acc=0.90625
# [37/100] training 43.8% loss=0.27293, acc=0.89062
# [37/100] training 43.9% loss=0.18376, acc=0.93750
# [37/100] training 44.1% loss=0.18679, acc=0.92188
# [37/100] training 44.3% loss=0.14197, acc=0.95312
# [37/100] training 44.4% loss=0.27391, acc=0.90625
# [37/100] training 44.6% loss=0.29061, acc=0.89062
# [37/100] training 44.7% loss=0.34105, acc=0.87500
# [37/100] training 45.0% loss=0.12563, acc=0.93750
# [37/100] training 45.1% loss=0.38533, acc=0.89062
# [37/100] training 45.3% loss=0.24777, acc=0.89062
# [37/100] training 45.5% loss=0.08066, acc=0.98438
# [37/100] training 45.6% loss=0.30120, acc=0.89062
# [37/100] training 45.8% loss=0.11298, acc=0.96875
# [37/100] training 45.9% loss=0.14161, acc=0.95312
# [37/100] training 46.2% loss=0.08628, acc=0.95312
# [37/100] training 46.3% loss=0.13892, acc=0.92188
# [37/100] training 46.5% loss=0.36412, acc=0.85938
# [37/100] training 46.6% loss=0.20960, acc=0.90625
# [37/100] training 46.8% loss=0.18932, acc=0.93750
# [37/100] training 47.0% loss=0.16214, acc=0.92188
# [37/100] training 47.2% loss=0.23934, acc=0.85938
# [37/100] training 47.4% loss=0.23285, acc=0.89062
# [37/100] training 47.5% loss=0.24690, acc=0.92188
# [37/100] training 47.7% loss=0.22371, acc=0.92188
# [37/100] training 47.8% loss=0.35035, acc=0.84375
# [37/100] training 48.0% loss=0.15635, acc=0.95312
# [37/100] training 48.3% loss=0.09979, acc=0.93750
# [37/100] training 48.4% loss=0.06093, acc=1.00000
# [37/100] training 48.6% loss=0.12398, acc=0.96875
# [37/100] training 48.7% loss=0.32180, acc=0.87500
# [37/100] training 48.9% loss=0.12196, acc=0.95312
# [37/100] training 49.0% loss=0.17125, acc=0.92188
# [37/100] training 49.2% loss=0.30084, acc=0.89062
# [37/100] training 49.3% loss=0.15575, acc=0.93750
# [37/100] training 49.6% loss=0.25818, acc=0.90625
# [37/100] training 49.8% loss=0.17323, acc=0.93750
# [37/100] training 49.9% loss=0.23898, acc=0.92188
# [37/100] training 50.1% loss=0.15153, acc=0.92188
# [37/100] training 50.2% loss=0.25985, acc=0.89062
# [37/100] training 50.4% loss=0.38672, acc=0.84375
# [37/100] training 50.6% loss=0.19522, acc=0.90625
# [37/100] training 50.8% loss=0.22579, acc=0.89062
# [37/100] training 51.0% loss=0.36690, acc=0.84375
# [37/100] training 51.1% loss=0.25009, acc=0.92188
# [37/100] training 51.3% loss=0.20570, acc=0.93750
# [37/100] training 51.4% loss=0.22103, acc=0.93750
# [37/100] training 51.7% loss=0.21541, acc=0.87500
# [37/100] training 51.8% loss=0.21251, acc=0.92188
# [37/100] training 52.0% loss=0.22923, acc=0.89062
# [37/100] training 52.1% loss=0.19612, acc=0.92188
# [37/100] training 52.3% loss=0.32995, acc=0.90625
# [37/100] training 52.5% loss=0.09645, acc=0.96875
# [37/100] training 52.6% loss=0.22477, acc=0.90625
# [37/100] training 52.9% loss=0.28565, acc=0.87500
# [37/100] training 53.0% loss=0.18397, acc=0.92188
# [37/100] training 53.2% loss=0.19871, acc=0.95312
# [37/100] training 53.3% loss=0.14911, acc=0.95312
# [37/100] training 53.5% loss=0.15307, acc=0.95312
# [37/100] training 53.7% loss=0.05296, acc=0.98438
# [37/100] training 53.8% loss=0.25524, acc=0.81250
# [37/100] training 54.1% loss=0.33851, acc=0.90625
# [37/100] training 54.2% loss=0.14152, acc=0.93750
# [37/100] training 54.4% loss=0.19117, acc=0.95312
# [37/100] training 54.5% loss=0.27021, acc=0.92188
# [37/100] training 54.7% loss=0.31061, acc=0.87500
# [37/100] training 54.8% loss=0.11705, acc=0.96875
# [37/100] training 55.1% loss=0.21044, acc=0.87500
# [37/100] training 55.3% loss=0.18522, acc=0.92188
# [37/100] training 55.4% loss=0.15351, acc=0.96875
# [37/100] training 55.6% loss=0.16084, acc=0.92188
# [37/100] training 55.7% loss=0.15608, acc=0.92188
# [37/100] training 55.9% loss=0.19359, acc=0.92188
# [37/100] training 56.0% loss=0.09542, acc=0.96875
# [37/100] training 56.3% loss=0.35232, acc=0.87500
# [37/100] training 56.5% loss=0.20998, acc=0.89062
# [37/100] training 56.6% loss=0.18115, acc=0.92188
# [37/100] training 56.8% loss=0.27790, acc=0.90625
# [37/100] training 56.9% loss=0.18994, acc=0.90625
# [37/100] training 57.1% loss=0.24777, acc=0.93750
# [37/100] training 57.2% loss=0.18335, acc=0.93750
# [37/100] training 57.5% loss=0.15028, acc=0.93750
# [37/100] training 57.6% loss=0.19212, acc=0.90625
# [37/100] training 57.8% loss=0.16201, acc=0.93750
# [37/100] training 58.0% loss=0.13810, acc=0.92188
# [37/100] training 58.1% loss=0.16040, acc=0.96875
# [37/100] training 58.3% loss=0.13997, acc=0.92188
# [37/100] training 58.4% loss=0.25991, acc=0.93750
# [37/100] training 58.7% loss=0.30251, acc=0.92188
# [37/100] training 58.8% loss=0.25034, acc=0.90625
# [37/100] training 59.0% loss=0.14304, acc=0.93750
# [37/100] training 59.2% loss=0.18888, acc=0.93750
# [37/100] training 59.3% loss=0.16346, acc=0.95312
# [37/100] training 59.5% loss=0.19482, acc=0.90625
# [37/100] training 59.7% loss=0.23513, acc=0.92188
# [37/100] training 59.9% loss=0.14287, acc=0.95312
# [37/100] training 60.0% loss=0.17349, acc=0.93750
# [37/100] training 60.2% loss=0.22928, acc=0.90625
# [37/100] training 60.3% loss=0.19617, acc=0.93750
# [37/100] training 60.5% loss=0.20815, acc=0.89062
# [37/100] training 60.8% loss=0.19226, acc=0.90625
# [37/100] training 60.9% loss=0.28567, acc=0.85938
# [37/100] training 61.1% loss=0.20413, acc=0.87500
# [37/100] training 61.2% loss=0.23397, acc=0.90625
# [37/100] training 61.4% loss=0.25963, acc=0.85938
# [37/100] training 61.5% loss=0.37874, acc=0.82812
# [37/100] training 61.7% loss=0.22262, acc=0.92188
# [37/100] training 62.0% loss=0.25616, acc=0.92188
# [37/100] training 62.1% loss=0.21368, acc=0.89062
# [37/100] training 62.3% loss=0.13709, acc=0.95312
# [37/100] training 62.4% loss=0.19337, acc=0.89062
# [37/100] training 62.6% loss=0.24348, acc=0.89062
# [37/100] training 62.7% loss=0.24303, acc=0.90625
# [37/100] training 62.9% loss=0.15688, acc=0.95312
# [37/100] training 63.1% loss=0.20683, acc=0.92188
# [37/100] training 63.3% loss=0.18017, acc=0.92188
# [37/100] training 63.5% loss=0.21781, acc=0.90625
# [37/100] training 63.6% loss=0.30491, acc=0.92188
# [37/100] training 63.8% loss=0.41428, acc=0.85938
# [37/100] training 63.9% loss=0.19166, acc=0.96875
# [37/100] training 64.2% loss=0.16815, acc=0.93750
# [37/100] training 64.3% loss=0.21754, acc=0.90625
# [37/100] training 64.5% loss=0.18524, acc=0.89062
# [37/100] training 64.7% loss=0.17770, acc=0.93750
# [37/100] training 64.8% loss=0.35204, acc=0.85938
# [37/100] training 65.0% loss=0.15072, acc=0.93750
# [37/100] training 65.1% loss=0.18741, acc=0.90625
# [37/100] training 65.4% loss=0.12765, acc=0.96875
# [37/100] training 65.5% loss=0.11553, acc=0.96875
# [37/100] training 65.7% loss=0.20292, acc=0.92188
# [37/100] training 65.8% loss=0.24841, acc=0.87500
# [37/100] training 66.0% loss=0.17444, acc=0.95312
# [37/100] training 66.2% loss=0.14505, acc=0.93750
# [37/100] training 66.3% loss=0.23544, acc=0.87500
# [37/100] training 66.6% loss=0.15015, acc=0.90625
# [37/100] training 66.7% loss=0.13709, acc=0.93750
# [37/100] training 66.9% loss=0.13919, acc=0.95312
# [37/100] training 67.0% loss=0.20825, acc=0.92188
# [37/100] training 67.2% loss=0.13794, acc=0.95312
# [37/100] training 67.4% loss=0.12807, acc=0.93750
# [37/100] training 67.6% loss=0.13689, acc=0.92188
# [37/100] training 67.8% loss=0.14395, acc=0.92188
# [37/100] training 67.9% loss=0.12641, acc=0.95312
# [37/100] training 68.1% loss=0.24613, acc=0.93750
# [37/100] training 68.2% loss=0.16543, acc=0.93750
# [37/100] training 68.4% loss=0.16518, acc=0.93750
# [37/100] training 68.5% loss=0.19557, acc=0.92188
# [37/100] training 68.8% loss=0.14275, acc=0.93750
# [37/100] training 69.0% loss=0.18339, acc=0.93750
# [37/100] training 69.1% loss=0.18819, acc=0.89062
# [37/100] training 69.3% loss=0.15029, acc=0.90625
# [37/100] training 69.4% loss=0.18737, acc=0.90625
# [37/100] training 69.6% loss=0.19927, acc=0.90625
# [37/100] training 69.7% loss=0.18763, acc=0.92188
# [37/100] training 70.0% loss=0.36181, acc=0.82812
# [37/100] training 70.2% loss=0.20782, acc=0.93750
# [37/100] training 70.3% loss=0.19551, acc=0.93750
# [37/100] training 70.5% loss=0.22560, acc=0.90625
# [37/100] training 70.6% loss=0.14894, acc=0.92188
# [37/100] training 70.8% loss=0.18183, acc=0.90625
# [37/100] training 71.0% loss=0.25835, acc=0.92188
# [37/100] training 71.2% loss=0.12657, acc=0.93750
# [37/100] training 71.3% loss=0.09963, acc=0.96875
# [37/100] training 71.5% loss=0.23098, acc=0.90625
# [37/100] training 71.7% loss=0.24250, acc=0.90625
# [37/100] training 71.8% loss=0.32881, acc=0.90625
# [37/100] training 72.0% loss=0.10740, acc=0.95312
# [37/100] training 72.2% loss=0.14626, acc=0.95312
# [37/100] training 72.4% loss=0.21717, acc=0.92188
# [37/100] training 72.5% loss=0.22920, acc=0.89062
# [37/100] training 72.7% loss=0.26832, acc=0.89062
# [37/100] training 72.9% loss=0.14486, acc=0.95312
# [37/100] training 73.0% loss=0.12929, acc=0.95312
# [37/100] training 73.3% loss=0.26683, acc=0.87500
# [37/100] training 73.4% loss=0.17220, acc=0.93750
# [37/100] training 73.6% loss=0.12788, acc=0.98438
# [37/100] training 73.7% loss=0.17702, acc=0.95312
# [37/100] training 73.9% loss=0.09922, acc=0.96875
# [37/100] training 74.0% loss=0.17355, acc=0.89062
# [37/100] training 74.2% loss=0.13398, acc=0.96875
# [37/100] training 74.5% loss=0.12852, acc=0.93750
# [37/100] training 74.6% loss=0.35223, acc=0.87500
# [37/100] training 74.8% loss=0.35997, acc=0.87500
# [37/100] training 74.9% loss=0.31275, acc=0.85938
# [37/100] training 75.1% loss=0.12534, acc=0.93750
# [37/100] training 75.2% loss=0.15034, acc=0.92188
# [37/100] training 75.4% loss=0.21514, acc=0.89062
# [37/100] training 75.7% loss=0.20275, acc=0.90625
# [37/100] training 75.8% loss=0.25753, acc=0.89062
# [37/100] training 76.0% loss=0.16041, acc=0.92188
# [37/100] training 76.1% loss=0.24177, acc=0.89062
# [37/100] training 76.3% loss=0.12757, acc=0.93750
# [37/100] training 76.4% loss=0.14087, acc=0.96875
# [37/100] training 76.7% loss=0.17211, acc=0.93750
# [37/100] training 76.8% loss=0.12014, acc=0.92188
# [37/100] training 77.0% loss=0.14878, acc=0.93750
# [37/100] training 77.2% loss=0.21794, acc=0.96875
# [37/100] training 77.3% loss=0.08904, acc=0.98438
# [37/100] training 77.5% loss=0.23095, acc=0.90625
# [37/100] training 77.6% loss=0.19346, acc=0.90625
# [37/100] training 77.9% loss=0.21849, acc=0.92188
# [37/100] training 78.0% loss=0.20073, acc=0.92188
# [37/100] training 78.2% loss=0.32839, acc=0.90625
# [37/100] training 78.4% loss=0.18398, acc=0.89062
# [37/100] training 78.5% loss=0.19811, acc=0.93750
# [37/100] training 78.7% loss=0.26914, acc=0.90625
# [37/100] training 78.8% loss=0.12945, acc=0.93750
# [37/100] training 79.1% loss=0.12277, acc=0.98438
# [37/100] training 79.2% loss=0.17724, acc=0.93750
# [37/100] training 79.4% loss=0.20278, acc=0.92188
# [37/100] training 79.5% loss=0.13213, acc=0.95312
# [37/100] training 79.7% loss=0.07369, acc=1.00000
# [37/100] training 79.9% loss=0.13990, acc=0.93750
# [37/100] training 80.1% loss=0.13328, acc=0.93750
# [37/100] training 80.3% loss=0.33725, acc=0.89062
# [37/100] training 80.4% loss=0.14397, acc=0.95312
# [37/100] training 80.6% loss=0.22745, acc=0.89062
# [37/100] training 80.7% loss=0.19986, acc=0.93750
# [37/100] training 80.9% loss=0.20266, acc=0.90625
# [37/100] training 81.2% loss=0.20338, acc=0.89062
# [37/100] training 81.3% loss=0.21845, acc=0.89062
# [37/100] training 81.5% loss=0.18869, acc=0.93750
# [37/100] training 81.6% loss=0.44525, acc=0.81250
# [37/100] training 81.8% loss=0.18928, acc=0.93750
# [37/100] training 81.9% loss=0.35868, acc=0.90625
# [37/100] training 82.1% loss=0.19849, acc=0.92188
# [37/100] training 82.2% loss=0.20574, acc=0.95312
# [37/100] training 82.5% loss=0.12164, acc=0.96875
# [37/100] training 82.7% loss=0.27216, acc=0.90625
# [37/100] training 82.8% loss=0.20451, acc=0.95312
# [37/100] training 83.0% loss=0.11762, acc=0.95312
# [37/100] training 83.1% loss=0.14966, acc=0.92188
# [37/100] training 83.3% loss=0.17570, acc=0.96875
# [37/100] training 83.5% loss=0.18015, acc=0.93750
# [37/100] training 83.7% loss=0.30169, acc=0.87500
# [37/100] training 83.9% loss=0.16034, acc=0.89062
# [37/100] training 84.0% loss=0.11474, acc=0.95312
# [37/100] training 84.2% loss=0.09021, acc=0.95312
# [37/100] training 84.3% loss=0.19587, acc=0.92188
# [37/100] training 84.5% loss=0.17247, acc=0.95312
# [37/100] training 84.7% loss=0.22834, acc=0.90625
# [37/100] training 84.9% loss=0.18257, acc=0.93750
# [37/100] training 85.0% loss=0.31164, acc=0.89062
# [37/100] training 85.2% loss=0.20090, acc=0.90625
# [37/100] training 85.4% loss=0.11702, acc=0.96875
# [37/100] training 85.5% loss=0.28587, acc=0.87500
# [37/100] training 85.8% loss=0.16165, acc=0.95312
# [37/100] training 85.9% loss=0.13560, acc=0.93750
# [37/100] training 86.1% loss=0.28972, acc=0.89062
# [37/100] training 86.2% loss=0.10953, acc=0.95312
# [37/100] training 86.4% loss=0.24655, acc=0.90625
# [37/100] training 86.6% loss=0.13970, acc=0.93750
# [37/100] training 86.7% loss=0.16978, acc=0.92188
# [37/100] training 87.0% loss=0.20039, acc=0.92188
# [37/100] training 87.1% loss=0.24423, acc=0.89062
# [37/100] training 87.3% loss=0.13946, acc=0.95312
# [37/100] training 87.4% loss=0.30838, acc=0.85938
# [37/100] training 87.6% loss=0.18668, acc=0.92188
# [37/100] training 87.7% loss=0.29480, acc=0.90625
# [37/100] training 87.9% loss=0.25119, acc=0.90625
# [37/100] training 88.2% loss=0.17894, acc=0.95312
# [37/100] training 88.3% loss=0.20477, acc=0.93750
# [37/100] training 88.5% loss=0.20594, acc=0.89062
# [37/100] training 88.6% loss=0.10707, acc=0.96875
# [37/100] training 88.8% loss=0.16700, acc=0.92188
# [37/100] training 88.9% loss=0.35670, acc=0.87500
# [37/100] training 89.2% loss=0.15225, acc=0.92188
# [37/100] training 89.4% loss=0.17965, acc=0.96875
# [37/100] training 89.5% loss=0.26989, acc=0.89062
# [37/100] training 89.7% loss=0.22569, acc=0.93750
# [37/100] training 89.8% loss=0.18421, acc=0.96875
# [37/100] training 90.0% loss=0.12213, acc=0.96875
# [37/100] training 90.1% loss=0.16617, acc=0.93750
# [37/100] training 90.4% loss=0.22866, acc=0.89062
# [37/100] training 90.5% loss=0.14658, acc=0.93750
# [37/100] training 90.7% loss=0.10018, acc=0.98438
# [37/100] training 90.9% loss=0.15675, acc=0.93750
# [37/100] training 91.0% loss=0.14953, acc=0.93750
# [37/100] training 91.2% loss=0.16553, acc=0.93750
# [37/100] training 91.3% loss=0.32479, acc=0.89062
# [37/100] training 91.6% loss=0.28366, acc=0.85938
# [37/100] training 91.7% loss=0.09701, acc=0.95312
# [37/100] training 91.9% loss=0.23676, acc=0.90625
# [37/100] training 92.1% loss=0.18168, acc=0.90625
# [37/100] training 92.2% loss=0.15485, acc=0.93750
# [37/100] training 92.4% loss=0.14267, acc=0.95312
# [37/100] training 92.6% loss=0.18960, acc=0.92188
# [37/100] training 92.8% loss=0.20782, acc=0.92188
# [37/100] training 92.9% loss=0.21601, acc=0.93750
# [37/100] training 93.1% loss=0.45620, acc=0.82812
# [37/100] training 93.2% loss=0.17442, acc=0.93750
# [37/100] training 93.4% loss=0.23598, acc=0.89062
# [37/100] training 93.7% loss=0.14478, acc=0.93750
# [37/100] training 93.8% loss=0.22844, acc=0.85938
# [37/100] training 94.0% loss=0.13601, acc=0.95312
# [37/100] training 94.1% loss=0.18498, acc=0.93750
# [37/100] training 94.3% loss=0.17624, acc=0.90625
# [37/100] training 94.4% loss=0.19021, acc=0.93750
# [37/100] training 94.6% loss=0.10359, acc=0.96875
# [37/100] training 94.9% loss=0.15200, acc=0.93750
# [37/100] training 95.0% loss=0.26368, acc=0.87500
# [37/100] training 95.2% loss=0.54479, acc=0.84375
# [37/100] training 95.3% loss=0.17790, acc=0.92188
# [37/100] training 95.5% loss=0.11012, acc=0.98438
# [37/100] training 95.6% loss=0.29463, acc=0.89062
# [37/100] training 95.8% loss=0.21058, acc=0.87500
# [37/100] training 96.0% loss=0.22363, acc=0.92188
# [37/100] training 96.2% loss=0.14103, acc=0.95312
# [37/100] training 96.4% loss=0.13584, acc=0.98438
# [37/100] training 96.5% loss=0.18648, acc=0.95312
# [37/100] training 96.7% loss=0.14046, acc=0.93750
# [37/100] training 96.8% loss=0.22166, acc=0.89062
# [37/100] training 97.1% loss=0.10425, acc=0.98438
# [37/100] training 97.2% loss=0.16048, acc=0.95312
# [37/100] training 97.4% loss=0.23341, acc=0.93750
# [37/100] training 97.6% loss=0.30241, acc=0.90625
# [37/100] training 97.7% loss=0.17098, acc=0.95312
# [37/100] training 97.9% loss=0.14900, acc=0.92188
# [37/100] training 98.0% loss=0.13072, acc=0.93750
# [37/100] training 98.3% loss=0.29719, acc=0.85938
# [37/100] training 98.4% loss=0.13552, acc=0.96875
# [37/100] training 98.6% loss=0.34682, acc=0.84375
# [37/100] training 98.7% loss=0.25065, acc=0.90625
# [37/100] training 98.9% loss=0.12316, acc=0.96875
# [37/100] training 99.1% loss=0.15437, acc=0.95312
# [37/100] training 99.2% loss=0.15134, acc=0.92188
# [37/100] training 99.5% loss=0.36374, acc=0.85938
# [37/100] training 99.6% loss=0.26215, acc=0.89062
# [37/100] training 99.8% loss=0.18777, acc=0.92188
# [37/100] training 99.9% loss=0.14825, acc=0.95312
# [37/100] testing 0.9% loss=0.12457, acc=0.95312
# [37/100] testing 1.8% loss=0.43415, acc=0.85938
# [37/100] testing 2.2% loss=0.27374, acc=0.93750
# [37/100] testing 3.1% loss=0.31857, acc=0.85938
# [37/100] testing 3.5% loss=0.10178, acc=0.96875
# [37/100] testing 4.4% loss=0.21459, acc=0.92188
# [37/100] testing 4.8% loss=0.29436, acc=0.87500
# [37/100] testing 5.7% loss=0.30227, acc=0.84375
# [37/100] testing 6.6% loss=0.11575, acc=0.96875
# [37/100] testing 7.0% loss=0.09445, acc=0.96875
# [37/100] testing 7.9% loss=0.24220, acc=0.89062
# [37/100] testing 8.3% loss=0.31475, acc=0.87500
# [37/100] testing 9.2% loss=0.31918, acc=0.89062
# [37/100] testing 9.7% loss=0.13718, acc=0.93750
# [37/100] testing 10.5% loss=0.22716, acc=0.90625
# [37/100] testing 11.0% loss=0.31238, acc=0.85938
# [37/100] testing 11.8% loss=0.25600, acc=0.84375
# [37/100] testing 12.7% loss=0.39388, acc=0.84375
# [37/100] testing 13.2% loss=0.22928, acc=0.89062
# [37/100] testing 14.0% loss=0.42464, acc=0.89062
# [37/100] testing 14.5% loss=0.27050, acc=0.90625
# [37/100] testing 15.4% loss=0.21142, acc=0.90625
# [37/100] testing 15.8% loss=0.16916, acc=0.90625
# [37/100] testing 16.7% loss=0.33529, acc=0.82812
# [37/100] testing 17.5% loss=0.22018, acc=0.90625
# [37/100] testing 18.0% loss=0.28847, acc=0.89062
# [37/100] testing 18.9% loss=0.13866, acc=0.93750
# [37/100] testing 19.3% loss=0.32690, acc=0.89062
# [37/100] testing 20.2% loss=0.30485, acc=0.85938
# [37/100] testing 20.6% loss=0.27481, acc=0.90625
# [37/100] testing 21.5% loss=0.20630, acc=0.93750
# [37/100] testing 21.9% loss=0.42231, acc=0.84375
# [37/100] testing 22.8% loss=0.46039, acc=0.89062
# [37/100] testing 23.7% loss=0.39177, acc=0.85938
# [37/100] testing 24.1% loss=0.20111, acc=0.93750
# [37/100] testing 25.0% loss=0.37180, acc=0.89062
# [37/100] testing 25.4% loss=0.10696, acc=0.95312
# [37/100] testing 26.3% loss=0.35002, acc=0.85938
# [37/100] testing 26.8% loss=0.29365, acc=0.92188
# [37/100] testing 27.6% loss=0.16798, acc=0.95312
# [37/100] testing 28.5% loss=0.23723, acc=0.89062
# [37/100] testing 29.0% loss=0.20979, acc=0.89062
# [37/100] testing 29.8% loss=0.26958, acc=0.93750
# [37/100] testing 30.3% loss=0.25658, acc=0.92188
# [37/100] testing 31.1% loss=0.27575, acc=0.87500
# [37/100] testing 31.6% loss=0.16322, acc=0.93750
# [37/100] testing 32.5% loss=0.31860, acc=0.85938
# [37/100] testing 32.9% loss=0.35316, acc=0.90625
# [37/100] testing 33.8% loss=0.27122, acc=0.90625
# [37/100] testing 34.7% loss=0.38092, acc=0.89062
# [37/100] testing 35.1% loss=0.17617, acc=0.93750
# [37/100] testing 36.0% loss=0.32106, acc=0.89062
# [37/100] testing 36.4% loss=0.27127, acc=0.90625
# [37/100] testing 37.3% loss=0.25941, acc=0.93750
# [37/100] testing 37.7% loss=0.38196, acc=0.82812
# [37/100] testing 38.6% loss=0.15663, acc=0.93750
# [37/100] testing 39.5% loss=0.29339, acc=0.93750
# [37/100] testing 39.9% loss=0.22050, acc=0.90625
# [37/100] testing 40.8% loss=0.30869, acc=0.92188
# [37/100] testing 41.2% loss=0.19571, acc=0.95312
# [37/100] testing 42.1% loss=0.27860, acc=0.89062
# [37/100] testing 42.5% loss=0.18972, acc=0.95312
# [37/100] testing 43.4% loss=0.35022, acc=0.87500
# [37/100] testing 43.9% loss=0.13442, acc=0.95312
# [37/100] testing 44.7% loss=0.32540, acc=0.87500
# [37/100] testing 45.6% loss=0.24889, acc=0.90625
# [37/100] testing 46.1% loss=0.28183, acc=0.85938
# [37/100] testing 46.9% loss=0.16318, acc=0.95312
# [37/100] testing 47.4% loss=0.09634, acc=0.95312
# [37/100] testing 48.3% loss=0.37528, acc=0.89062
# [37/100] testing 48.7% loss=0.41517, acc=0.84375
# [37/100] testing 49.6% loss=0.42110, acc=0.84375
# [37/100] testing 50.4% loss=0.20049, acc=0.92188
# [37/100] testing 50.9% loss=0.36363, acc=0.89062
# [37/100] testing 51.8% loss=0.36996, acc=0.84375
# [37/100] testing 52.2% loss=0.21630, acc=0.92188
# [37/100] testing 53.1% loss=0.22778, acc=0.92188
# [37/100] testing 53.5% loss=0.21854, acc=0.92188
# [37/100] testing 54.4% loss=0.32895, acc=0.89062
# [37/100] testing 54.8% loss=0.41549, acc=0.84375
# [37/100] testing 55.7% loss=0.16292, acc=0.92188
# [37/100] testing 56.6% loss=0.27778, acc=0.87500
# [37/100] testing 57.0% loss=0.42651, acc=0.85938
# [37/100] testing 57.9% loss=0.21805, acc=0.90625
# [37/100] testing 58.3% loss=0.31489, acc=0.85938
# [37/100] testing 59.2% loss=0.29868, acc=0.85938
# [37/100] testing 59.7% loss=0.28191, acc=0.90625
# [37/100] testing 60.5% loss=0.40409, acc=0.85938
# [37/100] testing 61.4% loss=0.11648, acc=0.95312
# [37/100] testing 61.9% loss=0.21564, acc=0.90625
# [37/100] testing 62.7% loss=0.23280, acc=0.90625
# [37/100] testing 63.2% loss=0.34820, acc=0.87500
# [37/100] testing 64.0% loss=0.46707, acc=0.82812
# [37/100] testing 64.5% loss=0.16818, acc=0.93750
# [37/100] testing 65.4% loss=0.19778, acc=0.92188
# [37/100] testing 65.8% loss=0.32800, acc=0.89062
# [37/100] testing 66.7% loss=0.24462, acc=0.89062
# [37/100] testing 67.6% loss=0.38407, acc=0.89062
# [37/100] testing 68.0% loss=0.12986, acc=0.96875
# [37/100] testing 68.9% loss=0.32223, acc=0.90625
# [37/100] testing 69.3% loss=0.25367, acc=0.85938
# [37/100] testing 70.2% loss=0.42053, acc=0.85938
# [37/100] testing 70.6% loss=0.33571, acc=0.85938
# [37/100] testing 71.5% loss=0.35240, acc=0.89062
# [37/100] testing 72.4% loss=0.15998, acc=0.92188
# [37/100] testing 72.8% loss=0.21596, acc=0.90625
# [37/100] testing 73.7% loss=0.23836, acc=0.89062
# [37/100] testing 74.1% loss=0.35604, acc=0.85938
# [37/100] testing 75.0% loss=0.24176, acc=0.85938
# [37/100] testing 75.4% loss=0.43506, acc=0.85938
# [37/100] testing 76.3% loss=0.06447, acc=0.98438
# [37/100] testing 76.8% loss=0.25921, acc=0.87500
# [37/100] testing 77.6% loss=0.23554, acc=0.93750
# [37/100] testing 78.5% loss=0.29302, acc=0.87500
# [37/100] testing 79.0% loss=0.21763, acc=0.89062
# [37/100] testing 79.8% loss=0.30016, acc=0.85938
# [37/100] testing 80.3% loss=0.30685, acc=0.87500
# [37/100] testing 81.2% loss=0.29076, acc=0.87500
# [37/100] testing 81.6% loss=0.18404, acc=0.90625
# [37/100] testing 82.5% loss=0.23173, acc=0.92188
# [37/100] testing 83.3% loss=0.19244, acc=0.95312
# [37/100] testing 83.8% loss=0.16530, acc=0.93750
# [37/100] testing 84.7% loss=0.33624, acc=0.85938
# [37/100] testing 85.1% loss=0.26057, acc=0.89062
# [37/100] testing 86.0% loss=0.27420, acc=0.90625
# [37/100] testing 86.4% loss=0.35709, acc=0.81250
# [37/100] testing 87.3% loss=0.23208, acc=0.89062
# [37/100] testing 87.7% loss=0.20196, acc=0.92188
# [37/100] testing 88.6% loss=0.21956, acc=0.90625
# [37/100] testing 89.5% loss=0.51154, acc=0.81250
# [37/100] testing 89.9% loss=0.19818, acc=0.93750
# [37/100] testing 90.8% loss=0.29721, acc=0.95312
# [37/100] testing 91.2% loss=0.12862, acc=0.95312
# [37/100] testing 92.1% loss=0.34267, acc=0.89062
# [37/100] testing 92.6% loss=0.28838, acc=0.85938
# [37/100] testing 93.4% loss=0.27391, acc=0.89062
# [37/100] testing 94.3% loss=0.09155, acc=0.96875
# [37/100] testing 94.7% loss=0.26031, acc=0.89062
# [37/100] testing 95.6% loss=0.46013, acc=0.81250
# [37/100] testing 96.1% loss=0.18749, acc=0.92188
# [37/100] testing 96.9% loss=0.28142, acc=0.87500
# [37/100] testing 97.4% loss=0.14273, acc=0.95312
# [37/100] testing 98.3% loss=0.25639, acc=0.89062
# [37/100] testing 98.7% loss=0.27772, acc=0.90625
# [37/100] testing 99.6% loss=0.29574, acc=0.89062
# [38/100] training 0.2% loss=0.30574, acc=0.84375
# [38/100] training 0.4% loss=0.28344, acc=0.89062
# [38/100] training 0.5% loss=0.12569, acc=0.92188
# [38/100] training 0.8% loss=0.12570, acc=0.95312
# [38/100] training 0.9% loss=0.22878, acc=0.92188
# [38/100] training 1.1% loss=0.22156, acc=0.93750
# [38/100] training 1.2% loss=0.21295, acc=0.90625
# [38/100] training 1.4% loss=0.22400, acc=0.95312
# [38/100] training 1.6% loss=0.19639, acc=0.90625
# [38/100] training 1.8% loss=0.17116, acc=0.95312
# [38/100] training 2.0% loss=0.33005, acc=0.85938
# [38/100] training 2.1% loss=0.22991, acc=0.90625
# [38/100] training 2.3% loss=0.16493, acc=0.90625
# [38/100] training 2.4% loss=0.17972, acc=0.92188
# [38/100] training 2.6% loss=0.12696, acc=0.95312
# [38/100] training 2.7% loss=0.20765, acc=0.92188
# [38/100] training 3.0% loss=0.19705, acc=0.89062
# [38/100] training 3.2% loss=0.16119, acc=0.93750
# [38/100] training 3.3% loss=0.34348, acc=0.87500
# [38/100] training 3.5% loss=0.17252, acc=0.93750
# [38/100] training 3.6% loss=0.35912, acc=0.85938
# [38/100] training 3.8% loss=0.16028, acc=0.93750
# [38/100] training 3.9% loss=0.19903, acc=0.92188
# [38/100] training 4.2% loss=0.12069, acc=0.95312
# [38/100] training 4.4% loss=0.14095, acc=0.93750
# [38/100] training 4.5% loss=0.11434, acc=0.93750
# [38/100] training 4.7% loss=0.30146, acc=0.85938
# [38/100] training 4.8% loss=0.16089, acc=0.93750
# [38/100] training 5.0% loss=0.14522, acc=0.93750
# [38/100] training 5.2% loss=0.28233, acc=0.92188
# [38/100] training 5.4% loss=0.11530, acc=0.96875
# [38/100] training 5.5% loss=0.27247, acc=0.85938
# [38/100] training 5.7% loss=0.22046, acc=0.92188
# [38/100] training 5.9% loss=0.24430, acc=0.89062
# [38/100] training 6.0% loss=0.25427, acc=0.87500
# [38/100] training 6.3% loss=0.23729, acc=0.90625
# [38/100] training 6.4% loss=0.26676, acc=0.89062
# [38/100] training 6.6% loss=0.29321, acc=0.87500
# [38/100] training 6.7% loss=0.27215, acc=0.87500
# [38/100] training 6.9% loss=0.28715, acc=0.89062
# [38/100] training 7.1% loss=0.19795, acc=0.92188
# [38/100] training 7.2% loss=0.28679, acc=0.84375
# [38/100] training 7.5% loss=0.24759, acc=0.89062
# [38/100] training 7.6% loss=0.26849, acc=0.90625
# [38/100] training 7.8% loss=0.21247, acc=0.89062
# [38/100] training 7.9% loss=0.25442, acc=0.85938
# [38/100] training 8.1% loss=0.23587, acc=0.85938
# [38/100] training 8.2% loss=0.25224, acc=0.89062
# [38/100] training 8.4% loss=0.31724, acc=0.90625
# [38/100] training 8.7% loss=0.21215, acc=0.89062
# [38/100] training 8.8% loss=0.19084, acc=0.90625
# [38/100] training 9.0% loss=0.25228, acc=0.87500
# [38/100] training 9.1% loss=0.22606, acc=0.90625
# [38/100] training 9.3% loss=0.37574, acc=0.89062
# [38/100] training 9.4% loss=0.12028, acc=0.92188
# [38/100] training 9.7% loss=0.28299, acc=0.90625
# [38/100] training 9.9% loss=0.35743, acc=0.82812
# [38/100] training 10.0% loss=0.26496, acc=0.87500
# [38/100] training 10.2% loss=0.20862, acc=0.90625
# [38/100] training 10.3% loss=0.16817, acc=0.92188
# [38/100] training 10.5% loss=0.27607, acc=0.87500
# [38/100] training 10.6% loss=0.23964, acc=0.90625
# [38/100] training 10.9% loss=0.21552, acc=0.89062
# [38/100] training 11.0% loss=0.26280, acc=0.85938
# [38/100] training 11.2% loss=0.19068, acc=0.92188
# [38/100] training 11.4% loss=0.27086, acc=0.90625
# [38/100] training 11.5% loss=0.43314, acc=0.87500
# [38/100] training 11.7% loss=0.13123, acc=0.98438
# [38/100] training 11.8% loss=0.18296, acc=0.93750
# [38/100] training 12.1% loss=0.18934, acc=0.90625
# [38/100] training 12.2% loss=0.16177, acc=0.90625
# [38/100] training 12.4% loss=0.16012, acc=0.95312
# [38/100] training 12.6% loss=0.29843, acc=0.92188
# [38/100] training 12.7% loss=0.18344, acc=0.90625
# [38/100] training 12.9% loss=0.30025, acc=0.90625
# [38/100] training 13.0% loss=0.17197, acc=0.96875
# [38/100] training 13.3% loss=0.11968, acc=0.95312
# [38/100] training 13.4% loss=0.18169, acc=0.89062
# [38/100] training 13.6% loss=0.24024, acc=0.89062
# [38/100] training 13.7% loss=0.26524, acc=0.93750
# [38/100] training 13.9% loss=0.31037, acc=0.93750
# [38/100] training 14.1% loss=0.23237, acc=0.96875
# [38/100] training 14.3% loss=0.11791, acc=0.98438
# [38/100] training 14.5% loss=0.18978, acc=0.93750
# [38/100] training 14.6% loss=0.22043, acc=0.89062
# [38/100] training 14.8% loss=0.16388, acc=0.90625
# [38/100] training 14.9% loss=0.17910, acc=0.93750
# [38/100] training 15.1% loss=0.27097, acc=0.89062
# [38/100] training 15.4% loss=0.25621, acc=0.90625
# [38/100] training 15.5% loss=0.30480, acc=0.89062
# [38/100] training 15.7% loss=0.30459, acc=0.85938
# [38/100] training 15.8% loss=0.20594, acc=0.90625
# [38/100] training 16.0% loss=0.26856, acc=0.90625
# [38/100] training 16.1% loss=0.41851, acc=0.85938
# [38/100] training 16.3% loss=0.28016, acc=0.89062
# [38/100] training 16.4% loss=0.22388, acc=0.92188
# [38/100] training 16.7% loss=0.25330, acc=0.85938
# [38/100] training 16.9% loss=0.24087, acc=0.90625
# [38/100] training 17.0% loss=0.21290, acc=0.89062
# [38/100] training 17.2% loss=0.18579, acc=0.90625
# [38/100] training 17.3% loss=0.34548, acc=0.87500
# [38/100] training 17.5% loss=0.22894, acc=0.90625
# [38/100] training 17.7% loss=0.18790, acc=0.90625
# [38/100] training 17.9% loss=0.22563, acc=0.89062
# [38/100] training 18.1% loss=0.30597, acc=0.87500
# [38/100] training 18.2% loss=0.22807, acc=0.90625
# [38/100] training 18.4% loss=0.31660, acc=0.85938
# [38/100] training 18.5% loss=0.21690, acc=0.93750
# [38/100] training 18.8% loss=0.19448, acc=0.93750
# [38/100] training 18.9% loss=0.15189, acc=0.96875
# [38/100] training 19.1% loss=0.25149, acc=0.90625
# [38/100] training 19.2% loss=0.15484, acc=0.95312
# [38/100] training 19.4% loss=0.09578, acc=0.96875
# [38/100] training 19.6% loss=0.19734, acc=0.90625
# [38/100] training 19.7% loss=0.35458, acc=0.89062
# [38/100] training 20.0% loss=0.11301, acc=0.93750
# [38/100] training 20.1% loss=0.20038, acc=0.89062
# [38/100] training 20.3% loss=0.24251, acc=0.92188
# [38/100] training 20.4% loss=0.29498, acc=0.87500
# [38/100] training 20.6% loss=0.28225, acc=0.89062
# [38/100] training 20.8% loss=0.13341, acc=0.95312
# [38/100] training 20.9% loss=0.15636, acc=0.92188
# [38/100] training 21.2% loss=0.24228, acc=0.92188
# [38/100] training 21.3% loss=0.18497, acc=0.92188
# [38/100] training 21.5% loss=0.30922, acc=0.89062
# [38/100] training 21.6% loss=0.06294, acc=0.96875
# [38/100] training 21.8% loss=0.15696, acc=0.93750
# [38/100] training 21.9% loss=0.23672, acc=0.90625
# [38/100] training 22.2% loss=0.24930, acc=0.89062
# [38/100] training 22.4% loss=0.24381, acc=0.89062
# [38/100] training 22.5% loss=0.12854, acc=0.96875
# [38/100] training 22.7% loss=0.24525, acc=0.92188
# [38/100] training 22.8% loss=0.25407, acc=0.90625
# [38/100] training 23.0% loss=0.12048, acc=0.95312
# [38/100] training 23.1% loss=0.29786, acc=0.87500
# [38/100] training 23.4% loss=0.30245, acc=0.85938
# [38/100] training 23.6% loss=0.28489, acc=0.92188
# [38/100] training 23.7% loss=0.18823, acc=0.95312
# [38/100] training 23.9% loss=0.19384, acc=0.92188
# [38/100] training 24.0% loss=0.14581, acc=0.92188
# [38/100] training 24.2% loss=0.17873, acc=0.92188
# [38/100] training 24.3% loss=0.30244, acc=0.92188
# [38/100] training 24.6% loss=0.18806, acc=0.90625
# [38/100] training 24.7% loss=0.28224, acc=0.90625
# [38/100] training 24.9% loss=0.16659, acc=0.90625
# [38/100] training 25.1% loss=0.22673, acc=0.90625
# [38/100] training 25.2% loss=0.17154, acc=0.95312
# [38/100] training 25.4% loss=0.24058, acc=0.87500
# [38/100] training 25.6% loss=0.14417, acc=0.95312
# [38/100] training 25.8% loss=0.16685, acc=0.95312
# [38/100] training 25.9% loss=0.09455, acc=0.98438
# [38/100] training 26.1% loss=0.13554, acc=0.93750
# [38/100] training 26.3% loss=0.18221, acc=0.92188
# [38/100] training 26.4% loss=0.14328, acc=0.93750
# [38/100] training 26.6% loss=0.06874, acc=0.98438
# [38/100] training 26.8% loss=0.15967, acc=0.90625
# [38/100] training 27.0% loss=0.13766, acc=0.93750
# [38/100] training 27.1% loss=0.22907, acc=0.95312
# [38/100] training 27.3% loss=0.18972, acc=0.89062
# [38/100] training 27.4% loss=0.08174, acc=0.96875
# [38/100] training 27.6% loss=0.37897, acc=0.89062
# [38/100] training 27.9% loss=0.13969, acc=0.95312
# [38/100] training 28.0% loss=0.22802, acc=0.95312
# [38/100] training 28.2% loss=0.24867, acc=0.92188
# [38/100] training 28.3% loss=0.11110, acc=0.96875
# [38/100] training 28.5% loss=0.34025, acc=0.84375
# [38/100] training 28.6% loss=0.18189, acc=0.96875
# [38/100] training 28.8% loss=0.16684, acc=0.93750
# [38/100] training 29.1% loss=0.18886, acc=0.92188
# [38/100] training 29.2% loss=0.19029, acc=0.93750
# [38/100] training 29.4% loss=0.22991, acc=0.90625
# [38/100] training 29.5% loss=0.11438, acc=0.95312
# [38/100] training 29.7% loss=0.21243, acc=0.92188
# [38/100] training 29.8% loss=0.13233, acc=0.95312
# [38/100] training 30.0% loss=0.30036, acc=0.85938
# [38/100] training 30.2% loss=0.19017, acc=0.90625
# [38/100] training 30.4% loss=0.10304, acc=0.95312
# [38/100] training 30.6% loss=0.25490, acc=0.89062
# [38/100] training 30.7% loss=0.24859, acc=0.92188
# [38/100] training 30.9% loss=0.27913, acc=0.87500
# [38/100] training 31.0% loss=0.11758, acc=0.93750
# [38/100] training 31.3% loss=0.13990, acc=0.95312
# [38/100] training 31.4% loss=0.23828, acc=0.92188
# [38/100] training 31.6% loss=0.21793, acc=0.93750
# [38/100] training 31.8% loss=0.21555, acc=0.92188
# [38/100] training 31.9% loss=0.21260, acc=0.90625
# [38/100] training 32.1% loss=0.20089, acc=0.89062
# [38/100] training 32.2% loss=0.31012, acc=0.90625
# [38/100] training 32.5% loss=0.12618, acc=0.96875
# [38/100] training 32.6% loss=0.18832, acc=0.90625
# [38/100] training 32.8% loss=0.20627, acc=0.95312
# [38/100] training 32.9% loss=0.24427, acc=0.89062
# [38/100] training 33.1% loss=0.13312, acc=0.95312
# [38/100] training 33.3% loss=0.23141, acc=0.87500
# [38/100] training 33.4% loss=0.13436, acc=0.93750
# [38/100] training 33.7% loss=0.17108, acc=0.93750
# [38/100] training 33.8% loss=0.14611, acc=0.95312
# [38/100] training 34.0% loss=0.22941, acc=0.87500
# [38/100] training 34.1% loss=0.21093, acc=0.92188
# [38/100] training 34.3% loss=0.22620, acc=0.92188
# [38/100] training 34.5% loss=0.17283, acc=0.95312
# [38/100] training 34.7% loss=0.17668, acc=0.93750
# [38/100] training 34.9% loss=0.18979, acc=0.89062
# [38/100] training 35.0% loss=0.18406, acc=0.92188
# [38/100] training 35.2% loss=0.25230, acc=0.92188
# [38/100] training 35.3% loss=0.24966, acc=0.90625
# [38/100] training 35.5% loss=0.18808, acc=0.92188
# [38/100] training 35.6% loss=0.34904, acc=0.89062
# [38/100] training 35.9% loss=0.28545, acc=0.85938
# [38/100] training 36.1% loss=0.35095, acc=0.84375
# [38/100] training 36.2% loss=0.24868, acc=0.90625
# [38/100] training 36.4% loss=0.31554, acc=0.87500
# [38/100] training 36.5% loss=0.25374, acc=0.89062
# [38/100] training 36.7% loss=0.19133, acc=0.93750
# [38/100] training 36.8% loss=0.20324, acc=0.89062
# [38/100] training 37.1% loss=0.30876, acc=0.84375
# [38/100] training 37.3% loss=0.39453, acc=0.81250
# [38/100] training 37.4% loss=0.30170, acc=0.84375
# [38/100] training 37.6% loss=0.32199, acc=0.90625
# [38/100] training 37.7% loss=0.19004, acc=0.92188
# [38/100] training 37.9% loss=0.37923, acc=0.85938
# [38/100] training 38.1% loss=0.36719, acc=0.85938
# [38/100] training 38.3% loss=0.17094, acc=0.93750
# [38/100] training 38.4% loss=0.09392, acc=0.95312
# [38/100] training 38.6% loss=0.14631, acc=0.93750
# [38/100] training 38.8% loss=0.38624, acc=0.85938
# [38/100] training 38.9% loss=0.26650, acc=0.87500
# [38/100] training 39.1% loss=0.18297, acc=0.93750
# [38/100] training 39.3% loss=0.30422, acc=0.89062
# [38/100] training 39.5% loss=0.29137, acc=0.90625
# [38/100] training 39.6% loss=0.26432, acc=0.89062
# [38/100] training 39.8% loss=0.14441, acc=0.92188
# [38/100] training 40.0% loss=0.20163, acc=0.90625
# [38/100] training 40.1% loss=0.47800, acc=0.85938
# [38/100] training 40.4% loss=0.24748, acc=0.90625
# [38/100] training 40.5% loss=0.29929, acc=0.90625
# [38/100] training 40.7% loss=0.22760, acc=0.90625
# [38/100] training 40.8% loss=0.28165, acc=0.87500
# [38/100] training 41.0% loss=0.22974, acc=0.90625
# [38/100] training 41.1% loss=0.30572, acc=0.85938
# [38/100] training 41.3% loss=0.40258, acc=0.76562
# [38/100] training 41.6% loss=0.27180, acc=0.87500
# [38/100] training 41.7% loss=0.28278, acc=0.89062
# [38/100] training 41.9% loss=0.24806, acc=0.90625
# [38/100] training 42.0% loss=0.29240, acc=0.89062
# [38/100] training 42.2% loss=0.20876, acc=0.93750
# [38/100] training 42.3% loss=0.16019, acc=0.98438
# [38/100] training 42.5% loss=0.19872, acc=0.92188
# [38/100] training 42.8% loss=0.18245, acc=0.93750
# [38/100] training 42.9% loss=0.19664, acc=0.90625
# [38/100] training 43.1% loss=0.24093, acc=0.92188
# [38/100] training 43.2% loss=0.19598, acc=0.90625
# [38/100] training 43.4% loss=0.25364, acc=0.92188
# [38/100] training 43.5% loss=0.22363, acc=0.90625
# [38/100] training 43.8% loss=0.21620, acc=0.92188
# [38/100] training 43.9% loss=0.25182, acc=0.82812
# [38/100] training 44.1% loss=0.16508, acc=0.95312
# [38/100] training 44.3% loss=0.25872, acc=0.90625
# [38/100] training 44.4% loss=0.27597, acc=0.87500
# [38/100] training 44.6% loss=0.30460, acc=0.81250
# [38/100] training 44.7% loss=0.26897, acc=0.87500
# [38/100] training 45.0% loss=0.14026, acc=0.92188
# [38/100] training 45.1% loss=0.36369, acc=0.84375
# [38/100] training 45.3% loss=0.18435, acc=0.92188
# [38/100] training 45.5% loss=0.11112, acc=0.98438
# [38/100] training 45.6% loss=0.24114, acc=0.92188
# [38/100] training 45.8% loss=0.12183, acc=0.95312
# [38/100] training 45.9% loss=0.17002, acc=0.90625
# [38/100] training 46.2% loss=0.14977, acc=0.93750
# [38/100] training 46.3% loss=0.13408, acc=0.93750
# [38/100] training 46.5% loss=0.34062, acc=0.87500
# [38/100] training 46.6% loss=0.20662, acc=0.92188
# [38/100] training 46.8% loss=0.19820, acc=0.89062
# [38/100] training 47.0% loss=0.28555, acc=0.90625
# [38/100] training 47.2% loss=0.22954, acc=0.89062
# [38/100] training 47.4% loss=0.24570, acc=0.90625
# [38/100] training 47.5% loss=0.28357, acc=0.85938
# [38/100] training 47.7% loss=0.25044, acc=0.89062
# [38/100] training 47.8% loss=0.26488, acc=0.92188
# [38/100] training 48.0% loss=0.26249, acc=0.87500
# [38/100] training 48.3% loss=0.09488, acc=0.98438
# [38/100] training 48.4% loss=0.13438, acc=0.95312
# [38/100] training 48.6% loss=0.18240, acc=0.95312
# [38/100] training 48.7% loss=0.27274, acc=0.89062
# [38/100] training 48.9% loss=0.19587, acc=0.93750
# [38/100] training 49.0% loss=0.15479, acc=0.93750
# [38/100] training 49.2% loss=0.31336, acc=0.87500
# [38/100] training 49.3% loss=0.19058, acc=0.92188
# [38/100] training 49.6% loss=0.21141, acc=0.90625
# [38/100] training 49.8% loss=0.21181, acc=0.90625
# [38/100] training 49.9% loss=0.31483, acc=0.89062
# [38/100] training 50.1% loss=0.21060, acc=0.92188
# [38/100] training 50.2% loss=0.27167, acc=0.87500
# [38/100] training 50.4% loss=0.35954, acc=0.87500
# [38/100] training 50.6% loss=0.24455, acc=0.92188
# [38/100] training 50.8% loss=0.22814, acc=0.96875
# [38/100] training 51.0% loss=0.29444, acc=0.82812
# [38/100] training 51.1% loss=0.26831, acc=0.84375
# [38/100] training 51.3% loss=0.39184, acc=0.85938
# [38/100] training 51.4% loss=0.27917, acc=0.92188
# [38/100] training 51.7% loss=0.21462, acc=0.90625
# [38/100] training 51.8% loss=0.25633, acc=0.90625
# [38/100] training 52.0% loss=0.22601, acc=0.93750
# [38/100] training 52.1% loss=0.30883, acc=0.92188
# [38/100] training 52.3% loss=0.44342, acc=0.79688
# [38/100] training 52.5% loss=0.12985, acc=0.96875
# [38/100] training 52.6% loss=0.27384, acc=0.87500
# [38/100] training 52.9% loss=0.30350, acc=0.82812
# [38/100] training 53.0% loss=0.18053, acc=0.93750
# [38/100] training 53.2% loss=0.23076, acc=0.92188
# [38/100] training 53.3% loss=0.15867, acc=0.93750
# [38/100] training 53.5% loss=0.35060, acc=0.84375
# [38/100] training 53.7% loss=0.27886, acc=0.87500
# [38/100] training 53.8% loss=0.34993, acc=0.82812
# [38/100] training 54.1% loss=0.38045, acc=0.82812
# [38/100] training 54.2% loss=0.28576, acc=0.85938
# [38/100] training 54.4% loss=0.32946, acc=0.84375
# [38/100] training 54.5% loss=0.27702, acc=0.89062
# [38/100] training 54.7% loss=0.39139, acc=0.90625
# [38/100] training 54.8% loss=0.17428, acc=0.92188
# [38/100] training 55.1% loss=0.23066, acc=0.89062
# [38/100] training 55.3% loss=0.20695, acc=0.92188
# [38/100] training 55.4% loss=0.28593, acc=0.85938
# [38/100] training 55.6% loss=0.19615, acc=0.93750
# [38/100] training 55.7% loss=0.18015, acc=0.95312
# [38/100] training 55.9% loss=0.26853, acc=0.87500
# [38/100] training 56.0% loss=0.18711, acc=0.89062
# [38/100] training 56.3% loss=0.54086, acc=0.81250
# [38/100] training 56.5% loss=0.22370, acc=0.93750
# [38/100] training 56.6% loss=0.35971, acc=0.82812
# [38/100] training 56.8% loss=0.30398, acc=0.85938
# [38/100] training 56.9% loss=0.21446, acc=0.89062
# [38/100] training 57.1% loss=0.24178, acc=0.89062
# [38/100] training 57.2% loss=0.25231, acc=0.87500
# [38/100] training 57.5% loss=0.19418, acc=0.93750
# [38/100] training 57.6% loss=0.22231, acc=0.90625
# [38/100] training 57.8% loss=0.11463, acc=0.93750
# [38/100] training 58.0% loss=0.20906, acc=0.95312
# [38/100] training 58.1% loss=0.28463, acc=0.87500
# [38/100] training 58.3% loss=0.18075, acc=0.93750
# [38/100] training 58.4% loss=0.32280, acc=0.90625
# [38/100] training 58.7% loss=0.30419, acc=0.89062
# [38/100] training 58.8% loss=0.23352, acc=0.92188
# [38/100] training 59.0% loss=0.21653, acc=0.87500
# [38/100] training 59.2% loss=0.20343, acc=0.93750
# [38/100] training 59.3% loss=0.32630, acc=0.87500
# [38/100] training 59.5% loss=0.27938, acc=0.84375
# [38/100] training 59.7% loss=0.29040, acc=0.87500
# [38/100] training 59.9% loss=0.25138, acc=0.90625
# [38/100] training 60.0% loss=0.22414, acc=0.92188
# [38/100] training 60.2% loss=0.18405, acc=0.87500
# [38/100] training 60.3% loss=0.25482, acc=0.89062
# [38/100] training 60.5% loss=0.21865, acc=0.87500
# [38/100] training 60.8% loss=0.19501, acc=0.93750
# [38/100] training 60.9% loss=0.29588, acc=0.90625
# [38/100] training 61.1% loss=0.29656, acc=0.89062
# [38/100] training 61.2% loss=0.23771, acc=0.92188
# [38/100] training 61.4% loss=0.31920, acc=0.85938
# [38/100] training 61.5% loss=0.41281, acc=0.89062
# [38/100] training 61.7% loss=0.29855, acc=0.87500
# [38/100] training 62.0% loss=0.28674, acc=0.89062
# [38/100] training 62.1% loss=0.33026, acc=0.89062
# [38/100] training 62.3% loss=0.15475, acc=0.95312
# [38/100] training 62.4% loss=0.22570, acc=0.87500
# [38/100] training 62.6% loss=0.31720, acc=0.82812
# [38/100] training 62.7% loss=0.16198, acc=0.92188
# [38/100] training 62.9% loss=0.24518, acc=0.93750
# [38/100] training 63.1% loss=0.25516, acc=0.89062
# [38/100] training 63.3% loss=0.21340, acc=0.92188
# [38/100] training 63.5% loss=0.21626, acc=0.93750
# [38/100] training 63.6% loss=0.29484, acc=0.87500
# [38/100] training 63.8% loss=0.37228, acc=0.87500
# [38/100] training 63.9% loss=0.15774, acc=0.96875
# [38/100] training 64.2% loss=0.22104, acc=0.90625
# [38/100] training 64.3% loss=0.30980, acc=0.89062
# [38/100] training 64.5% loss=0.20241, acc=0.92188
# [38/100] training 64.7% loss=0.20639, acc=0.92188
# [38/100] training 64.8% loss=0.30903, acc=0.89062
# [38/100] training 65.0% loss=0.21036, acc=0.90625
# [38/100] training 65.1% loss=0.23772, acc=0.85938
# [38/100] training 65.4% loss=0.18549, acc=0.95312
# [38/100] training 65.5% loss=0.12442, acc=0.93750
# [38/100] training 65.7% loss=0.15665, acc=0.92188
# [38/100] training 65.8% loss=0.21868, acc=0.87500
# [38/100] training 66.0% loss=0.14201, acc=0.98438
# [38/100] training 66.2% loss=0.10752, acc=0.93750
# [38/100] training 66.3% loss=0.22862, acc=0.92188
# [38/100] training 66.6% loss=0.25907, acc=0.90625
# [38/100] training 66.7% loss=0.13772, acc=0.93750
# [38/100] training 66.9% loss=0.22970, acc=0.90625
# [38/100] training 67.0% loss=0.17724, acc=0.93750
# [38/100] training 67.2% loss=0.16002, acc=0.96875
# [38/100] training 67.4% loss=0.11616, acc=0.96875
# [38/100] training 67.6% loss=0.19990, acc=0.90625
# [38/100] training 67.8% loss=0.15119, acc=0.95312
# [38/100] training 67.9% loss=0.09863, acc=0.96875
# [38/100] training 68.1% loss=0.25668, acc=0.92188
# [38/100] training 68.2% loss=0.14506, acc=0.95312
# [38/100] training 68.4% loss=0.16551, acc=0.93750
# [38/100] training 68.5% loss=0.25910, acc=0.90625
# [38/100] training 68.8% loss=0.19722, acc=0.90625
# [38/100] training 69.0% loss=0.44815, acc=0.87500
# [38/100] training 69.1% loss=0.14416, acc=0.92188
# [38/100] training 69.3% loss=0.22405, acc=0.90625
# [38/100] training 69.4% loss=0.27970, acc=0.87500
# [38/100] training 69.6% loss=0.18052, acc=0.90625
# [38/100] training 69.7% loss=0.17904, acc=0.92188
# [38/100] training 70.0% loss=0.29570, acc=0.89062
# [38/100] training 70.2% loss=0.20798, acc=0.93750
# [38/100] training 70.3% loss=0.25969, acc=0.90625
# [38/100] training 70.5% loss=0.19096, acc=0.96875
# [38/100] training 70.6% loss=0.14159, acc=0.95312
# [38/100] training 70.8% loss=0.25299, acc=0.92188
# [38/100] training 71.0% loss=0.17000, acc=0.93750
# [38/100] training 71.2% loss=0.14871, acc=0.93750
# [38/100] training 71.3% loss=0.12595, acc=0.96875
# [38/100] training 71.5% loss=0.28896, acc=0.95312
# [38/100] training 71.7% loss=0.25004, acc=0.89062
# [38/100] training 71.8% loss=0.28578, acc=0.90625
# [38/100] training 72.0% loss=0.07899, acc=0.98438
# [38/100] training 72.2% loss=0.22447, acc=0.92188
# [38/100] training 72.4% loss=0.23240, acc=0.90625
# [38/100] training 72.5% loss=0.28350, acc=0.87500
# [38/100] training 72.7% loss=0.36748, acc=0.81250
# [38/100] training 72.9% loss=0.15183, acc=0.93750
# [38/100] training 73.0% loss=0.14676, acc=0.96875
# [38/100] training 73.3% loss=0.30330, acc=0.89062
# [38/100] training 73.4% loss=0.14173, acc=0.98438
# [38/100] training 73.6% loss=0.19218, acc=0.89062
# [38/100] training 73.7% loss=0.23441, acc=0.92188
# [38/100] training 73.9% loss=0.09941, acc=0.98438
# [38/100] training 74.0% loss=0.20818, acc=0.89062
# [38/100] training 74.2% loss=0.17074, acc=0.92188
# [38/100] training 74.5% loss=0.15423, acc=0.96875
# [38/100] training 74.6% loss=0.47680, acc=0.82812
# [38/100] training 74.8% loss=0.40907, acc=0.85938
# [38/100] training 74.9% loss=0.30903, acc=0.87500
# [38/100] training 75.1% loss=0.20872, acc=0.90625
# [38/100] training 75.2% loss=0.20694, acc=0.87500
# [38/100] training 75.4% loss=0.23359, acc=0.93750
# [38/100] training 75.7% loss=0.17590, acc=0.93750
# [38/100] training 75.8% loss=0.32013, acc=0.92188
# [38/100] training 76.0% loss=0.16783, acc=0.92188
# [38/100] training 76.1% loss=0.20674, acc=0.90625
# [38/100] training 76.3% loss=0.17640, acc=0.95312
# [38/100] training 76.4% loss=0.30911, acc=0.78125
# [38/100] training 76.7% loss=0.12809, acc=0.95312
# [38/100] training 76.8% loss=0.10744, acc=0.96875
# [38/100] training 77.0% loss=0.08392, acc=0.98438
# [38/100] training 77.2% loss=0.16550, acc=0.93750
# [38/100] training 77.3% loss=0.07740, acc=0.98438
# [38/100] training 77.5% loss=0.20151, acc=0.90625
# [38/100] training 77.6% loss=0.28279, acc=0.90625
# [38/100] training 77.9% loss=0.21324, acc=0.89062
# [38/100] training 78.0% loss=0.13941, acc=0.92188
# [38/100] training 78.2% loss=0.20315, acc=0.92188
# [38/100] training 78.4% loss=0.10102, acc=0.95312
# [38/100] training 78.5% loss=0.35824, acc=0.89062
# [38/100] training 78.7% loss=0.23525, acc=0.87500
# [38/100] training 78.8% loss=0.12380, acc=0.96875
# [38/100] training 79.1% loss=0.13647, acc=0.93750
# [38/100] training 79.2% loss=0.15871, acc=0.96875
# [38/100] training 79.4% loss=0.19873, acc=0.95312
# [38/100] training 79.5% loss=0.18100, acc=0.95312
# [38/100] training 79.7% loss=0.09344, acc=0.98438
# [38/100] training 79.9% loss=0.12176, acc=0.95312
# [38/100] training 80.1% loss=0.12033, acc=0.95312
# [38/100] training 80.3% loss=0.26734, acc=0.89062
# [38/100] training 80.4% loss=0.12271, acc=0.96875
# [38/100] training 80.6% loss=0.18243, acc=0.90625
# [38/100] training 80.7% loss=0.20279, acc=0.90625
# [38/100] training 80.9% loss=0.28226, acc=0.84375
# [38/100] training 81.2% loss=0.21466, acc=0.92188
# [38/100] training 81.3% loss=0.20764, acc=0.90625
# [38/100] training 81.5% loss=0.23517, acc=0.90625
# [38/100] training 81.6% loss=0.27654, acc=0.85938
# [38/100] training 81.8% loss=0.15709, acc=0.93750
# [38/100] training 81.9% loss=0.34640, acc=0.90625
# [38/100] training 82.1% loss=0.10743, acc=0.96875
# [38/100] training 82.2% loss=0.22682, acc=0.89062
# [38/100] training 82.5% loss=0.15229, acc=0.92188
# [38/100] training 82.7% loss=0.23373, acc=0.92188
# [38/100] training 82.8% loss=0.11354, acc=0.95312
# [38/100] training 83.0% loss=0.18569, acc=0.95312
# [38/100] training 83.1% loss=0.15425, acc=0.93750
# [38/100] training 83.3% loss=0.18823, acc=0.95312
# [38/100] training 83.5% loss=0.11937, acc=0.98438
# [38/100] training 83.7% loss=0.36520, acc=0.84375
# [38/100] training 83.9% loss=0.22624, acc=0.90625
# [38/100] training 84.0% loss=0.13897, acc=0.95312
# [38/100] training 84.2% loss=0.08828, acc=1.00000
# [38/100] training 84.3% loss=0.22177, acc=0.92188
# [38/100] training 84.5% loss=0.09725, acc=0.96875
# [38/100] training 84.7% loss=0.23119, acc=0.87500
# [38/100] training 84.9% loss=0.15018, acc=0.93750
# [38/100] training 85.0% loss=0.14624, acc=0.92188
# [38/100] training 85.2% loss=0.17234, acc=0.95312
# [38/100] training 85.4% loss=0.30738, acc=0.90625
# [38/100] training 85.5% loss=0.18655, acc=0.92188
# [38/100] training 85.8% loss=0.23658, acc=0.90625
# [38/100] training 85.9% loss=0.11186, acc=0.95312
# [38/100] training 86.1% loss=0.16326, acc=0.95312
# [38/100] training 86.2% loss=0.14197, acc=0.95312
# [38/100] training 86.4% loss=0.26625, acc=0.87500
# [38/100] training 86.6% loss=0.17983, acc=0.93750
# [38/100] training 86.7% loss=0.17685, acc=0.90625
# [38/100] training 87.0% loss=0.26534, acc=0.87500
# [38/100] training 87.1% loss=0.15165, acc=0.95312
# [38/100] training 87.3% loss=0.12513, acc=0.96875
# [38/100] training 87.4% loss=0.23001, acc=0.92188
# [38/100] training 87.6% loss=0.11837, acc=0.92188
# [38/100] training 87.7% loss=0.29499, acc=0.90625
# [38/100] training 87.9% loss=0.21535, acc=0.90625
# [38/100] training 88.2% loss=0.17094, acc=0.92188
# [38/100] training 88.3% loss=0.24211, acc=0.92188
# [38/100] training 88.5% loss=0.19620, acc=0.92188
# [38/100] training 88.6% loss=0.13526, acc=0.92188
# [38/100] training 88.8% loss=0.15345, acc=0.95312
# [38/100] training 88.9% loss=0.21055, acc=0.89062
# [38/100] training 89.2% loss=0.19912, acc=0.90625
# [38/100] training 89.4% loss=0.19477, acc=0.92188
# [38/100] training 89.5% loss=0.20587, acc=0.92188
# [38/100] training 89.7% loss=0.19368, acc=0.90625
# [38/100] training 89.8% loss=0.19037, acc=0.93750
# [38/100] training 90.0% loss=0.11372, acc=0.95312
# [38/100] training 90.1% loss=0.23670, acc=0.90625
# [38/100] training 90.4% loss=0.16260, acc=0.93750
# [38/100] training 90.5% loss=0.13271, acc=0.93750
# [38/100] training 90.7% loss=0.23201, acc=0.92188
# [38/100] training 90.9% loss=0.08144, acc=0.96875
# [38/100] training 91.0% loss=0.15661, acc=0.95312
# [38/100] training 91.2% loss=0.19523, acc=0.90625
# [38/100] training 91.3% loss=0.36966, acc=0.89062
# [38/100] training 91.6% loss=0.25479, acc=0.90625
# [38/100] training 91.7% loss=0.13945, acc=0.95312
# [38/100] training 91.9% loss=0.18365, acc=0.92188
# [38/100] training 92.1% loss=0.16511, acc=0.90625
# [38/100] training 92.2% loss=0.11571, acc=0.93750
# [38/100] training 92.4% loss=0.24104, acc=0.92188
# [38/100] training 92.6% loss=0.29957, acc=0.85938
# [38/100] training 92.8% loss=0.17010, acc=0.95312
# [38/100] training 92.9% loss=0.25396, acc=0.89062
# [38/100] training 93.1% loss=0.39953, acc=0.89062
# [38/100] training 93.2% loss=0.20938, acc=0.93750
# [38/100] training 93.4% loss=0.25828, acc=0.85938
# [38/100] training 93.7% loss=0.15844, acc=0.95312
# [38/100] training 93.8% loss=0.25154, acc=0.89062
# [38/100] training 94.0% loss=0.17733, acc=0.95312
# [38/100] training 94.1% loss=0.19084, acc=0.92188
# [38/100] training 94.3% loss=0.22528, acc=0.92188
# [38/100] training 94.4% loss=0.20515, acc=0.93750
# [38/100] training 94.6% loss=0.19055, acc=0.93750
# [38/100] training 94.9% loss=0.18890, acc=0.90625
# [38/100] training 95.0% loss=0.36284, acc=0.87500
# [38/100] training 95.2% loss=0.38844, acc=0.84375
# [38/100] training 95.3% loss=0.18619, acc=0.93750
# [38/100] training 95.5% loss=0.09550, acc=0.98438
# [38/100] training 95.6% loss=0.27154, acc=0.85938
# [38/100] training 95.8% loss=0.18068, acc=0.93750
# [38/100] training 96.0% loss=0.18718, acc=0.93750
# [38/100] training 96.2% loss=0.10595, acc=0.95312
# [38/100] training 96.4% loss=0.12790, acc=0.96875
# [38/100] training 96.5% loss=0.24893, acc=0.92188
# [38/100] training 96.7% loss=0.16282, acc=0.95312
# [38/100] training 96.8% loss=0.26245, acc=0.92188
# [38/100] training 97.1% loss=0.11878, acc=0.93750
# [38/100] training 97.2% loss=0.19997, acc=0.93750
# [38/100] training 97.4% loss=0.17745, acc=0.90625
# [38/100] training 97.6% loss=0.23092, acc=0.92188
# [38/100] training 97.7% loss=0.20624, acc=0.90625
# [38/100] training 97.9% loss=0.11385, acc=0.95312
# [38/100] training 98.0% loss=0.14685, acc=0.96875
# [38/100] training 98.3% loss=0.21493, acc=0.89062
# [38/100] training 98.4% loss=0.26549, acc=0.92188
# [38/100] training 98.6% loss=0.26655, acc=0.85938
# [38/100] training 98.7% loss=0.23289, acc=0.93750
# [38/100] training 98.9% loss=0.20237, acc=0.90625
# [38/100] training 99.1% loss=0.15602, acc=0.92188
# [38/100] training 99.2% loss=0.17892, acc=0.90625
# [38/100] training 99.5% loss=0.32769, acc=0.85938
# [38/100] training 99.6% loss=0.18967, acc=0.90625
# [38/100] training 99.8% loss=0.17779, acc=0.92188
# [38/100] training 99.9% loss=0.07418, acc=0.98438
# [38/100] testing 0.9% loss=0.14702, acc=0.93750
# [38/100] testing 1.8% loss=0.48470, acc=0.82812
# [38/100] testing 2.2% loss=0.24434, acc=0.92188
# [38/100] testing 3.1% loss=0.37028, acc=0.85938
# [38/100] testing 3.5% loss=0.11310, acc=0.98438
# [38/100] testing 4.4% loss=0.28516, acc=0.87500
# [38/100] testing 4.8% loss=0.30867, acc=0.87500
# [38/100] testing 5.7% loss=0.20018, acc=0.92188
# [38/100] testing 6.6% loss=0.12998, acc=0.95312
# [38/100] testing 7.0% loss=0.12919, acc=0.95312
# [38/100] testing 7.9% loss=0.30621, acc=0.89062
# [38/100] testing 8.3% loss=0.29011, acc=0.93750
# [38/100] testing 9.2% loss=0.22667, acc=0.90625
# [38/100] testing 9.7% loss=0.09856, acc=0.98438
# [38/100] testing 10.5% loss=0.23348, acc=0.93750
# [38/100] testing 11.0% loss=0.25741, acc=0.90625
# [38/100] testing 11.8% loss=0.19372, acc=0.92188
# [38/100] testing 12.7% loss=0.33915, acc=0.84375
# [38/100] testing 13.2% loss=0.27173, acc=0.92188
# [38/100] testing 14.0% loss=0.36271, acc=0.92188
# [38/100] testing 14.5% loss=0.18745, acc=0.92188
# [38/100] testing 15.4% loss=0.31922, acc=0.87500
# [38/100] testing 15.8% loss=0.20236, acc=0.90625
# [38/100] testing 16.7% loss=0.29808, acc=0.85938
# [38/100] testing 17.5% loss=0.18692, acc=0.93750
# [38/100] testing 18.0% loss=0.27302, acc=0.92188
# [38/100] testing 18.9% loss=0.07689, acc=0.95312
# [38/100] testing 19.3% loss=0.33644, acc=0.85938
# [38/100] testing 20.2% loss=0.34924, acc=0.93750
# [38/100] testing 20.6% loss=0.30810, acc=0.87500
# [38/100] testing 21.5% loss=0.30453, acc=0.92188
# [38/100] testing 21.9% loss=0.40468, acc=0.84375
# [38/100] testing 22.8% loss=0.25453, acc=0.92188
# [38/100] testing 23.7% loss=0.28366, acc=0.89062
# [38/100] testing 24.1% loss=0.24895, acc=0.89062
# [38/100] testing 25.0% loss=0.22830, acc=0.90625
# [38/100] testing 25.4% loss=0.09757, acc=0.95312
# [38/100] testing 26.3% loss=0.30318, acc=0.89062
# [38/100] testing 26.8% loss=0.23219, acc=0.90625
# [38/100] testing 27.6% loss=0.21902, acc=0.90625
# [38/100] testing 28.5% loss=0.18656, acc=0.93750
# [38/100] testing 29.0% loss=0.25501, acc=0.90625
# [38/100] testing 29.8% loss=0.37908, acc=0.93750
# [38/100] testing 30.3% loss=0.32408, acc=0.90625
# [38/100] testing 31.1% loss=0.24303, acc=0.92188
# [38/100] testing 31.6% loss=0.23013, acc=0.90625
# [38/100] testing 32.5% loss=0.21933, acc=0.92188
# [38/100] testing 32.9% loss=0.48193, acc=0.87500
# [38/100] testing 33.8% loss=0.25152, acc=0.87500
# [38/100] testing 34.7% loss=0.34858, acc=0.90625
# [38/100] testing 35.1% loss=0.22071, acc=0.93750
# [38/100] testing 36.0% loss=0.28793, acc=0.89062
# [38/100] testing 36.4% loss=0.32386, acc=0.90625
# [38/100] testing 37.3% loss=0.36100, acc=0.93750
# [38/100] testing 37.7% loss=0.38067, acc=0.85938
# [38/100] testing 38.6% loss=0.16146, acc=0.93750
# [38/100] testing 39.5% loss=0.27186, acc=0.90625
# [38/100] testing 39.9% loss=0.34044, acc=0.89062
# [38/100] testing 40.8% loss=0.25679, acc=0.90625
# [38/100] testing 41.2% loss=0.22331, acc=0.93750
# [38/100] testing 42.1% loss=0.21389, acc=0.93750
# [38/100] testing 42.5% loss=0.20056, acc=0.93750
# [38/100] testing 43.4% loss=0.31243, acc=0.89062
# [38/100] testing 43.9% loss=0.08169, acc=0.98438
# [38/100] testing 44.7% loss=0.31672, acc=0.87500
# [38/100] testing 45.6% loss=0.22631, acc=0.90625
# [38/100] testing 46.1% loss=0.35169, acc=0.87500
# [38/100] testing 46.9% loss=0.27865, acc=0.90625
# [38/100] testing 47.4% loss=0.10578, acc=0.93750
# [38/100] testing 48.3% loss=0.41117, acc=0.85938
# [38/100] testing 48.7% loss=0.40660, acc=0.82812
# [38/100] testing 49.6% loss=0.42355, acc=0.82812
# [38/100] testing 50.4% loss=0.24783, acc=0.92188
# [38/100] testing 50.9% loss=0.29473, acc=0.87500
# [38/100] testing 51.8% loss=0.20530, acc=0.92188
# [38/100] testing 52.2% loss=0.23494, acc=0.89062
# [38/100] testing 53.1% loss=0.23129, acc=0.92188
# [38/100] testing 53.5% loss=0.21320, acc=0.92188
# [38/100] testing 54.4% loss=0.48192, acc=0.79688
# [38/100] testing 54.8% loss=0.28978, acc=0.87500
# [38/100] testing 55.7% loss=0.23393, acc=0.92188
# [38/100] testing 56.6% loss=0.28628, acc=0.89062
# [38/100] testing 57.0% loss=0.26968, acc=0.93750
# [38/100] testing 57.9% loss=0.34081, acc=0.84375
# [38/100] testing 58.3% loss=0.36341, acc=0.89062
# [38/100] testing 59.2% loss=0.20365, acc=0.92188
# [38/100] testing 59.7% loss=0.21790, acc=0.92188
# [38/100] testing 60.5% loss=0.40919, acc=0.84375
# [38/100] testing 61.4% loss=0.18637, acc=0.92188
# [38/100] testing 61.9% loss=0.25831, acc=0.92188
# [38/100] testing 62.7% loss=0.22404, acc=0.92188
# [38/100] testing 63.2% loss=0.42001, acc=0.85938
# [38/100] testing 64.0% loss=0.52247, acc=0.81250
# [38/100] testing 64.5% loss=0.20477, acc=0.89062
# [38/100] testing 65.4% loss=0.17074, acc=0.95312
# [38/100] testing 65.8% loss=0.37512, acc=0.84375
# [38/100] testing 66.7% loss=0.22697, acc=0.90625
# [38/100] testing 67.6% loss=0.44436, acc=0.89062
# [38/100] testing 68.0% loss=0.07402, acc=0.98438
# [38/100] testing 68.9% loss=0.30183, acc=0.92188
# [38/100] testing 69.3% loss=0.27205, acc=0.87500
# [38/100] testing 70.2% loss=0.43764, acc=0.84375
# [38/100] testing 70.6% loss=0.35468, acc=0.87500
# [38/100] testing 71.5% loss=0.34193, acc=0.89062
# [38/100] testing 72.4% loss=0.14236, acc=0.92188
# [38/100] testing 72.8% loss=0.10875, acc=0.96875
# [38/100] testing 73.7% loss=0.14878, acc=0.93750
# [38/100] testing 74.1% loss=0.37345, acc=0.87500
# [38/100] testing 75.0% loss=0.26362, acc=0.89062
# [38/100] testing 75.4% loss=0.31472, acc=0.90625
# [38/100] testing 76.3% loss=0.10442, acc=0.96875
# [38/100] testing 76.8% loss=0.35089, acc=0.85938
# [38/100] testing 77.6% loss=0.28769, acc=0.87500
# [38/100] testing 78.5% loss=0.39703, acc=0.84375
# [38/100] testing 79.0% loss=0.21361, acc=0.89062
# [38/100] testing 79.8% loss=0.33052, acc=0.87500
# [38/100] testing 80.3% loss=0.16736, acc=0.89062
# [38/100] testing 81.2% loss=0.38876, acc=0.85938
# [38/100] testing 81.6% loss=0.17133, acc=0.96875
# [38/100] testing 82.5% loss=0.14007, acc=0.93750
# [38/100] testing 83.3% loss=0.30198, acc=0.93750
# [38/100] testing 83.8% loss=0.12717, acc=0.98438
# [38/100] testing 84.7% loss=0.40822, acc=0.84375
# [38/100] testing 85.1% loss=0.22866, acc=0.90625
# [38/100] testing 86.0% loss=0.25289, acc=0.90625
# [38/100] testing 86.4% loss=0.31400, acc=0.85938
# [38/100] testing 87.3% loss=0.23897, acc=0.87500
# [38/100] testing 87.7% loss=0.24579, acc=0.87500
# [38/100] testing 88.6% loss=0.25002, acc=0.92188
# [38/100] testing 89.5% loss=0.39259, acc=0.84375
# [38/100] testing 89.9% loss=0.24395, acc=0.90625
# [38/100] testing 90.8% loss=0.35763, acc=0.95312
# [38/100] testing 91.2% loss=0.14430, acc=0.92188
# [38/100] testing 92.1% loss=0.44005, acc=0.85938
# [38/100] testing 92.6% loss=0.36787, acc=0.85938
# [38/100] testing 93.4% loss=0.36006, acc=0.82812
# [38/100] testing 94.3% loss=0.10575, acc=0.96875
# [38/100] testing 94.7% loss=0.17011, acc=0.90625
# [38/100] testing 95.6% loss=0.39225, acc=0.84375
# [38/100] testing 96.1% loss=0.14444, acc=0.95312
# [38/100] testing 96.9% loss=0.34629, acc=0.82812
# [38/100] testing 97.4% loss=0.11816, acc=0.96875
# [38/100] testing 98.3% loss=0.19604, acc=0.92188
# [38/100] testing 98.7% loss=0.20459, acc=0.90625
# [38/100] testing 99.6% loss=0.32287, acc=0.90625
# [39/100] training 0.2% loss=0.21037, acc=0.90625
# [39/100] training 0.4% loss=0.43489, acc=0.82812
# [39/100] training 0.5% loss=0.13755, acc=0.95312
# [39/100] training 0.8% loss=0.13008, acc=0.95312
# [39/100] training 0.9% loss=0.18133, acc=0.93750
# [39/100] training 1.1% loss=0.23431, acc=0.92188
# [39/100] training 1.2% loss=0.26620, acc=0.89062
# [39/100] training 1.4% loss=0.17578, acc=0.95312
# [39/100] training 1.6% loss=0.14150, acc=0.93750
# [39/100] training 1.8% loss=0.15328, acc=0.92188
# [39/100] training 2.0% loss=0.24953, acc=0.89062
# [39/100] training 2.1% loss=0.21249, acc=0.89062
# [39/100] training 2.3% loss=0.14474, acc=0.92188
# [39/100] training 2.4% loss=0.17894, acc=0.90625
# [39/100] training 2.6% loss=0.10164, acc=0.95312
# [39/100] training 2.7% loss=0.23437, acc=0.92188
# [39/100] training 3.0% loss=0.21934, acc=0.92188
# [39/100] training 3.2% loss=0.17487, acc=0.92188
# [39/100] training 3.3% loss=0.27883, acc=0.90625
# [39/100] training 3.5% loss=0.15592, acc=0.92188
# [39/100] training 3.6% loss=0.28996, acc=0.85938
# [39/100] training 3.8% loss=0.21109, acc=0.90625
# [39/100] training 3.9% loss=0.15702, acc=0.96875
# [39/100] training 4.2% loss=0.15059, acc=0.93750
# [39/100] training 4.4% loss=0.14739, acc=0.96875
# [39/100] training 4.5% loss=0.18169, acc=0.93750
# [39/100] training 4.7% loss=0.25433, acc=0.90625
# [39/100] training 4.8% loss=0.23064, acc=0.89062
# [39/100] training 5.0% loss=0.11991, acc=0.96875
# [39/100] training 5.2% loss=0.14793, acc=0.95312
# [39/100] training 5.4% loss=0.06466, acc=0.98438
# [39/100] training 5.5% loss=0.21070, acc=0.92188
# [39/100] training 5.7% loss=0.18565, acc=0.92188
# [39/100] training 5.9% loss=0.20240, acc=0.92188
# [39/100] training 6.0% loss=0.17750, acc=0.93750
# [39/100] training 6.3% loss=0.22013, acc=0.89062
# [39/100] training 6.4% loss=0.17857, acc=0.92188
# [39/100] training 6.6% loss=0.19236, acc=0.90625
# [39/100] training 6.7% loss=0.31259, acc=0.82812
# [39/100] training 6.9% loss=0.17433, acc=0.92188
# [39/100] training 7.1% loss=0.27314, acc=0.90625
# [39/100] training 7.2% loss=0.21166, acc=0.89062
# [39/100] training 7.5% loss=0.18034, acc=0.93750
# [39/100] training 7.6% loss=0.26186, acc=0.89062
# [39/100] training 7.8% loss=0.14046, acc=0.93750
# [39/100] training 7.9% loss=0.21522, acc=0.89062
# [39/100] training 8.1% loss=0.13419, acc=0.95312
# [39/100] training 8.2% loss=0.21610, acc=0.96875
# [39/100] training 8.4% loss=0.31308, acc=0.84375
# [39/100] training 8.7% loss=0.13361, acc=0.93750
# [39/100] training 8.8% loss=0.19666, acc=0.89062
# [39/100] training 9.0% loss=0.24464, acc=0.90625
# [39/100] training 9.1% loss=0.15119, acc=0.96875
# [39/100] training 9.3% loss=0.38024, acc=0.90625
# [39/100] training 9.4% loss=0.13815, acc=0.95312
# [39/100] training 9.7% loss=0.15983, acc=0.93750
# [39/100] training 9.9% loss=0.30818, acc=0.87500
# [39/100] training 10.0% loss=0.16342, acc=0.90625
# [39/100] training 10.2% loss=0.17119, acc=0.90625
# [39/100] training 10.3% loss=0.15366, acc=0.93750
# [39/100] training 10.5% loss=0.25170, acc=0.90625
# [39/100] training 10.6% loss=0.15084, acc=0.92188
# [39/100] training 10.9% loss=0.14723, acc=0.93750
# [39/100] training 11.0% loss=0.26634, acc=0.90625
# [39/100] training 11.2% loss=0.21812, acc=0.93750
# [39/100] training 11.4% loss=0.22045, acc=0.89062
# [39/100] training 11.5% loss=0.37121, acc=0.84375
# [39/100] training 11.7% loss=0.13417, acc=0.95312
# [39/100] training 11.8% loss=0.17566, acc=0.93750
# [39/100] training 12.1% loss=0.25189, acc=0.85938
# [39/100] training 12.2% loss=0.14638, acc=0.93750
# [39/100] training 12.4% loss=0.11965, acc=0.96875
# [39/100] training 12.6% loss=0.27192, acc=0.93750
# [39/100] training 12.7% loss=0.18546, acc=0.92188
# [39/100] training 12.9% loss=0.24936, acc=0.92188
# [39/100] training 13.0% loss=0.14492, acc=0.93750
# [39/100] training 13.3% loss=0.20296, acc=0.92188
# [39/100] training 13.4% loss=0.24686, acc=0.90625
# [39/100] training 13.6% loss=0.15232, acc=0.95312
# [39/100] training 13.7% loss=0.29996, acc=0.85938
# [39/100] training 13.9% loss=0.22888, acc=0.90625
# [39/100] training 14.1% loss=0.29728, acc=0.92188
# [39/100] training 14.3% loss=0.15027, acc=0.93750
# [39/100] training 14.5% loss=0.18791, acc=0.96875
# [39/100] training 14.6% loss=0.17662, acc=0.93750
# [39/100] training 14.8% loss=0.22673, acc=0.87500
# [39/100] training 14.9% loss=0.11513, acc=0.95312
# [39/100] training 15.1% loss=0.27945, acc=0.90625
# [39/100] training 15.4% loss=0.24946, acc=0.89062
# [39/100] training 15.5% loss=0.12458, acc=0.96875
# [39/100] training 15.7% loss=0.34074, acc=0.87500
# [39/100] training 15.8% loss=0.13283, acc=0.95312
# [39/100] training 16.0% loss=0.22817, acc=0.90625
# [39/100] training 16.1% loss=0.25801, acc=0.87500
# [39/100] training 16.3% loss=0.22798, acc=0.90625
# [39/100] training 16.4% loss=0.18596, acc=0.93750
# [39/100] training 16.7% loss=0.27581, acc=0.89062
# [39/100] training 16.9% loss=0.25528, acc=0.90625
# [39/100] training 17.0% loss=0.18481, acc=0.90625
# [39/100] training 17.2% loss=0.15900, acc=0.92188
# [39/100] training 17.3% loss=0.26546, acc=0.89062
# [39/100] training 17.5% loss=0.29037, acc=0.93750
# [39/100] training 17.7% loss=0.15298, acc=0.93750
# [39/100] training 17.9% loss=0.23305, acc=0.92188
# [39/100] training 18.1% loss=0.24318, acc=0.87500
# [39/100] training 18.2% loss=0.24817, acc=0.92188
# [39/100] training 18.4% loss=0.25505, acc=0.87500
# [39/100] training 18.5% loss=0.20965, acc=0.92188
# [39/100] training 18.8% loss=0.21734, acc=0.93750
# [39/100] training 18.9% loss=0.15978, acc=0.95312
# [39/100] training 19.1% loss=0.28629, acc=0.87500
# [39/100] training 19.2% loss=0.10034, acc=0.98438
# [39/100] training 19.4% loss=0.16723, acc=0.92188
# [39/100] training 19.6% loss=0.20088, acc=0.89062
# [39/100] training 19.7% loss=0.20732, acc=0.90625
# [39/100] training 20.0% loss=0.21997, acc=0.92188
# [39/100] training 20.1% loss=0.29658, acc=0.87500
# [39/100] training 20.3% loss=0.22800, acc=0.89062
# [39/100] training 20.4% loss=0.16707, acc=0.92188
# [39/100] training 20.6% loss=0.23143, acc=0.90625
# [39/100] training 20.8% loss=0.18558, acc=0.89062
# [39/100] training 20.9% loss=0.21589, acc=0.90625
# [39/100] training 21.2% loss=0.22584, acc=0.93750
# [39/100] training 21.3% loss=0.26302, acc=0.92188
# [39/100] training 21.5% loss=0.30424, acc=0.93750
# [39/100] training 21.6% loss=0.07400, acc=0.98438
# [39/100] training 21.8% loss=0.17957, acc=0.90625
# [39/100] training 21.9% loss=0.20640, acc=0.89062
# [39/100] training 22.2% loss=0.20210, acc=0.89062
# [39/100] training 22.4% loss=0.26001, acc=0.90625
# [39/100] training 22.5% loss=0.11397, acc=0.96875
# [39/100] training 22.7% loss=0.19345, acc=0.90625
# [39/100] training 22.8% loss=0.24167, acc=0.89062
# [39/100] training 23.0% loss=0.07250, acc=1.00000
# [39/100] training 23.1% loss=0.34215, acc=0.84375
# [39/100] training 23.4% loss=0.27063, acc=0.84375
# [39/100] training 23.6% loss=0.32581, acc=0.90625
# [39/100] training 23.7% loss=0.18047, acc=0.90625
# [39/100] training 23.9% loss=0.23455, acc=0.89062
# [39/100] training 24.0% loss=0.14684, acc=0.95312
# [39/100] training 24.2% loss=0.12398, acc=0.93750
# [39/100] training 24.3% loss=0.20645, acc=0.95312
# [39/100] training 24.6% loss=0.27262, acc=0.87500
# [39/100] training 24.7% loss=0.34136, acc=0.87500
# [39/100] training 24.9% loss=0.32807, acc=0.89062
# [39/100] training 25.1% loss=0.27882, acc=0.89062
# [39/100] training 25.2% loss=0.11662, acc=0.95312
# [39/100] training 25.4% loss=0.17105, acc=0.95312
# [39/100] training 25.6% loss=0.09634, acc=0.96875
# [39/100] training 25.8% loss=0.19745, acc=0.90625
# [39/100] training 25.9% loss=0.16489, acc=0.92188
# [39/100] training 26.1% loss=0.09081, acc=0.96875
# [39/100] training 26.3% loss=0.20731, acc=0.90625
# [39/100] training 26.4% loss=0.12686, acc=0.95312
# [39/100] training 26.6% loss=0.06411, acc=0.98438
# [39/100] training 26.8% loss=0.14570, acc=0.92188
# [39/100] training 27.0% loss=0.21324, acc=0.92188
# [39/100] training 27.1% loss=0.12347, acc=0.96875
# [39/100] training 27.3% loss=0.18235, acc=0.92188
# [39/100] training 27.4% loss=0.13315, acc=0.95312
# [39/100] training 27.6% loss=0.27462, acc=0.95312
# [39/100] training 27.9% loss=0.11890, acc=0.95312
# [39/100] training 28.0% loss=0.35352, acc=0.84375
# [39/100] training 28.2% loss=0.15911, acc=0.95312
# [39/100] training 28.3% loss=0.08676, acc=0.95312
# [39/100] training 28.5% loss=0.21109, acc=0.92188
# [39/100] training 28.6% loss=0.30043, acc=0.90625
# [39/100] training 28.8% loss=0.16593, acc=0.93750
# [39/100] training 29.1% loss=0.17381, acc=0.93750
# [39/100] training 29.2% loss=0.12893, acc=0.96875
# [39/100] training 29.4% loss=0.26428, acc=0.89062
# [39/100] training 29.5% loss=0.15605, acc=0.95312
# [39/100] training 29.7% loss=0.26496, acc=0.85938
# [39/100] training 29.8% loss=0.16619, acc=0.93750
# [39/100] training 30.0% loss=0.22332, acc=0.89062
# [39/100] training 30.2% loss=0.18620, acc=0.92188
# [39/100] training 30.4% loss=0.12772, acc=0.95312
# [39/100] training 30.6% loss=0.30496, acc=0.90625
# [39/100] training 30.7% loss=0.18924, acc=0.93750
# [39/100] training 30.9% loss=0.35670, acc=0.87500
# [39/100] training 31.0% loss=0.10160, acc=0.95312
# [39/100] training 31.3% loss=0.13358, acc=0.95312
# [39/100] training 31.4% loss=0.32251, acc=0.84375
# [39/100] training 31.6% loss=0.25740, acc=0.89062
# [39/100] training 31.8% loss=0.16332, acc=0.95312
# [39/100] training 31.9% loss=0.28156, acc=0.87500
# [39/100] training 32.1% loss=0.18412, acc=0.92188
# [39/100] training 32.2% loss=0.22825, acc=0.93750
# [39/100] training 32.5% loss=0.10407, acc=0.95312
# [39/100] training 32.6% loss=0.23190, acc=0.90625
# [39/100] training 32.8% loss=0.20295, acc=0.95312
# [39/100] training 32.9% loss=0.23808, acc=0.90625
# [39/100] training 33.1% loss=0.12344, acc=0.93750
# [39/100] training 33.3% loss=0.26253, acc=0.85938
# [39/100] training 33.4% loss=0.13298, acc=0.96875
# [39/100] training 33.7% loss=0.16264, acc=0.93750
# [39/100] training 33.8% loss=0.23985, acc=0.87500
# [39/100] training 34.0% loss=0.23928, acc=0.89062
# [39/100] training 34.1% loss=0.18737, acc=0.92188
# [39/100] training 34.3% loss=0.23826, acc=0.89062
# [39/100] training 34.5% loss=0.19406, acc=0.90625
# [39/100] training 34.7% loss=0.15507, acc=0.93750
# [39/100] training 34.9% loss=0.15613, acc=0.93750
# [39/100] training 35.0% loss=0.13989, acc=0.96875
# [39/100] training 35.2% loss=0.24803, acc=0.89062
# [39/100] training 35.3% loss=0.21893, acc=0.92188
# [39/100] training 35.5% loss=0.15276, acc=0.95312
# [39/100] training 35.6% loss=0.35276, acc=0.87500
# [39/100] training 35.9% loss=0.15927, acc=0.92188
# [39/100] training 36.1% loss=0.32036, acc=0.84375
# [39/100] training 36.2% loss=0.19499, acc=0.93750
# [39/100] training 36.4% loss=0.22023, acc=0.93750
# [39/100] training 36.5% loss=0.24033, acc=0.92188
# [39/100] training 36.7% loss=0.20367, acc=0.89062
# [39/100] training 36.8% loss=0.12467, acc=0.96875
# [39/100] training 37.1% loss=0.26956, acc=0.87500
# [39/100] training 37.3% loss=0.21259, acc=0.90625
# [39/100] training 37.4% loss=0.12979, acc=0.93750
# [39/100] training 37.6% loss=0.09136, acc=0.98438
# [39/100] training 37.7% loss=0.29178, acc=0.90625
# [39/100] training 37.9% loss=0.30030, acc=0.92188
# [39/100] training 38.1% loss=0.39588, acc=0.84375
# [39/100] training 38.3% loss=0.16869, acc=0.93750
# [39/100] training 38.4% loss=0.07415, acc=0.98438
# [39/100] training 38.6% loss=0.14007, acc=0.96875
# [39/100] training 38.8% loss=0.15733, acc=0.92188
# [39/100] training 38.9% loss=0.15448, acc=0.90625
# [39/100] training 39.1% loss=0.26342, acc=0.90625
# [39/100] training 39.3% loss=0.21930, acc=0.90625
# [39/100] training 39.5% loss=0.27070, acc=0.85938
# [39/100] training 39.6% loss=0.18540, acc=0.95312
# [39/100] training 39.8% loss=0.08584, acc=0.98438
# [39/100] training 40.0% loss=0.22119, acc=0.89062
# [39/100] training 40.1% loss=0.32422, acc=0.87500
# [39/100] training 40.4% loss=0.11324, acc=0.95312
# [39/100] training 40.5% loss=0.25471, acc=0.92188
# [39/100] training 40.7% loss=0.20189, acc=0.90625
# [39/100] training 40.8% loss=0.15066, acc=0.93750
# [39/100] training 41.0% loss=0.11675, acc=0.95312
# [39/100] training 41.1% loss=0.22561, acc=0.89062
# [39/100] training 41.3% loss=0.25551, acc=0.90625
# [39/100] training 41.6% loss=0.31193, acc=0.89062
# [39/100] training 41.7% loss=0.30133, acc=0.90625
# [39/100] training 41.9% loss=0.16536, acc=0.92188
# [39/100] training 42.0% loss=0.27733, acc=0.87500
# [39/100] training 42.2% loss=0.18324, acc=0.93750
# [39/100] training 42.3% loss=0.14135, acc=0.95312
# [39/100] training 42.5% loss=0.15521, acc=0.95312
# [39/100] training 42.8% loss=0.17951, acc=0.92188
# [39/100] training 42.9% loss=0.17238, acc=0.93750
# [39/100] training 43.1% loss=0.22581, acc=0.89062
# [39/100] training 43.2% loss=0.12263, acc=0.95312
# [39/100] training 43.4% loss=0.14801, acc=0.95312
# [39/100] training 43.5% loss=0.23863, acc=0.90625
# [39/100] training 43.8% loss=0.28881, acc=0.87500
# [39/100] training 43.9% loss=0.14394, acc=0.93750
# [39/100] training 44.1% loss=0.14629, acc=0.95312
# [39/100] training 44.3% loss=0.11496, acc=0.95312
# [39/100] training 44.4% loss=0.29012, acc=0.90625
# [39/100] training 44.6% loss=0.20059, acc=0.89062
# [39/100] training 44.7% loss=0.32330, acc=0.90625
# [39/100] training 45.0% loss=0.15387, acc=0.93750
# [39/100] training 45.1% loss=0.24733, acc=0.92188
# [39/100] training 45.3% loss=0.18765, acc=0.92188
# [39/100] training 45.5% loss=0.06933, acc=0.98438
# [39/100] training 45.6% loss=0.18710, acc=0.95312
# [39/100] training 45.8% loss=0.09978, acc=0.98438
# [39/100] training 45.9% loss=0.14556, acc=0.93750
# [39/100] training 46.2% loss=0.06403, acc=0.98438
# [39/100] training 46.3% loss=0.16660, acc=0.90625
# [39/100] training 46.5% loss=0.26207, acc=0.89062
# [39/100] training 46.6% loss=0.17970, acc=0.92188
# [39/100] training 46.8% loss=0.18781, acc=0.90625
# [39/100] training 47.0% loss=0.20581, acc=0.92188
# [39/100] training 47.2% loss=0.28223, acc=0.84375
# [39/100] training 47.4% loss=0.16266, acc=0.95312
# [39/100] training 47.5% loss=0.19605, acc=0.92188
# [39/100] training 47.7% loss=0.16951, acc=0.93750
# [39/100] training 47.8% loss=0.35600, acc=0.92188
# [39/100] training 48.0% loss=0.18410, acc=0.95312
# [39/100] training 48.3% loss=0.09483, acc=0.96875
# [39/100] training 48.4% loss=0.13214, acc=0.93750
# [39/100] training 48.6% loss=0.15435, acc=0.93750
# [39/100] training 48.7% loss=0.24305, acc=0.87500
# [39/100] training 48.9% loss=0.21414, acc=0.95312
# [39/100] training 49.0% loss=0.09433, acc=0.96875
# [39/100] training 49.2% loss=0.23191, acc=0.87500
# [39/100] training 49.3% loss=0.16019, acc=0.93750
# [39/100] training 49.6% loss=0.22636, acc=0.92188
# [39/100] training 49.8% loss=0.16739, acc=0.93750
# [39/100] training 49.9% loss=0.23687, acc=0.90625
# [39/100] training 50.1% loss=0.10704, acc=0.95312
# [39/100] training 50.2% loss=0.25463, acc=0.90625
# [39/100] training 50.4% loss=0.42189, acc=0.85938
# [39/100] training 50.6% loss=0.23727, acc=0.89062
# [39/100] training 50.8% loss=0.23056, acc=0.87500
# [39/100] training 51.0% loss=0.29966, acc=0.87500
# [39/100] training 51.1% loss=0.22760, acc=0.92188
# [39/100] training 51.3% loss=0.23499, acc=0.89062
# [39/100] training 51.4% loss=0.19225, acc=0.93750
# [39/100] training 51.7% loss=0.19553, acc=0.92188
# [39/100] training 51.8% loss=0.23665, acc=0.89062
# [39/100] training 52.0% loss=0.20644, acc=0.89062
# [39/100] training 52.1% loss=0.16849, acc=0.95312
# [39/100] training 52.3% loss=0.27954, acc=0.93750
# [39/100] training 52.5% loss=0.06274, acc=0.98438
# [39/100] training 52.6% loss=0.20784, acc=0.89062
# [39/100] training 52.9% loss=0.25679, acc=0.87500
# [39/100] training 53.0% loss=0.16589, acc=0.89062
# [39/100] training 53.2% loss=0.21749, acc=0.96875
# [39/100] training 53.3% loss=0.12830, acc=0.95312
# [39/100] training 53.5% loss=0.18148, acc=0.90625
# [39/100] training 53.7% loss=0.07904, acc=0.98438
# [39/100] training 53.8% loss=0.22934, acc=0.87500
# [39/100] training 54.1% loss=0.25926, acc=0.89062
# [39/100] training 54.2% loss=0.13352, acc=0.90625
# [39/100] training 54.4% loss=0.16451, acc=0.96875
# [39/100] training 54.5% loss=0.24941, acc=0.93750
# [39/100] training 54.7% loss=0.38131, acc=0.87500
# [39/100] training 54.8% loss=0.14848, acc=0.93750
# [39/100] training 55.1% loss=0.21941, acc=0.85938
# [39/100] training 55.3% loss=0.18138, acc=0.93750
# [39/100] training 55.4% loss=0.14436, acc=0.93750
# [39/100] training 55.6% loss=0.14382, acc=0.93750
# [39/100] training 55.7% loss=0.11094, acc=0.93750
# [39/100] training 55.9% loss=0.16273, acc=0.93750
# [39/100] training 56.0% loss=0.12698, acc=0.92188
# [39/100] training 56.3% loss=0.50949, acc=0.81250
# [39/100] training 56.5% loss=0.17646, acc=0.93750
# [39/100] training 56.6% loss=0.18616, acc=0.93750
# [39/100] training 56.8% loss=0.27165, acc=0.84375
# [39/100] training 56.9% loss=0.24624, acc=0.89062
# [39/100] training 57.1% loss=0.27985, acc=0.89062
# [39/100] training 57.2% loss=0.16968, acc=0.93750
# [39/100] training 57.5% loss=0.19522, acc=0.90625
# [39/100] training 57.6% loss=0.17328, acc=0.90625
# [39/100] training 57.8% loss=0.14329, acc=0.95312
# [39/100] training 58.0% loss=0.12010, acc=0.96875
# [39/100] training 58.1% loss=0.14155, acc=0.92188
# [39/100] training 58.3% loss=0.08033, acc=1.00000
# [39/100] training 58.4% loss=0.28457, acc=0.92188
# [39/100] training 58.7% loss=0.26604, acc=0.92188
# [39/100] training 58.8% loss=0.32391, acc=0.84375
# [39/100] training 59.0% loss=0.12730, acc=0.98438
# [39/100] training 59.2% loss=0.18812, acc=0.90625
# [39/100] training 59.3% loss=0.12472, acc=0.98438
# [39/100] training 59.5% loss=0.19828, acc=0.90625
# [39/100] training 59.7% loss=0.20567, acc=0.89062
# [39/100] training 59.9% loss=0.12984, acc=0.93750
# [39/100] training 60.0% loss=0.19376, acc=0.92188
# [39/100] training 60.2% loss=0.14459, acc=0.93750
# [39/100] training 60.3% loss=0.20243, acc=0.90625
# [39/100] training 60.5% loss=0.25653, acc=0.85938
# [39/100] training 60.8% loss=0.15573, acc=0.93750
# [39/100] training 60.9% loss=0.26395, acc=0.90625
# [39/100] training 61.1% loss=0.20510, acc=0.92188
# [39/100] training 61.2% loss=0.14560, acc=0.93750
# [39/100] training 61.4% loss=0.26340, acc=0.90625
# [39/100] training 61.5% loss=0.39538, acc=0.84375
# [39/100] training 61.7% loss=0.20214, acc=0.92188
# [39/100] training 62.0% loss=0.39556, acc=0.85938
# [39/100] training 62.1% loss=0.26374, acc=0.84375
# [39/100] training 62.3% loss=0.10513, acc=0.98438
# [39/100] training 62.4% loss=0.29600, acc=0.87500
# [39/100] training 62.6% loss=0.22910, acc=0.89062
# [39/100] training 62.7% loss=0.12803, acc=0.93750
# [39/100] training 62.9% loss=0.16737, acc=0.92188
# [39/100] training 63.1% loss=0.22237, acc=0.92188
# [39/100] training 63.3% loss=0.15897, acc=0.93750
# [39/100] training 63.5% loss=0.26700, acc=0.95312
# [39/100] training 63.6% loss=0.23953, acc=0.92188
# [39/100] training 63.8% loss=0.25092, acc=0.90625
# [39/100] training 63.9% loss=0.14737, acc=0.95312
# [39/100] training 64.2% loss=0.14625, acc=0.96875
# [39/100] training 64.3% loss=0.24244, acc=0.89062
# [39/100] training 64.5% loss=0.17345, acc=0.90625
# [39/100] training 64.7% loss=0.25540, acc=0.87500
# [39/100] training 64.8% loss=0.36449, acc=0.87500
# [39/100] training 65.0% loss=0.20030, acc=0.89062
# [39/100] training 65.1% loss=0.20018, acc=0.95312
# [39/100] training 65.4% loss=0.17012, acc=0.93750
# [39/100] training 65.5% loss=0.13432, acc=0.95312
# [39/100] training 65.7% loss=0.21183, acc=0.95312
# [39/100] training 65.8% loss=0.19467, acc=0.89062
# [39/100] training 66.0% loss=0.20762, acc=0.93750
# [39/100] training 66.2% loss=0.09537, acc=0.96875
# [39/100] training 66.3% loss=0.26887, acc=0.92188
# [39/100] training 66.6% loss=0.19423, acc=0.93750
# [39/100] training 66.7% loss=0.16609, acc=0.95312
# [39/100] training 66.9% loss=0.23038, acc=0.89062
# [39/100] training 67.0% loss=0.19810, acc=0.90625
# [39/100] training 67.2% loss=0.10146, acc=0.96875
# [39/100] training 67.4% loss=0.16793, acc=0.93750
# [39/100] training 67.6% loss=0.22475, acc=0.89062
# [39/100] training 67.8% loss=0.12029, acc=0.93750
# [39/100] training 67.9% loss=0.10603, acc=0.98438
# [39/100] training 68.1% loss=0.12187, acc=0.95312
# [39/100] training 68.2% loss=0.12611, acc=0.93750
# [39/100] training 68.4% loss=0.20475, acc=0.95312
# [39/100] training 68.5% loss=0.25561, acc=0.90625
# [39/100] training 68.8% loss=0.24194, acc=0.89062
# [39/100] training 69.0% loss=0.29879, acc=0.92188
# [39/100] training 69.1% loss=0.15960, acc=0.93750
# [39/100] training 69.3% loss=0.20757, acc=0.87500
# [39/100] training 69.4% loss=0.18244, acc=0.92188
# [39/100] training 69.6% loss=0.30408, acc=0.87500
# [39/100] training 69.7% loss=0.27123, acc=0.90625
# [39/100] training 70.0% loss=0.27611, acc=0.89062
# [39/100] training 70.2% loss=0.36087, acc=0.87500
# [39/100] training 70.3% loss=0.22212, acc=0.92188
# [39/100] training 70.5% loss=0.28380, acc=0.87500
# [39/100] training 70.6% loss=0.18549, acc=0.92188
# [39/100] training 70.8% loss=0.22110, acc=0.89062
# [39/100] training 71.0% loss=0.25630, acc=0.89062
# [39/100] training 71.2% loss=0.20419, acc=0.92188
# [39/100] training 71.3% loss=0.15502, acc=0.93750
# [39/100] training 71.5% loss=0.29781, acc=0.92188
# [39/100] training 71.7% loss=0.28062, acc=0.84375
# [39/100] training 71.8% loss=0.25329, acc=0.89062
# [39/100] training 72.0% loss=0.15085, acc=0.95312
# [39/100] training 72.2% loss=0.21792, acc=0.92188
# [39/100] training 72.4% loss=0.21745, acc=0.87500
# [39/100] training 72.5% loss=0.20678, acc=0.90625
# [39/100] training 72.7% loss=0.29358, acc=0.89062
# [39/100] training 72.9% loss=0.18953, acc=0.92188
# [39/100] training 73.0% loss=0.14907, acc=0.92188
# [39/100] training 73.3% loss=0.31803, acc=0.85938
# [39/100] training 73.4% loss=0.17853, acc=0.93750
# [39/100] training 73.6% loss=0.11328, acc=0.96875
# [39/100] training 73.7% loss=0.19009, acc=0.92188
# [39/100] training 73.9% loss=0.22076, acc=0.92188
# [39/100] training 74.0% loss=0.24911, acc=0.90625
# [39/100] training 74.2% loss=0.23154, acc=0.92188
# [39/100] training 74.5% loss=0.20075, acc=0.90625
# [39/100] training 74.6% loss=0.33852, acc=0.87500
# [39/100] training 74.8% loss=0.30313, acc=0.89062
# [39/100] training 74.9% loss=0.32023, acc=0.87500
# [39/100] training 75.1% loss=0.13720, acc=0.96875
# [39/100] training 75.2% loss=0.24648, acc=0.89062
# [39/100] training 75.4% loss=0.22266, acc=0.90625
# [39/100] training 75.7% loss=0.20185, acc=0.95312
# [39/100] training 75.8% loss=0.24320, acc=0.92188
# [39/100] training 76.0% loss=0.19164, acc=0.90625
# [39/100] training 76.1% loss=0.32364, acc=0.84375
# [39/100] training 76.3% loss=0.17554, acc=0.93750
# [39/100] training 76.4% loss=0.26426, acc=0.84375
# [39/100] training 76.7% loss=0.18604, acc=0.92188
# [39/100] training 76.8% loss=0.16279, acc=0.95312
# [39/100] training 77.0% loss=0.11931, acc=0.96875
# [39/100] training 77.2% loss=0.26533, acc=0.87500
# [39/100] training 77.3% loss=0.12800, acc=0.96875
# [39/100] training 77.5% loss=0.25719, acc=0.87500
# [39/100] training 77.6% loss=0.18572, acc=0.90625
# [39/100] training 77.9% loss=0.22588, acc=0.92188
# [39/100] training 78.0% loss=0.15935, acc=0.93750
# [39/100] training 78.2% loss=0.23235, acc=0.85938
# [39/100] training 78.4% loss=0.07973, acc=0.96875
# [39/100] training 78.5% loss=0.26993, acc=0.90625
# [39/100] training 78.7% loss=0.20669, acc=0.90625
# [39/100] training 78.8% loss=0.13977, acc=0.96875
# [39/100] training 79.1% loss=0.10144, acc=0.96875
# [39/100] training 79.2% loss=0.18440, acc=0.95312
# [39/100] training 79.4% loss=0.24238, acc=0.87500
# [39/100] training 79.5% loss=0.20968, acc=0.90625
# [39/100] training 79.7% loss=0.09735, acc=0.96875
# [39/100] training 79.9% loss=0.15107, acc=0.92188
# [39/100] training 80.1% loss=0.11635, acc=0.95312
# [39/100] training 80.3% loss=0.24307, acc=0.92188
# [39/100] training 80.4% loss=0.16937, acc=0.95312
# [39/100] training 80.6% loss=0.21775, acc=0.85938
# [39/100] training 80.7% loss=0.23461, acc=0.89062
# [39/100] training 80.9% loss=0.24415, acc=0.89062
# [39/100] training 81.2% loss=0.31342, acc=0.87500
# [39/100] training 81.3% loss=0.27192, acc=0.85938
# [39/100] training 81.5% loss=0.15013, acc=0.95312
# [39/100] training 81.6% loss=0.32867, acc=0.89062
# [39/100] training 81.8% loss=0.26895, acc=0.90625
# [39/100] training 81.9% loss=0.38264, acc=0.87500
# [39/100] training 82.1% loss=0.12383, acc=0.93750
# [39/100] training 82.2% loss=0.17047, acc=0.92188
# [39/100] training 82.5% loss=0.10663, acc=0.96875
# [39/100] training 82.7% loss=0.24007, acc=0.92188
# [39/100] training 82.8% loss=0.14223, acc=0.96875
# [39/100] training 83.0% loss=0.13279, acc=0.95312
# [39/100] training 83.1% loss=0.23328, acc=0.92188
# [39/100] training 83.3% loss=0.23995, acc=0.93750
# [39/100] training 83.5% loss=0.11751, acc=0.96875
# [39/100] training 83.7% loss=0.33680, acc=0.87500
# [39/100] training 83.9% loss=0.22646, acc=0.92188
# [39/100] training 84.0% loss=0.17409, acc=0.93750
# [39/100] training 84.2% loss=0.10705, acc=0.96875
# [39/100] training 84.3% loss=0.17418, acc=0.90625
# [39/100] training 84.5% loss=0.24042, acc=0.93750
# [39/100] training 84.7% loss=0.22907, acc=0.89062
# [39/100] training 84.9% loss=0.12505, acc=0.95312
# [39/100] training 85.0% loss=0.18542, acc=0.90625
# [39/100] training 85.2% loss=0.22911, acc=0.92188
# [39/100] training 85.4% loss=0.16916, acc=0.95312
# [39/100] training 85.5% loss=0.12888, acc=0.92188
# [39/100] training 85.8% loss=0.14301, acc=0.95312
# [39/100] training 85.9% loss=0.15134, acc=0.92188
# [39/100] training 86.1% loss=0.25096, acc=0.85938
# [39/100] training 86.2% loss=0.10924, acc=0.95312
# [39/100] training 86.4% loss=0.31838, acc=0.89062
# [39/100] training 86.6% loss=0.26121, acc=0.90625
# [39/100] training 86.7% loss=0.21536, acc=0.85938
# [39/100] training 87.0% loss=0.34329, acc=0.85938
# [39/100] training 87.1% loss=0.23435, acc=0.89062
# [39/100] training 87.3% loss=0.17412, acc=0.90625
# [39/100] training 87.4% loss=0.24406, acc=0.87500
# [39/100] training 87.6% loss=0.18854, acc=0.93750
# [39/100] training 87.7% loss=0.25997, acc=0.92188
# [39/100] training 87.9% loss=0.23686, acc=0.89062
# [39/100] training 88.2% loss=0.16510, acc=0.92188
# [39/100] training 88.3% loss=0.28179, acc=0.93750
# [39/100] training 88.5% loss=0.17890, acc=0.89062
# [39/100] training 88.6% loss=0.09140, acc=0.98438
# [39/100] training 88.8% loss=0.13348, acc=0.95312
# [39/100] training 88.9% loss=0.20566, acc=0.92188
# [39/100] training 89.2% loss=0.25843, acc=0.90625
# [39/100] training 89.4% loss=0.16186, acc=0.90625
# [39/100] training 89.5% loss=0.21412, acc=0.92188
# [39/100] training 89.7% loss=0.21357, acc=0.93750
# [39/100] training 89.8% loss=0.19506, acc=0.95312
# [39/100] training 90.0% loss=0.12267, acc=0.95312
# [39/100] training 90.1% loss=0.22779, acc=0.92188
# [39/100] training 90.4% loss=0.17646, acc=0.93750
# [39/100] training 90.5% loss=0.13980, acc=0.93750
# [39/100] training 90.7% loss=0.09748, acc=0.96875
# [39/100] training 90.9% loss=0.17864, acc=0.95312
# [39/100] training 91.0% loss=0.15430, acc=0.93750
# [39/100] training 91.2% loss=0.27102, acc=0.85938
# [39/100] training 91.3% loss=0.43375, acc=0.87500
# [39/100] training 91.6% loss=0.26479, acc=0.92188
# [39/100] training 91.7% loss=0.17206, acc=0.95312
# [39/100] training 91.9% loss=0.24393, acc=0.89062
# [39/100] training 92.1% loss=0.11464, acc=0.95312
# [39/100] training 92.2% loss=0.11831, acc=0.95312
# [39/100] training 92.4% loss=0.21013, acc=0.92188
# [39/100] training 92.6% loss=0.31820, acc=0.85938
# [39/100] training 92.8% loss=0.19512, acc=0.90625
# [39/100] training 92.9% loss=0.25039, acc=0.92188
# [39/100] training 93.1% loss=0.54922, acc=0.82812
# [39/100] training 93.2% loss=0.29479, acc=0.87500
# [39/100] training 93.4% loss=0.20685, acc=0.90625
# [39/100] training 93.7% loss=0.25984, acc=0.87500
# [39/100] training 93.8% loss=0.20608, acc=0.87500
# [39/100] training 94.0% loss=0.15518, acc=0.93750
# [39/100] training 94.1% loss=0.18302, acc=0.93750
# [39/100] training 94.3% loss=0.13271, acc=0.96875
# [39/100] training 94.4% loss=0.19225, acc=0.93750
# [39/100] training 94.6% loss=0.15598, acc=0.93750
# [39/100] training 94.9% loss=0.15554, acc=0.92188
# [39/100] training 95.0% loss=0.37812, acc=0.89062
# [39/100] training 95.2% loss=0.38440, acc=0.89062
# [39/100] training 95.3% loss=0.21537, acc=0.90625
# [39/100] training 95.5% loss=0.16239, acc=0.93750
# [39/100] training 95.6% loss=0.21487, acc=0.93750
# [39/100] training 95.8% loss=0.24359, acc=0.85938
# [39/100] training 96.0% loss=0.19603, acc=0.95312
# [39/100] training 96.2% loss=0.11436, acc=0.96875
# [39/100] training 96.4% loss=0.15900, acc=0.98438
# [39/100] training 96.5% loss=0.20036, acc=0.92188
# [39/100] training 96.7% loss=0.18362, acc=0.93750
# [39/100] training 96.8% loss=0.28587, acc=0.89062
# [39/100] training 97.1% loss=0.16402, acc=0.93750
# [39/100] training 97.2% loss=0.13029, acc=0.96875
# [39/100] training 97.4% loss=0.16665, acc=0.95312
# [39/100] training 97.6% loss=0.37726, acc=0.87500
# [39/100] training 97.7% loss=0.22458, acc=0.89062
# [39/100] training 97.9% loss=0.17322, acc=0.90625
# [39/100] training 98.0% loss=0.17995, acc=0.90625
# [39/100] training 98.3% loss=0.15887, acc=0.95312
# [39/100] training 98.4% loss=0.13984, acc=0.90625
# [39/100] training 98.6% loss=0.28363, acc=0.87500
# [39/100] training 98.7% loss=0.24129, acc=0.93750
# [39/100] training 98.9% loss=0.19181, acc=0.93750
# [39/100] training 99.1% loss=0.19572, acc=0.92188
# [39/100] training 99.2% loss=0.15760, acc=0.93750
# [39/100] training 99.5% loss=0.32662, acc=0.87500
# [39/100] training 99.6% loss=0.24198, acc=0.90625
# [39/100] training 99.8% loss=0.16591, acc=0.95312
# [39/100] training 99.9% loss=0.10000, acc=0.92188
# [39/100] testing 0.9% loss=0.14004, acc=0.90625
# [39/100] testing 1.8% loss=0.42706, acc=0.85938
# [39/100] testing 2.2% loss=0.29623, acc=0.90625
# [39/100] testing 3.1% loss=0.45162, acc=0.79688
# [39/100] testing 3.5% loss=0.13045, acc=0.95312
# [39/100] testing 4.4% loss=0.24257, acc=0.84375
# [39/100] testing 4.8% loss=0.31716, acc=0.84375
# [39/100] testing 5.7% loss=0.29930, acc=0.87500
# [39/100] testing 6.6% loss=0.18851, acc=0.90625
# [39/100] testing 7.0% loss=0.08291, acc=0.96875
# [39/100] testing 7.9% loss=0.30929, acc=0.87500
# [39/100] testing 8.3% loss=0.28738, acc=0.87500
# [39/100] testing 9.2% loss=0.37084, acc=0.87500
# [39/100] testing 9.7% loss=0.12749, acc=0.93750
# [39/100] testing 10.5% loss=0.24611, acc=0.89062
# [39/100] testing 11.0% loss=0.23022, acc=0.85938
# [39/100] testing 11.8% loss=0.26121, acc=0.90625
# [39/100] testing 12.7% loss=0.35784, acc=0.85938
# [39/100] testing 13.2% loss=0.28359, acc=0.89062
# [39/100] testing 14.0% loss=0.28620, acc=0.92188
# [39/100] testing 14.5% loss=0.19964, acc=0.92188
# [39/100] testing 15.4% loss=0.34939, acc=0.92188
# [39/100] testing 15.8% loss=0.15255, acc=0.93750
# [39/100] testing 16.7% loss=0.31074, acc=0.89062
# [39/100] testing 17.5% loss=0.25020, acc=0.90625
# [39/100] testing 18.0% loss=0.21521, acc=0.89062
# [39/100] testing 18.9% loss=0.09046, acc=0.96875
# [39/100] testing 19.3% loss=0.36057, acc=0.89062
# [39/100] testing 20.2% loss=0.27226, acc=0.92188
# [39/100] testing 20.6% loss=0.29575, acc=0.89062
# [39/100] testing 21.5% loss=0.13556, acc=0.96875
# [39/100] testing 21.9% loss=0.38147, acc=0.87500
# [39/100] testing 22.8% loss=0.44375, acc=0.84375
# [39/100] testing 23.7% loss=0.40341, acc=0.87500
# [39/100] testing 24.1% loss=0.20770, acc=0.90625
# [39/100] testing 25.0% loss=0.27276, acc=0.90625
# [39/100] testing 25.4% loss=0.09467, acc=0.96875
# [39/100] testing 26.3% loss=0.37700, acc=0.87500
# [39/100] testing 26.8% loss=0.40599, acc=0.87500
# [39/100] testing 27.6% loss=0.25799, acc=0.93750
# [39/100] testing 28.5% loss=0.28004, acc=0.92188
# [39/100] testing 29.0% loss=0.25464, acc=0.90625
# [39/100] testing 29.8% loss=0.36489, acc=0.89062
# [39/100] testing 30.3% loss=0.22408, acc=0.92188
# [39/100] testing 31.1% loss=0.20666, acc=0.92188
# [39/100] testing 31.6% loss=0.19610, acc=0.95312
# [39/100] testing 32.5% loss=0.20933, acc=0.89062
# [39/100] testing 32.9% loss=0.35974, acc=0.89062
# [39/100] testing 33.8% loss=0.27871, acc=0.85938
# [39/100] testing 34.7% loss=0.37446, acc=0.90625
# [39/100] testing 35.1% loss=0.20042, acc=0.90625
# [39/100] testing 36.0% loss=0.22860, acc=0.92188
# [39/100] testing 36.4% loss=0.27401, acc=0.93750
# [39/100] testing 37.3% loss=0.34369, acc=0.90625
# [39/100] testing 37.7% loss=0.47738, acc=0.84375
# [39/100] testing 38.6% loss=0.19716, acc=0.93750
# [39/100] testing 39.5% loss=0.16968, acc=0.96875
# [39/100] testing 39.9% loss=0.36628, acc=0.84375
# [39/100] testing 40.8% loss=0.30842, acc=0.89062
# [39/100] testing 41.2% loss=0.29939, acc=0.93750
# [39/100] testing 42.1% loss=0.22606, acc=0.90625
# [39/100] testing 42.5% loss=0.13026, acc=0.96875
# [39/100] testing 43.4% loss=0.29587, acc=0.90625
# [39/100] testing 43.9% loss=0.09565, acc=0.95312
# [39/100] testing 44.7% loss=0.27240, acc=0.85938
# [39/100] testing 45.6% loss=0.25734, acc=0.89062
# [39/100] testing 46.1% loss=0.34183, acc=0.85938
# [39/100] testing 46.9% loss=0.24001, acc=0.89062
# [39/100] testing 47.4% loss=0.15291, acc=0.95312
# [39/100] testing 48.3% loss=0.32901, acc=0.85938
# [39/100] testing 48.7% loss=0.35023, acc=0.85938
# [39/100] testing 49.6% loss=0.55154, acc=0.82812
# [39/100] testing 50.4% loss=0.24346, acc=0.93750
# [39/100] testing 50.9% loss=0.36267, acc=0.89062
# [39/100] testing 51.8% loss=0.22362, acc=0.90625
# [39/100] testing 52.2% loss=0.21608, acc=0.87500
# [39/100] testing 53.1% loss=0.20938, acc=0.90625
# [39/100] testing 53.5% loss=0.24994, acc=0.90625
# [39/100] testing 54.4% loss=0.35493, acc=0.82812
# [39/100] testing 54.8% loss=0.26327, acc=0.85938
# [39/100] testing 55.7% loss=0.17237, acc=0.93750
# [39/100] testing 56.6% loss=0.26415, acc=0.93750
# [39/100] testing 57.0% loss=0.38277, acc=0.87500
# [39/100] testing 57.9% loss=0.30631, acc=0.87500
# [39/100] testing 58.3% loss=0.36617, acc=0.87500
# [39/100] testing 59.2% loss=0.37235, acc=0.82812
# [39/100] testing 59.7% loss=0.27641, acc=0.89062
# [39/100] testing 60.5% loss=0.38850, acc=0.84375
# [39/100] testing 61.4% loss=0.11354, acc=0.93750
# [39/100] testing 61.9% loss=0.22537, acc=0.89062
# [39/100] testing 62.7% loss=0.14338, acc=0.93750
# [39/100] testing 63.2% loss=0.40664, acc=0.85938
# [39/100] testing 64.0% loss=0.52967, acc=0.81250
# [39/100] testing 64.5% loss=0.18117, acc=0.90625
# [39/100] testing 65.4% loss=0.21634, acc=0.95312
# [39/100] testing 65.8% loss=0.36896, acc=0.89062
# [39/100] testing 66.7% loss=0.22138, acc=0.87500
# [39/100] testing 67.6% loss=0.38077, acc=0.90625
# [39/100] testing 68.0% loss=0.11238, acc=0.95312
# [39/100] testing 68.9% loss=0.28161, acc=0.90625
# [39/100] testing 69.3% loss=0.38504, acc=0.84375
# [39/100] testing 70.2% loss=0.43082, acc=0.81250
# [39/100] testing 70.6% loss=0.34146, acc=0.84375
# [39/100] testing 71.5% loss=0.33803, acc=0.89062
# [39/100] testing 72.4% loss=0.17082, acc=0.90625
# [39/100] testing 72.8% loss=0.15936, acc=0.92188
# [39/100] testing 73.7% loss=0.11816, acc=0.92188
# [39/100] testing 74.1% loss=0.33309, acc=0.90625
# [39/100] testing 75.0% loss=0.17152, acc=0.90625
# [39/100] testing 75.4% loss=0.34951, acc=0.87500
# [39/100] testing 76.3% loss=0.07843, acc=0.96875
# [39/100] testing 76.8% loss=0.23164, acc=0.90625
# [39/100] testing 77.6% loss=0.17769, acc=0.90625
# [39/100] testing 78.5% loss=0.33616, acc=0.81250
# [39/100] testing 79.0% loss=0.36982, acc=0.87500
# [39/100] testing 79.8% loss=0.34670, acc=0.84375
# [39/100] testing 80.3% loss=0.31913, acc=0.87500
# [39/100] testing 81.2% loss=0.48801, acc=0.82812
# [39/100] testing 81.6% loss=0.16075, acc=0.95312
# [39/100] testing 82.5% loss=0.19411, acc=0.92188
# [39/100] testing 83.3% loss=0.21672, acc=0.95312
# [39/100] testing 83.8% loss=0.14808, acc=0.96875
# [39/100] testing 84.7% loss=0.29266, acc=0.85938
# [39/100] testing 85.1% loss=0.21807, acc=0.89062
# [39/100] testing 86.0% loss=0.24072, acc=0.92188
# [39/100] testing 86.4% loss=0.41979, acc=0.82812
# [39/100] testing 87.3% loss=0.28507, acc=0.84375
# [39/100] testing 87.7% loss=0.22613, acc=0.92188
# [39/100] testing 88.6% loss=0.32380, acc=0.90625
# [39/100] testing 89.5% loss=0.52374, acc=0.81250
# [39/100] testing 89.9% loss=0.17142, acc=0.92188
# [39/100] testing 90.8% loss=0.29071, acc=0.93750
# [39/100] testing 91.2% loss=0.13915, acc=0.95312
# [39/100] testing 92.1% loss=0.38905, acc=0.87500
# [39/100] testing 92.6% loss=0.32389, acc=0.82812
# [39/100] testing 93.4% loss=0.32801, acc=0.85938
# [39/100] testing 94.3% loss=0.14017, acc=0.95312
# [39/100] testing 94.7% loss=0.22731, acc=0.90625
# [39/100] testing 95.6% loss=0.41771, acc=0.84375
# [39/100] testing 96.1% loss=0.14288, acc=0.93750
# [39/100] testing 96.9% loss=0.25061, acc=0.90625
# [39/100] testing 97.4% loss=0.16047, acc=0.96875
# [39/100] testing 98.3% loss=0.22829, acc=0.87500
# [39/100] testing 98.7% loss=0.26886, acc=0.90625
# [39/100] testing 99.6% loss=0.28100, acc=0.87500
# [40/100] training 0.2% loss=0.26106, acc=0.90625
# [40/100] training 0.4% loss=0.39701, acc=0.85938
# [40/100] training 0.5% loss=0.12768, acc=0.95312
# [40/100] training 0.8% loss=0.21250, acc=0.90625
# [40/100] training 0.9% loss=0.18018, acc=0.93750
# [40/100] training 1.1% loss=0.32146, acc=0.90625
# [40/100] training 1.2% loss=0.26189, acc=0.87500
# [40/100] training 1.4% loss=0.11803, acc=0.96875
# [40/100] training 1.6% loss=0.12031, acc=0.95312
# [40/100] training 1.8% loss=0.19492, acc=0.95312
# [40/100] training 2.0% loss=0.27015, acc=0.90625
# [40/100] training 2.1% loss=0.30479, acc=0.82812
# [40/100] training 2.3% loss=0.17804, acc=0.92188
# [40/100] training 2.4% loss=0.21515, acc=0.92188
# [40/100] training 2.6% loss=0.14915, acc=0.92188
# [40/100] training 2.7% loss=0.38009, acc=0.87500
# [40/100] training 3.0% loss=0.17841, acc=0.93750
# [40/100] training 3.2% loss=0.10059, acc=0.98438
# [40/100] training 3.3% loss=0.42176, acc=0.89062
# [40/100] training 3.5% loss=0.22061, acc=0.89062
# [40/100] training 3.6% loss=0.20097, acc=0.90625
# [40/100] training 3.8% loss=0.16675, acc=0.92188
# [40/100] training 3.9% loss=0.20807, acc=0.90625
# [40/100] training 4.2% loss=0.11393, acc=0.96875
# [40/100] training 4.4% loss=0.16544, acc=0.95312
# [40/100] training 4.5% loss=0.12038, acc=0.93750
# [40/100] training 4.7% loss=0.33537, acc=0.89062
# [40/100] training 4.8% loss=0.21017, acc=0.92188
# [40/100] training 5.0% loss=0.07727, acc=0.98438
# [40/100] training 5.2% loss=0.16847, acc=0.93750
# [40/100] training 5.4% loss=0.10796, acc=0.96875
# [40/100] training 5.5% loss=0.20512, acc=0.89062
# [40/100] training 5.7% loss=0.19766, acc=0.95312
# [40/100] training 5.9% loss=0.20646, acc=0.95312
# [40/100] training 6.0% loss=0.21433, acc=0.89062
# [40/100] training 6.3% loss=0.17087, acc=0.90625
# [40/100] training 6.4% loss=0.08215, acc=1.00000
# [40/100] training 6.6% loss=0.19565, acc=0.90625
# [40/100] training 6.7% loss=0.27329, acc=0.85938
# [40/100] training 6.9% loss=0.22236, acc=0.87500
# [40/100] training 7.1% loss=0.18090, acc=0.93750
# [40/100] training 7.2% loss=0.27677, acc=0.89062
# [40/100] training 7.5% loss=0.23236, acc=0.92188
# [40/100] training 7.6% loss=0.31575, acc=0.93750
# [40/100] training 7.8% loss=0.21080, acc=0.92188
# [40/100] training 7.9% loss=0.18900, acc=0.89062
# [40/100] training 8.1% loss=0.15618, acc=0.98438
# [40/100] training 8.2% loss=0.23242, acc=0.90625
# [40/100] training 8.4% loss=0.28045, acc=0.87500
# [40/100] training 8.7% loss=0.15033, acc=0.95312
# [40/100] training 8.8% loss=0.19205, acc=0.92188
# [40/100] training 9.0% loss=0.13420, acc=0.95312
# [40/100] training 9.1% loss=0.15556, acc=0.96875
# [40/100] training 9.3% loss=0.32043, acc=0.92188
# [40/100] training 9.4% loss=0.16001, acc=0.93750
# [40/100] training 9.7% loss=0.15082, acc=0.95312
# [40/100] training 9.9% loss=0.30074, acc=0.87500
# [40/100] training 10.0% loss=0.19991, acc=0.90625
# [40/100] training 10.2% loss=0.18165, acc=0.90625
# [40/100] training 10.3% loss=0.15767, acc=0.92188
# [40/100] training 10.5% loss=0.30139, acc=0.89062
# [40/100] training 10.6% loss=0.18849, acc=0.87500
# [40/100] training 10.9% loss=0.11080, acc=0.95312
# [40/100] training 11.0% loss=0.27389, acc=0.89062
# [40/100] training 11.2% loss=0.10952, acc=0.95312
# [40/100] training 11.4% loss=0.20688, acc=0.87500
# [40/100] training 11.5% loss=0.34194, acc=0.87500
# [40/100] training 11.7% loss=0.16278, acc=0.93750
# [40/100] training 11.8% loss=0.20383, acc=0.87500
# [40/100] training 12.1% loss=0.14459, acc=0.93750
# [40/100] training 12.2% loss=0.09031, acc=0.96875
# [40/100] training 12.4% loss=0.20024, acc=0.90625
# [40/100] training 12.6% loss=0.27338, acc=0.93750
# [40/100] training 12.7% loss=0.26194, acc=0.90625
# [40/100] training 12.9% loss=0.27093, acc=0.92188
# [40/100] training 13.0% loss=0.13459, acc=0.92188
# [40/100] training 13.3% loss=0.25948, acc=0.92188
# [40/100] training 13.4% loss=0.25287, acc=0.87500
# [40/100] training 13.6% loss=0.14042, acc=0.95312
# [40/100] training 13.7% loss=0.28053, acc=0.89062
# [40/100] training 13.9% loss=0.33529, acc=0.92188
# [40/100] training 14.1% loss=0.25489, acc=0.93750
# [40/100] training 14.3% loss=0.16684, acc=0.95312
# [40/100] training 14.5% loss=0.20713, acc=0.90625
# [40/100] training 14.6% loss=0.25322, acc=0.90625
# [40/100] training 14.8% loss=0.21094, acc=0.89062
# [40/100] training 14.9% loss=0.14563, acc=0.95312
# [40/100] training 15.1% loss=0.30267, acc=0.85938
# [40/100] training 15.4% loss=0.23885, acc=0.87500
# [40/100] training 15.5% loss=0.20472, acc=0.93750
# [40/100] training 15.7% loss=0.26338, acc=0.93750
# [40/100] training 15.8% loss=0.14129, acc=0.95312
# [40/100] training 16.0% loss=0.24853, acc=0.92188
# [40/100] training 16.1% loss=0.17535, acc=0.93750
# [40/100] training 16.3% loss=0.22539, acc=0.89062
# [40/100] training 16.4% loss=0.22020, acc=0.87500
# [40/100] training 16.7% loss=0.27623, acc=0.87500
# [40/100] training 16.9% loss=0.25588, acc=0.89062
# [40/100] training 17.0% loss=0.21039, acc=0.89062
# [40/100] training 17.2% loss=0.14320, acc=0.96875
# [40/100] training 17.3% loss=0.25181, acc=0.92188
# [40/100] training 17.5% loss=0.23159, acc=0.92188
# [40/100] training 17.7% loss=0.17093, acc=0.93750
# [40/100] training 17.9% loss=0.25112, acc=0.90625
# [40/100] training 18.1% loss=0.23259, acc=0.89062
# [40/100] training 18.2% loss=0.19329, acc=0.89062
# [40/100] training 18.4% loss=0.25717, acc=0.92188
# [40/100] training 18.5% loss=0.21979, acc=0.93750
# [40/100] training 18.8% loss=0.15408, acc=0.92188
# [40/100] training 18.9% loss=0.14591, acc=0.92188
# [40/100] training 19.1% loss=0.29049, acc=0.85938
# [40/100] training 19.2% loss=0.11587, acc=0.96875
# [40/100] training 19.4% loss=0.19311, acc=0.93750
# [40/100] training 19.6% loss=0.17758, acc=0.95312
# [40/100] training 19.7% loss=0.20670, acc=0.92188
# [40/100] training 20.0% loss=0.13718, acc=0.96875
# [40/100] training 20.1% loss=0.18819, acc=0.92188
# [40/100] training 20.3% loss=0.18559, acc=0.95312
# [40/100] training 20.4% loss=0.24927, acc=0.89062
# [40/100] training 20.6% loss=0.25269, acc=0.90625
# [40/100] training 20.8% loss=0.10135, acc=0.96875
# [40/100] training 20.9% loss=0.19656, acc=0.90625
# [40/100] training 21.2% loss=0.25559, acc=0.90625
# [40/100] training 21.3% loss=0.14057, acc=0.96875
# [40/100] training 21.5% loss=0.31791, acc=0.89062
# [40/100] training 21.6% loss=0.09437, acc=0.96875
# [40/100] training 21.8% loss=0.17035, acc=0.90625
# [40/100] training 21.9% loss=0.17877, acc=0.95312
# [40/100] training 22.2% loss=0.19745, acc=0.92188
# [40/100] training 22.4% loss=0.24553, acc=0.89062
# [40/100] training 22.5% loss=0.09809, acc=0.96875
# [40/100] training 22.7% loss=0.19272, acc=0.92188
# [40/100] training 22.8% loss=0.24809, acc=0.89062
# [40/100] training 23.0% loss=0.09337, acc=0.96875
# [40/100] training 23.1% loss=0.32586, acc=0.85938
# [40/100] training 23.4% loss=0.26128, acc=0.85938
# [40/100] training 23.6% loss=0.26718, acc=0.93750
# [40/100] training 23.7% loss=0.15733, acc=0.95312
# [40/100] training 23.9% loss=0.21364, acc=0.90625
# [40/100] training 24.0% loss=0.13871, acc=0.98438
# [40/100] training 24.2% loss=0.24256, acc=0.90625
# [40/100] training 24.3% loss=0.22173, acc=0.95312
# [40/100] training 24.6% loss=0.23391, acc=0.92188
# [40/100] training 24.7% loss=0.27577, acc=0.90625
# [40/100] training 24.9% loss=0.24079, acc=0.90625
# [40/100] training 25.1% loss=0.26383, acc=0.89062
# [40/100] training 25.2% loss=0.12749, acc=0.95312
# [40/100] training 25.4% loss=0.17813, acc=0.93750
# [40/100] training 25.6% loss=0.13984, acc=0.93750
# [40/100] training 25.8% loss=0.15246, acc=0.95312
# [40/100] training 25.9% loss=0.12057, acc=0.98438
# [40/100] training 26.1% loss=0.12384, acc=0.95312
# [40/100] training 26.3% loss=0.23358, acc=0.89062
# [40/100] training 26.4% loss=0.19649, acc=0.93750
# [40/100] training 26.6% loss=0.04703, acc=0.98438
# [40/100] training 26.8% loss=0.11243, acc=0.93750
# [40/100] training 27.0% loss=0.16091, acc=0.95312
# [40/100] training 27.1% loss=0.25263, acc=0.92188
# [40/100] training 27.3% loss=0.10753, acc=0.93750
# [40/100] training 27.4% loss=0.14790, acc=0.93750
# [40/100] training 27.6% loss=0.44181, acc=0.85938
# [40/100] training 27.9% loss=0.23386, acc=0.89062
# [40/100] training 28.0% loss=0.19696, acc=0.92188
# [40/100] training 28.2% loss=0.19736, acc=0.92188
# [40/100] training 28.3% loss=0.14146, acc=0.95312
# [40/100] training 28.5% loss=0.21029, acc=0.90625
# [40/100] training 28.6% loss=0.21386, acc=0.93750
# [40/100] training 28.8% loss=0.13407, acc=0.93750
# [40/100] training 29.1% loss=0.20769, acc=0.92188
# [40/100] training 29.2% loss=0.16937, acc=0.90625
# [40/100] training 29.4% loss=0.28003, acc=0.89062
# [40/100] training 29.5% loss=0.10242, acc=0.96875
# [40/100] training 29.7% loss=0.23048, acc=0.90625
# [40/100] training 29.8% loss=0.14157, acc=0.90625
# [40/100] training 30.0% loss=0.19488, acc=0.92188
# [40/100] training 30.2% loss=0.13204, acc=0.92188
# [40/100] training 30.4% loss=0.06938, acc=0.98438
# [40/100] training 30.6% loss=0.34533, acc=0.89062
# [40/100] training 30.7% loss=0.21190, acc=0.92188
# [40/100] training 30.9% loss=0.31710, acc=0.93750
# [40/100] training 31.0% loss=0.10642, acc=0.93750
# [40/100] training 31.3% loss=0.18152, acc=0.95312
# [40/100] training 31.4% loss=0.34421, acc=0.84375
# [40/100] training 31.6% loss=0.18093, acc=0.93750
# [40/100] training 31.8% loss=0.16271, acc=0.92188
# [40/100] training 31.9% loss=0.23850, acc=0.90625
# [40/100] training 32.1% loss=0.19621, acc=0.89062
# [40/100] training 32.2% loss=0.23668, acc=0.90625
# [40/100] training 32.5% loss=0.10093, acc=0.96875
# [40/100] training 32.6% loss=0.23616, acc=0.90625
# [40/100] training 32.8% loss=0.15406, acc=0.95312
# [40/100] training 32.9% loss=0.25968, acc=0.87500
# [40/100] training 33.1% loss=0.12789, acc=0.93750
# [40/100] training 33.3% loss=0.23336, acc=0.89062
# [40/100] training 33.4% loss=0.10667, acc=0.95312
# [40/100] training 33.7% loss=0.17243, acc=0.93750
# [40/100] training 33.8% loss=0.16997, acc=0.90625
# [40/100] training 34.0% loss=0.22075, acc=0.92188
# [40/100] training 34.1% loss=0.22323, acc=0.92188
# [40/100] training 34.3% loss=0.27073, acc=0.90625
# [40/100] training 34.5% loss=0.32198, acc=0.87500
# [40/100] training 34.7% loss=0.15168, acc=0.93750
# [40/100] training 34.9% loss=0.14994, acc=0.92188
# [40/100] training 35.0% loss=0.12310, acc=0.98438
# [40/100] training 35.2% loss=0.28087, acc=0.85938
# [40/100] training 35.3% loss=0.19049, acc=0.89062
# [40/100] training 35.5% loss=0.16622, acc=0.90625
# [40/100] training 35.6% loss=0.29260, acc=0.85938
# [40/100] training 35.9% loss=0.18289, acc=0.92188
# [40/100] training 36.1% loss=0.30771, acc=0.87500
# [40/100] training 36.2% loss=0.14616, acc=0.96875
# [40/100] training 36.4% loss=0.18999, acc=0.95312
# [40/100] training 36.5% loss=0.22892, acc=0.93750
# [40/100] training 36.7% loss=0.23562, acc=0.92188
# [40/100] training 36.8% loss=0.11514, acc=0.95312
# [40/100] training 37.1% loss=0.39497, acc=0.85938
# [40/100] training 37.3% loss=0.22150, acc=0.92188
# [40/100] training 37.4% loss=0.23580, acc=0.87500
# [40/100] training 37.6% loss=0.13184, acc=0.95312
# [40/100] training 37.7% loss=0.30276, acc=0.85938
# [40/100] training 37.9% loss=0.14803, acc=0.92188
# [40/100] training 38.1% loss=0.36347, acc=0.87500
# [40/100] training 38.3% loss=0.20984, acc=0.92188
# [40/100] training 38.4% loss=0.04296, acc=1.00000
# [40/100] training 38.6% loss=0.14849, acc=0.93750
# [40/100] training 38.8% loss=0.28878, acc=0.89062
# [40/100] training 38.9% loss=0.24959, acc=0.93750
# [40/100] training 39.1% loss=0.30098, acc=0.87500
# [40/100] training 39.3% loss=0.28024, acc=0.89062
# [40/100] training 39.5% loss=0.26491, acc=0.90625
# [40/100] training 39.6% loss=0.16465, acc=0.96875
# [40/100] training 39.8% loss=0.10568, acc=0.93750
# [40/100] training 40.0% loss=0.22685, acc=0.92188
# [40/100] training 40.1% loss=0.31500, acc=0.87500
# [40/100] training 40.4% loss=0.12915, acc=0.93750
# [40/100] training 40.5% loss=0.28886, acc=0.92188
# [40/100] training 40.7% loss=0.18886, acc=0.90625
# [40/100] training 40.8% loss=0.11164, acc=0.95312
# [40/100] training 41.0% loss=0.10929, acc=0.95312
# [40/100] training 41.1% loss=0.37987, acc=0.84375
# [40/100] training 41.3% loss=0.21944, acc=0.87500
# [40/100] training 41.6% loss=0.22633, acc=0.90625
# [40/100] training 41.7% loss=0.26949, acc=0.93750
# [40/100] training 41.9% loss=0.13542, acc=0.95312
# [40/100] training 42.0% loss=0.15434, acc=0.92188
# [40/100] training 42.2% loss=0.15017, acc=0.96875
# [40/100] training 42.3% loss=0.17696, acc=0.92188
# [40/100] training 42.5% loss=0.17937, acc=0.93750
# [40/100] training 42.8% loss=0.16863, acc=0.95312
# [40/100] training 42.9% loss=0.18869, acc=0.95312
# [40/100] training 43.1% loss=0.19172, acc=0.95312
# [40/100] training 43.2% loss=0.11045, acc=0.96875
# [40/100] training 43.4% loss=0.26922, acc=0.89062
# [40/100] training 43.5% loss=0.25853, acc=0.89062
# [40/100] training 43.8% loss=0.27029, acc=0.92188
# [40/100] training 43.9% loss=0.20974, acc=0.93750
# [40/100] training 44.1% loss=0.15310, acc=0.96875
# [40/100] training 44.3% loss=0.16078, acc=0.93750
# [40/100] training 44.4% loss=0.25629, acc=0.93750
# [40/100] training 44.6% loss=0.23771, acc=0.87500
# [40/100] training 44.7% loss=0.26681, acc=0.92188
# [40/100] training 45.0% loss=0.10827, acc=0.93750
# [40/100] training 45.1% loss=0.27735, acc=0.92188
# [40/100] training 45.3% loss=0.21904, acc=0.90625
# [40/100] training 45.5% loss=0.04769, acc=1.00000
# [40/100] training 45.6% loss=0.16878, acc=0.92188
# [40/100] training 45.8% loss=0.09317, acc=0.98438
# [40/100] training 45.9% loss=0.16701, acc=0.93750
# [40/100] training 46.2% loss=0.12411, acc=0.95312
# [40/100] training 46.3% loss=0.13673, acc=0.96875
# [40/100] training 46.5% loss=0.23187, acc=0.92188
# [40/100] training 46.6% loss=0.12907, acc=0.95312
# [40/100] training 46.8% loss=0.14515, acc=0.96875
# [40/100] training 47.0% loss=0.20648, acc=0.89062
# [40/100] training 47.2% loss=0.32887, acc=0.92188
# [40/100] training 47.4% loss=0.16386, acc=0.95312
# [40/100] training 47.5% loss=0.26987, acc=0.87500
# [40/100] training 47.7% loss=0.15908, acc=0.90625
# [40/100] training 47.8% loss=0.20705, acc=0.93750
# [40/100] training 48.0% loss=0.11001, acc=0.95312
# [40/100] training 48.3% loss=0.09777, acc=0.96875
# [40/100] training 48.4% loss=0.10253, acc=0.93750
# [40/100] training 48.6% loss=0.12926, acc=0.96875
# [40/100] training 48.7% loss=0.17351, acc=0.92188
# [40/100] training 48.9% loss=0.19084, acc=0.95312
# [40/100] training 49.0% loss=0.10392, acc=0.95312
# [40/100] training 49.2% loss=0.26138, acc=0.85938
# [40/100] training 49.3% loss=0.15023, acc=0.93750
# [40/100] training 49.6% loss=0.31062, acc=0.93750
# [40/100] training 49.8% loss=0.20032, acc=0.89062
# [40/100] training 49.9% loss=0.22134, acc=0.90625
# [40/100] training 50.1% loss=0.11302, acc=0.92188
# [40/100] training 50.2% loss=0.18030, acc=0.89062
# [40/100] training 50.4% loss=0.39636, acc=0.87500
# [40/100] training 50.6% loss=0.14973, acc=0.93750
# [40/100] training 50.8% loss=0.26786, acc=0.81250
# [40/100] training 51.0% loss=0.27867, acc=0.90625
# [40/100] training 51.1% loss=0.19951, acc=0.95312
# [40/100] training 51.3% loss=0.21935, acc=0.93750
# [40/100] training 51.4% loss=0.23643, acc=0.92188
# [40/100] training 51.7% loss=0.18065, acc=0.89062
# [40/100] training 51.8% loss=0.15737, acc=0.92188
# [40/100] training 52.0% loss=0.22313, acc=0.89062
# [40/100] training 52.1% loss=0.18839, acc=0.92188
# [40/100] training 52.3% loss=0.28353, acc=0.93750
# [40/100] training 52.5% loss=0.10676, acc=0.95312
# [40/100] training 52.6% loss=0.20479, acc=0.90625
# [40/100] training 52.9% loss=0.20088, acc=0.93750
# [40/100] training 53.0% loss=0.11508, acc=0.95312
# [40/100] training 53.2% loss=0.13943, acc=0.95312
# [40/100] training 53.3% loss=0.18946, acc=0.93750
# [40/100] training 53.5% loss=0.17805, acc=0.89062
# [40/100] training 53.7% loss=0.04341, acc=1.00000
# [40/100] training 53.8% loss=0.31410, acc=0.90625
# [40/100] training 54.1% loss=0.32736, acc=0.85938
# [40/100] training 54.2% loss=0.12919, acc=0.92188
# [40/100] training 54.4% loss=0.17188, acc=0.92188
# [40/100] training 54.5% loss=0.35114, acc=0.85938
# [40/100] training 54.7% loss=0.32675, acc=0.82812
# [40/100] training 54.8% loss=0.14535, acc=0.93750
# [40/100] training 55.1% loss=0.17530, acc=0.93750
# [40/100] training 55.3% loss=0.12772, acc=0.95312
# [40/100] training 55.4% loss=0.20306, acc=0.89062
# [40/100] training 55.6% loss=0.11006, acc=0.98438
# [40/100] training 55.7% loss=0.15436, acc=0.92188
# [40/100] training 55.9% loss=0.17805, acc=0.92188
# [40/100] training 56.0% loss=0.12200, acc=0.96875
# [40/100] training 56.3% loss=0.30956, acc=0.84375
# [40/100] training 56.5% loss=0.20080, acc=0.90625
# [40/100] training 56.6% loss=0.11384, acc=0.95312
# [40/100] training 56.8% loss=0.24719, acc=0.87500
# [40/100] training 56.9% loss=0.25938, acc=0.87500
# [40/100] training 57.1% loss=0.22216, acc=0.90625
# [40/100] training 57.2% loss=0.17056, acc=0.92188
# [40/100] training 57.5% loss=0.18533, acc=0.90625
# [40/100] training 57.6% loss=0.18199, acc=0.92188
# [40/100] training 57.8% loss=0.12197, acc=0.95312
# [40/100] training 58.0% loss=0.11146, acc=0.96875
# [40/100] training 58.1% loss=0.14902, acc=0.93750
# [40/100] training 58.3% loss=0.10235, acc=0.95312
# [40/100] training 58.4% loss=0.21421, acc=0.92188
# [40/100] training 58.7% loss=0.22158, acc=0.92188
# [40/100] training 58.8% loss=0.24646, acc=0.90625
# [40/100] training 59.0% loss=0.12498, acc=0.93750
# [40/100] training 59.2% loss=0.16307, acc=0.92188
# [40/100] training 59.3% loss=0.19636, acc=0.93750
# [40/100] training 59.5% loss=0.19013, acc=0.93750
# [40/100] training 59.7% loss=0.24438, acc=0.90625
# [40/100] training 59.9% loss=0.15815, acc=0.95312
# [40/100] training 60.0% loss=0.16766, acc=0.92188
# [40/100] training 60.2% loss=0.26040, acc=0.89062
# [40/100] training 60.3% loss=0.15616, acc=0.92188
# [40/100] training 60.5% loss=0.16248, acc=0.95312
# [40/100] training 60.8% loss=0.22003, acc=0.89062
# [40/100] training 60.9% loss=0.33082, acc=0.90625
# [40/100] training 61.1% loss=0.20417, acc=0.92188
# [40/100] training 61.2% loss=0.16404, acc=0.93750
# [40/100] training 61.4% loss=0.20678, acc=0.92188
# [40/100] training 61.5% loss=0.40313, acc=0.87500
# [40/100] training 61.7% loss=0.15579, acc=0.95312
# [40/100] training 62.0% loss=0.34029, acc=0.89062
# [40/100] training 62.1% loss=0.28002, acc=0.90625
# [40/100] training 62.3% loss=0.11802, acc=0.95312
# [40/100] training 62.4% loss=0.27862, acc=0.87500
# [40/100] training 62.6% loss=0.22126, acc=0.89062
# [40/100] training 62.7% loss=0.12666, acc=0.95312
# [40/100] training 62.9% loss=0.24449, acc=0.90625
# [40/100] training 63.1% loss=0.21641, acc=0.89062
# [40/100] training 63.3% loss=0.14022, acc=0.95312
# [40/100] training 63.5% loss=0.23430, acc=0.92188
# [40/100] training 63.6% loss=0.26646, acc=0.87500
# [40/100] training 63.8% loss=0.26085, acc=0.92188
# [40/100] training 63.9% loss=0.18056, acc=0.90625
# [40/100] training 64.2% loss=0.17004, acc=0.92188
# [40/100] training 64.3% loss=0.26975, acc=0.90625
# [40/100] training 64.5% loss=0.19282, acc=0.90625
# [40/100] training 64.7% loss=0.16242, acc=0.93750
# [40/100] training 64.8% loss=0.39661, acc=0.79688
# [40/100] training 65.0% loss=0.19802, acc=0.92188
# [40/100] training 65.1% loss=0.26917, acc=0.92188
# [40/100] training 65.4% loss=0.09556, acc=0.96875
# [40/100] training 65.5% loss=0.12139, acc=0.95312
# [40/100] training 65.7% loss=0.10659, acc=0.95312
# [40/100] training 65.8% loss=0.27041, acc=0.84375
# [40/100] training 66.0% loss=0.18633, acc=0.93750
# [40/100] training 66.2% loss=0.07578, acc=0.98438
# [40/100] training 66.3% loss=0.16360, acc=0.92188
# [40/100] training 66.6% loss=0.17429, acc=0.92188
# [40/100] training 66.7% loss=0.12434, acc=0.96875
# [40/100] training 66.9% loss=0.22576, acc=0.92188
# [40/100] training 67.0% loss=0.14403, acc=0.96875
# [40/100] training 67.2% loss=0.19468, acc=0.96875
# [40/100] training 67.4% loss=0.15563, acc=0.93750
# [40/100] training 67.6% loss=0.12194, acc=0.96875
# [40/100] training 67.8% loss=0.26886, acc=0.90625
# [40/100] training 67.9% loss=0.12750, acc=0.93750
# [40/100] training 68.1% loss=0.23364, acc=0.92188
# [40/100] training 68.2% loss=0.14968, acc=0.96875
# [40/100] training 68.4% loss=0.16122, acc=0.96875
# [40/100] training 68.5% loss=0.24247, acc=0.87500
# [40/100] training 68.8% loss=0.16477, acc=0.90625
# [40/100] training 69.0% loss=0.35097, acc=0.92188
# [40/100] training 69.1% loss=0.35408, acc=0.89062
# [40/100] training 69.3% loss=0.09744, acc=0.95312
# [40/100] training 69.4% loss=0.12446, acc=0.98438
# [40/100] training 69.6% loss=0.16251, acc=0.92188
# [40/100] training 69.7% loss=0.30318, acc=0.89062
# [40/100] training 70.0% loss=0.21529, acc=0.92188
# [40/100] training 70.2% loss=0.28116, acc=0.90625
# [40/100] training 70.3% loss=0.21124, acc=0.93750
# [40/100] training 70.5% loss=0.14670, acc=0.96875
# [40/100] training 70.6% loss=0.09705, acc=0.98438
# [40/100] training 70.8% loss=0.30337, acc=0.84375
# [40/100] training 71.0% loss=0.22100, acc=0.92188
# [40/100] training 71.2% loss=0.11822, acc=0.95312
# [40/100] training 71.3% loss=0.12817, acc=0.95312
# [40/100] training 71.5% loss=0.21097, acc=0.93750
# [40/100] training 71.7% loss=0.23457, acc=0.90625
# [40/100] training 71.8% loss=0.26004, acc=0.92188
# [40/100] training 72.0% loss=0.10110, acc=0.96875
# [40/100] training 72.2% loss=0.31022, acc=0.84375
# [40/100] training 72.4% loss=0.16376, acc=0.90625
# [40/100] training 72.5% loss=0.22439, acc=0.89062
# [40/100] training 72.7% loss=0.32739, acc=0.85938
# [40/100] training 72.9% loss=0.11803, acc=0.95312
# [40/100] training 73.0% loss=0.13809, acc=0.93750
# [40/100] training 73.3% loss=0.27057, acc=0.89062
# [40/100] training 73.4% loss=0.15137, acc=0.95312
# [40/100] training 73.6% loss=0.10185, acc=0.98438
# [40/100] training 73.7% loss=0.19788, acc=0.93750
# [40/100] training 73.9% loss=0.17366, acc=0.93750
# [40/100] training 74.0% loss=0.25384, acc=0.89062
# [40/100] training 74.2% loss=0.13779, acc=0.93750
# [40/100] training 74.5% loss=0.16753, acc=0.93750
# [40/100] training 74.6% loss=0.32165, acc=0.87500
# [40/100] training 74.8% loss=0.24368, acc=0.92188
# [40/100] training 74.9% loss=0.28073, acc=0.89062
# [40/100] training 75.1% loss=0.17980, acc=0.92188
# [40/100] training 75.2% loss=0.14626, acc=0.93750
# [40/100] training 75.4% loss=0.24943, acc=0.89062
# [40/100] training 75.7% loss=0.16602, acc=0.95312
# [40/100] training 75.8% loss=0.23888, acc=0.87500
# [40/100] training 76.0% loss=0.17486, acc=0.92188
# [40/100] training 76.1% loss=0.18506, acc=0.92188
# [40/100] training 76.3% loss=0.11940, acc=0.95312
# [40/100] training 76.4% loss=0.25957, acc=0.85938
# [40/100] training 76.7% loss=0.20320, acc=0.92188
# [40/100] training 76.8% loss=0.18066, acc=0.95312
# [40/100] training 77.0% loss=0.07314, acc=0.96875
# [40/100] training 77.2% loss=0.19048, acc=0.90625
# [40/100] training 77.3% loss=0.19638, acc=0.89062
# [40/100] training 77.5% loss=0.26535, acc=0.90625
# [40/100] training 77.6% loss=0.16138, acc=0.93750
# [40/100] training 77.9% loss=0.18412, acc=0.89062
# [40/100] training 78.0% loss=0.20777, acc=0.92188
# [40/100] training 78.2% loss=0.23498, acc=0.92188
# [40/100] training 78.4% loss=0.12826, acc=0.96875
# [40/100] training 78.5% loss=0.32193, acc=0.89062
# [40/100] training 78.7% loss=0.21053, acc=0.92188
# [40/100] training 78.8% loss=0.14024, acc=0.96875
# [40/100] training 79.1% loss=0.14847, acc=0.95312
# [40/100] training 79.2% loss=0.18779, acc=0.92188
# [40/100] training 79.4% loss=0.17631, acc=0.92188
# [40/100] training 79.5% loss=0.11543, acc=0.95312
# [40/100] training 79.7% loss=0.06807, acc=1.00000
# [40/100] training 79.9% loss=0.13431, acc=0.95312
# [40/100] training 80.1% loss=0.13111, acc=0.95312
# [40/100] training 80.3% loss=0.24631, acc=0.93750
# [40/100] training 80.4% loss=0.17858, acc=0.92188
# [40/100] training 80.6% loss=0.20472, acc=0.93750
# [40/100] training 80.7% loss=0.22160, acc=0.90625
# [40/100] training 80.9% loss=0.21082, acc=0.90625
# [40/100] training 81.2% loss=0.29526, acc=0.84375
# [40/100] training 81.3% loss=0.19778, acc=0.95312
# [40/100] training 81.5% loss=0.18712, acc=0.93750
# [40/100] training 81.6% loss=0.31144, acc=0.84375
# [40/100] training 81.8% loss=0.13967, acc=0.93750
# [40/100] training 81.9% loss=0.42321, acc=0.87500
# [40/100] training 82.1% loss=0.14800, acc=0.95312
# [40/100] training 82.2% loss=0.15480, acc=0.93750
# [40/100] training 82.5% loss=0.14992, acc=0.95312
# [40/100] training 82.7% loss=0.18627, acc=0.92188
# [40/100] training 82.8% loss=0.17005, acc=0.95312
# [40/100] training 83.0% loss=0.15652, acc=0.93750
# [40/100] training 83.1% loss=0.16277, acc=0.93750
# [40/100] training 83.3% loss=0.13056, acc=0.93750
# [40/100] training 83.5% loss=0.11622, acc=0.92188
# [40/100] training 83.7% loss=0.40502, acc=0.84375
# [40/100] training 83.9% loss=0.17749, acc=0.93750
# [40/100] training 84.0% loss=0.06282, acc=0.98438
# [40/100] training 84.2% loss=0.14815, acc=0.93750
# [40/100] training 84.3% loss=0.12806, acc=0.93750
# [40/100] training 84.5% loss=0.20237, acc=0.93750
# [40/100] training 84.7% loss=0.18247, acc=0.93750
# [40/100] training 84.9% loss=0.15355, acc=0.95312
# [40/100] training 85.0% loss=0.11037, acc=0.98438
# [40/100] training 85.2% loss=0.19579, acc=0.93750
# [40/100] training 85.4% loss=0.09982, acc=0.95312
# [40/100] training 85.5% loss=0.21713, acc=0.90625
# [40/100] training 85.8% loss=0.17457, acc=0.93750
# [40/100] training 85.9% loss=0.17558, acc=0.93750
# [40/100] training 86.1% loss=0.18061, acc=0.89062
# [40/100] training 86.2% loss=0.09366, acc=0.95312
# [40/100] training 86.4% loss=0.33986, acc=0.89062
# [40/100] training 86.6% loss=0.20342, acc=0.92188
# [40/100] training 86.7% loss=0.18811, acc=0.93750
# [40/100] training 87.0% loss=0.19978, acc=0.95312
# [40/100] training 87.1% loss=0.13343, acc=0.95312
# [40/100] training 87.3% loss=0.13799, acc=0.96875
# [40/100] training 87.4% loss=0.20366, acc=0.93750
# [40/100] training 87.6% loss=0.18778, acc=0.92188
# [40/100] training 87.7% loss=0.28157, acc=0.89062
# [40/100] training 87.9% loss=0.25450, acc=0.90625
# [40/100] training 88.2% loss=0.14158, acc=0.96875
# [40/100] training 88.3% loss=0.24086, acc=0.92188
# [40/100] training 88.5% loss=0.18401, acc=0.89062
# [40/100] training 88.6% loss=0.15827, acc=0.95312
# [40/100] training 88.8% loss=0.18802, acc=0.92188
# [40/100] training 88.9% loss=0.28185, acc=0.89062
# [40/100] training 89.2% loss=0.16331, acc=0.92188
# [40/100] training 89.4% loss=0.17689, acc=0.93750
# [40/100] training 89.5% loss=0.22374, acc=0.89062
# [40/100] training 89.7% loss=0.25628, acc=0.90625
# [40/100] training 89.8% loss=0.20530, acc=0.93750
# [40/100] training 90.0% loss=0.17688, acc=0.89062
# [40/100] training 90.1% loss=0.20198, acc=0.92188
# [40/100] training 90.4% loss=0.22065, acc=0.92188
# [40/100] training 90.5% loss=0.15851, acc=0.92188
# [40/100] training 90.7% loss=0.08850, acc=1.00000
# [40/100] training 90.9% loss=0.17602, acc=0.95312
# [40/100] training 91.0% loss=0.15985, acc=0.95312
# [40/100] training 91.2% loss=0.16147, acc=0.92188
# [40/100] training 91.3% loss=0.25558, acc=0.92188
# [40/100] training 91.6% loss=0.23422, acc=0.92188
# [40/100] training 91.7% loss=0.26651, acc=0.89062
# [40/100] training 91.9% loss=0.23554, acc=0.90625
# [40/100] training 92.1% loss=0.12236, acc=0.93750
# [40/100] training 92.2% loss=0.08172, acc=0.96875
# [40/100] training 92.4% loss=0.22194, acc=0.87500
# [40/100] training 92.6% loss=0.39706, acc=0.84375
# [40/100] training 92.8% loss=0.21265, acc=0.92188
# [40/100] training 92.9% loss=0.20787, acc=0.92188
# [40/100] training 93.1% loss=0.51869, acc=0.81250
# [40/100] training 93.2% loss=0.28467, acc=0.87500
# [40/100] training 93.4% loss=0.23346, acc=0.92188
# [40/100] training 93.7% loss=0.20518, acc=0.92188
# [40/100] training 93.8% loss=0.15097, acc=0.95312
# [40/100] training 94.0% loss=0.15706, acc=0.95312
# [40/100] training 94.1% loss=0.16433, acc=0.92188
# [40/100] training 94.3% loss=0.12990, acc=0.95312
# [40/100] training 94.4% loss=0.21143, acc=0.92188
# [40/100] training 94.6% loss=0.09694, acc=0.96875
# [40/100] training 94.9% loss=0.14326, acc=0.92188
# [40/100] training 95.0% loss=0.20317, acc=0.87500
# [40/100] training 95.2% loss=0.33371, acc=0.89062
# [40/100] training 95.3% loss=0.28580, acc=0.92188
# [40/100] training 95.5% loss=0.17805, acc=0.90625
# [40/100] training 95.6% loss=0.25640, acc=0.92188
# [40/100] training 95.8% loss=0.23432, acc=0.90625
# [40/100] training 96.0% loss=0.21159, acc=0.93750
# [40/100] training 96.2% loss=0.15082, acc=0.95312
# [40/100] training 96.4% loss=0.14224, acc=0.96875
# [40/100] training 96.5% loss=0.17722, acc=0.95312
# [40/100] training 96.7% loss=0.13038, acc=0.95312
# [40/100] training 96.8% loss=0.20200, acc=0.90625
# [40/100] training 97.1% loss=0.16203, acc=0.93750
# [40/100] training 97.2% loss=0.15154, acc=0.96875
# [40/100] training 97.4% loss=0.19822, acc=0.92188
# [40/100] training 97.6% loss=0.29629, acc=0.87500
# [40/100] training 97.7% loss=0.09535, acc=0.95312
# [40/100] training 97.9% loss=0.16680, acc=0.95312
# [40/100] training 98.0% loss=0.17184, acc=0.95312
# [40/100] training 98.3% loss=0.27437, acc=0.90625
# [40/100] training 98.4% loss=0.14455, acc=0.95312
# [40/100] training 98.6% loss=0.34046, acc=0.85938
# [40/100] training 98.7% loss=0.28071, acc=0.92188
# [40/100] training 98.9% loss=0.17639, acc=0.92188
# [40/100] training 99.1% loss=0.18780, acc=0.92188
# [40/100] training 99.2% loss=0.18977, acc=0.93750
# [40/100] training 99.5% loss=0.21989, acc=0.90625
# [40/100] training 99.6% loss=0.18536, acc=0.92188
# [40/100] training 99.8% loss=0.15839, acc=0.92188
# [40/100] training 99.9% loss=0.05086, acc=0.98438
# [40/100] testing 0.9% loss=0.16598, acc=0.92188
# [40/100] testing 1.8% loss=0.39653, acc=0.82812
# [40/100] testing 2.2% loss=0.28128, acc=0.90625
# [40/100] testing 3.1% loss=0.40372, acc=0.89062
# [40/100] testing 3.5% loss=0.11651, acc=0.96875
# [40/100] testing 4.4% loss=0.27737, acc=0.87500
# [40/100] testing 4.8% loss=0.21428, acc=0.92188
# [40/100] testing 5.7% loss=0.22801, acc=0.87500
# [40/100] testing 6.6% loss=0.14332, acc=0.92188
# [40/100] testing 7.0% loss=0.09270, acc=0.93750
# [40/100] testing 7.9% loss=0.26612, acc=0.90625
# [40/100] testing 8.3% loss=0.26524, acc=0.90625
# [40/100] testing 9.2% loss=0.26310, acc=0.90625
# [40/100] testing 9.7% loss=0.14851, acc=0.89062
# [40/100] testing 10.5% loss=0.27011, acc=0.85938
# [40/100] testing 11.0% loss=0.28267, acc=0.87500
# [40/100] testing 11.8% loss=0.20006, acc=0.89062
# [40/100] testing 12.7% loss=0.37485, acc=0.89062
# [40/100] testing 13.2% loss=0.25741, acc=0.90625
# [40/100] testing 14.0% loss=0.36851, acc=0.89062
# [40/100] testing 14.5% loss=0.18608, acc=0.92188
# [40/100] testing 15.4% loss=0.32885, acc=0.87500
# [40/100] testing 15.8% loss=0.22623, acc=0.87500
# [40/100] testing 16.7% loss=0.17598, acc=0.92188
# [40/100] testing 17.5% loss=0.16480, acc=0.95312
# [40/100] testing 18.0% loss=0.23725, acc=0.90625
# [40/100] testing 18.9% loss=0.09481, acc=0.95312
# [40/100] testing 19.3% loss=0.27336, acc=0.89062
# [40/100] testing 20.2% loss=0.35435, acc=0.90625
# [40/100] testing 20.6% loss=0.35180, acc=0.89062
# [40/100] testing 21.5% loss=0.31531, acc=0.92188
# [40/100] testing 21.9% loss=0.46380, acc=0.84375
# [40/100] testing 22.8% loss=0.23278, acc=0.92188
# [40/100] testing 23.7% loss=0.42734, acc=0.81250
# [40/100] testing 24.1% loss=0.17360, acc=0.92188
# [40/100] testing 25.0% loss=0.26497, acc=0.89062
# [40/100] testing 25.4% loss=0.10363, acc=0.96875
# [40/100] testing 26.3% loss=0.28795, acc=0.89062
# [40/100] testing 26.8% loss=0.28241, acc=0.90625
# [40/100] testing 27.6% loss=0.22357, acc=0.92188
# [40/100] testing 28.5% loss=0.21761, acc=0.90625
# [40/100] testing 29.0% loss=0.20199, acc=0.93750
# [40/100] testing 29.8% loss=0.41349, acc=0.87500
# [40/100] testing 30.3% loss=0.28513, acc=0.93750
# [40/100] testing 31.1% loss=0.25515, acc=0.87500
# [40/100] testing 31.6% loss=0.21822, acc=0.93750
# [40/100] testing 32.5% loss=0.20559, acc=0.90625
# [40/100] testing 32.9% loss=0.46637, acc=0.89062
# [40/100] testing 33.8% loss=0.30065, acc=0.89062
# [40/100] testing 34.7% loss=0.45009, acc=0.87500
# [40/100] testing 35.1% loss=0.12091, acc=0.95312
# [40/100] testing 36.0% loss=0.28207, acc=0.90625
# [40/100] testing 36.4% loss=0.26843, acc=0.89062
# [40/100] testing 37.3% loss=0.26503, acc=0.93750
# [40/100] testing 37.7% loss=0.44460, acc=0.82812
# [40/100] testing 38.6% loss=0.19712, acc=0.96875
# [40/100] testing 39.5% loss=0.22700, acc=0.93750
# [40/100] testing 39.9% loss=0.23806, acc=0.92188
# [40/100] testing 40.8% loss=0.28696, acc=0.93750
# [40/100] testing 41.2% loss=0.22853, acc=0.90625
# [40/100] testing 42.1% loss=0.26265, acc=0.90625
# [40/100] testing 42.5% loss=0.25013, acc=0.92188
# [40/100] testing 43.4% loss=0.37498, acc=0.87500
# [40/100] testing 43.9% loss=0.08325, acc=0.96875
# [40/100] testing 44.7% loss=0.25818, acc=0.89062
# [40/100] testing 45.6% loss=0.20937, acc=0.92188
# [40/100] testing 46.1% loss=0.36594, acc=0.85938
# [40/100] testing 46.9% loss=0.23725, acc=0.90625
# [40/100] testing 47.4% loss=0.16555, acc=0.92188
# [40/100] testing 48.3% loss=0.42059, acc=0.87500
# [40/100] testing 48.7% loss=0.37259, acc=0.85938
# [40/100] testing 49.6% loss=0.45544, acc=0.82812
# [40/100] testing 50.4% loss=0.19647, acc=0.90625
# [40/100] testing 50.9% loss=0.34019, acc=0.89062
# [40/100] testing 51.8% loss=0.26913, acc=0.90625
# [40/100] testing 52.2% loss=0.21589, acc=0.90625
# [40/100] testing 53.1% loss=0.24642, acc=0.90625
# [40/100] testing 53.5% loss=0.20171, acc=0.93750
# [40/100] testing 54.4% loss=0.33447, acc=0.84375
# [40/100] testing 54.8% loss=0.37577, acc=0.85938
# [40/100] testing 55.7% loss=0.15131, acc=0.93750
# [40/100] testing 56.6% loss=0.30073, acc=0.87500
# [40/100] testing 57.0% loss=0.38198, acc=0.84375
# [40/100] testing 57.9% loss=0.22577, acc=0.87500
# [40/100] testing 58.3% loss=0.27381, acc=0.90625
# [40/100] testing 59.2% loss=0.25042, acc=0.90625
# [40/100] testing 59.7% loss=0.25203, acc=0.89062
# [40/100] testing 60.5% loss=0.40590, acc=0.87500
# [40/100] testing 61.4% loss=0.10007, acc=0.95312
# [40/100] testing 61.9% loss=0.19868, acc=0.90625
# [40/100] testing 62.7% loss=0.18150, acc=0.90625
# [40/100] testing 63.2% loss=0.45290, acc=0.84375
# [40/100] testing 64.0% loss=0.44541, acc=0.90625
# [40/100] testing 64.5% loss=0.22478, acc=0.90625
# [40/100] testing 65.4% loss=0.25644, acc=0.90625
# [40/100] testing 65.8% loss=0.32611, acc=0.87500
# [40/100] testing 66.7% loss=0.23170, acc=0.95312
# [40/100] testing 67.6% loss=0.46496, acc=0.90625
# [40/100] testing 68.0% loss=0.11138, acc=0.95312
# [40/100] testing 68.9% loss=0.21486, acc=0.93750
# [40/100] testing 69.3% loss=0.26384, acc=0.84375
# [40/100] testing 70.2% loss=0.32098, acc=0.85938
# [40/100] testing 70.6% loss=0.37565, acc=0.82812
# [40/100] testing 71.5% loss=0.39795, acc=0.85938
# [40/100] testing 72.4% loss=0.10052, acc=0.95312
# [40/100] testing 72.8% loss=0.20916, acc=0.89062
# [40/100] testing 73.7% loss=0.08985, acc=0.95312
# [40/100] testing 74.1% loss=0.41561, acc=0.87500
# [40/100] testing 75.0% loss=0.18330, acc=0.90625
# [40/100] testing 75.4% loss=0.33494, acc=0.85938
# [40/100] testing 76.3% loss=0.08779, acc=0.96875
# [40/100] testing 76.8% loss=0.25877, acc=0.85938
# [40/100] testing 77.6% loss=0.16236, acc=0.93750
# [40/100] testing 78.5% loss=0.29069, acc=0.82812
# [40/100] testing 79.0% loss=0.22025, acc=0.90625
# [40/100] testing 79.8% loss=0.29965, acc=0.84375
# [40/100] testing 80.3% loss=0.40507, acc=0.82812
# [40/100] testing 81.2% loss=0.48279, acc=0.84375
# [40/100] testing 81.6% loss=0.21731, acc=0.93750
# [40/100] testing 82.5% loss=0.16746, acc=0.92188
# [40/100] testing 83.3% loss=0.18225, acc=0.96875
# [40/100] testing 83.8% loss=0.17294, acc=0.95312
# [40/100] testing 84.7% loss=0.21315, acc=0.92188
# [40/100] testing 85.1% loss=0.24241, acc=0.92188
# [40/100] testing 86.0% loss=0.26395, acc=0.93750
# [40/100] testing 86.4% loss=0.44385, acc=0.84375
# [40/100] testing 87.3% loss=0.28760, acc=0.82812
# [40/100] testing 87.7% loss=0.27406, acc=0.90625
# [40/100] testing 88.6% loss=0.26824, acc=0.89062
# [40/100] testing 89.5% loss=0.52976, acc=0.82812
# [40/100] testing 89.9% loss=0.17209, acc=0.92188
# [40/100] testing 90.8% loss=0.35867, acc=0.90625
# [40/100] testing 91.2% loss=0.17064, acc=0.92188
# [40/100] testing 92.1% loss=0.29941, acc=0.85938
# [40/100] testing 92.6% loss=0.43402, acc=0.89062
# [40/100] testing 93.4% loss=0.31410, acc=0.84375
# [40/100] testing 94.3% loss=0.06848, acc=1.00000
# [40/100] testing 94.7% loss=0.19729, acc=0.92188
# [40/100] testing 95.6% loss=0.34645, acc=0.85938
# [40/100] testing 96.1% loss=0.19007, acc=0.92188
# [40/100] testing 96.9% loss=0.27925, acc=0.87500
# [40/100] testing 97.4% loss=0.11452, acc=0.96875
# [40/100] testing 98.3% loss=0.13857, acc=0.92188
# [40/100] testing 98.7% loss=0.17477, acc=0.92188
# [40/100] testing 99.6% loss=0.35543, acc=0.87500
# [41/100] training 0.2% loss=0.24058, acc=0.90625
# [41/100] training 0.4% loss=0.39885, acc=0.87500
# [41/100] training 0.5% loss=0.15345, acc=0.95312
# [41/100] training 0.8% loss=0.13272, acc=0.95312
# [41/100] training 0.9% loss=0.12489, acc=0.96875
# [41/100] training 1.1% loss=0.19782, acc=0.95312
# [41/100] training 1.2% loss=0.21286, acc=0.90625
# [41/100] training 1.4% loss=0.15304, acc=0.92188
# [41/100] training 1.6% loss=0.19254, acc=0.92188
# [41/100] training 1.8% loss=0.19777, acc=0.93750
# [41/100] training 2.0% loss=0.30919, acc=0.87500
# [41/100] training 2.1% loss=0.23566, acc=0.87500
# [41/100] training 2.3% loss=0.13013, acc=0.95312
# [41/100] training 2.4% loss=0.34777, acc=0.87500
# [41/100] training 2.6% loss=0.14513, acc=0.92188
# [41/100] training 2.7% loss=0.27883, acc=0.85938
# [41/100] training 3.0% loss=0.17491, acc=0.93750
# [41/100] training 3.2% loss=0.16338, acc=0.96875
# [41/100] training 3.3% loss=0.32373, acc=0.89062
# [41/100] training 3.5% loss=0.22545, acc=0.90625
# [41/100] training 3.6% loss=0.26973, acc=0.89062
# [41/100] training 3.8% loss=0.20793, acc=0.89062
# [41/100] training 3.9% loss=0.20036, acc=0.89062
# [41/100] training 4.2% loss=0.11002, acc=0.95312
# [41/100] training 4.4% loss=0.21328, acc=0.93750
# [41/100] training 4.5% loss=0.19558, acc=0.87500
# [41/100] training 4.7% loss=0.28015, acc=0.90625
# [41/100] training 4.8% loss=0.18277, acc=0.90625
# [41/100] training 5.0% loss=0.12341, acc=0.96875
# [41/100] training 5.2% loss=0.17449, acc=0.93750
# [41/100] training 5.4% loss=0.09581, acc=0.96875
# [41/100] training 5.5% loss=0.17448, acc=0.92188
# [41/100] training 5.7% loss=0.24434, acc=0.90625
# [41/100] training 5.9% loss=0.21350, acc=0.90625
# [41/100] training 6.0% loss=0.16514, acc=0.92188
# [41/100] training 6.3% loss=0.21266, acc=0.90625
# [41/100] training 6.4% loss=0.08613, acc=0.98438
# [41/100] training 6.6% loss=0.22894, acc=0.89062
# [41/100] training 6.7% loss=0.33668, acc=0.89062
# [41/100] training 6.9% loss=0.19209, acc=0.90625
# [41/100] training 7.1% loss=0.14529, acc=0.95312
# [41/100] training 7.2% loss=0.21160, acc=0.90625
# [41/100] training 7.5% loss=0.19168, acc=0.92188
# [41/100] training 7.6% loss=0.27856, acc=0.87500
# [41/100] training 7.8% loss=0.19233, acc=0.95312
# [41/100] training 7.9% loss=0.22155, acc=0.89062
# [41/100] training 8.1% loss=0.10778, acc=0.98438
# [41/100] training 8.2% loss=0.29535, acc=0.89062
# [41/100] training 8.4% loss=0.21800, acc=0.90625
# [41/100] training 8.7% loss=0.21625, acc=0.93750
# [41/100] training 8.8% loss=0.17605, acc=0.92188
# [41/100] training 9.0% loss=0.22640, acc=0.90625
# [41/100] training 9.1% loss=0.23447, acc=0.92188
# [41/100] training 9.3% loss=0.29500, acc=0.93750
# [41/100] training 9.4% loss=0.13326, acc=0.93750
# [41/100] training 9.7% loss=0.12583, acc=0.92188
# [41/100] training 9.9% loss=0.23815, acc=0.89062
# [41/100] training 10.0% loss=0.20730, acc=0.89062
# [41/100] training 10.2% loss=0.14517, acc=0.93750
# [41/100] training 10.3% loss=0.18965, acc=0.90625
# [41/100] training 10.5% loss=0.21452, acc=0.93750
# [41/100] training 10.6% loss=0.15773, acc=0.95312
# [41/100] training 10.9% loss=0.14396, acc=0.95312
# [41/100] training 11.0% loss=0.29631, acc=0.89062
# [41/100] training 11.2% loss=0.16796, acc=0.93750
# [41/100] training 11.4% loss=0.16354, acc=0.95312
# [41/100] training 11.5% loss=0.48295, acc=0.85938
# [41/100] training 11.7% loss=0.12265, acc=0.98438
# [41/100] training 11.8% loss=0.19402, acc=0.92188
# [41/100] training 12.1% loss=0.22288, acc=0.93750
# [41/100] training 12.2% loss=0.21711, acc=0.89062
# [41/100] training 12.4% loss=0.21804, acc=0.90625
# [41/100] training 12.6% loss=0.28579, acc=0.92188
# [41/100] training 12.7% loss=0.19509, acc=0.93750
# [41/100] training 12.9% loss=0.21945, acc=0.90625
# [41/100] training 13.0% loss=0.16378, acc=0.93750
# [41/100] training 13.3% loss=0.20399, acc=0.90625
# [41/100] training 13.4% loss=0.18217, acc=0.95312
# [41/100] training 13.6% loss=0.18894, acc=0.92188
# [41/100] training 13.7% loss=0.24253, acc=0.89062
# [41/100] training 13.9% loss=0.25278, acc=0.93750
# [41/100] training 14.1% loss=0.29057, acc=0.90625
# [41/100] training 14.3% loss=0.08448, acc=0.98438
# [41/100] training 14.5% loss=0.16092, acc=0.93750
# [41/100] training 14.6% loss=0.26794, acc=0.89062
# [41/100] training 14.8% loss=0.21068, acc=0.89062
# [41/100] training 14.9% loss=0.11882, acc=0.96875
# [41/100] training 15.1% loss=0.27207, acc=0.85938
# [41/100] training 15.4% loss=0.22979, acc=0.89062
# [41/100] training 15.5% loss=0.20495, acc=0.93750
# [41/100] training 15.7% loss=0.30113, acc=0.92188
# [41/100] training 15.8% loss=0.13029, acc=0.95312
# [41/100] training 16.0% loss=0.33342, acc=0.89062
# [41/100] training 16.1% loss=0.25286, acc=0.90625
# [41/100] training 16.3% loss=0.22791, acc=0.92188
# [41/100] training 16.4% loss=0.17208, acc=0.92188
# [41/100] training 16.7% loss=0.26144, acc=0.87500
# [41/100] training 16.9% loss=0.21103, acc=0.90625
# [41/100] training 17.0% loss=0.18382, acc=0.90625
# [41/100] training 17.2% loss=0.14053, acc=0.93750
# [41/100] training 17.3% loss=0.12514, acc=0.95312
# [41/100] training 17.5% loss=0.15897, acc=0.93750
# [41/100] training 17.7% loss=0.15309, acc=0.95312
# [41/100] training 17.9% loss=0.23413, acc=0.92188
# [41/100] training 18.1% loss=0.24342, acc=0.89062
# [41/100] training 18.2% loss=0.23680, acc=0.93750
# [41/100] training 18.4% loss=0.26721, acc=0.87500
# [41/100] training 18.5% loss=0.22481, acc=0.90625
# [41/100] training 18.8% loss=0.15342, acc=0.93750
# [41/100] training 18.9% loss=0.10325, acc=0.96875
# [41/100] training 19.1% loss=0.27220, acc=0.85938
# [41/100] training 19.2% loss=0.10007, acc=0.98438
# [41/100] training 19.4% loss=0.17352, acc=0.92188
# [41/100] training 19.6% loss=0.23032, acc=0.90625
# [41/100] training 19.7% loss=0.26789, acc=0.87500
# [41/100] training 20.0% loss=0.16914, acc=0.93750
# [41/100] training 20.1% loss=0.15497, acc=0.93750
# [41/100] training 20.3% loss=0.17818, acc=0.92188
# [41/100] training 20.4% loss=0.21733, acc=0.90625
# [41/100] training 20.6% loss=0.30964, acc=0.89062
# [41/100] training 20.8% loss=0.14214, acc=0.93750
# [41/100] training 20.9% loss=0.15411, acc=0.93750
# [41/100] training 21.2% loss=0.20995, acc=0.95312
# [41/100] training 21.3% loss=0.27655, acc=0.89062
# [41/100] training 21.5% loss=0.25074, acc=0.90625
# [41/100] training 21.6% loss=0.10738, acc=0.96875
# [41/100] training 21.8% loss=0.12954, acc=0.93750
# [41/100] training 21.9% loss=0.14851, acc=0.93750
# [41/100] training 22.2% loss=0.21164, acc=0.90625
# [41/100] training 22.4% loss=0.14067, acc=0.93750
# [41/100] training 22.5% loss=0.11108, acc=0.96875
# [41/100] training 22.7% loss=0.20719, acc=0.89062
# [41/100] training 22.8% loss=0.23950, acc=0.90625
# [41/100] training 23.0% loss=0.14745, acc=0.92188
# [41/100] training 23.1% loss=0.25118, acc=0.89062
# [41/100] training 23.4% loss=0.31521, acc=0.85938
# [41/100] training 23.6% loss=0.26522, acc=0.92188
# [41/100] training 23.7% loss=0.16253, acc=0.96875
# [41/100] training 23.9% loss=0.19183, acc=0.89062
# [41/100] training 24.0% loss=0.21215, acc=0.89062
# [41/100] training 24.2% loss=0.16936, acc=0.90625
# [41/100] training 24.3% loss=0.28176, acc=0.87500
# [41/100] training 24.6% loss=0.23564, acc=0.89062
# [41/100] training 24.7% loss=0.26787, acc=0.90625
# [41/100] training 24.9% loss=0.14936, acc=0.93750
# [41/100] training 25.1% loss=0.36119, acc=0.84375
# [41/100] training 25.2% loss=0.13820, acc=0.95312
# [41/100] training 25.4% loss=0.24870, acc=0.87500
# [41/100] training 25.6% loss=0.23687, acc=0.93750
# [41/100] training 25.8% loss=0.25592, acc=0.89062
# [41/100] training 25.9% loss=0.18799, acc=0.90625
# [41/100] training 26.1% loss=0.19555, acc=0.90625
# [41/100] training 26.3% loss=0.15863, acc=0.90625
# [41/100] training 26.4% loss=0.11910, acc=0.95312
# [41/100] training 26.6% loss=0.06979, acc=0.96875
# [41/100] training 26.8% loss=0.14065, acc=0.93750
# [41/100] training 27.0% loss=0.15762, acc=0.90625
# [41/100] training 27.1% loss=0.12771, acc=0.93750
# [41/100] training 27.3% loss=0.18991, acc=0.89062
# [41/100] training 27.4% loss=0.15589, acc=0.93750
# [41/100] training 27.6% loss=0.35996, acc=0.90625
# [41/100] training 27.9% loss=0.16549, acc=0.95312
# [41/100] training 28.0% loss=0.30651, acc=0.87500
# [41/100] training 28.2% loss=0.15880, acc=0.93750
# [41/100] training 28.3% loss=0.11261, acc=0.92188
# [41/100] training 28.5% loss=0.20994, acc=0.90625
# [41/100] training 28.6% loss=0.13376, acc=0.96875
# [41/100] training 28.8% loss=0.13908, acc=0.93750
# [41/100] training 29.1% loss=0.11570, acc=0.98438
# [41/100] training 29.2% loss=0.09729, acc=0.95312
# [41/100] training 29.4% loss=0.28775, acc=0.90625
# [41/100] training 29.5% loss=0.08653, acc=0.96875
# [41/100] training 29.7% loss=0.19973, acc=0.89062
# [41/100] training 29.8% loss=0.17509, acc=0.93750
# [41/100] training 30.0% loss=0.23717, acc=0.93750
# [41/100] training 30.2% loss=0.13244, acc=0.93750
# [41/100] training 30.4% loss=0.08055, acc=0.98438
# [41/100] training 30.6% loss=0.24632, acc=0.89062
# [41/100] training 30.7% loss=0.22681, acc=0.89062
# [41/100] training 30.9% loss=0.21757, acc=0.95312
# [41/100] training 31.0% loss=0.07346, acc=0.95312
# [41/100] training 31.3% loss=0.17734, acc=0.95312
# [41/100] training 31.4% loss=0.26464, acc=0.85938
# [41/100] training 31.6% loss=0.28428, acc=0.92188
# [41/100] training 31.8% loss=0.16147, acc=0.93750
# [41/100] training 31.9% loss=0.26089, acc=0.87500
# [41/100] training 32.1% loss=0.24925, acc=0.89062
# [41/100] training 32.2% loss=0.24464, acc=0.85938
# [41/100] training 32.5% loss=0.15487, acc=0.95312
# [41/100] training 32.6% loss=0.22025, acc=0.92188
# [41/100] training 32.8% loss=0.16508, acc=0.93750
# [41/100] training 32.9% loss=0.28199, acc=0.90625
# [41/100] training 33.1% loss=0.18616, acc=0.89062
# [41/100] training 33.3% loss=0.17896, acc=0.92188
# [41/100] training 33.4% loss=0.15362, acc=0.93750
# [41/100] training 33.7% loss=0.21669, acc=0.87500
# [41/100] training 33.8% loss=0.24117, acc=0.87500
# [41/100] training 34.0% loss=0.18346, acc=0.90625
# [41/100] training 34.1% loss=0.16768, acc=0.95312
# [41/100] training 34.3% loss=0.27174, acc=0.92188
# [41/100] training 34.5% loss=0.24684, acc=0.90625
# [41/100] training 34.7% loss=0.14539, acc=0.96875
# [41/100] training 34.9% loss=0.14504, acc=0.93750
# [41/100] training 35.0% loss=0.15362, acc=0.92188
# [41/100] training 35.2% loss=0.24408, acc=0.85938
# [41/100] training 35.3% loss=0.25209, acc=0.84375
# [41/100] training 35.5% loss=0.22847, acc=0.87500
# [41/100] training 35.6% loss=0.30796, acc=0.87500
# [41/100] training 35.9% loss=0.25332, acc=0.87500
# [41/100] training 36.1% loss=0.35688, acc=0.85938
# [41/100] training 36.2% loss=0.20425, acc=0.90625
# [41/100] training 36.4% loss=0.17433, acc=0.93750
# [41/100] training 36.5% loss=0.19761, acc=0.95312
# [41/100] training 36.7% loss=0.16814, acc=0.93750
# [41/100] training 36.8% loss=0.13240, acc=0.96875
# [41/100] training 37.1% loss=0.19944, acc=0.93750
# [41/100] training 37.3% loss=0.21370, acc=0.92188
# [41/100] training 37.4% loss=0.18693, acc=0.92188
# [41/100] training 37.6% loss=0.09803, acc=0.96875
# [41/100] training 37.7% loss=0.27009, acc=0.87500
# [41/100] training 37.9% loss=0.17060, acc=0.92188
# [41/100] training 38.1% loss=0.25539, acc=0.90625
# [41/100] training 38.3% loss=0.16681, acc=0.92188
# [41/100] training 38.4% loss=0.08705, acc=0.96875
# [41/100] training 38.6% loss=0.21863, acc=0.90625
# [41/100] training 38.8% loss=0.19817, acc=0.92188
# [41/100] training 38.9% loss=0.18565, acc=0.92188
# [41/100] training 39.1% loss=0.23620, acc=0.92188
# [41/100] training 39.3% loss=0.23110, acc=0.89062
# [41/100] training 39.5% loss=0.22718, acc=0.89062
# [41/100] training 39.6% loss=0.09209, acc=0.98438
# [41/100] training 39.8% loss=0.12104, acc=0.93750
# [41/100] training 40.0% loss=0.20295, acc=0.92188
# [41/100] training 40.1% loss=0.29443, acc=0.89062
# [41/100] training 40.4% loss=0.15286, acc=0.92188
# [41/100] training 40.5% loss=0.29775, acc=0.87500
# [41/100] training 40.7% loss=0.19919, acc=0.93750
# [41/100] training 40.8% loss=0.11710, acc=0.96875
# [41/100] training 41.0% loss=0.17094, acc=0.92188
# [41/100] training 41.1% loss=0.30075, acc=0.90625
# [41/100] training 41.3% loss=0.21452, acc=0.92188
# [41/100] training 41.6% loss=0.26945, acc=0.89062
# [41/100] training 41.7% loss=0.20798, acc=0.93750
# [41/100] training 41.9% loss=0.17392, acc=0.93750
# [41/100] training 42.0% loss=0.25738, acc=0.89062
# [41/100] training 42.2% loss=0.25667, acc=0.90625
# [41/100] training 42.3% loss=0.15686, acc=0.95312
# [41/100] training 42.5% loss=0.13858, acc=0.93750
# [41/100] training 42.8% loss=0.15411, acc=0.93750
# [41/100] training 42.9% loss=0.14007, acc=0.95312
# [41/100] training 43.1% loss=0.25569, acc=0.90625
# [41/100] training 43.2% loss=0.12055, acc=0.93750
# [41/100] training 43.4% loss=0.15782, acc=0.95312
# [41/100] training 43.5% loss=0.24732, acc=0.90625
# [41/100] training 43.8% loss=0.25718, acc=0.92188
# [41/100] training 43.9% loss=0.15183, acc=0.95312
# [41/100] training 44.1% loss=0.24478, acc=0.93750
# [41/100] training 44.3% loss=0.18479, acc=0.92188
# [41/100] training 44.4% loss=0.26818, acc=0.90625
# [41/100] training 44.6% loss=0.24059, acc=0.87500
# [41/100] training 44.7% loss=0.31587, acc=0.90625
# [41/100] training 45.0% loss=0.11261, acc=0.96875
# [41/100] training 45.1% loss=0.28343, acc=0.90625
# [41/100] training 45.3% loss=0.16923, acc=0.92188
# [41/100] training 45.5% loss=0.10538, acc=0.96875
# [41/100] training 45.6% loss=0.32154, acc=0.87500
# [41/100] training 45.8% loss=0.13646, acc=0.96875
# [41/100] training 45.9% loss=0.10611, acc=0.93750
# [41/100] training 46.2% loss=0.10002, acc=0.96875
# [41/100] training 46.3% loss=0.12812, acc=0.92188
# [41/100] training 46.5% loss=0.26980, acc=0.89062
# [41/100] training 46.6% loss=0.26937, acc=0.90625
# [41/100] training 46.8% loss=0.12392, acc=0.93750
# [41/100] training 47.0% loss=0.23663, acc=0.90625
# [41/100] training 47.2% loss=0.17729, acc=0.90625
# [41/100] training 47.4% loss=0.26661, acc=0.87500
# [41/100] training 47.5% loss=0.24995, acc=0.90625
# [41/100] training 47.7% loss=0.19651, acc=0.92188
# [41/100] training 47.8% loss=0.29300, acc=0.87500
# [41/100] training 48.0% loss=0.10120, acc=0.96875
# [41/100] training 48.3% loss=0.08499, acc=0.98438
# [41/100] training 48.4% loss=0.07436, acc=1.00000
# [41/100] training 48.6% loss=0.13358, acc=0.93750
# [41/100] training 48.7% loss=0.27460, acc=0.90625
# [41/100] training 48.9% loss=0.16196, acc=0.93750
# [41/100] training 49.0% loss=0.08386, acc=0.96875
# [41/100] training 49.2% loss=0.19588, acc=0.90625
# [41/100] training 49.3% loss=0.21264, acc=0.92188
# [41/100] training 49.6% loss=0.24344, acc=0.92188
# [41/100] training 49.8% loss=0.23415, acc=0.92188
# [41/100] training 49.9% loss=0.26444, acc=0.89062
# [41/100] training 50.1% loss=0.14104, acc=0.95312
# [41/100] training 50.2% loss=0.29571, acc=0.84375
# [41/100] training 50.4% loss=0.34647, acc=0.87500
# [41/100] training 50.6% loss=0.18353, acc=0.92188
# [41/100] training 50.8% loss=0.28171, acc=0.87500
# [41/100] training 51.0% loss=0.22399, acc=0.90625
# [41/100] training 51.1% loss=0.27012, acc=0.82812
# [41/100] training 51.3% loss=0.22774, acc=0.90625
# [41/100] training 51.4% loss=0.16791, acc=0.93750
# [41/100] training 51.7% loss=0.22541, acc=0.90625
# [41/100] training 51.8% loss=0.28563, acc=0.85938
# [41/100] training 52.0% loss=0.21949, acc=0.90625
# [41/100] training 52.1% loss=0.19750, acc=0.92188
# [41/100] training 52.3% loss=0.21697, acc=0.92188
# [41/100] training 52.5% loss=0.08836, acc=0.96875
# [41/100] training 52.6% loss=0.15504, acc=0.90625
# [41/100] training 52.9% loss=0.23479, acc=0.92188
# [41/100] training 53.0% loss=0.11713, acc=0.92188
# [41/100] training 53.2% loss=0.25296, acc=0.92188
# [41/100] training 53.3% loss=0.19063, acc=0.93750
# [41/100] training 53.5% loss=0.23660, acc=0.90625
# [41/100] training 53.7% loss=0.05123, acc=1.00000
# [41/100] training 53.8% loss=0.27252, acc=0.85938
# [41/100] training 54.1% loss=0.42370, acc=0.82812
# [41/100] training 54.2% loss=0.09141, acc=0.98438
# [41/100] training 54.4% loss=0.16533, acc=0.95312
# [41/100] training 54.5% loss=0.29128, acc=0.92188
# [41/100] training 54.7% loss=0.26527, acc=0.92188
# [41/100] training 54.8% loss=0.11087, acc=0.96875
# [41/100] training 55.1% loss=0.23628, acc=0.89062
# [41/100] training 55.3% loss=0.15393, acc=0.95312
# [41/100] training 55.4% loss=0.14473, acc=0.95312
# [41/100] training 55.6% loss=0.16277, acc=0.92188
# [41/100] training 55.7% loss=0.11643, acc=0.95312
# [41/100] training 55.9% loss=0.07409, acc=0.98438
# [41/100] training 56.0% loss=0.13114, acc=0.95312
# [41/100] training 56.3% loss=0.39623, acc=0.81250
# [41/100] training 56.5% loss=0.11949, acc=0.93750
# [41/100] training 56.6% loss=0.23555, acc=0.85938
# [41/100] training 56.8% loss=0.20700, acc=0.93750
# [41/100] training 56.9% loss=0.14978, acc=0.93750
# [41/100] training 57.1% loss=0.34033, acc=0.84375
# [41/100] training 57.2% loss=0.10347, acc=0.93750
# [41/100] training 57.5% loss=0.24986, acc=0.89062
# [41/100] training 57.6% loss=0.14268, acc=0.96875
# [41/100] training 57.8% loss=0.17404, acc=0.93750
# [41/100] training 58.0% loss=0.22742, acc=0.92188
# [41/100] training 58.1% loss=0.12070, acc=0.96875
# [41/100] training 58.3% loss=0.19148, acc=0.90625
# [41/100] training 58.4% loss=0.24479, acc=0.89062
# [41/100] training 58.7% loss=0.27582, acc=0.92188
# [41/100] training 58.8% loss=0.23944, acc=0.92188
# [41/100] training 59.0% loss=0.13003, acc=0.96875
# [41/100] training 59.2% loss=0.18761, acc=0.87500
# [41/100] training 59.3% loss=0.13443, acc=0.96875
# [41/100] training 59.5% loss=0.20952, acc=0.93750
# [41/100] training 59.7% loss=0.12980, acc=0.95312
# [41/100] training 59.9% loss=0.15893, acc=0.96875
# [41/100] training 60.0% loss=0.22040, acc=0.92188
# [41/100] training 60.2% loss=0.16724, acc=0.90625
# [41/100] training 60.3% loss=0.13615, acc=0.95312
# [41/100] training 60.5% loss=0.29507, acc=0.87500
# [41/100] training 60.8% loss=0.22653, acc=0.90625
# [41/100] training 60.9% loss=0.21744, acc=0.93750
# [41/100] training 61.1% loss=0.23080, acc=0.93750
# [41/100] training 61.2% loss=0.17743, acc=0.93750
# [41/100] training 61.4% loss=0.29412, acc=0.87500
# [41/100] training 61.5% loss=0.33246, acc=0.85938
# [41/100] training 61.7% loss=0.11344, acc=0.96875
# [41/100] training 62.0% loss=0.29263, acc=0.87500
# [41/100] training 62.1% loss=0.23191, acc=0.90625
# [41/100] training 62.3% loss=0.06089, acc=0.98438
# [41/100] training 62.4% loss=0.28589, acc=0.85938
# [41/100] training 62.6% loss=0.24541, acc=0.90625
# [41/100] training 62.7% loss=0.08753, acc=0.98438
# [41/100] training 62.9% loss=0.26407, acc=0.92188
# [41/100] training 63.1% loss=0.23214, acc=0.93750
# [41/100] training 63.3% loss=0.24034, acc=0.90625
# [41/100] training 63.5% loss=0.22329, acc=0.95312
# [41/100] training 63.6% loss=0.22646, acc=0.89062
# [41/100] training 63.8% loss=0.30497, acc=0.85938
# [41/100] training 63.9% loss=0.14555, acc=0.95312
# [41/100] training 64.2% loss=0.18378, acc=0.90625
# [41/100] training 64.3% loss=0.24647, acc=0.89062
# [41/100] training 64.5% loss=0.22693, acc=0.90625
# [41/100] training 64.7% loss=0.20399, acc=0.87500
# [41/100] training 64.8% loss=0.37820, acc=0.85938
# [41/100] training 65.0% loss=0.19194, acc=0.95312
# [41/100] training 65.1% loss=0.16860, acc=0.92188
# [41/100] training 65.4% loss=0.14139, acc=0.96875
# [41/100] training 65.5% loss=0.12681, acc=0.93750
# [41/100] training 65.7% loss=0.12764, acc=0.96875
# [41/100] training 65.8% loss=0.19025, acc=0.93750
# [41/100] training 66.0% loss=0.15108, acc=0.95312
# [41/100] training 66.2% loss=0.08584, acc=0.95312
# [41/100] training 66.3% loss=0.19748, acc=0.90625
# [41/100] training 66.6% loss=0.13547, acc=0.92188
# [41/100] training 66.7% loss=0.19309, acc=0.93750
# [41/100] training 66.9% loss=0.31020, acc=0.89062
# [41/100] training 67.0% loss=0.18817, acc=0.93750
# [41/100] training 67.2% loss=0.14171, acc=0.95312
# [41/100] training 67.4% loss=0.06808, acc=0.98438
# [41/100] training 67.6% loss=0.09511, acc=0.96875
# [41/100] training 67.8% loss=0.13525, acc=0.95312
# [41/100] training 67.9% loss=0.07192, acc=0.96875
# [41/100] training 68.1% loss=0.18049, acc=0.92188
# [41/100] training 68.2% loss=0.11564, acc=0.98438
# [41/100] training 68.4% loss=0.33343, acc=0.87500
# [41/100] training 68.5% loss=0.24043, acc=0.93750
# [41/100] training 68.8% loss=0.24579, acc=0.87500
# [41/100] training 69.0% loss=0.30761, acc=0.90625
# [41/100] training 69.1% loss=0.16463, acc=0.93750
# [41/100] training 69.3% loss=0.21219, acc=0.89062
# [41/100] training 69.4% loss=0.23114, acc=0.93750
# [41/100] training 69.6% loss=0.19034, acc=0.93750
# [41/100] training 69.7% loss=0.21191, acc=0.90625
# [41/100] training 70.0% loss=0.32757, acc=0.85938
# [41/100] training 70.2% loss=0.19401, acc=0.87500
# [41/100] training 70.3% loss=0.17002, acc=0.95312
# [41/100] training 70.5% loss=0.18769, acc=0.92188
# [41/100] training 70.6% loss=0.12621, acc=0.98438
# [41/100] training 70.8% loss=0.19842, acc=0.90625
# [41/100] training 71.0% loss=0.27317, acc=0.89062
# [41/100] training 71.2% loss=0.15770, acc=0.92188
# [41/100] training 71.3% loss=0.08802, acc=0.96875
# [41/100] training 71.5% loss=0.29275, acc=0.95312
# [41/100] training 71.7% loss=0.21013, acc=0.90625
# [41/100] training 71.8% loss=0.20569, acc=0.93750
# [41/100] training 72.0% loss=0.10648, acc=0.95312
# [41/100] training 72.2% loss=0.23210, acc=0.92188
# [41/100] training 72.4% loss=0.23263, acc=0.90625
# [41/100] training 72.5% loss=0.22395, acc=0.90625
# [41/100] training 72.7% loss=0.31242, acc=0.92188
# [41/100] training 72.9% loss=0.15116, acc=0.96875
# [41/100] training 73.0% loss=0.08768, acc=0.96875
# [41/100] training 73.3% loss=0.29382, acc=0.82812
# [41/100] training 73.4% loss=0.18064, acc=0.92188
# [41/100] training 73.6% loss=0.14823, acc=0.93750
# [41/100] training 73.7% loss=0.21500, acc=0.92188
# [41/100] training 73.9% loss=0.16815, acc=0.93750
# [41/100] training 74.0% loss=0.24297, acc=0.89062
# [41/100] training 74.2% loss=0.14131, acc=0.93750
# [41/100] training 74.5% loss=0.13141, acc=0.95312
# [41/100] training 74.6% loss=0.36213, acc=0.81250
# [41/100] training 74.8% loss=0.29057, acc=0.87500
# [41/100] training 74.9% loss=0.31059, acc=0.82812
# [41/100] training 75.1% loss=0.10800, acc=0.98438
# [41/100] training 75.2% loss=0.16136, acc=0.92188
# [41/100] training 75.4% loss=0.19836, acc=0.87500
# [41/100] training 75.7% loss=0.21700, acc=0.87500
# [41/100] training 75.8% loss=0.21864, acc=0.89062
# [41/100] training 76.0% loss=0.16362, acc=0.92188
# [41/100] training 76.1% loss=0.19837, acc=0.92188
# [41/100] training 76.3% loss=0.14675, acc=0.93750
# [41/100] training 76.4% loss=0.35934, acc=0.81250
# [41/100] training 76.7% loss=0.12863, acc=0.93750
# [41/100] training 76.8% loss=0.12423, acc=0.95312
# [41/100] training 77.0% loss=0.08519, acc=0.96875
# [41/100] training 77.2% loss=0.12228, acc=0.93750
# [41/100] training 77.3% loss=0.09797, acc=0.96875
# [41/100] training 77.5% loss=0.14837, acc=0.93750
# [41/100] training 77.6% loss=0.21982, acc=0.90625
# [41/100] training 77.9% loss=0.17629, acc=0.90625
# [41/100] training 78.0% loss=0.22954, acc=0.90625
# [41/100] training 78.2% loss=0.12889, acc=0.93750
# [41/100] training 78.4% loss=0.09135, acc=0.95312
# [41/100] training 78.5% loss=0.30686, acc=0.87500
# [41/100] training 78.7% loss=0.24073, acc=0.87500
# [41/100] training 78.8% loss=0.11201, acc=0.96875
# [41/100] training 79.1% loss=0.13411, acc=0.93750
# [41/100] training 79.2% loss=0.16610, acc=0.95312
# [41/100] training 79.4% loss=0.17986, acc=0.93750
# [41/100] training 79.5% loss=0.17602, acc=0.92188
# [41/100] training 79.7% loss=0.11200, acc=0.95312
# [41/100] training 79.9% loss=0.16677, acc=0.95312
# [41/100] training 80.1% loss=0.14838, acc=0.95312
# [41/100] training 80.3% loss=0.25454, acc=0.89062
# [41/100] training 80.4% loss=0.14260, acc=0.96875
# [41/100] training 80.6% loss=0.20942, acc=0.93750
# [41/100] training 80.7% loss=0.13294, acc=0.96875
# [41/100] training 80.9% loss=0.20214, acc=0.93750
# [41/100] training 81.2% loss=0.27905, acc=0.90625
# [41/100] training 81.3% loss=0.17756, acc=0.93750
# [41/100] training 81.5% loss=0.18779, acc=0.93750
# [41/100] training 81.6% loss=0.32651, acc=0.87500
# [41/100] training 81.8% loss=0.34708, acc=0.87500
# [41/100] training 81.9% loss=0.36572, acc=0.92188
# [41/100] training 82.1% loss=0.10291, acc=0.96875
# [41/100] training 82.2% loss=0.18025, acc=0.92188
# [41/100] training 82.5% loss=0.20439, acc=0.92188
# [41/100] training 82.7% loss=0.17212, acc=0.95312
# [41/100] training 82.8% loss=0.17945, acc=0.90625
# [41/100] training 83.0% loss=0.16316, acc=0.95312
# [41/100] training 83.1% loss=0.16576, acc=0.92188
# [41/100] training 83.3% loss=0.09612, acc=0.96875
# [41/100] training 83.5% loss=0.13492, acc=0.96875
# [41/100] training 83.7% loss=0.28119, acc=0.90625
# [41/100] training 83.9% loss=0.19142, acc=0.92188
# [41/100] training 84.0% loss=0.09095, acc=0.98438
# [41/100] training 84.2% loss=0.09347, acc=0.95312
# [41/100] training 84.3% loss=0.15248, acc=0.95312
# [41/100] training 84.5% loss=0.14128, acc=0.92188
# [41/100] training 84.7% loss=0.29120, acc=0.89062
# [41/100] training 84.9% loss=0.16151, acc=0.93750
# [41/100] training 85.0% loss=0.19718, acc=0.92188
# [41/100] training 85.2% loss=0.17645, acc=0.93750
# [41/100] training 85.4% loss=0.07963, acc=0.96875
# [41/100] training 85.5% loss=0.26412, acc=0.87500
# [41/100] training 85.8% loss=0.14663, acc=0.96875
# [41/100] training 85.9% loss=0.15565, acc=0.93750
# [41/100] training 86.1% loss=0.20015, acc=0.90625
# [41/100] training 86.2% loss=0.13771, acc=0.95312
# [41/100] training 86.4% loss=0.25189, acc=0.89062
# [41/100] training 86.6% loss=0.16994, acc=0.95312
# [41/100] training 86.7% loss=0.16344, acc=0.92188
# [41/100] training 87.0% loss=0.22596, acc=0.93750
# [41/100] training 87.1% loss=0.16860, acc=0.93750
# [41/100] training 87.3% loss=0.15891, acc=0.93750
# [41/100] training 87.4% loss=0.18183, acc=0.89062
# [41/100] training 87.6% loss=0.20447, acc=0.89062
# [41/100] training 87.7% loss=0.25123, acc=0.93750
# [41/100] training 87.9% loss=0.17122, acc=0.92188
# [41/100] training 88.2% loss=0.21001, acc=0.89062
# [41/100] training 88.3% loss=0.27354, acc=0.92188
# [41/100] training 88.5% loss=0.18592, acc=0.93750
# [41/100] training 88.6% loss=0.09761, acc=0.96875
# [41/100] training 88.8% loss=0.14208, acc=0.95312
# [41/100] training 88.9% loss=0.21686, acc=0.85938
# [41/100] training 89.2% loss=0.18572, acc=0.90625
# [41/100] training 89.4% loss=0.20874, acc=0.95312
# [41/100] training 89.5% loss=0.19643, acc=0.92188
# [41/100] training 89.7% loss=0.25403, acc=0.90625
# [41/100] training 89.8% loss=0.21884, acc=0.95312
# [41/100] training 90.0% loss=0.20357, acc=0.89062
# [41/100] training 90.1% loss=0.14465, acc=0.95312
# [41/100] training 90.4% loss=0.21436, acc=0.90625
# [41/100] training 90.5% loss=0.13556, acc=0.95312
# [41/100] training 90.7% loss=0.11084, acc=0.95312
# [41/100] training 90.9% loss=0.15779, acc=0.93750
# [41/100] training 91.0% loss=0.17078, acc=0.93750
# [41/100] training 91.2% loss=0.17135, acc=0.93750
# [41/100] training 91.3% loss=0.30071, acc=0.87500
# [41/100] training 91.6% loss=0.21098, acc=0.93750
# [41/100] training 91.7% loss=0.18933, acc=0.92188
# [41/100] training 91.9% loss=0.27732, acc=0.89062
# [41/100] training 92.1% loss=0.18038, acc=0.85938
# [41/100] training 92.2% loss=0.10125, acc=0.95312
# [41/100] training 92.4% loss=0.21632, acc=0.90625
# [41/100] training 92.6% loss=0.24082, acc=0.85938
# [41/100] training 92.8% loss=0.30834, acc=0.87500
# [41/100] training 92.9% loss=0.28112, acc=0.92188
# [41/100] training 93.1% loss=0.41914, acc=0.85938
# [41/100] training 93.2% loss=0.31390, acc=0.92188
# [41/100] training 93.4% loss=0.26908, acc=0.90625
# [41/100] training 93.7% loss=0.17586, acc=0.95312
# [41/100] training 93.8% loss=0.15037, acc=0.93750
# [41/100] training 94.0% loss=0.15263, acc=0.95312
# [41/100] training 94.1% loss=0.21614, acc=0.95312
# [41/100] training 94.3% loss=0.11162, acc=0.96875
# [41/100] training 94.4% loss=0.19718, acc=0.93750
# [41/100] training 94.6% loss=0.08672, acc=0.98438
# [41/100] training 94.9% loss=0.11641, acc=0.96875
# [41/100] training 95.0% loss=0.21384, acc=0.93750
# [41/100] training 95.2% loss=0.22434, acc=0.92188
# [41/100] training 95.3% loss=0.27575, acc=0.90625
# [41/100] training 95.5% loss=0.15890, acc=0.95312
# [41/100] training 95.6% loss=0.22839, acc=0.95312
# [41/100] training 95.8% loss=0.20980, acc=0.92188
# [41/100] training 96.0% loss=0.22768, acc=0.90625
# [41/100] training 96.2% loss=0.13241, acc=0.96875
# [41/100] training 96.4% loss=0.14287, acc=0.96875
# [41/100] training 96.5% loss=0.18850, acc=0.92188
# [41/100] training 96.7% loss=0.12444, acc=0.96875
# [41/100] training 96.8% loss=0.24840, acc=0.85938
# [41/100] training 97.1% loss=0.22409, acc=0.89062
# [41/100] training 97.2% loss=0.16593, acc=0.96875
# [41/100] training 97.4% loss=0.21534, acc=0.92188
# [41/100] training 97.6% loss=0.29005, acc=0.89062
# [41/100] training 97.7% loss=0.14213, acc=0.93750
# [41/100] training 97.9% loss=0.11954, acc=0.95312
# [41/100] training 98.0% loss=0.15553, acc=0.95312
# [41/100] training 98.3% loss=0.19970, acc=0.92188
# [41/100] training 98.4% loss=0.15415, acc=0.95312
# [41/100] training 98.6% loss=0.33866, acc=0.85938
# [41/100] training 98.7% loss=0.19462, acc=0.95312
# [41/100] training 98.9% loss=0.17085, acc=0.92188
# [41/100] training 99.1% loss=0.17338, acc=0.92188
# [41/100] training 99.2% loss=0.12276, acc=0.95312
# [41/100] training 99.5% loss=0.25758, acc=0.92188
# [41/100] training 99.6% loss=0.22957, acc=0.89062
# [41/100] training 99.8% loss=0.14742, acc=0.95312
# [41/100] training 99.9% loss=0.06303, acc=0.96875
# [41/100] testing 0.9% loss=0.15290, acc=0.90625
# [41/100] testing 1.8% loss=0.37288, acc=0.84375
# [41/100] testing 2.2% loss=0.23583, acc=0.92188
# [41/100] testing 3.1% loss=0.36762, acc=0.85938
# [41/100] testing 3.5% loss=0.13645, acc=0.95312
# [41/100] testing 4.4% loss=0.22492, acc=0.90625
# [41/100] testing 4.8% loss=0.35585, acc=0.85938
# [41/100] testing 5.7% loss=0.17262, acc=0.90625
# [41/100] testing 6.6% loss=0.16940, acc=0.92188
# [41/100] testing 7.0% loss=0.10353, acc=0.93750
# [41/100] testing 7.9% loss=0.30320, acc=0.84375
# [41/100] testing 8.3% loss=0.27192, acc=0.89062
# [41/100] testing 9.2% loss=0.26131, acc=0.90625
# [41/100] testing 9.7% loss=0.12251, acc=0.93750
# [41/100] testing 10.5% loss=0.20524, acc=0.89062
# [41/100] testing 11.0% loss=0.32784, acc=0.89062
# [41/100] testing 11.8% loss=0.12260, acc=0.93750
# [41/100] testing 12.7% loss=0.31306, acc=0.90625
# [41/100] testing 13.2% loss=0.21134, acc=0.89062
# [41/100] testing 14.0% loss=0.41195, acc=0.90625
# [41/100] testing 14.5% loss=0.24140, acc=0.87500
# [41/100] testing 15.4% loss=0.32694, acc=0.93750
# [41/100] testing 15.8% loss=0.19509, acc=0.93750
# [41/100] testing 16.7% loss=0.23263, acc=0.87500
# [41/100] testing 17.5% loss=0.17427, acc=0.92188
# [41/100] testing 18.0% loss=0.19239, acc=0.92188
# [41/100] testing 18.9% loss=0.13610, acc=0.92188
# [41/100] testing 19.3% loss=0.38916, acc=0.87500
# [41/100] testing 20.2% loss=0.33497, acc=0.89062
# [41/100] testing 20.6% loss=0.27677, acc=0.87500
# [41/100] testing 21.5% loss=0.21084, acc=0.93750
# [41/100] testing 21.9% loss=0.33570, acc=0.85938
# [41/100] testing 22.8% loss=0.27913, acc=0.90625
# [41/100] testing 23.7% loss=0.32038, acc=0.85938
# [41/100] testing 24.1% loss=0.23723, acc=0.89062
# [41/100] testing 25.0% loss=0.25546, acc=0.89062
# [41/100] testing 25.4% loss=0.17772, acc=0.92188
# [41/100] testing 26.3% loss=0.30506, acc=0.90625
# [41/100] testing 26.8% loss=0.31213, acc=0.89062
# [41/100] testing 27.6% loss=0.18122, acc=0.93750
# [41/100] testing 28.5% loss=0.17539, acc=0.93750
# [41/100] testing 29.0% loss=0.18091, acc=0.95312
# [41/100] testing 29.8% loss=0.38365, acc=0.90625
# [41/100] testing 30.3% loss=0.26569, acc=0.92188
# [41/100] testing 31.1% loss=0.23673, acc=0.87500
# [41/100] testing 31.6% loss=0.28050, acc=0.90625
# [41/100] testing 32.5% loss=0.22772, acc=0.89062
# [41/100] testing 32.9% loss=0.42195, acc=0.89062
# [41/100] testing 33.8% loss=0.35338, acc=0.85938
# [41/100] testing 34.7% loss=0.36122, acc=0.90625
# [41/100] testing 35.1% loss=0.18230, acc=0.93750
# [41/100] testing 36.0% loss=0.26433, acc=0.89062
# [41/100] testing 36.4% loss=0.25585, acc=0.89062
# [41/100] testing 37.3% loss=0.33369, acc=0.87500
# [41/100] testing 37.7% loss=0.44414, acc=0.81250
# [41/100] testing 38.6% loss=0.22209, acc=0.93750
# [41/100] testing 39.5% loss=0.26628, acc=0.93750
# [41/100] testing 39.9% loss=0.25621, acc=0.89062
# [41/100] testing 40.8% loss=0.28978, acc=0.92188
# [41/100] testing 41.2% loss=0.31809, acc=0.93750
# [41/100] testing 42.1% loss=0.24715, acc=0.92188
# [41/100] testing 42.5% loss=0.19986, acc=0.90625
# [41/100] testing 43.4% loss=0.26685, acc=0.92188
# [41/100] testing 43.9% loss=0.12506, acc=0.95312
# [41/100] testing 44.7% loss=0.31650, acc=0.84375
# [41/100] testing 45.6% loss=0.20457, acc=0.92188
# [41/100] testing 46.1% loss=0.30832, acc=0.85938
# [41/100] testing 46.9% loss=0.24269, acc=0.90625
# [41/100] testing 47.4% loss=0.12909, acc=0.95312
# [41/100] testing 48.3% loss=0.37557, acc=0.89062
# [41/100] testing 48.7% loss=0.36846, acc=0.84375
# [41/100] testing 49.6% loss=0.41612, acc=0.85938
# [41/100] testing 50.4% loss=0.19951, acc=0.92188
# [41/100] testing 50.9% loss=0.30488, acc=0.89062
# [41/100] testing 51.8% loss=0.22351, acc=0.90625
# [41/100] testing 52.2% loss=0.26837, acc=0.89062
# [41/100] testing 53.1% loss=0.16631, acc=0.93750
# [41/100] testing 53.5% loss=0.24578, acc=0.90625
# [41/100] testing 54.4% loss=0.39735, acc=0.87500
# [41/100] testing 54.8% loss=0.25513, acc=0.92188
# [41/100] testing 55.7% loss=0.11485, acc=0.96875
# [41/100] testing 56.6% loss=0.26695, acc=0.90625
# [41/100] testing 57.0% loss=0.43943, acc=0.84375
# [41/100] testing 57.9% loss=0.24868, acc=0.87500
# [41/100] testing 58.3% loss=0.32874, acc=0.89062
# [41/100] testing 59.2% loss=0.20226, acc=0.92188
# [41/100] testing 59.7% loss=0.27108, acc=0.87500
# [41/100] testing 60.5% loss=0.34836, acc=0.84375
# [41/100] testing 61.4% loss=0.09182, acc=0.98438
# [41/100] testing 61.9% loss=0.22228, acc=0.90625
# [41/100] testing 62.7% loss=0.14709, acc=0.93750
# [41/100] testing 63.2% loss=0.41231, acc=0.89062
# [41/100] testing 64.0% loss=0.43324, acc=0.87500
# [41/100] testing 64.5% loss=0.14940, acc=0.93750
# [41/100] testing 65.4% loss=0.16759, acc=0.95312
# [41/100] testing 65.8% loss=0.30268, acc=0.89062
# [41/100] testing 66.7% loss=0.20460, acc=0.92188
# [41/100] testing 67.6% loss=0.38534, acc=0.89062
# [41/100] testing 68.0% loss=0.13012, acc=0.96875
# [41/100] testing 68.9% loss=0.25622, acc=0.92188
# [41/100] testing 69.3% loss=0.24148, acc=0.89062
# [41/100] testing 70.2% loss=0.46184, acc=0.84375
# [41/100] testing 70.6% loss=0.18551, acc=0.92188
# [41/100] testing 71.5% loss=0.37727, acc=0.87500
# [41/100] testing 72.4% loss=0.09595, acc=0.96875
# [41/100] testing 72.8% loss=0.13002, acc=0.95312
# [41/100] testing 73.7% loss=0.14627, acc=0.95312
# [41/100] testing 74.1% loss=0.42444, acc=0.89062
# [41/100] testing 75.0% loss=0.16660, acc=0.92188
# [41/100] testing 75.4% loss=0.39402, acc=0.85938
# [41/100] testing 76.3% loss=0.07684, acc=0.98438
# [41/100] testing 76.8% loss=0.18250, acc=0.89062
# [41/100] testing 77.6% loss=0.22061, acc=0.92188
# [41/100] testing 78.5% loss=0.40321, acc=0.85938
# [41/100] testing 79.0% loss=0.28052, acc=0.90625
# [41/100] testing 79.8% loss=0.27962, acc=0.89062
# [41/100] testing 80.3% loss=0.19686, acc=0.87500
# [41/100] testing 81.2% loss=0.37542, acc=0.89062
# [41/100] testing 81.6% loss=0.26253, acc=0.89062
# [41/100] testing 82.5% loss=0.19671, acc=0.92188
# [41/100] testing 83.3% loss=0.24159, acc=0.95312
# [41/100] testing 83.8% loss=0.14323, acc=0.95312
# [41/100] testing 84.7% loss=0.38331, acc=0.82812
# [41/100] testing 85.1% loss=0.20621, acc=0.90625
# [41/100] testing 86.0% loss=0.29350, acc=0.89062
# [41/100] testing 86.4% loss=0.46549, acc=0.82812
# [41/100] testing 87.3% loss=0.20514, acc=0.89062
# [41/100] testing 87.7% loss=0.25644, acc=0.90625
# [41/100] testing 88.6% loss=0.24187, acc=0.87500
# [41/100] testing 89.5% loss=0.49781, acc=0.81250
# [41/100] testing 89.9% loss=0.19931, acc=0.90625
# [41/100] testing 90.8% loss=0.32112, acc=0.90625
# [41/100] testing 91.2% loss=0.15646, acc=0.90625
# [41/100] testing 92.1% loss=0.32940, acc=0.90625
# [41/100] testing 92.6% loss=0.40317, acc=0.84375
# [41/100] testing 93.4% loss=0.33025, acc=0.84375
# [41/100] testing 94.3% loss=0.12349, acc=0.92188
# [41/100] testing 94.7% loss=0.18004, acc=0.93750
# [41/100] testing 95.6% loss=0.41965, acc=0.87500
# [41/100] testing 96.1% loss=0.15201, acc=0.95312
# [41/100] testing 96.9% loss=0.29350, acc=0.87500
# [41/100] testing 97.4% loss=0.15960, acc=0.95312
# [41/100] testing 98.3% loss=0.16050, acc=0.90625
# [41/100] testing 98.7% loss=0.32209, acc=0.90625
# [41/100] testing 99.6% loss=0.29538, acc=0.90625
# [42/100] training 0.2% loss=0.23195, acc=0.90625
# [42/100] training 0.4% loss=0.35904, acc=0.85938
# [42/100] training 0.5% loss=0.18549, acc=0.93750
# [42/100] training 0.8% loss=0.17640, acc=0.93750
# [42/100] training 0.9% loss=0.14871, acc=0.95312
# [42/100] training 1.1% loss=0.23891, acc=0.93750
# [42/100] training 1.2% loss=0.18410, acc=0.93750
# [42/100] training 1.4% loss=0.26287, acc=0.90625
# [42/100] training 1.6% loss=0.11837, acc=0.95312
# [42/100] training 1.8% loss=0.15070, acc=0.93750
# [42/100] training 2.0% loss=0.26953, acc=0.87500
# [42/100] training 2.1% loss=0.20791, acc=0.87500
# [42/100] training 2.3% loss=0.13940, acc=0.95312
# [42/100] training 2.4% loss=0.21495, acc=0.90625
# [42/100] training 2.6% loss=0.14758, acc=0.93750
# [42/100] training 2.7% loss=0.35675, acc=0.87500
# [42/100] training 3.0% loss=0.16652, acc=0.92188
# [42/100] training 3.2% loss=0.14529, acc=0.96875
# [42/100] training 3.3% loss=0.28118, acc=0.92188
# [42/100] training 3.5% loss=0.12082, acc=0.96875
# [42/100] training 3.6% loss=0.22438, acc=0.92188
# [42/100] training 3.8% loss=0.17116, acc=0.90625
# [42/100] training 3.9% loss=0.15031, acc=0.92188
# [42/100] training 4.2% loss=0.13528, acc=0.92188
# [42/100] training 4.4% loss=0.15855, acc=0.93750
# [42/100] training 4.5% loss=0.22284, acc=0.93750
# [42/100] training 4.7% loss=0.29052, acc=0.87500
# [42/100] training 4.8% loss=0.19170, acc=0.90625
# [42/100] training 5.0% loss=0.14902, acc=0.93750
# [42/100] training 5.2% loss=0.35289, acc=0.85938
# [42/100] training 5.4% loss=0.10945, acc=0.96875
# [42/100] training 5.5% loss=0.26832, acc=0.89062
# [42/100] training 5.7% loss=0.22122, acc=0.90625
# [42/100] training 5.9% loss=0.25571, acc=0.90625
# [42/100] training 6.0% loss=0.28845, acc=0.84375
# [42/100] training 6.3% loss=0.16604, acc=0.90625
# [42/100] training 6.4% loss=0.12449, acc=0.96875
# [42/100] training 6.6% loss=0.20453, acc=0.90625
# [42/100] training 6.7% loss=0.23929, acc=0.89062
# [42/100] training 6.9% loss=0.19125, acc=0.90625
# [42/100] training 7.1% loss=0.18837, acc=0.95312
# [42/100] training 7.2% loss=0.31972, acc=0.87500
# [42/100] training 7.5% loss=0.12878, acc=0.93750
# [42/100] training 7.6% loss=0.27302, acc=0.89062
# [42/100] training 7.8% loss=0.17411, acc=0.92188
# [42/100] training 7.9% loss=0.16999, acc=0.89062
# [42/100] training 8.1% loss=0.11307, acc=0.95312
# [42/100] training 8.2% loss=0.23642, acc=0.90625
# [42/100] training 8.4% loss=0.27865, acc=0.93750
# [42/100] training 8.7% loss=0.23325, acc=0.87500
# [42/100] training 8.8% loss=0.26376, acc=0.85938
# [42/100] training 9.0% loss=0.15255, acc=0.92188
# [42/100] training 9.1% loss=0.33541, acc=0.92188
# [42/100] training 9.3% loss=0.33595, acc=0.89062
# [42/100] training 9.4% loss=0.15299, acc=0.93750
# [42/100] training 9.7% loss=0.18368, acc=0.93750
# [42/100] training 9.9% loss=0.31053, acc=0.87500
# [42/100] training 10.0% loss=0.18836, acc=0.90625
# [42/100] training 10.2% loss=0.16842, acc=0.92188
# [42/100] training 10.3% loss=0.18423, acc=0.92188
# [42/100] training 10.5% loss=0.24691, acc=0.90625
# [42/100] training 10.6% loss=0.13624, acc=0.92188
# [42/100] training 10.9% loss=0.16388, acc=0.89062
# [42/100] training 11.0% loss=0.28691, acc=0.90625
# [42/100] training 11.2% loss=0.08156, acc=0.96875
# [42/100] training 11.4% loss=0.16327, acc=0.92188
# [42/100] training 11.5% loss=0.35409, acc=0.85938
# [42/100] training 11.7% loss=0.06125, acc=1.00000
# [42/100] training 11.8% loss=0.15876, acc=0.93750
# [42/100] training 12.1% loss=0.12761, acc=0.95312
# [42/100] training 12.2% loss=0.17553, acc=0.92188
# [42/100] training 12.4% loss=0.14184, acc=0.93750
# [42/100] training 12.6% loss=0.20931, acc=0.92188
# [42/100] training 12.7% loss=0.12607, acc=0.95312
# [42/100] training 12.9% loss=0.29670, acc=0.87500
# [42/100] training 13.0% loss=0.11829, acc=0.92188
# [42/100] training 13.3% loss=0.19817, acc=0.92188
# [42/100] training 13.4% loss=0.23415, acc=0.89062
# [42/100] training 13.6% loss=0.18866, acc=0.93750
# [42/100] training 13.7% loss=0.44853, acc=0.87500
# [42/100] training 13.9% loss=0.37943, acc=0.85938
# [42/100] training 14.1% loss=0.20987, acc=0.90625
# [42/100] training 14.3% loss=0.16589, acc=0.95312
# [42/100] training 14.5% loss=0.22194, acc=0.92188
# [42/100] training 14.6% loss=0.20070, acc=0.93750
# [42/100] training 14.8% loss=0.21059, acc=0.90625
# [42/100] training 14.9% loss=0.12107, acc=0.96875
# [42/100] training 15.1% loss=0.23044, acc=0.87500
# [42/100] training 15.4% loss=0.22826, acc=0.89062
# [42/100] training 15.5% loss=0.10009, acc=0.96875
# [42/100] training 15.7% loss=0.37723, acc=0.89062
# [42/100] training 15.8% loss=0.15565, acc=0.92188
# [42/100] training 16.0% loss=0.29128, acc=0.87500
# [42/100] training 16.1% loss=0.34817, acc=0.85938
# [42/100] training 16.3% loss=0.22595, acc=0.90625
# [42/100] training 16.4% loss=0.22644, acc=0.87500
# [42/100] training 16.7% loss=0.31559, acc=0.85938
# [42/100] training 16.9% loss=0.26691, acc=0.87500
# [42/100] training 17.0% loss=0.17725, acc=0.95312
# [42/100] training 17.2% loss=0.20164, acc=0.92188
# [42/100] training 17.3% loss=0.25935, acc=0.93750
# [42/100] training 17.5% loss=0.19880, acc=0.92188
# [42/100] training 17.7% loss=0.17582, acc=0.92188
# [42/100] training 17.9% loss=0.23905, acc=0.90625
# [42/100] training 18.1% loss=0.34059, acc=0.87500
# [42/100] training 18.2% loss=0.25907, acc=0.90625
# [42/100] training 18.4% loss=0.28832, acc=0.89062
# [42/100] training 18.5% loss=0.21079, acc=0.92188
# [42/100] training 18.8% loss=0.20039, acc=0.87500
# [42/100] training 18.9% loss=0.10370, acc=0.95312
# [42/100] training 19.1% loss=0.27739, acc=0.89062
# [42/100] training 19.2% loss=0.12622, acc=0.95312
# [42/100] training 19.4% loss=0.20969, acc=0.92188
# [42/100] training 19.6% loss=0.19182, acc=0.90625
# [42/100] training 19.7% loss=0.29708, acc=0.87500
# [42/100] training 20.0% loss=0.10446, acc=0.95312
# [42/100] training 20.1% loss=0.17265, acc=0.96875
# [42/100] training 20.3% loss=0.25385, acc=0.93750
# [42/100] training 20.4% loss=0.22454, acc=0.87500
# [42/100] training 20.6% loss=0.27011, acc=0.92188
# [42/100] training 20.8% loss=0.18555, acc=0.92188
# [42/100] training 20.9% loss=0.23801, acc=0.92188
# [42/100] training 21.2% loss=0.25475, acc=0.90625
# [42/100] training 21.3% loss=0.20375, acc=0.90625
# [42/100] training 21.5% loss=0.23208, acc=0.92188
# [42/100] training 21.6% loss=0.11000, acc=0.96875
# [42/100] training 21.8% loss=0.18053, acc=0.90625
# [42/100] training 21.9% loss=0.19515, acc=0.90625
# [42/100] training 22.2% loss=0.19606, acc=0.90625
# [42/100] training 22.4% loss=0.21722, acc=0.87500
# [42/100] training 22.5% loss=0.12117, acc=0.96875
# [42/100] training 22.7% loss=0.22658, acc=0.92188
# [42/100] training 22.8% loss=0.19830, acc=0.93750
# [42/100] training 23.0% loss=0.09291, acc=0.96875
# [42/100] training 23.1% loss=0.21037, acc=0.92188
# [42/100] training 23.4% loss=0.33995, acc=0.85938
# [42/100] training 23.6% loss=0.28505, acc=0.89062
# [42/100] training 23.7% loss=0.17621, acc=0.92188
# [42/100] training 23.9% loss=0.22289, acc=0.90625
# [42/100] training 24.0% loss=0.16380, acc=0.89062
# [42/100] training 24.2% loss=0.21924, acc=0.89062
# [42/100] training 24.3% loss=0.23807, acc=0.90625
# [42/100] training 24.6% loss=0.23472, acc=0.90625
# [42/100] training 24.7% loss=0.26136, acc=0.95312
# [42/100] training 24.9% loss=0.19038, acc=0.95312
# [42/100] training 25.1% loss=0.26633, acc=0.93750
# [42/100] training 25.2% loss=0.17306, acc=0.95312
# [42/100] training 25.4% loss=0.18691, acc=0.90625
# [42/100] training 25.6% loss=0.17260, acc=0.95312
# [42/100] training 25.8% loss=0.18396, acc=0.92188
# [42/100] training 25.9% loss=0.16155, acc=0.92188
# [42/100] training 26.1% loss=0.11166, acc=0.96875
# [42/100] training 26.3% loss=0.11992, acc=0.96875
# [42/100] training 26.4% loss=0.12041, acc=0.95312
# [42/100] training 26.6% loss=0.07016, acc=0.96875
# [42/100] training 26.8% loss=0.21275, acc=0.92188
# [42/100] training 27.0% loss=0.25574, acc=0.90625
# [42/100] training 27.1% loss=0.14707, acc=0.93750
# [42/100] training 27.3% loss=0.14825, acc=0.92188
# [42/100] training 27.4% loss=0.07222, acc=0.98438
# [42/100] training 27.6% loss=0.27847, acc=0.93750
# [42/100] training 27.9% loss=0.12067, acc=0.96875
# [42/100] training 28.0% loss=0.23079, acc=0.90625
# [42/100] training 28.2% loss=0.16581, acc=0.93750
# [42/100] training 28.3% loss=0.08212, acc=0.95312
# [42/100] training 28.5% loss=0.16843, acc=0.90625
# [42/100] training 28.6% loss=0.22353, acc=0.93750
# [42/100] training 28.8% loss=0.18064, acc=0.93750
# [42/100] training 29.1% loss=0.16938, acc=0.93750
# [42/100] training 29.2% loss=0.12316, acc=0.95312
# [42/100] training 29.4% loss=0.23953, acc=0.87500
# [42/100] training 29.5% loss=0.10345, acc=0.96875
# [42/100] training 29.7% loss=0.31237, acc=0.87500
# [42/100] training 29.8% loss=0.15563, acc=0.95312
# [42/100] training 30.0% loss=0.15463, acc=0.93750
# [42/100] training 30.2% loss=0.18025, acc=0.93750
# [42/100] training 30.4% loss=0.04757, acc=0.98438
# [42/100] training 30.6% loss=0.17604, acc=0.95312
# [42/100] training 30.7% loss=0.17021, acc=0.90625
# [42/100] training 30.9% loss=0.39449, acc=0.90625
# [42/100] training 31.0% loss=0.09559, acc=0.96875
# [42/100] training 31.3% loss=0.18355, acc=0.96875
# [42/100] training 31.4% loss=0.33499, acc=0.84375
# [42/100] training 31.6% loss=0.26128, acc=0.87500
# [42/100] training 31.8% loss=0.15644, acc=0.92188
# [42/100] training 31.9% loss=0.18735, acc=0.92188
# [42/100] training 32.1% loss=0.20678, acc=0.93750
# [42/100] training 32.2% loss=0.27285, acc=0.89062
# [42/100] training 32.5% loss=0.13257, acc=0.93750
# [42/100] training 32.6% loss=0.24278, acc=0.89062
# [42/100] training 32.8% loss=0.21279, acc=0.93750
# [42/100] training 32.9% loss=0.30190, acc=0.90625
# [42/100] training 33.1% loss=0.08619, acc=0.96875
# [42/100] training 33.3% loss=0.17804, acc=0.92188
# [42/100] training 33.4% loss=0.13663, acc=0.93750
# [42/100] training 33.7% loss=0.18529, acc=0.92188
# [42/100] training 33.8% loss=0.20192, acc=0.92188
# [42/100] training 34.0% loss=0.16630, acc=0.90625
# [42/100] training 34.1% loss=0.10654, acc=0.96875
# [42/100] training 34.3% loss=0.18334, acc=0.93750
# [42/100] training 34.5% loss=0.25399, acc=0.89062
# [42/100] training 34.7% loss=0.11043, acc=0.95312
# [42/100] training 34.9% loss=0.13024, acc=0.92188
# [42/100] training 35.0% loss=0.16711, acc=0.96875
# [42/100] training 35.2% loss=0.17691, acc=0.89062
# [42/100] training 35.3% loss=0.21380, acc=0.93750
# [42/100] training 35.5% loss=0.16023, acc=0.90625
# [42/100] training 35.6% loss=0.34100, acc=0.87500
# [42/100] training 35.9% loss=0.24545, acc=0.90625
# [42/100] training 36.1% loss=0.24531, acc=0.92188
# [42/100] training 36.2% loss=0.14417, acc=0.93750
# [42/100] training 36.4% loss=0.24084, acc=0.92188
# [42/100] training 36.5% loss=0.16610, acc=0.95312
# [42/100] training 36.7% loss=0.17478, acc=0.93750
# [42/100] training 36.8% loss=0.13222, acc=0.95312
# [42/100] training 37.1% loss=0.25508, acc=0.89062
# [42/100] training 37.3% loss=0.17258, acc=0.90625
# [42/100] training 37.4% loss=0.16933, acc=0.92188
# [42/100] training 37.6% loss=0.22155, acc=0.90625
# [42/100] training 37.7% loss=0.26344, acc=0.89062
# [42/100] training 37.9% loss=0.27723, acc=0.89062
# [42/100] training 38.1% loss=0.36066, acc=0.89062
# [42/100] training 38.3% loss=0.22977, acc=0.93750
# [42/100] training 38.4% loss=0.07834, acc=0.98438
# [42/100] training 38.6% loss=0.15233, acc=0.93750
# [42/100] training 38.8% loss=0.27109, acc=0.85938
# [42/100] training 38.9% loss=0.19679, acc=0.90625
# [42/100] training 39.1% loss=0.27959, acc=0.90625
# [42/100] training 39.3% loss=0.19177, acc=0.92188
# [42/100] training 39.5% loss=0.22633, acc=0.93750
# [42/100] training 39.6% loss=0.20640, acc=0.93750
# [42/100] training 39.8% loss=0.09032, acc=0.96875
# [42/100] training 40.0% loss=0.21979, acc=0.93750
# [42/100] training 40.1% loss=0.32205, acc=0.87500
# [42/100] training 40.4% loss=0.12746, acc=0.90625
# [42/100] training 40.5% loss=0.20533, acc=0.92188
# [42/100] training 40.7% loss=0.21347, acc=0.93750
# [42/100] training 40.8% loss=0.12997, acc=0.96875
# [42/100] training 41.0% loss=0.12910, acc=0.93750
# [42/100] training 41.1% loss=0.32967, acc=0.89062
# [42/100] training 41.3% loss=0.23712, acc=0.87500
# [42/100] training 41.6% loss=0.32893, acc=0.89062
# [42/100] training 41.7% loss=0.26054, acc=0.92188
# [42/100] training 41.9% loss=0.16465, acc=0.93750
# [42/100] training 42.0% loss=0.17861, acc=0.90625
# [42/100] training 42.2% loss=0.21482, acc=0.92188
# [42/100] training 42.3% loss=0.16377, acc=0.93750
# [42/100] training 42.5% loss=0.09826, acc=0.98438
# [42/100] training 42.8% loss=0.15099, acc=0.92188
# [42/100] training 42.9% loss=0.12218, acc=0.93750
# [42/100] training 43.1% loss=0.13548, acc=0.95312
# [42/100] training 43.2% loss=0.09924, acc=0.96875
# [42/100] training 43.4% loss=0.19677, acc=0.90625
# [42/100] training 43.5% loss=0.21989, acc=0.85938
# [42/100] training 43.8% loss=0.24364, acc=0.92188
# [42/100] training 43.9% loss=0.23048, acc=0.93750
# [42/100] training 44.1% loss=0.11379, acc=0.96875
# [42/100] training 44.3% loss=0.19483, acc=0.93750
# [42/100] training 44.4% loss=0.19036, acc=0.92188
# [42/100] training 44.6% loss=0.37351, acc=0.84375
# [42/100] training 44.7% loss=0.29274, acc=0.85938
# [42/100] training 45.0% loss=0.11809, acc=0.92188
# [42/100] training 45.1% loss=0.24773, acc=0.92188
# [42/100] training 45.3% loss=0.26187, acc=0.87500
# [42/100] training 45.5% loss=0.11998, acc=0.95312
# [42/100] training 45.6% loss=0.24449, acc=0.89062
# [42/100] training 45.8% loss=0.14966, acc=0.93750
# [42/100] training 45.9% loss=0.16428, acc=0.92188
# [42/100] training 46.2% loss=0.17188, acc=0.95312
# [42/100] training 46.3% loss=0.10545, acc=0.95312
# [42/100] training 46.5% loss=0.34723, acc=0.89062
# [42/100] training 46.6% loss=0.16380, acc=0.92188
# [42/100] training 46.8% loss=0.14351, acc=0.95312
# [42/100] training 47.0% loss=0.24491, acc=0.90625
# [42/100] training 47.2% loss=0.27659, acc=0.85938
# [42/100] training 47.4% loss=0.17205, acc=0.93750
# [42/100] training 47.5% loss=0.25002, acc=0.89062
# [42/100] training 47.7% loss=0.16126, acc=0.93750
# [42/100] training 47.8% loss=0.22601, acc=0.93750
# [42/100] training 48.0% loss=0.18089, acc=0.93750
# [42/100] training 48.3% loss=0.06966, acc=0.98438
# [42/100] training 48.4% loss=0.06349, acc=0.98438
# [42/100] training 48.6% loss=0.10583, acc=0.95312
# [42/100] training 48.7% loss=0.28252, acc=0.87500
# [42/100] training 48.9% loss=0.12461, acc=0.93750
# [42/100] training 49.0% loss=0.09350, acc=0.96875
# [42/100] training 49.2% loss=0.18900, acc=0.89062
# [42/100] training 49.3% loss=0.18624, acc=0.93750
# [42/100] training 49.6% loss=0.25055, acc=0.93750
# [42/100] training 49.8% loss=0.23063, acc=0.89062
# [42/100] training 49.9% loss=0.19978, acc=0.92188
# [42/100] training 50.1% loss=0.09514, acc=0.96875
# [42/100] training 50.2% loss=0.17391, acc=0.93750
# [42/100] training 50.4% loss=0.33377, acc=0.89062
# [42/100] training 50.6% loss=0.17092, acc=0.93750
# [42/100] training 50.8% loss=0.25781, acc=0.89062
# [42/100] training 51.0% loss=0.26594, acc=0.89062
# [42/100] training 51.1% loss=0.22420, acc=0.93750
# [42/100] training 51.3% loss=0.22849, acc=0.90625
# [42/100] training 51.4% loss=0.15607, acc=0.93750
# [42/100] training 51.7% loss=0.16228, acc=0.92188
# [42/100] training 51.8% loss=0.32949, acc=0.81250
# [42/100] training 52.0% loss=0.19853, acc=0.89062
# [42/100] training 52.1% loss=0.15159, acc=0.95312
# [42/100] training 52.3% loss=0.18142, acc=0.93750
# [42/100] training 52.5% loss=0.09519, acc=0.95312
# [42/100] training 52.6% loss=0.21405, acc=0.93750
# [42/100] training 52.9% loss=0.43221, acc=0.84375
# [42/100] training 53.0% loss=0.07039, acc=0.96875
# [42/100] training 53.2% loss=0.25296, acc=0.90625
# [42/100] training 53.3% loss=0.08881, acc=0.98438
# [42/100] training 53.5% loss=0.16729, acc=0.93750
# [42/100] training 53.7% loss=0.07173, acc=0.98438
# [42/100] training 53.8% loss=0.25192, acc=0.87500
# [42/100] training 54.1% loss=0.31890, acc=0.84375
# [42/100] training 54.2% loss=0.11033, acc=0.96875
# [42/100] training 54.4% loss=0.15189, acc=0.93750
# [42/100] training 54.5% loss=0.24788, acc=0.90625
# [42/100] training 54.7% loss=0.31560, acc=0.85938
# [42/100] training 54.8% loss=0.13204, acc=0.96875
# [42/100] training 55.1% loss=0.18130, acc=0.93750
# [42/100] training 55.3% loss=0.10467, acc=0.95312
# [42/100] training 55.4% loss=0.16251, acc=0.93750
# [42/100] training 55.6% loss=0.16050, acc=0.93750
# [42/100] training 55.7% loss=0.21063, acc=0.93750
# [42/100] training 55.9% loss=0.11977, acc=0.93750
# [42/100] training 56.0% loss=0.18425, acc=0.93750
# [42/100] training 56.3% loss=0.37757, acc=0.85938
# [42/100] training 56.5% loss=0.20641, acc=0.90625
# [42/100] training 56.6% loss=0.22108, acc=0.90625
# [42/100] training 56.8% loss=0.24720, acc=0.90625
# [42/100] training 56.9% loss=0.11909, acc=0.95312
# [42/100] training 57.1% loss=0.21310, acc=0.90625
# [42/100] training 57.2% loss=0.09481, acc=0.98438
# [42/100] training 57.5% loss=0.25716, acc=0.85938
# [42/100] training 57.6% loss=0.15663, acc=0.92188
# [42/100] training 57.8% loss=0.16065, acc=0.92188
# [42/100] training 58.0% loss=0.17106, acc=0.87500
# [42/100] training 58.1% loss=0.10892, acc=0.95312
# [42/100] training 58.3% loss=0.11335, acc=0.95312
# [42/100] training 58.4% loss=0.27816, acc=0.90625
# [42/100] training 58.7% loss=0.29258, acc=0.92188
# [42/100] training 58.8% loss=0.36606, acc=0.84375
# [42/100] training 59.0% loss=0.10346, acc=0.98438
# [42/100] training 59.2% loss=0.18768, acc=0.89062
# [42/100] training 59.3% loss=0.15443, acc=0.93750
# [42/100] training 59.5% loss=0.17707, acc=0.92188
# [42/100] training 59.7% loss=0.18466, acc=0.89062
# [42/100] training 59.9% loss=0.14030, acc=0.93750
# [42/100] training 60.0% loss=0.13998, acc=0.93750
# [42/100] training 60.2% loss=0.17509, acc=0.90625
# [42/100] training 60.3% loss=0.15574, acc=0.92188
# [42/100] training 60.5% loss=0.25524, acc=0.93750
# [42/100] training 60.8% loss=0.19725, acc=0.90625
# [42/100] training 60.9% loss=0.29784, acc=0.85938
# [42/100] training 61.1% loss=0.26716, acc=0.92188
# [42/100] training 61.2% loss=0.20172, acc=0.93750
# [42/100] training 61.4% loss=0.27231, acc=0.87500
# [42/100] training 61.5% loss=0.39820, acc=0.82812
# [42/100] training 61.7% loss=0.18592, acc=0.93750
# [42/100] training 62.0% loss=0.21390, acc=0.89062
# [42/100] training 62.1% loss=0.27722, acc=0.92188
# [42/100] training 62.3% loss=0.13888, acc=0.93750
# [42/100] training 62.4% loss=0.20774, acc=0.92188
# [42/100] training 62.6% loss=0.29817, acc=0.87500
# [42/100] training 62.7% loss=0.14058, acc=0.92188
# [42/100] training 62.9% loss=0.17756, acc=0.93750
# [42/100] training 63.1% loss=0.17105, acc=0.95312
# [42/100] training 63.3% loss=0.18556, acc=0.90625
# [42/100] training 63.5% loss=0.14890, acc=0.93750
# [42/100] training 63.6% loss=0.43415, acc=0.82812
# [42/100] training 63.8% loss=0.25715, acc=0.92188
# [42/100] training 63.9% loss=0.12504, acc=0.96875
# [42/100] training 64.2% loss=0.19698, acc=0.92188
# [42/100] training 64.3% loss=0.23358, acc=0.93750
# [42/100] training 64.5% loss=0.21261, acc=0.89062
# [42/100] training 64.7% loss=0.14722, acc=0.96875
# [42/100] training 64.8% loss=0.44785, acc=0.78125
# [42/100] training 65.0% loss=0.16140, acc=0.96875
# [42/100] training 65.1% loss=0.22302, acc=0.90625
# [42/100] training 65.4% loss=0.13775, acc=0.96875
# [42/100] training 65.5% loss=0.09841, acc=0.98438
# [42/100] training 65.7% loss=0.15974, acc=0.92188
# [42/100] training 65.8% loss=0.14206, acc=0.93750
# [42/100] training 66.0% loss=0.18218, acc=0.93750
# [42/100] training 66.2% loss=0.09148, acc=0.95312
# [42/100] training 66.3% loss=0.18641, acc=0.92188
# [42/100] training 66.6% loss=0.06799, acc=0.96875
# [42/100] training 66.7% loss=0.08166, acc=0.96875
# [42/100] training 66.9% loss=0.24787, acc=0.89062
# [42/100] training 67.0% loss=0.12943, acc=0.90625
# [42/100] training 67.2% loss=0.13392, acc=0.92188
# [42/100] training 67.4% loss=0.12871, acc=0.93750
# [42/100] training 67.6% loss=0.20867, acc=0.90625
# [42/100] training 67.8% loss=0.13887, acc=0.96875
# [42/100] training 67.9% loss=0.11697, acc=0.95312
# [42/100] training 68.1% loss=0.34161, acc=0.87500
# [42/100] training 68.2% loss=0.15395, acc=0.93750
# [42/100] training 68.4% loss=0.12968, acc=0.93750
# [42/100] training 68.5% loss=0.17757, acc=0.89062
# [42/100] training 68.8% loss=0.17954, acc=0.92188
# [42/100] training 69.0% loss=0.32761, acc=0.90625
# [42/100] training 69.1% loss=0.23619, acc=0.93750
# [42/100] training 69.3% loss=0.22554, acc=0.87500
# [42/100] training 69.4% loss=0.17334, acc=0.95312
# [42/100] training 69.6% loss=0.15552, acc=0.93750
# [42/100] training 69.7% loss=0.16011, acc=0.92188
# [42/100] training 70.0% loss=0.29498, acc=0.87500
# [42/100] training 70.2% loss=0.22737, acc=0.89062
# [42/100] training 70.3% loss=0.17308, acc=0.96875
# [42/100] training 70.5% loss=0.17055, acc=0.95312
# [42/100] training 70.6% loss=0.13823, acc=0.96875
# [42/100] training 70.8% loss=0.16549, acc=0.90625
# [42/100] training 71.0% loss=0.26788, acc=0.90625
# [42/100] training 71.2% loss=0.12051, acc=0.95312
# [42/100] training 71.3% loss=0.18027, acc=0.98438
# [42/100] training 71.5% loss=0.25794, acc=0.93750
# [42/100] training 71.7% loss=0.22137, acc=0.90625
# [42/100] training 71.8% loss=0.26560, acc=0.93750
# [42/100] training 72.0% loss=0.09432, acc=0.95312
# [42/100] training 72.2% loss=0.23450, acc=0.92188
# [42/100] training 72.4% loss=0.18607, acc=0.90625
# [42/100] training 72.5% loss=0.24349, acc=0.87500
# [42/100] training 72.7% loss=0.25282, acc=0.85938
# [42/100] training 72.9% loss=0.17468, acc=0.96875
# [42/100] training 73.0% loss=0.13535, acc=0.93750
# [42/100] training 73.3% loss=0.25844, acc=0.87500
# [42/100] training 73.4% loss=0.11978, acc=0.95312
# [42/100] training 73.6% loss=0.09538, acc=0.96875
# [42/100] training 73.7% loss=0.20761, acc=0.92188
# [42/100] training 73.9% loss=0.14734, acc=0.93750
# [42/100] training 74.0% loss=0.20603, acc=0.92188
# [42/100] training 74.2% loss=0.12192, acc=0.93750
# [42/100] training 74.5% loss=0.17783, acc=0.93750
# [42/100] training 74.6% loss=0.32828, acc=0.84375
# [42/100] training 74.8% loss=0.33688, acc=0.89062
# [42/100] training 74.9% loss=0.21815, acc=0.90625
# [42/100] training 75.1% loss=0.17552, acc=0.92188
# [42/100] training 75.2% loss=0.11816, acc=0.95312
# [42/100] training 75.4% loss=0.25251, acc=0.89062
# [42/100] training 75.7% loss=0.21247, acc=0.89062
# [42/100] training 75.8% loss=0.22656, acc=0.90625
# [42/100] training 76.0% loss=0.13583, acc=0.95312
# [42/100] training 76.1% loss=0.22991, acc=0.93750
# [42/100] training 76.3% loss=0.11493, acc=0.96875
# [42/100] training 76.4% loss=0.21222, acc=0.93750
# [42/100] training 76.7% loss=0.13747, acc=0.92188
# [42/100] training 76.8% loss=0.13516, acc=0.95312
# [42/100] training 77.0% loss=0.09200, acc=0.95312
# [42/100] training 77.2% loss=0.13602, acc=0.95312
# [42/100] training 77.3% loss=0.07134, acc=0.96875
# [42/100] training 77.5% loss=0.17769, acc=0.92188
# [42/100] training 77.6% loss=0.18717, acc=0.93750
# [42/100] training 77.9% loss=0.16101, acc=0.90625
# [42/100] training 78.0% loss=0.12891, acc=0.95312
# [42/100] training 78.2% loss=0.23942, acc=0.89062
# [42/100] training 78.4% loss=0.07240, acc=1.00000
# [42/100] training 78.5% loss=0.25485, acc=0.93750
# [42/100] training 78.7% loss=0.18989, acc=0.92188
# [42/100] training 78.8% loss=0.13779, acc=0.95312
# [42/100] training 79.1% loss=0.09518, acc=0.95312
# [42/100] training 79.2% loss=0.15127, acc=0.95312
# [42/100] training 79.4% loss=0.20680, acc=0.93750
# [42/100] training 79.5% loss=0.18406, acc=0.92188
# [42/100] training 79.7% loss=0.13275, acc=0.96875
# [42/100] training 79.9% loss=0.11429, acc=0.92188
# [42/100] training 80.1% loss=0.13054, acc=0.95312
# [42/100] training 80.3% loss=0.15778, acc=0.95312
# [42/100] training 80.4% loss=0.22784, acc=0.92188
# [42/100] training 80.6% loss=0.21598, acc=0.92188
# [42/100] training 80.7% loss=0.08844, acc=0.98438
# [42/100] training 80.9% loss=0.23930, acc=0.89062
# [42/100] training 81.2% loss=0.21329, acc=0.92188
# [42/100] training 81.3% loss=0.17553, acc=0.90625
# [42/100] training 81.5% loss=0.18006, acc=0.90625
# [42/100] training 81.6% loss=0.36271, acc=0.85938
# [42/100] training 81.8% loss=0.24957, acc=0.87500
# [42/100] training 81.9% loss=0.31068, acc=0.92188
# [42/100] training 82.1% loss=0.27813, acc=0.89062
# [42/100] training 82.2% loss=0.23294, acc=0.92188
# [42/100] training 82.5% loss=0.16146, acc=0.95312
# [42/100] training 82.7% loss=0.17304, acc=0.96875
# [42/100] training 82.8% loss=0.19238, acc=0.93750
# [42/100] training 83.0% loss=0.14626, acc=0.95312
# [42/100] training 83.1% loss=0.14481, acc=0.96875
# [42/100] training 83.3% loss=0.25687, acc=0.93750
# [42/100] training 83.5% loss=0.12306, acc=0.92188
# [42/100] training 83.7% loss=0.31152, acc=0.90625
# [42/100] training 83.9% loss=0.22623, acc=0.89062
# [42/100] training 84.0% loss=0.10515, acc=0.95312
# [42/100] training 84.2% loss=0.16268, acc=0.92188
# [42/100] training 84.3% loss=0.12092, acc=0.95312
# [42/100] training 84.5% loss=0.09215, acc=0.96875
# [42/100] training 84.7% loss=0.13945, acc=0.93750
# [42/100] training 84.9% loss=0.15986, acc=0.92188
# [42/100] training 85.0% loss=0.11818, acc=0.96875
# [42/100] training 85.2% loss=0.18404, acc=0.92188
# [42/100] training 85.4% loss=0.11718, acc=0.92188
# [42/100] training 85.5% loss=0.16548, acc=0.92188
# [42/100] training 85.8% loss=0.22158, acc=0.93750
# [42/100] training 85.9% loss=0.15540, acc=0.93750
# [42/100] training 86.1% loss=0.20190, acc=0.93750
# [42/100] training 86.2% loss=0.10420, acc=0.95312
# [42/100] training 86.4% loss=0.30251, acc=0.90625
# [42/100] training 86.6% loss=0.16238, acc=0.92188
# [42/100] training 86.7% loss=0.14724, acc=0.92188
# [42/100] training 87.0% loss=0.26308, acc=0.92188
# [42/100] training 87.1% loss=0.13220, acc=0.96875
# [42/100] training 87.3% loss=0.18005, acc=0.95312
# [42/100] training 87.4% loss=0.18531, acc=0.92188
# [42/100] training 87.6% loss=0.28821, acc=0.85938
# [42/100] training 87.7% loss=0.25560, acc=0.92188
# [42/100] training 87.9% loss=0.20510, acc=0.92188
# [42/100] training 88.2% loss=0.16794, acc=0.93750
# [42/100] training 88.3% loss=0.29122, acc=0.87500
# [42/100] training 88.5% loss=0.17235, acc=0.93750
# [42/100] training 88.6% loss=0.14433, acc=0.95312
# [42/100] training 88.8% loss=0.14930, acc=0.93750
# [42/100] training 88.9% loss=0.21318, acc=0.92188
# [42/100] training 89.2% loss=0.19669, acc=0.92188
# [42/100] training 89.4% loss=0.15645, acc=0.93750
# [42/100] training 89.5% loss=0.20268, acc=0.92188
# [42/100] training 89.7% loss=0.26695, acc=0.89062
# [42/100] training 89.8% loss=0.14563, acc=0.95312
# [42/100] training 90.0% loss=0.12362, acc=0.93750
# [42/100] training 90.1% loss=0.27717, acc=0.90625
# [42/100] training 90.4% loss=0.20463, acc=0.92188
# [42/100] training 90.5% loss=0.08303, acc=0.96875
# [42/100] training 90.7% loss=0.10738, acc=0.95312
# [42/100] training 90.9% loss=0.20220, acc=0.93750
# [42/100] training 91.0% loss=0.14917, acc=0.95312
# [42/100] training 91.2% loss=0.14006, acc=0.95312
# [42/100] training 91.3% loss=0.24283, acc=0.93750
# [42/100] training 91.6% loss=0.29633, acc=0.90625
# [42/100] training 91.7% loss=0.19327, acc=0.92188
# [42/100] training 91.9% loss=0.33229, acc=0.87500
# [42/100] training 92.1% loss=0.15907, acc=0.92188
# [42/100] training 92.2% loss=0.13612, acc=0.93750
# [42/100] training 92.4% loss=0.26452, acc=0.89062
# [42/100] training 92.6% loss=0.25738, acc=0.90625
# [42/100] training 92.8% loss=0.17915, acc=0.93750
# [42/100] training 92.9% loss=0.37037, acc=0.90625
# [42/100] training 93.1% loss=0.38712, acc=0.85938
# [42/100] training 93.2% loss=0.27102, acc=0.93750
# [42/100] training 93.4% loss=0.22557, acc=0.90625
# [42/100] training 93.7% loss=0.19455, acc=0.92188
# [42/100] training 93.8% loss=0.12195, acc=0.95312
# [42/100] training 94.0% loss=0.11249, acc=0.96875
# [42/100] training 94.1% loss=0.16511, acc=0.95312
# [42/100] training 94.3% loss=0.15667, acc=0.89062
# [42/100] training 94.4% loss=0.19926, acc=0.92188
# [42/100] training 94.6% loss=0.09962, acc=0.96875
# [42/100] training 94.9% loss=0.17378, acc=0.93750
# [42/100] training 95.0% loss=0.20878, acc=0.90625
# [42/100] training 95.2% loss=0.40771, acc=0.87500
# [42/100] training 95.3% loss=0.26029, acc=0.87500
# [42/100] training 95.5% loss=0.13402, acc=0.93750
# [42/100] training 95.6% loss=0.21145, acc=0.95312
# [42/100] training 95.8% loss=0.29392, acc=0.87500
# [42/100] training 96.0% loss=0.26426, acc=0.89062
# [42/100] training 96.2% loss=0.10049, acc=0.98438
# [42/100] training 96.4% loss=0.20053, acc=0.92188
# [42/100] training 96.5% loss=0.25199, acc=0.90625
# [42/100] training 96.7% loss=0.11701, acc=0.96875
# [42/100] training 96.8% loss=0.18898, acc=0.92188
# [42/100] training 97.1% loss=0.20298, acc=0.89062
# [42/100] training 97.2% loss=0.19760, acc=0.92188
# [42/100] training 97.4% loss=0.23017, acc=0.90625
# [42/100] training 97.6% loss=0.30276, acc=0.85938
# [42/100] training 97.7% loss=0.15860, acc=0.90625
# [42/100] training 97.9% loss=0.11273, acc=0.95312
# [42/100] training 98.0% loss=0.20841, acc=0.95312
# [42/100] training 98.3% loss=0.20449, acc=0.89062
# [42/100] training 98.4% loss=0.23975, acc=0.85938
# [42/100] training 98.6% loss=0.36036, acc=0.82812
# [42/100] training 98.7% loss=0.29088, acc=0.89062
# [42/100] training 98.9% loss=0.20564, acc=0.90625
# [42/100] training 99.1% loss=0.16641, acc=0.93750
# [42/100] training 99.2% loss=0.11059, acc=0.95312
# [42/100] training 99.5% loss=0.23997, acc=0.89062
# [42/100] training 99.6% loss=0.15816, acc=0.92188
# [42/100] training 99.8% loss=0.10600, acc=0.95312
# [42/100] training 99.9% loss=0.03187, acc=0.98438
# [42/100] testing 0.9% loss=0.10779, acc=0.93750
# [42/100] testing 1.8% loss=0.50439, acc=0.82812
# [42/100] testing 2.2% loss=0.26347, acc=0.92188
# [42/100] testing 3.1% loss=0.34619, acc=0.89062
# [42/100] testing 3.5% loss=0.10852, acc=0.95312
# [42/100] testing 4.4% loss=0.23626, acc=0.90625
# [42/100] testing 4.8% loss=0.31945, acc=0.84375
# [42/100] testing 5.7% loss=0.26273, acc=0.87500
# [42/100] testing 6.6% loss=0.20651, acc=0.92188
# [42/100] testing 7.0% loss=0.13595, acc=0.92188
# [42/100] testing 7.9% loss=0.33508, acc=0.87500
# [42/100] testing 8.3% loss=0.29968, acc=0.90625
# [42/100] testing 9.2% loss=0.33229, acc=0.90625
# [42/100] testing 9.7% loss=0.14713, acc=0.95312
# [42/100] testing 10.5% loss=0.19510, acc=0.89062
# [42/100] testing 11.0% loss=0.33934, acc=0.87500
# [42/100] testing 11.8% loss=0.18449, acc=0.95312
# [42/100] testing 12.7% loss=0.51480, acc=0.82812
# [42/100] testing 13.2% loss=0.25784, acc=0.89062
# [42/100] testing 14.0% loss=0.40560, acc=0.90625
# [42/100] testing 14.5% loss=0.23647, acc=0.87500
# [42/100] testing 15.4% loss=0.30106, acc=0.93750
# [42/100] testing 15.8% loss=0.17895, acc=0.90625
# [42/100] testing 16.7% loss=0.37184, acc=0.85938
# [42/100] testing 17.5% loss=0.22771, acc=0.92188
# [42/100] testing 18.0% loss=0.28022, acc=0.89062
# [42/100] testing 18.9% loss=0.09448, acc=0.95312
# [42/100] testing 19.3% loss=0.35373, acc=0.90625
# [42/100] testing 20.2% loss=0.39328, acc=0.85938
# [42/100] testing 20.6% loss=0.34621, acc=0.84375
# [42/100] testing 21.5% loss=0.19704, acc=0.93750
# [42/100] testing 21.9% loss=0.44328, acc=0.87500
# [42/100] testing 22.8% loss=0.29754, acc=0.89062
# [42/100] testing 23.7% loss=0.38329, acc=0.89062
# [42/100] testing 24.1% loss=0.19132, acc=0.90625
# [42/100] testing 25.0% loss=0.47980, acc=0.84375
# [42/100] testing 25.4% loss=0.14520, acc=0.95312
# [42/100] testing 26.3% loss=0.36599, acc=0.89062
# [42/100] testing 26.8% loss=0.33069, acc=0.90625
# [42/100] testing 27.6% loss=0.16535, acc=0.95312
# [42/100] testing 28.5% loss=0.29410, acc=0.90625
# [42/100] testing 29.0% loss=0.16410, acc=0.95312
# [42/100] testing 29.8% loss=0.48309, acc=0.85938
# [42/100] testing 30.3% loss=0.25455, acc=0.90625
# [42/100] testing 31.1% loss=0.39717, acc=0.84375
# [42/100] testing 31.6% loss=0.24469, acc=0.90625
# [42/100] testing 32.5% loss=0.30547, acc=0.90625
# [42/100] testing 32.9% loss=0.46403, acc=0.89062
# [42/100] testing 33.8% loss=0.41307, acc=0.85938
# [42/100] testing 34.7% loss=0.41245, acc=0.85938
# [42/100] testing 35.1% loss=0.20766, acc=0.90625
# [42/100] testing 36.0% loss=0.34743, acc=0.87500
# [42/100] testing 36.4% loss=0.32210, acc=0.90625
# [42/100] testing 37.3% loss=0.35403, acc=0.89062
# [42/100] testing 37.7% loss=0.46783, acc=0.81250
# [42/100] testing 38.6% loss=0.22720, acc=0.95312
# [42/100] testing 39.5% loss=0.27004, acc=0.92188
# [42/100] testing 39.9% loss=0.26824, acc=0.89062
# [42/100] testing 40.8% loss=0.35785, acc=0.87500
# [42/100] testing 41.2% loss=0.31795, acc=0.93750
# [42/100] testing 42.1% loss=0.25812, acc=0.90625
# [42/100] testing 42.5% loss=0.19095, acc=0.92188
# [42/100] testing 43.4% loss=0.32780, acc=0.89062
# [42/100] testing 43.9% loss=0.11323, acc=0.96875
# [42/100] testing 44.7% loss=0.22417, acc=0.89062
# [42/100] testing 45.6% loss=0.34324, acc=0.90625
# [42/100] testing 46.1% loss=0.28652, acc=0.89062
# [42/100] testing 46.9% loss=0.25463, acc=0.90625
# [42/100] testing 47.4% loss=0.12248, acc=0.93750
# [42/100] testing 48.3% loss=0.43612, acc=0.89062
# [42/100] testing 48.7% loss=0.25824, acc=0.84375
# [42/100] testing 49.6% loss=0.55397, acc=0.81250
# [42/100] testing 50.4% loss=0.19923, acc=0.90625
# [42/100] testing 50.9% loss=0.31565, acc=0.90625
# [42/100] testing 51.8% loss=0.24605, acc=0.89062
# [42/100] testing 52.2% loss=0.20485, acc=0.89062
# [42/100] testing 53.1% loss=0.21476, acc=0.90625
# [42/100] testing 53.5% loss=0.17862, acc=0.92188
# [42/100] testing 54.4% loss=0.41481, acc=0.84375
# [42/100] testing 54.8% loss=0.36224, acc=0.82812
# [42/100] testing 55.7% loss=0.16642, acc=0.89062
# [42/100] testing 56.6% loss=0.38565, acc=0.89062
# [42/100] testing 57.0% loss=0.45234, acc=0.84375
# [42/100] testing 57.9% loss=0.26515, acc=0.89062
# [42/100] testing 58.3% loss=0.44137, acc=0.87500
# [42/100] testing 59.2% loss=0.27024, acc=0.87500
# [42/100] testing 59.7% loss=0.32325, acc=0.82812
# [42/100] testing 60.5% loss=0.39019, acc=0.82812
# [42/100] testing 61.4% loss=0.13925, acc=0.93750
# [42/100] testing 61.9% loss=0.25463, acc=0.90625
# [42/100] testing 62.7% loss=0.21156, acc=0.90625
# [42/100] testing 63.2% loss=0.49586, acc=0.85938
# [42/100] testing 64.0% loss=0.44927, acc=0.84375
# [42/100] testing 64.5% loss=0.18747, acc=0.93750
# [42/100] testing 65.4% loss=0.19976, acc=0.92188
# [42/100] testing 65.8% loss=0.35143, acc=0.90625
# [42/100] testing 66.7% loss=0.20763, acc=0.90625
# [42/100] testing 67.6% loss=0.53679, acc=0.85938
# [42/100] testing 68.0% loss=0.06507, acc=0.98438
# [42/100] testing 68.9% loss=0.33990, acc=0.90625
# [42/100] testing 69.3% loss=0.36209, acc=0.84375
# [42/100] testing 70.2% loss=0.39000, acc=0.85938
# [42/100] testing 70.6% loss=0.35335, acc=0.85938
# [42/100] testing 71.5% loss=0.44258, acc=0.89062
# [42/100] testing 72.4% loss=0.20428, acc=0.93750
# [42/100] testing 72.8% loss=0.21282, acc=0.93750
# [42/100] testing 73.7% loss=0.14207, acc=0.93750
# [42/100] testing 74.1% loss=0.40634, acc=0.89062
# [42/100] testing 75.0% loss=0.26813, acc=0.89062
# [42/100] testing 75.4% loss=0.36888, acc=0.85938
# [42/100] testing 76.3% loss=0.06538, acc=0.96875
# [42/100] testing 76.8% loss=0.35545, acc=0.87500
# [42/100] testing 77.6% loss=0.17351, acc=0.92188
# [42/100] testing 78.5% loss=0.35025, acc=0.85938
# [42/100] testing 79.0% loss=0.29196, acc=0.87500
# [42/100] testing 79.8% loss=0.26963, acc=0.85938
# [42/100] testing 80.3% loss=0.34605, acc=0.85938
# [42/100] testing 81.2% loss=0.64273, acc=0.81250
# [42/100] testing 81.6% loss=0.27087, acc=0.90625
# [42/100] testing 82.5% loss=0.14684, acc=0.92188
# [42/100] testing 83.3% loss=0.15241, acc=0.96875
# [42/100] testing 83.8% loss=0.16834, acc=0.93750
# [42/100] testing 84.7% loss=0.37241, acc=0.87500
# [42/100] testing 85.1% loss=0.22049, acc=0.90625
# [42/100] testing 86.0% loss=0.29769, acc=0.92188
# [42/100] testing 86.4% loss=0.56046, acc=0.76562
# [42/100] testing 87.3% loss=0.28639, acc=0.87500
# [42/100] testing 87.7% loss=0.20173, acc=0.90625
# [42/100] testing 88.6% loss=0.16129, acc=0.93750
# [42/100] testing 89.5% loss=0.50452, acc=0.81250
# [42/100] testing 89.9% loss=0.22108, acc=0.90625
# [42/100] testing 90.8% loss=0.35864, acc=0.93750
# [42/100] testing 91.2% loss=0.14485, acc=0.93750
# [42/100] testing 92.1% loss=0.32976, acc=0.87500
# [42/100] testing 92.6% loss=0.33267, acc=0.87500
# [42/100] testing 93.4% loss=0.28070, acc=0.87500
# [42/100] testing 94.3% loss=0.14918, acc=0.93750
# [42/100] testing 94.7% loss=0.17227, acc=0.95312
# [42/100] testing 95.6% loss=0.52828, acc=0.84375
# [42/100] testing 96.1% loss=0.28294, acc=0.87500
# [42/100] testing 96.9% loss=0.27120, acc=0.89062
# [42/100] testing 97.4% loss=0.12965, acc=0.96875
# [42/100] testing 98.3% loss=0.22888, acc=0.90625
# [42/100] testing 98.7% loss=0.22273, acc=0.89062
# [42/100] testing 99.6% loss=0.42582, acc=0.87500
# [43/100] training 0.2% loss=0.32363, acc=0.82812
# [43/100] training 0.4% loss=0.38717, acc=0.90625
# [43/100] training 0.5% loss=0.14795, acc=0.96875
# [43/100] training 0.8% loss=0.15431, acc=0.93750
# [43/100] training 0.9% loss=0.17836, acc=0.92188
# [43/100] training 1.1% loss=0.21761, acc=0.93750
# [43/100] training 1.2% loss=0.24590, acc=0.85938
# [43/100] training 1.4% loss=0.19757, acc=0.93750
# [43/100] training 1.6% loss=0.11003, acc=0.96875
# [43/100] training 1.8% loss=0.17083, acc=0.95312
# [43/100] training 2.0% loss=0.23216, acc=0.92188
# [43/100] training 2.1% loss=0.16094, acc=0.92188
# [43/100] training 2.3% loss=0.13569, acc=0.95312
# [43/100] training 2.4% loss=0.11949, acc=0.93750
# [43/100] training 2.6% loss=0.20183, acc=0.90625
# [43/100] training 2.7% loss=0.23361, acc=0.90625
# [43/100] training 3.0% loss=0.22035, acc=0.92188
# [43/100] training 3.2% loss=0.13083, acc=0.95312
# [43/100] training 3.3% loss=0.34076, acc=0.92188
# [43/100] training 3.5% loss=0.16880, acc=0.89062
# [43/100] training 3.6% loss=0.17020, acc=0.90625
# [43/100] training 3.8% loss=0.20256, acc=0.95312
# [43/100] training 3.9% loss=0.12683, acc=0.93750
# [43/100] training 4.2% loss=0.10203, acc=0.96875
# [43/100] training 4.4% loss=0.12883, acc=0.95312
# [43/100] training 4.5% loss=0.12607, acc=0.96875
# [43/100] training 4.7% loss=0.33341, acc=0.84375
# [43/100] training 4.8% loss=0.16488, acc=0.92188
# [43/100] training 5.0% loss=0.11324, acc=0.92188
# [43/100] training 5.2% loss=0.21416, acc=0.95312
# [43/100] training 5.4% loss=0.09505, acc=0.98438
# [43/100] training 5.5% loss=0.17112, acc=0.93750
# [43/100] training 5.7% loss=0.25581, acc=0.90625
# [43/100] training 5.9% loss=0.16423, acc=0.92188
# [43/100] training 6.0% loss=0.22548, acc=0.89062
# [43/100] training 6.3% loss=0.21810, acc=0.92188
# [43/100] training 6.4% loss=0.14906, acc=0.93750
# [43/100] training 6.6% loss=0.12519, acc=0.98438
# [43/100] training 6.7% loss=0.32303, acc=0.89062
# [43/100] training 6.9% loss=0.17911, acc=0.93750
# [43/100] training 7.1% loss=0.16787, acc=0.89062
# [43/100] training 7.2% loss=0.28709, acc=0.89062
# [43/100] training 7.5% loss=0.12773, acc=0.95312
# [43/100] training 7.6% loss=0.22029, acc=0.92188
# [43/100] training 7.8% loss=0.35159, acc=0.85938
# [43/100] training 7.9% loss=0.18603, acc=0.89062
# [43/100] training 8.1% loss=0.12350, acc=0.96875
# [43/100] training 8.2% loss=0.21975, acc=0.93750
# [43/100] training 8.4% loss=0.21464, acc=0.92188
# [43/100] training 8.7% loss=0.20236, acc=0.89062
# [43/100] training 8.8% loss=0.15377, acc=0.95312
# [43/100] training 9.0% loss=0.18360, acc=0.87500
# [43/100] training 9.1% loss=0.17895, acc=0.92188
# [43/100] training 9.3% loss=0.18944, acc=0.95312
# [43/100] training 9.4% loss=0.15082, acc=0.93750
# [43/100] training 9.7% loss=0.15676, acc=0.95312
# [43/100] training 9.9% loss=0.24845, acc=0.89062
# [43/100] training 10.0% loss=0.20461, acc=0.87500
# [43/100] training 10.2% loss=0.21182, acc=0.92188
# [43/100] training 10.3% loss=0.12035, acc=0.95312
# [43/100] training 10.5% loss=0.34111, acc=0.87500
# [43/100] training 10.6% loss=0.16182, acc=0.90625
# [43/100] training 10.9% loss=0.11521, acc=0.95312
# [43/100] training 11.0% loss=0.22594, acc=0.90625
# [43/100] training 11.2% loss=0.08723, acc=0.98438
# [43/100] training 11.4% loss=0.19044, acc=0.89062
# [43/100] training 11.5% loss=0.38398, acc=0.85938
# [43/100] training 11.7% loss=0.08302, acc=0.98438
# [43/100] training 11.8% loss=0.15542, acc=0.92188
# [43/100] training 12.1% loss=0.16075, acc=0.93750
# [43/100] training 12.2% loss=0.16438, acc=0.93750
# [43/100] training 12.4% loss=0.31019, acc=0.87500
# [43/100] training 12.6% loss=0.21672, acc=0.93750
# [43/100] training 12.7% loss=0.21613, acc=0.93750
# [43/100] training 12.9% loss=0.32098, acc=0.84375
# [43/100] training 13.0% loss=0.14070, acc=0.93750
# [43/100] training 13.3% loss=0.17614, acc=0.93750
# [43/100] training 13.4% loss=0.30577, acc=0.81250
# [43/100] training 13.6% loss=0.19369, acc=0.87500
# [43/100] training 13.7% loss=0.32070, acc=0.90625
# [43/100] training 13.9% loss=0.22670, acc=0.95312
# [43/100] training 14.1% loss=0.22342, acc=0.93750
# [43/100] training 14.3% loss=0.14913, acc=0.93750
# [43/100] training 14.5% loss=0.13787, acc=0.96875
# [43/100] training 14.6% loss=0.13785, acc=0.95312
# [43/100] training 14.8% loss=0.30444, acc=0.89062
# [43/100] training 14.9% loss=0.11322, acc=0.96875
# [43/100] training 15.1% loss=0.25543, acc=0.89062
# [43/100] training 15.4% loss=0.21451, acc=0.90625
# [43/100] training 15.5% loss=0.16334, acc=0.92188
# [43/100] training 15.7% loss=0.31999, acc=0.89062
# [43/100] training 15.8% loss=0.14510, acc=0.93750
# [43/100] training 16.0% loss=0.19457, acc=0.92188
# [43/100] training 16.1% loss=0.45707, acc=0.84375
# [43/100] training 16.3% loss=0.28387, acc=0.85938
# [43/100] training 16.4% loss=0.17807, acc=0.92188
# [43/100] training 16.7% loss=0.28454, acc=0.90625
# [43/100] training 16.9% loss=0.22083, acc=0.90625
# [43/100] training 17.0% loss=0.15869, acc=0.93750
# [43/100] training 17.2% loss=0.16225, acc=0.95312
# [43/100] training 17.3% loss=0.21848, acc=0.90625
# [43/100] training 17.5% loss=0.25252, acc=0.87500
# [43/100] training 17.7% loss=0.22574, acc=0.90625
# [43/100] training 17.9% loss=0.21604, acc=0.93750
# [43/100] training 18.1% loss=0.28287, acc=0.89062
# [43/100] training 18.2% loss=0.25141, acc=0.90625
# [43/100] training 18.4% loss=0.25082, acc=0.90625
# [43/100] training 18.5% loss=0.18288, acc=0.93750
# [43/100] training 18.8% loss=0.14781, acc=0.93750
# [43/100] training 18.9% loss=0.06868, acc=1.00000
# [43/100] training 19.1% loss=0.22193, acc=0.89062
# [43/100] training 19.2% loss=0.17575, acc=0.93750
# [43/100] training 19.4% loss=0.25036, acc=0.90625
# [43/100] training 19.6% loss=0.19589, acc=0.92188
# [43/100] training 19.7% loss=0.15525, acc=0.92188
# [43/100] training 20.0% loss=0.10877, acc=0.93750
# [43/100] training 20.1% loss=0.15094, acc=0.93750
# [43/100] training 20.3% loss=0.25393, acc=0.90625
# [43/100] training 20.4% loss=0.16901, acc=0.95312
# [43/100] training 20.6% loss=0.18784, acc=0.95312
# [43/100] training 20.8% loss=0.13688, acc=0.93750
# [43/100] training 20.9% loss=0.14448, acc=0.95312
# [43/100] training 21.2% loss=0.18025, acc=0.96875
# [43/100] training 21.3% loss=0.22634, acc=0.90625
# [43/100] training 21.5% loss=0.27136, acc=0.92188
# [43/100] training 21.6% loss=0.09005, acc=0.93750
# [43/100] training 21.8% loss=0.14443, acc=0.93750
# [43/100] training 21.9% loss=0.13910, acc=0.93750
# [43/100] training 22.2% loss=0.20282, acc=0.92188
# [43/100] training 22.4% loss=0.22430, acc=0.89062
# [43/100] training 22.5% loss=0.13311, acc=0.95312
# [43/100] training 22.7% loss=0.25123, acc=0.90625
# [43/100] training 22.8% loss=0.15637, acc=0.95312
# [43/100] training 23.0% loss=0.09025, acc=0.96875
# [43/100] training 23.1% loss=0.26682, acc=0.92188
# [43/100] training 23.4% loss=0.29431, acc=0.85938
# [43/100] training 23.6% loss=0.21441, acc=0.92188
# [43/100] training 23.7% loss=0.16931, acc=0.93750
# [43/100] training 23.9% loss=0.20853, acc=0.89062
# [43/100] training 24.0% loss=0.10377, acc=0.95312
# [43/100] training 24.2% loss=0.11803, acc=0.95312
# [43/100] training 24.3% loss=0.21211, acc=0.92188
# [43/100] training 24.6% loss=0.18657, acc=0.90625
# [43/100] training 24.7% loss=0.27457, acc=0.90625
# [43/100] training 24.9% loss=0.20935, acc=0.90625
# [43/100] training 25.1% loss=0.22544, acc=0.90625
# [43/100] training 25.2% loss=0.14204, acc=0.92188
# [43/100] training 25.4% loss=0.18799, acc=0.92188
# [43/100] training 25.6% loss=0.14604, acc=0.92188
# [43/100] training 25.8% loss=0.24157, acc=0.85938
# [43/100] training 25.9% loss=0.19429, acc=0.93750
# [43/100] training 26.1% loss=0.16562, acc=0.93750
# [43/100] training 26.3% loss=0.12179, acc=0.96875
# [43/100] training 26.4% loss=0.15473, acc=0.92188
# [43/100] training 26.6% loss=0.06892, acc=0.96875
# [43/100] training 26.8% loss=0.13161, acc=0.95312
# [43/100] training 27.0% loss=0.14840, acc=0.93750
# [43/100] training 27.1% loss=0.12689, acc=0.93750
# [43/100] training 27.3% loss=0.11164, acc=0.92188
# [43/100] training 27.4% loss=0.19991, acc=0.95312
# [43/100] training 27.6% loss=0.32570, acc=0.87500
# [43/100] training 27.9% loss=0.13049, acc=0.90625
# [43/100] training 28.0% loss=0.26015, acc=0.89062
# [43/100] training 28.2% loss=0.18083, acc=0.93750
# [43/100] training 28.3% loss=0.07687, acc=0.98438
# [43/100] training 28.5% loss=0.22725, acc=0.90625
# [43/100] training 28.6% loss=0.15930, acc=0.95312
# [43/100] training 28.8% loss=0.09376, acc=0.96875
# [43/100] training 29.1% loss=0.13185, acc=0.95312
# [43/100] training 29.2% loss=0.15699, acc=0.95312
# [43/100] training 29.4% loss=0.24866, acc=0.89062
# [43/100] training 29.5% loss=0.09383, acc=0.93750
# [43/100] training 29.7% loss=0.20119, acc=0.90625
# [43/100] training 29.8% loss=0.12894, acc=0.95312
# [43/100] training 30.0% loss=0.14236, acc=0.90625
# [43/100] training 30.2% loss=0.13212, acc=0.92188
# [43/100] training 30.4% loss=0.09552, acc=0.95312
# [43/100] training 30.6% loss=0.18553, acc=0.92188
# [43/100] training 30.7% loss=0.17217, acc=0.95312
# [43/100] training 30.9% loss=0.38459, acc=0.90625
# [43/100] training 31.0% loss=0.06723, acc=1.00000
# [43/100] training 31.3% loss=0.08703, acc=0.96875
# [43/100] training 31.4% loss=0.24653, acc=0.87500
# [43/100] training 31.6% loss=0.20005, acc=0.90625
# [43/100] training 31.8% loss=0.23475, acc=0.93750
# [43/100] training 31.9% loss=0.20139, acc=0.90625
# [43/100] training 32.1% loss=0.14489, acc=0.92188
# [43/100] training 32.2% loss=0.25716, acc=0.93750
# [43/100] training 32.5% loss=0.10535, acc=0.95312
# [43/100] training 32.6% loss=0.37743, acc=0.87500
# [43/100] training 32.8% loss=0.12054, acc=0.96875
# [43/100] training 32.9% loss=0.23667, acc=0.89062
# [43/100] training 33.1% loss=0.17588, acc=0.92188
# [43/100] training 33.3% loss=0.21582, acc=0.92188
# [43/100] training 33.4% loss=0.17265, acc=0.93750
# [43/100] training 33.7% loss=0.19935, acc=0.92188
# [43/100] training 33.8% loss=0.20172, acc=0.92188
# [43/100] training 34.0% loss=0.19922, acc=0.93750
# [43/100] training 34.1% loss=0.25890, acc=0.90625
# [43/100] training 34.3% loss=0.15145, acc=0.92188
# [43/100] training 34.5% loss=0.22705, acc=0.92188
# [43/100] training 34.7% loss=0.11469, acc=0.95312
# [43/100] training 34.9% loss=0.09063, acc=0.96875
# [43/100] training 35.0% loss=0.16768, acc=0.95312
# [43/100] training 35.2% loss=0.15684, acc=0.93750
# [43/100] training 35.3% loss=0.23090, acc=0.85938
# [43/100] training 35.5% loss=0.15098, acc=0.93750
# [43/100] training 35.6% loss=0.30547, acc=0.85938
# [43/100] training 35.9% loss=0.19189, acc=0.93750
# [43/100] training 36.1% loss=0.25201, acc=0.89062
# [43/100] training 36.2% loss=0.21511, acc=0.90625
# [43/100] training 36.4% loss=0.24230, acc=0.93750
# [43/100] training 36.5% loss=0.17682, acc=0.93750
# [43/100] training 36.7% loss=0.23902, acc=0.89062
# [43/100] training 36.8% loss=0.09098, acc=0.96875
# [43/100] training 37.1% loss=0.24764, acc=0.89062
# [43/100] training 37.3% loss=0.17465, acc=0.95312
# [43/100] training 37.4% loss=0.11715, acc=0.96875
# [43/100] training 37.6% loss=0.20950, acc=0.89062
# [43/100] training 37.7% loss=0.19702, acc=0.92188
# [43/100] training 37.9% loss=0.21983, acc=0.92188
# [43/100] training 38.1% loss=0.35349, acc=0.87500
# [43/100] training 38.3% loss=0.19581, acc=0.92188
# [43/100] training 38.4% loss=0.07819, acc=0.98438
# [43/100] training 38.6% loss=0.13098, acc=0.95312
# [43/100] training 38.8% loss=0.34520, acc=0.81250
# [43/100] training 38.9% loss=0.19284, acc=0.90625
# [43/100] training 39.1% loss=0.17254, acc=0.92188
# [43/100] training 39.3% loss=0.22861, acc=0.92188
# [43/100] training 39.5% loss=0.22951, acc=0.90625
# [43/100] training 39.6% loss=0.19815, acc=0.95312
# [43/100] training 39.8% loss=0.10889, acc=0.98438
# [43/100] training 40.0% loss=0.20531, acc=0.90625
# [43/100] training 40.1% loss=0.23231, acc=0.93750
# [43/100] training 40.4% loss=0.16271, acc=0.92188
# [43/100] training 40.5% loss=0.26423, acc=0.89062
# [43/100] training 40.7% loss=0.19362, acc=0.92188
# [43/100] training 40.8% loss=0.13817, acc=0.93750
# [43/100] training 41.0% loss=0.09325, acc=0.96875
# [43/100] training 41.1% loss=0.26087, acc=0.90625
# [43/100] training 41.3% loss=0.22780, acc=0.89062
# [43/100] training 41.6% loss=0.22488, acc=0.90625
# [43/100] training 41.7% loss=0.30462, acc=0.92188
# [43/100] training 41.9% loss=0.17142, acc=0.90625
# [43/100] training 42.0% loss=0.33911, acc=0.82812
# [43/100] training 42.2% loss=0.22395, acc=0.90625
# [43/100] training 42.3% loss=0.17098, acc=0.95312
# [43/100] training 42.5% loss=0.23977, acc=0.90625
# [43/100] training 42.8% loss=0.09849, acc=1.00000
# [43/100] training 42.9% loss=0.16994, acc=0.89062
# [43/100] training 43.1% loss=0.24034, acc=0.92188
# [43/100] training 43.2% loss=0.17040, acc=0.92188
# [43/100] training 43.4% loss=0.22636, acc=0.92188
# [43/100] training 43.5% loss=0.19718, acc=0.90625
# [43/100] training 43.8% loss=0.20738, acc=0.90625
# [43/100] training 43.9% loss=0.12975, acc=0.96875
# [43/100] training 44.1% loss=0.18210, acc=0.98438
# [43/100] training 44.3% loss=0.15837, acc=0.93750
# [43/100] training 44.4% loss=0.29019, acc=0.92188
# [43/100] training 44.6% loss=0.17749, acc=0.93750
# [43/100] training 44.7% loss=0.26515, acc=0.89062
# [43/100] training 45.0% loss=0.09045, acc=0.96875
# [43/100] training 45.1% loss=0.34150, acc=0.89062
# [43/100] training 45.3% loss=0.22220, acc=0.90625
# [43/100] training 45.5% loss=0.08049, acc=0.95312
# [43/100] training 45.6% loss=0.22068, acc=0.93750
# [43/100] training 45.8% loss=0.06930, acc=0.98438
# [43/100] training 45.9% loss=0.14357, acc=0.92188
# [43/100] training 46.2% loss=0.14317, acc=0.93750
# [43/100] training 46.3% loss=0.08459, acc=0.96875
# [43/100] training 46.5% loss=0.27702, acc=0.87500
# [43/100] training 46.6% loss=0.20078, acc=0.93750
# [43/100] training 46.8% loss=0.15522, acc=0.93750
# [43/100] training 47.0% loss=0.17901, acc=0.90625
# [43/100] training 47.2% loss=0.31887, acc=0.89062
# [43/100] training 47.4% loss=0.21119, acc=0.93750
# [43/100] training 47.5% loss=0.25964, acc=0.92188
# [43/100] training 47.7% loss=0.13880, acc=0.93750
# [43/100] training 47.8% loss=0.29472, acc=0.92188
# [43/100] training 48.0% loss=0.15924, acc=0.93750
# [43/100] training 48.3% loss=0.08709, acc=0.98438
# [43/100] training 48.4% loss=0.06742, acc=1.00000
# [43/100] training 48.6% loss=0.13663, acc=0.92188
# [43/100] training 48.7% loss=0.23222, acc=0.89062
# [43/100] training 48.9% loss=0.09653, acc=0.93750
# [43/100] training 49.0% loss=0.10890, acc=0.96875
# [43/100] training 49.2% loss=0.19140, acc=0.87500
# [43/100] training 49.3% loss=0.18119, acc=0.93750
# [43/100] training 49.6% loss=0.16887, acc=0.93750
# [43/100] training 49.8% loss=0.14450, acc=0.93750
# [43/100] training 49.9% loss=0.20867, acc=0.90625
# [43/100] training 50.1% loss=0.11477, acc=0.95312
# [43/100] training 50.2% loss=0.29371, acc=0.87500
# [43/100] training 50.4% loss=0.46677, acc=0.87500
# [43/100] training 50.6% loss=0.14912, acc=0.92188
# [43/100] training 50.8% loss=0.28470, acc=0.87500
# [43/100] training 51.0% loss=0.26399, acc=0.92188
# [43/100] training 51.1% loss=0.20891, acc=0.93750
# [43/100] training 51.3% loss=0.26248, acc=0.89062
# [43/100] training 51.4% loss=0.22209, acc=0.93750
# [43/100] training 51.7% loss=0.18309, acc=0.89062
# [43/100] training 51.8% loss=0.24669, acc=0.90625
# [43/100] training 52.0% loss=0.23792, acc=0.85938
# [43/100] training 52.1% loss=0.18807, acc=0.92188
# [43/100] training 52.3% loss=0.28482, acc=0.89062
# [43/100] training 52.5% loss=0.10079, acc=0.96875
# [43/100] training 52.6% loss=0.15980, acc=0.93750
# [43/100] training 52.9% loss=0.23488, acc=0.87500
# [43/100] training 53.0% loss=0.19876, acc=0.95312
# [43/100] training 53.2% loss=0.19705, acc=0.92188
# [43/100] training 53.3% loss=0.17734, acc=0.95312
# [43/100] training 53.5% loss=0.21662, acc=0.90625
# [43/100] training 53.7% loss=0.06695, acc=0.96875
# [43/100] training 53.8% loss=0.22088, acc=0.90625
# [43/100] training 54.1% loss=0.27823, acc=0.87500
# [43/100] training 54.2% loss=0.18858, acc=0.90625
# [43/100] training 54.4% loss=0.17422, acc=0.95312
# [43/100] training 54.5% loss=0.23492, acc=0.89062
# [43/100] training 54.7% loss=0.26904, acc=0.90625
# [43/100] training 54.8% loss=0.13084, acc=0.98438
# [43/100] training 55.1% loss=0.18235, acc=0.93750
# [43/100] training 55.3% loss=0.15330, acc=0.95312
# [43/100] training 55.4% loss=0.13696, acc=0.95312
# [43/100] training 55.6% loss=0.16099, acc=0.93750
# [43/100] training 55.7% loss=0.08745, acc=0.96875
# [43/100] training 55.9% loss=0.09099, acc=0.96875
# [43/100] training 56.0% loss=0.15186, acc=0.93750
# [43/100] training 56.3% loss=0.29904, acc=0.89062
# [43/100] training 56.5% loss=0.11500, acc=0.96875
# [43/100] training 56.6% loss=0.27995, acc=0.87500
# [43/100] training 56.8% loss=0.19195, acc=0.90625
# [43/100] training 56.9% loss=0.17243, acc=0.95312
# [43/100] training 57.1% loss=0.22497, acc=0.93750
# [43/100] training 57.2% loss=0.11807, acc=0.95312
# [43/100] training 57.5% loss=0.14264, acc=0.95312
# [43/100] training 57.6% loss=0.15716, acc=0.93750
# [43/100] training 57.8% loss=0.15953, acc=0.93750
# [43/100] training 58.0% loss=0.15911, acc=0.92188
# [43/100] training 58.1% loss=0.17214, acc=0.90625
# [43/100] training 58.3% loss=0.15083, acc=0.95312
# [43/100] training 58.4% loss=0.18201, acc=0.93750
# [43/100] training 58.7% loss=0.25602, acc=0.90625
# [43/100] training 58.8% loss=0.27501, acc=0.90625
# [43/100] training 59.0% loss=0.17201, acc=0.89062
# [43/100] training 59.2% loss=0.20805, acc=0.90625
# [43/100] training 59.3% loss=0.19333, acc=0.87500
# [43/100] training 59.5% loss=0.22390, acc=0.93750
# [43/100] training 59.7% loss=0.26749, acc=0.89062
# [43/100] training 59.9% loss=0.13851, acc=0.96875
# [43/100] training 60.0% loss=0.17924, acc=0.93750
# [43/100] training 60.2% loss=0.25901, acc=0.89062
# [43/100] training 60.3% loss=0.22530, acc=0.92188
# [43/100] training 60.5% loss=0.22730, acc=0.87500
# [43/100] training 60.8% loss=0.20722, acc=0.93750
# [43/100] training 60.9% loss=0.19264, acc=0.90625
# [43/100] training 61.1% loss=0.36009, acc=0.90625
# [43/100] training 61.2% loss=0.13982, acc=0.95312
# [43/100] training 61.4% loss=0.22979, acc=0.95312
# [43/100] training 61.5% loss=0.35098, acc=0.89062
# [43/100] training 61.7% loss=0.20427, acc=0.90625
# [43/100] training 62.0% loss=0.20116, acc=0.89062
# [43/100] training 62.1% loss=0.28527, acc=0.87500
# [43/100] training 62.3% loss=0.13776, acc=0.93750
# [43/100] training 62.4% loss=0.11789, acc=0.96875
# [43/100] training 62.6% loss=0.17038, acc=0.92188
# [43/100] training 62.7% loss=0.12720, acc=0.95312
# [43/100] training 62.9% loss=0.17006, acc=0.93750
# [43/100] training 63.1% loss=0.35819, acc=0.90625
# [43/100] training 63.3% loss=0.22742, acc=0.87500
# [43/100] training 63.5% loss=0.20341, acc=0.90625
# [43/100] training 63.6% loss=0.26418, acc=0.92188
# [43/100] training 63.8% loss=0.18762, acc=0.90625
# [43/100] training 63.9% loss=0.20462, acc=0.90625
# [43/100] training 64.2% loss=0.24566, acc=0.89062
# [43/100] training 64.3% loss=0.29643, acc=0.89062
# [43/100] training 64.5% loss=0.19382, acc=0.92188
# [43/100] training 64.7% loss=0.18844, acc=0.93750
# [43/100] training 64.8% loss=0.35529, acc=0.84375
# [43/100] training 65.0% loss=0.21714, acc=0.92188
# [43/100] training 65.1% loss=0.23514, acc=0.89062
# [43/100] training 65.4% loss=0.23280, acc=0.92188
# [43/100] training 65.5% loss=0.14730, acc=0.92188
# [43/100] training 65.7% loss=0.11638, acc=0.93750
# [43/100] training 65.8% loss=0.18101, acc=0.89062
# [43/100] training 66.0% loss=0.11141, acc=0.95312
# [43/100] training 66.2% loss=0.14522, acc=0.93750
# [43/100] training 66.3% loss=0.19332, acc=0.90625
# [43/100] training 66.6% loss=0.15854, acc=0.93750
# [43/100] training 66.7% loss=0.12851, acc=0.95312
# [43/100] training 66.9% loss=0.19797, acc=0.92188
# [43/100] training 67.0% loss=0.11244, acc=0.98438
# [43/100] training 67.2% loss=0.13280, acc=0.95312
# [43/100] training 67.4% loss=0.08793, acc=0.96875
# [43/100] training 67.6% loss=0.09028, acc=0.96875
# [43/100] training 67.8% loss=0.15630, acc=0.92188
# [43/100] training 67.9% loss=0.11071, acc=0.93750
# [43/100] training 68.1% loss=0.26953, acc=0.87500
# [43/100] training 68.2% loss=0.13417, acc=0.92188
# [43/100] training 68.4% loss=0.25852, acc=0.92188
# [43/100] training 68.5% loss=0.25727, acc=0.92188
# [43/100] training 68.8% loss=0.10434, acc=0.95312
# [43/100] training 69.0% loss=0.22008, acc=0.92188
# [43/100] training 69.1% loss=0.14514, acc=0.95312
# [43/100] training 69.3% loss=0.31712, acc=0.84375
# [43/100] training 69.4% loss=0.16944, acc=0.95312
# [43/100] training 69.6% loss=0.19735, acc=0.90625
# [43/100] training 69.7% loss=0.12400, acc=0.96875
# [43/100] training 70.0% loss=0.25871, acc=0.89062
# [43/100] training 70.2% loss=0.21277, acc=0.90625
# [43/100] training 70.3% loss=0.17117, acc=0.96875
# [43/100] training 70.5% loss=0.24948, acc=0.87500
# [43/100] training 70.6% loss=0.16104, acc=0.92188
# [43/100] training 70.8% loss=0.18147, acc=0.92188
# [43/100] training 71.0% loss=0.18279, acc=0.92188
# [43/100] training 71.2% loss=0.16359, acc=0.92188
# [43/100] training 71.3% loss=0.10827, acc=0.93750
# [43/100] training 71.5% loss=0.24557, acc=0.92188
# [43/100] training 71.7% loss=0.22969, acc=0.87500
# [43/100] training 71.8% loss=0.28766, acc=0.90625
# [43/100] training 72.0% loss=0.10500, acc=0.96875
# [43/100] training 72.2% loss=0.17612, acc=0.90625
# [43/100] training 72.4% loss=0.30372, acc=0.85938
# [43/100] training 72.5% loss=0.20095, acc=0.92188
# [43/100] training 72.7% loss=0.29586, acc=0.90625
# [43/100] training 72.9% loss=0.11718, acc=0.93750
# [43/100] training 73.0% loss=0.13978, acc=0.93750
# [43/100] training 73.3% loss=0.29321, acc=0.85938
# [43/100] training 73.4% loss=0.14934, acc=0.95312
# [43/100] training 73.6% loss=0.41349, acc=0.92188
# [43/100] training 73.7% loss=0.24117, acc=0.90625
# [43/100] training 73.9% loss=0.15207, acc=0.95312
# [43/100] training 74.0% loss=0.31718, acc=0.87500
# [43/100] training 74.2% loss=0.29967, acc=0.85938
# [43/100] training 74.5% loss=0.17661, acc=0.90625
# [43/100] training 74.6% loss=0.46934, acc=0.79688
# [43/100] training 74.8% loss=0.35868, acc=0.85938
# [43/100] training 74.9% loss=0.32175, acc=0.85938
# [43/100] training 75.1% loss=0.16300, acc=0.96875
# [43/100] training 75.2% loss=0.14974, acc=0.95312
# [43/100] training 75.4% loss=0.21606, acc=0.90625
# [43/100] training 75.7% loss=0.22303, acc=0.90625
# [43/100] training 75.8% loss=0.24899, acc=0.90625
# [43/100] training 76.0% loss=0.27184, acc=0.90625
# [43/100] training 76.1% loss=0.46167, acc=0.79688
# [43/100] training 76.3% loss=0.10700, acc=0.92188
# [43/100] training 76.4% loss=0.28888, acc=0.89062
# [43/100] training 76.7% loss=0.20665, acc=0.92188
# [43/100] training 76.8% loss=0.17678, acc=0.92188
# [43/100] training 77.0% loss=0.13637, acc=0.98438
# [43/100] training 77.2% loss=0.20286, acc=0.92188
# [43/100] training 77.3% loss=0.12278, acc=0.95312
# [43/100] training 77.5% loss=0.22917, acc=0.90625
# [43/100] training 77.6% loss=0.22119, acc=0.90625
# [43/100] training 77.9% loss=0.19458, acc=0.92188
# [43/100] training 78.0% loss=0.17078, acc=0.96875
# [43/100] training 78.2% loss=0.26750, acc=0.90625
# [43/100] training 78.4% loss=0.12457, acc=0.93750
# [43/100] training 78.5% loss=0.17432, acc=0.92188
# [43/100] training 78.7% loss=0.22038, acc=0.87500
# [43/100] training 78.8% loss=0.13224, acc=0.96875
# [43/100] training 79.1% loss=0.12514, acc=0.95312
# [43/100] training 79.2% loss=0.13959, acc=0.96875
# [43/100] training 79.4% loss=0.22008, acc=0.90625
# [43/100] training 79.5% loss=0.19320, acc=0.92188
# [43/100] training 79.7% loss=0.07363, acc=0.96875
# [43/100] training 79.9% loss=0.13305, acc=0.93750
# [43/100] training 80.1% loss=0.15463, acc=0.95312
# [43/100] training 80.3% loss=0.59223, acc=0.81250
# [43/100] training 80.4% loss=0.21386, acc=0.92188
# [43/100] training 80.6% loss=0.20855, acc=0.90625
# [43/100] training 80.7% loss=0.16983, acc=0.93750
# [43/100] training 80.9% loss=0.19099, acc=0.92188
# [43/100] training 81.2% loss=0.20282, acc=0.90625
# [43/100] training 81.3% loss=0.23205, acc=0.89062
# [43/100] training 81.5% loss=0.15303, acc=0.93750
# [43/100] training 81.6% loss=0.29910, acc=0.84375
# [43/100] training 81.8% loss=0.17154, acc=0.92188
# [43/100] training 81.9% loss=0.32766, acc=0.87500
# [43/100] training 82.1% loss=0.13657, acc=0.93750
# [43/100] training 82.2% loss=0.20541, acc=0.87500
# [43/100] training 82.5% loss=0.14367, acc=0.96875
# [43/100] training 82.7% loss=0.23968, acc=0.89062
# [43/100] training 82.8% loss=0.20366, acc=0.93750
# [43/100] training 83.0% loss=0.16933, acc=0.92188
# [43/100] training 83.1% loss=0.13045, acc=0.95312
# [43/100] training 83.3% loss=0.12705, acc=0.93750
# [43/100] training 83.5% loss=0.11550, acc=0.98438
# [43/100] training 83.7% loss=0.27562, acc=0.89062
# [43/100] training 83.9% loss=0.15061, acc=0.93750
# [43/100] training 84.0% loss=0.07743, acc=0.96875
# [43/100] training 84.2% loss=0.07323, acc=0.96875
# [43/100] training 84.3% loss=0.12558, acc=0.93750
# [43/100] training 84.5% loss=0.14244, acc=0.92188
# [43/100] training 84.7% loss=0.29279, acc=0.90625
# [43/100] training 84.9% loss=0.09715, acc=0.96875
# [43/100] training 85.0% loss=0.15242, acc=0.96875
# [43/100] training 85.2% loss=0.20339, acc=0.92188
# [43/100] training 85.4% loss=0.15223, acc=0.96875
# [43/100] training 85.5% loss=0.21257, acc=0.92188
# [43/100] training 85.8% loss=0.25439, acc=0.90625
# [43/100] training 85.9% loss=0.14322, acc=0.95312
# [43/100] training 86.1% loss=0.21534, acc=0.87500
# [43/100] training 86.2% loss=0.10562, acc=0.96875
# [43/100] training 86.4% loss=0.28980, acc=0.89062
# [43/100] training 86.6% loss=0.25500, acc=0.90625
# [43/100] training 86.7% loss=0.16385, acc=0.95312
# [43/100] training 87.0% loss=0.24987, acc=0.92188
# [43/100] training 87.1% loss=0.17082, acc=0.93750
# [43/100] training 87.3% loss=0.14053, acc=0.93750
# [43/100] training 87.4% loss=0.29099, acc=0.89062
# [43/100] training 87.6% loss=0.21252, acc=0.92188
# [43/100] training 87.7% loss=0.31567, acc=0.93750
# [43/100] training 87.9% loss=0.25853, acc=0.90625
# [43/100] training 88.2% loss=0.16344, acc=0.95312
# [43/100] training 88.3% loss=0.27889, acc=0.87500
# [43/100] training 88.5% loss=0.23380, acc=0.92188
# [43/100] training 88.6% loss=0.19740, acc=0.90625
# [43/100] training 88.8% loss=0.14129, acc=0.95312
# [43/100] training 88.9% loss=0.23155, acc=0.92188
# [43/100] training 89.2% loss=0.12368, acc=0.95312
# [43/100] training 89.4% loss=0.17653, acc=0.92188
# [43/100] training 89.5% loss=0.29358, acc=0.90625
# [43/100] training 89.7% loss=0.20462, acc=0.93750
# [43/100] training 89.8% loss=0.13018, acc=0.95312
# [43/100] training 90.0% loss=0.17132, acc=0.95312
# [43/100] training 90.1% loss=0.18197, acc=0.93750
# [43/100] training 90.4% loss=0.22368, acc=0.89062
# [43/100] training 90.5% loss=0.11372, acc=0.96875
# [43/100] training 90.7% loss=0.19969, acc=0.89062
# [43/100] training 90.9% loss=0.20748, acc=0.95312
# [43/100] training 91.0% loss=0.21471, acc=0.92188
# [43/100] training 91.2% loss=0.21080, acc=0.89062
# [43/100] training 91.3% loss=0.33315, acc=0.89062
# [43/100] training 91.6% loss=0.21679, acc=0.93750
# [43/100] training 91.7% loss=0.17877, acc=0.92188
# [43/100] training 91.9% loss=0.28282, acc=0.85938
# [43/100] training 92.1% loss=0.18757, acc=0.89062
# [43/100] training 92.2% loss=0.12141, acc=0.95312
# [43/100] training 92.4% loss=0.18029, acc=0.89062
# [43/100] training 92.6% loss=0.33104, acc=0.84375
# [43/100] training 92.8% loss=0.22989, acc=0.92188
# [43/100] training 92.9% loss=0.16554, acc=0.95312
# [43/100] training 93.1% loss=0.37673, acc=0.84375
# [43/100] training 93.2% loss=0.21575, acc=0.92188
# [43/100] training 93.4% loss=0.27864, acc=0.89062
# [43/100] training 93.7% loss=0.11083, acc=0.98438
# [43/100] training 93.8% loss=0.19399, acc=0.89062
# [43/100] training 94.0% loss=0.16958, acc=0.95312
# [43/100] training 94.1% loss=0.20110, acc=0.95312
# [43/100] training 94.3% loss=0.17391, acc=0.90625
# [43/100] training 94.4% loss=0.24024, acc=0.93750
# [43/100] training 94.6% loss=0.09433, acc=0.95312
# [43/100] training 94.9% loss=0.10133, acc=0.95312
# [43/100] training 95.0% loss=0.25387, acc=0.87500
# [43/100] training 95.2% loss=0.25857, acc=0.89062
# [43/100] training 95.3% loss=0.31236, acc=0.92188
# [43/100] training 95.5% loss=0.13812, acc=0.92188
# [43/100] training 95.6% loss=0.30449, acc=0.92188
# [43/100] training 95.8% loss=0.13917, acc=0.92188
# [43/100] training 96.0% loss=0.21786, acc=0.89062
# [43/100] training 96.2% loss=0.13544, acc=0.95312
# [43/100] training 96.4% loss=0.11285, acc=0.96875
# [43/100] training 96.5% loss=0.16506, acc=0.93750
# [43/100] training 96.7% loss=0.17967, acc=0.93750
# [43/100] training 96.8% loss=0.25345, acc=0.92188
# [43/100] training 97.1% loss=0.16249, acc=0.92188
# [43/100] training 97.2% loss=0.14953, acc=0.95312
# [43/100] training 97.4% loss=0.23073, acc=0.90625
# [43/100] training 97.6% loss=0.25910, acc=0.85938
# [43/100] training 97.7% loss=0.20419, acc=0.92188
# [43/100] training 97.9% loss=0.14220, acc=0.96875
# [43/100] training 98.0% loss=0.20929, acc=0.87500
# [43/100] training 98.3% loss=0.19430, acc=0.89062
# [43/100] training 98.4% loss=0.18322, acc=0.90625
# [43/100] training 98.6% loss=0.33476, acc=0.89062
# [43/100] training 98.7% loss=0.30900, acc=0.90625
# [43/100] training 98.9% loss=0.19128, acc=0.96875
# [43/100] training 99.1% loss=0.18602, acc=0.93750
# [43/100] training 99.2% loss=0.23267, acc=0.87500
# [43/100] training 99.5% loss=0.31701, acc=0.89062
# [43/100] training 99.6% loss=0.23508, acc=0.87500
# [43/100] training 99.8% loss=0.13252, acc=0.96875
# [43/100] training 99.9% loss=0.05079, acc=0.98438
# [43/100] testing 0.9% loss=0.13249, acc=0.95312
# [43/100] testing 1.8% loss=0.36920, acc=0.85938
# [43/100] testing 2.2% loss=0.30833, acc=0.89062
# [43/100] testing 3.1% loss=0.35112, acc=0.87500
# [43/100] testing 3.5% loss=0.12733, acc=0.95312
# [43/100] testing 4.4% loss=0.21515, acc=0.89062
# [43/100] testing 4.8% loss=0.32420, acc=0.85938
# [43/100] testing 5.7% loss=0.19948, acc=0.92188
# [43/100] testing 6.6% loss=0.13482, acc=0.95312
# [43/100] testing 7.0% loss=0.10259, acc=0.93750
# [43/100] testing 7.9% loss=0.32651, acc=0.87500
# [43/100] testing 8.3% loss=0.23732, acc=0.92188
# [43/100] testing 9.2% loss=0.24005, acc=0.92188
# [43/100] testing 9.7% loss=0.10443, acc=0.95312
# [43/100] testing 10.5% loss=0.21574, acc=0.90625
# [43/100] testing 11.0% loss=0.23757, acc=0.87500
# [43/100] testing 11.8% loss=0.24095, acc=0.93750
# [43/100] testing 12.7% loss=0.36965, acc=0.89062
# [43/100] testing 13.2% loss=0.35247, acc=0.87500
# [43/100] testing 14.0% loss=0.37162, acc=0.89062
# [43/100] testing 14.5% loss=0.26557, acc=0.89062
# [43/100] testing 15.4% loss=0.24386, acc=0.92188
# [43/100] testing 15.8% loss=0.20928, acc=0.92188
# [43/100] testing 16.7% loss=0.26903, acc=0.87500
# [43/100] testing 17.5% loss=0.25958, acc=0.90625
# [43/100] testing 18.0% loss=0.18466, acc=0.87500
# [43/100] testing 18.9% loss=0.07580, acc=0.95312
# [43/100] testing 19.3% loss=0.32124, acc=0.90625
# [43/100] testing 20.2% loss=0.33988, acc=0.92188
# [43/100] testing 20.6% loss=0.28336, acc=0.89062
# [43/100] testing 21.5% loss=0.20319, acc=0.92188
# [43/100] testing 21.9% loss=0.39187, acc=0.87500
# [43/100] testing 22.8% loss=0.25415, acc=0.92188
# [43/100] testing 23.7% loss=0.45500, acc=0.85938
# [43/100] testing 24.1% loss=0.14490, acc=0.93750
# [43/100] testing 25.0% loss=0.36056, acc=0.89062
# [43/100] testing 25.4% loss=0.16210, acc=0.93750
# [43/100] testing 26.3% loss=0.28468, acc=0.89062
# [43/100] testing 26.8% loss=0.36533, acc=0.85938
# [43/100] testing 27.6% loss=0.21262, acc=0.89062
# [43/100] testing 28.5% loss=0.21322, acc=0.93750
# [43/100] testing 29.0% loss=0.16896, acc=0.93750
# [43/100] testing 29.8% loss=0.38888, acc=0.87500
# [43/100] testing 30.3% loss=0.22274, acc=0.95312
# [43/100] testing 31.1% loss=0.35434, acc=0.90625
# [43/100] testing 31.6% loss=0.24728, acc=0.93750
# [43/100] testing 32.5% loss=0.27880, acc=0.90625
# [43/100] testing 32.9% loss=0.45242, acc=0.89062
# [43/100] testing 33.8% loss=0.30561, acc=0.90625
# [43/100] testing 34.7% loss=0.36260, acc=0.90625
# [43/100] testing 35.1% loss=0.11633, acc=0.96875
# [43/100] testing 36.0% loss=0.33428, acc=0.90625
# [43/100] testing 36.4% loss=0.30562, acc=0.89062
# [43/100] testing 37.3% loss=0.31813, acc=0.90625
# [43/100] testing 37.7% loss=0.42869, acc=0.82812
# [43/100] testing 38.6% loss=0.19726, acc=0.95312
# [43/100] testing 39.5% loss=0.25666, acc=0.95312
# [43/100] testing 39.9% loss=0.32499, acc=0.89062
# [43/100] testing 40.8% loss=0.29724, acc=0.92188
# [43/100] testing 41.2% loss=0.22199, acc=0.90625
# [43/100] testing 42.1% loss=0.16372, acc=0.90625
# [43/100] testing 42.5% loss=0.15492, acc=0.90625
# [43/100] testing 43.4% loss=0.35496, acc=0.90625
# [43/100] testing 43.9% loss=0.12407, acc=0.96875
# [43/100] testing 44.7% loss=0.28250, acc=0.85938
# [43/100] testing 45.6% loss=0.21984, acc=0.89062
# [43/100] testing 46.1% loss=0.36902, acc=0.84375
# [43/100] testing 46.9% loss=0.21319, acc=0.92188
# [43/100] testing 47.4% loss=0.12943, acc=0.93750
# [43/100] testing 48.3% loss=0.41110, acc=0.87500
# [43/100] testing 48.7% loss=0.33687, acc=0.84375
# [43/100] testing 49.6% loss=0.34649, acc=0.87500
# [43/100] testing 50.4% loss=0.19598, acc=0.90625
# [43/100] testing 50.9% loss=0.29388, acc=0.90625
# [43/100] testing 51.8% loss=0.21326, acc=0.87500
# [43/100] testing 52.2% loss=0.26359, acc=0.87500
# [43/100] testing 53.1% loss=0.25031, acc=0.92188
# [43/100] testing 53.5% loss=0.22987, acc=0.93750
# [43/100] testing 54.4% loss=0.27757, acc=0.85938
# [43/100] testing 54.8% loss=0.29320, acc=0.90625
# [43/100] testing 55.7% loss=0.12610, acc=0.93750
# [43/100] testing 56.6% loss=0.27859, acc=0.92188
# [43/100] testing 57.0% loss=0.38020, acc=0.87500
# [43/100] testing 57.9% loss=0.21847, acc=0.90625
# [43/100] testing 58.3% loss=0.34191, acc=0.89062
# [43/100] testing 59.2% loss=0.25451, acc=0.90625
# [43/100] testing 59.7% loss=0.31124, acc=0.87500
# [43/100] testing 60.5% loss=0.44175, acc=0.84375
# [43/100] testing 61.4% loss=0.12186, acc=0.95312
# [43/100] testing 61.9% loss=0.19775, acc=0.92188
# [43/100] testing 62.7% loss=0.24632, acc=0.92188
# [43/100] testing 63.2% loss=0.42562, acc=0.84375
# [43/100] testing 64.0% loss=0.46471, acc=0.85938
# [43/100] testing 64.5% loss=0.15688, acc=0.92188
# [43/100] testing 65.4% loss=0.12240, acc=0.96875
# [43/100] testing 65.8% loss=0.38771, acc=0.90625
# [43/100] testing 66.7% loss=0.24582, acc=0.89062
# [43/100] testing 67.6% loss=0.39217, acc=0.89062
# [43/100] testing 68.0% loss=0.14289, acc=0.95312
# [43/100] testing 68.9% loss=0.23105, acc=0.93750
# [43/100] testing 69.3% loss=0.22426, acc=0.90625
# [43/100] testing 70.2% loss=0.38681, acc=0.84375
# [43/100] testing 70.6% loss=0.34070, acc=0.84375
# [43/100] testing 71.5% loss=0.35843, acc=0.90625
# [43/100] testing 72.4% loss=0.17876, acc=0.92188
# [43/100] testing 72.8% loss=0.19921, acc=0.93750
# [43/100] testing 73.7% loss=0.12199, acc=0.96875
# [43/100] testing 74.1% loss=0.35307, acc=0.89062
# [43/100] testing 75.0% loss=0.26748, acc=0.85938
# [43/100] testing 75.4% loss=0.47214, acc=0.81250
# [43/100] testing 76.3% loss=0.05976, acc=0.98438
# [43/100] testing 76.8% loss=0.27912, acc=0.87500
# [43/100] testing 77.6% loss=0.19884, acc=0.90625
# [43/100] testing 78.5% loss=0.32191, acc=0.85938
# [43/100] testing 79.0% loss=0.28299, acc=0.92188
# [43/100] testing 79.8% loss=0.31336, acc=0.85938
# [43/100] testing 80.3% loss=0.23591, acc=0.90625
# [43/100] testing 81.2% loss=0.56014, acc=0.81250
# [43/100] testing 81.6% loss=0.20022, acc=0.92188
# [43/100] testing 82.5% loss=0.24563, acc=0.92188
# [43/100] testing 83.3% loss=0.20078, acc=0.95312
# [43/100] testing 83.8% loss=0.09959, acc=0.96875
# [43/100] testing 84.7% loss=0.30999, acc=0.85938
# [43/100] testing 85.1% loss=0.19375, acc=0.93750
# [43/100] testing 86.0% loss=0.27714, acc=0.92188
# [43/100] testing 86.4% loss=0.32372, acc=0.90625
# [43/100] testing 87.3% loss=0.29749, acc=0.87500
# [43/100] testing 87.7% loss=0.24630, acc=0.89062
# [43/100] testing 88.6% loss=0.27950, acc=0.89062
# [43/100] testing 89.5% loss=0.57861, acc=0.79688
# [43/100] testing 89.9% loss=0.12482, acc=0.93750
# [43/100] testing 90.8% loss=0.29937, acc=0.93750
# [43/100] testing 91.2% loss=0.19389, acc=0.93750
# [43/100] testing 92.1% loss=0.38016, acc=0.90625
# [43/100] testing 92.6% loss=0.31617, acc=0.87500
# [43/100] testing 93.4% loss=0.30823, acc=0.87500
# [43/100] testing 94.3% loss=0.10107, acc=0.96875
# [43/100] testing 94.7% loss=0.17638, acc=0.93750
# [43/100] testing 95.6% loss=0.45460, acc=0.84375
# [43/100] testing 96.1% loss=0.19385, acc=0.93750
# [43/100] testing 96.9% loss=0.29587, acc=0.87500
# [43/100] testing 97.4% loss=0.06666, acc=0.98438
# [43/100] testing 98.3% loss=0.24136, acc=0.87500
# [43/100] testing 98.7% loss=0.33230, acc=0.90625
# [43/100] testing 99.6% loss=0.31239, acc=0.90625
# [44/100] training 0.2% loss=0.35576, acc=0.87500
# [44/100] training 0.4% loss=0.38814, acc=0.87500
# [44/100] training 0.5% loss=0.13774, acc=0.95312
# [44/100] training 0.8% loss=0.11176, acc=0.95312
# [44/100] training 0.9% loss=0.14994, acc=0.92188
# [44/100] training 1.1% loss=0.24070, acc=0.95312
# [44/100] training 1.2% loss=0.25631, acc=0.89062
# [44/100] training 1.4% loss=0.13624, acc=0.92188
# [44/100] training 1.6% loss=0.17776, acc=0.92188
# [44/100] training 1.8% loss=0.18055, acc=0.96875
# [44/100] training 2.0% loss=0.29164, acc=0.87500
# [44/100] training 2.1% loss=0.19599, acc=0.89062
# [44/100] training 2.3% loss=0.15194, acc=0.93750
# [44/100] training 2.4% loss=0.34711, acc=0.87500
# [44/100] training 2.6% loss=0.10107, acc=0.95312
# [44/100] training 2.7% loss=0.17292, acc=0.95312
# [44/100] training 3.0% loss=0.16505, acc=0.92188
# [44/100] training 3.2% loss=0.12399, acc=0.95312
# [44/100] training 3.3% loss=0.32633, acc=0.87500
# [44/100] training 3.5% loss=0.14305, acc=0.92188
# [44/100] training 3.6% loss=0.30266, acc=0.85938
# [44/100] training 3.8% loss=0.18709, acc=0.95312
# [44/100] training 3.9% loss=0.14435, acc=0.93750
# [44/100] training 4.2% loss=0.11246, acc=0.95312
# [44/100] training 4.4% loss=0.16497, acc=0.96875
# [44/100] training 4.5% loss=0.16534, acc=0.92188
# [44/100] training 4.7% loss=0.28588, acc=0.85938
# [44/100] training 4.8% loss=0.25535, acc=0.85938
# [44/100] training 5.0% loss=0.08578, acc=0.95312
# [44/100] training 5.2% loss=0.26394, acc=0.90625
# [44/100] training 5.4% loss=0.16638, acc=0.95312
# [44/100] training 5.5% loss=0.23236, acc=0.92188
# [44/100] training 5.7% loss=0.22519, acc=0.89062
# [44/100] training 5.9% loss=0.21157, acc=0.92188
# [44/100] training 6.0% loss=0.27217, acc=0.87500
# [44/100] training 6.3% loss=0.20145, acc=0.92188
# [44/100] training 6.4% loss=0.15921, acc=0.93750
# [44/100] training 6.6% loss=0.16170, acc=0.92188
# [44/100] training 6.7% loss=0.32566, acc=0.85938
# [44/100] training 6.9% loss=0.12395, acc=0.93750
# [44/100] training 7.1% loss=0.13901, acc=0.95312
# [44/100] training 7.2% loss=0.19543, acc=0.92188
# [44/100] training 7.5% loss=0.19909, acc=0.92188
# [44/100] training 7.6% loss=0.27892, acc=0.92188
# [44/100] training 7.8% loss=0.17189, acc=0.95312
# [44/100] training 7.9% loss=0.21600, acc=0.87500
# [44/100] training 8.1% loss=0.15157, acc=0.92188
# [44/100] training 8.2% loss=0.20430, acc=0.92188
# [44/100] training 8.4% loss=0.21365, acc=0.90625
# [44/100] training 8.7% loss=0.26527, acc=0.87500
# [44/100] training 8.8% loss=0.12665, acc=0.96875
# [44/100] training 9.0% loss=0.28799, acc=0.89062
# [44/100] training 9.1% loss=0.24451, acc=0.93750
# [44/100] training 9.3% loss=0.32686, acc=0.87500
# [44/100] training 9.4% loss=0.14709, acc=0.92188
# [44/100] training 9.7% loss=0.15708, acc=0.93750
# [44/100] training 9.9% loss=0.23088, acc=0.90625
# [44/100] training 10.0% loss=0.30983, acc=0.89062
# [44/100] training 10.2% loss=0.22519, acc=0.90625
# [44/100] training 10.3% loss=0.17035, acc=0.93750
# [44/100] training 10.5% loss=0.23036, acc=0.92188
# [44/100] training 10.6% loss=0.15693, acc=0.93750
# [44/100] training 10.9% loss=0.18342, acc=0.90625
# [44/100] training 11.0% loss=0.29859, acc=0.90625
# [44/100] training 11.2% loss=0.12603, acc=0.95312
# [44/100] training 11.4% loss=0.24684, acc=0.92188
# [44/100] training 11.5% loss=0.36884, acc=0.82812
# [44/100] training 11.7% loss=0.09159, acc=1.00000
# [44/100] training 11.8% loss=0.20619, acc=0.92188
# [44/100] training 12.1% loss=0.18290, acc=0.92188
# [44/100] training 12.2% loss=0.15884, acc=0.92188
# [44/100] training 12.4% loss=0.18589, acc=0.89062
# [44/100] training 12.6% loss=0.28215, acc=0.90625
# [44/100] training 12.7% loss=0.17104, acc=0.92188
# [44/100] training 12.9% loss=0.17914, acc=0.90625
# [44/100] training 13.0% loss=0.15485, acc=0.90625
# [44/100] training 13.3% loss=0.13664, acc=0.93750
# [44/100] training 13.4% loss=0.22780, acc=0.92188
# [44/100] training 13.6% loss=0.14853, acc=0.95312
# [44/100] training 13.7% loss=0.31941, acc=0.87500
# [44/100] training 13.9% loss=0.35571, acc=0.89062
# [44/100] training 14.1% loss=0.27193, acc=0.95312
# [44/100] training 14.3% loss=0.12481, acc=0.95312
# [44/100] training 14.5% loss=0.19387, acc=0.95312
# [44/100] training 14.6% loss=0.22920, acc=0.93750
# [44/100] training 14.8% loss=0.16344, acc=0.92188
# [44/100] training 14.9% loss=0.10048, acc=0.98438
# [44/100] training 15.1% loss=0.31605, acc=0.85938
# [44/100] training 15.4% loss=0.20684, acc=0.90625
# [44/100] training 15.5% loss=0.15974, acc=0.95312
# [44/100] training 15.7% loss=0.26424, acc=0.92188
# [44/100] training 15.8% loss=0.23710, acc=0.89062
# [44/100] training 16.0% loss=0.25979, acc=0.89062
# [44/100] training 16.1% loss=0.27103, acc=0.89062
# [44/100] training 16.3% loss=0.17169, acc=0.90625
# [44/100] training 16.4% loss=0.18971, acc=0.90625
# [44/100] training 16.7% loss=0.19435, acc=0.89062
# [44/100] training 16.9% loss=0.18695, acc=0.93750
# [44/100] training 17.0% loss=0.17079, acc=0.90625
# [44/100] training 17.2% loss=0.15692, acc=0.92188
# [44/100] training 17.3% loss=0.20701, acc=0.90625
# [44/100] training 17.5% loss=0.20482, acc=0.93750
# [44/100] training 17.7% loss=0.13205, acc=0.92188
# [44/100] training 17.9% loss=0.23220, acc=0.90625
# [44/100] training 18.1% loss=0.22380, acc=0.89062
# [44/100] training 18.2% loss=0.20070, acc=0.95312
# [44/100] training 18.4% loss=0.21839, acc=0.87500
# [44/100] training 18.5% loss=0.25655, acc=0.90625
# [44/100] training 18.8% loss=0.15716, acc=0.90625
# [44/100] training 18.9% loss=0.14624, acc=0.95312
# [44/100] training 19.1% loss=0.28881, acc=0.84375
# [44/100] training 19.2% loss=0.12778, acc=0.95312
# [44/100] training 19.4% loss=0.22931, acc=0.92188
# [44/100] training 19.6% loss=0.13063, acc=0.96875
# [44/100] training 19.7% loss=0.23792, acc=0.92188
# [44/100] training 20.0% loss=0.11661, acc=0.96875
# [44/100] training 20.1% loss=0.27713, acc=0.89062
# [44/100] training 20.3% loss=0.27539, acc=0.93750
# [44/100] training 20.4% loss=0.25816, acc=0.90625
# [44/100] training 20.6% loss=0.38372, acc=0.92188
# [44/100] training 20.8% loss=0.15709, acc=0.89062
# [44/100] training 20.9% loss=0.14318, acc=0.93750
# [44/100] training 21.2% loss=0.24327, acc=0.93750
# [44/100] training 21.3% loss=0.33278, acc=0.85938
# [44/100] training 21.5% loss=0.20952, acc=0.92188
# [44/100] training 21.6% loss=0.13427, acc=0.96875
# [44/100] training 21.8% loss=0.12238, acc=0.98438
# [44/100] training 21.9% loss=0.20914, acc=0.95312
# [44/100] training 22.2% loss=0.26556, acc=0.85938
# [44/100] training 22.4% loss=0.18715, acc=0.95312
# [44/100] training 22.5% loss=0.11736, acc=0.96875
# [44/100] training 22.7% loss=0.12593, acc=0.95312
# [44/100] training 22.8% loss=0.26569, acc=0.84375
# [44/100] training 23.0% loss=0.07243, acc=0.98438
# [44/100] training 23.1% loss=0.22917, acc=0.93750
# [44/100] training 23.4% loss=0.25958, acc=0.90625
# [44/100] training 23.6% loss=0.32380, acc=0.90625
# [44/100] training 23.7% loss=0.10610, acc=0.95312
# [44/100] training 23.9% loss=0.14914, acc=0.93750
# [44/100] training 24.0% loss=0.10647, acc=0.95312
# [44/100] training 24.2% loss=0.16995, acc=0.93750
# [44/100] training 24.3% loss=0.27441, acc=0.92188
# [44/100] training 24.6% loss=0.23018, acc=0.92188
# [44/100] training 24.7% loss=0.27477, acc=0.89062
# [44/100] training 24.9% loss=0.09929, acc=0.93750
# [44/100] training 25.1% loss=0.23151, acc=0.92188
# [44/100] training 25.2% loss=0.17904, acc=0.90625
# [44/100] training 25.4% loss=0.21177, acc=0.92188
# [44/100] training 25.6% loss=0.13441, acc=0.96875
# [44/100] training 25.8% loss=0.17733, acc=0.92188
# [44/100] training 25.9% loss=0.17871, acc=0.90625
# [44/100] training 26.1% loss=0.15079, acc=0.92188
# [44/100] training 26.3% loss=0.20394, acc=0.92188
# [44/100] training 26.4% loss=0.16531, acc=0.92188
# [44/100] training 26.6% loss=0.07883, acc=0.96875
# [44/100] training 26.8% loss=0.13623, acc=0.95312
# [44/100] training 27.0% loss=0.15866, acc=0.96875
# [44/100] training 27.1% loss=0.12522, acc=0.93750
# [44/100] training 27.3% loss=0.26451, acc=0.85938
# [44/100] training 27.4% loss=0.13187, acc=0.93750
# [44/100] training 27.6% loss=0.38028, acc=0.87500
# [44/100] training 27.9% loss=0.17601, acc=0.92188
# [44/100] training 28.0% loss=0.34140, acc=0.85938
# [44/100] training 28.2% loss=0.17199, acc=0.95312
# [44/100] training 28.3% loss=0.15505, acc=0.90625
# [44/100] training 28.5% loss=0.17627, acc=0.93750
# [44/100] training 28.6% loss=0.28477, acc=0.89062
# [44/100] training 28.8% loss=0.14579, acc=0.95312
# [44/100] training 29.1% loss=0.18542, acc=0.92188
# [44/100] training 29.2% loss=0.15516, acc=0.93750
# [44/100] training 29.4% loss=0.23317, acc=0.87500
# [44/100] training 29.5% loss=0.14285, acc=0.95312
# [44/100] training 29.7% loss=0.14343, acc=0.93750
# [44/100] training 29.8% loss=0.16135, acc=0.92188
# [44/100] training 30.0% loss=0.27043, acc=0.87500
# [44/100] training 30.2% loss=0.16862, acc=0.93750
# [44/100] training 30.4% loss=0.11119, acc=0.95312
# [44/100] training 30.6% loss=0.32552, acc=0.89062
# [44/100] training 30.7% loss=0.15955, acc=0.93750
# [44/100] training 30.9% loss=0.28261, acc=0.90625
# [44/100] training 31.0% loss=0.12665, acc=0.95312
# [44/100] training 31.3% loss=0.14876, acc=0.96875
# [44/100] training 31.4% loss=0.31238, acc=0.81250
# [44/100] training 31.6% loss=0.23158, acc=0.87500
# [44/100] training 31.8% loss=0.14966, acc=0.89062
# [44/100] training 31.9% loss=0.24620, acc=0.90625
# [44/100] training 32.1% loss=0.14906, acc=0.96875
# [44/100] training 32.2% loss=0.26057, acc=0.89062
# [44/100] training 32.5% loss=0.09001, acc=0.96875
# [44/100] training 32.6% loss=0.17354, acc=0.95312
# [44/100] training 32.8% loss=0.18038, acc=0.93750
# [44/100] training 32.9% loss=0.23195, acc=0.89062
# [44/100] training 33.1% loss=0.10058, acc=0.96875
# [44/100] training 33.3% loss=0.25231, acc=0.89062
# [44/100] training 33.4% loss=0.12353, acc=0.95312
# [44/100] training 33.7% loss=0.22783, acc=0.89062
# [44/100] training 33.8% loss=0.19443, acc=0.92188
# [44/100] training 34.0% loss=0.18592, acc=0.92188
# [44/100] training 34.1% loss=0.15689, acc=0.95312
# [44/100] training 34.3% loss=0.21044, acc=0.90625
# [44/100] training 34.5% loss=0.29489, acc=0.89062
# [44/100] training 34.7% loss=0.12439, acc=0.93750
# [44/100] training 34.9% loss=0.11577, acc=0.95312
# [44/100] training 35.0% loss=0.16048, acc=0.95312
# [44/100] training 35.2% loss=0.29541, acc=0.89062
# [44/100] training 35.3% loss=0.22108, acc=0.95312
# [44/100] training 35.5% loss=0.21116, acc=0.87500
# [44/100] training 35.6% loss=0.30197, acc=0.89062
# [44/100] training 35.9% loss=0.24157, acc=0.92188
# [44/100] training 36.1% loss=0.27772, acc=0.89062
# [44/100] training 36.2% loss=0.22695, acc=0.92188
# [44/100] training 36.4% loss=0.31945, acc=0.89062
# [44/100] training 36.5% loss=0.19382, acc=0.95312
# [44/100] training 36.7% loss=0.24754, acc=0.90625
# [44/100] training 36.8% loss=0.08236, acc=0.98438
# [44/100] training 37.1% loss=0.28100, acc=0.90625
# [44/100] training 37.3% loss=0.30445, acc=0.93750
# [44/100] training 37.4% loss=0.13222, acc=0.95312
# [44/100] training 37.6% loss=0.11215, acc=0.96875
# [44/100] training 37.7% loss=0.26284, acc=0.87500
# [44/100] training 37.9% loss=0.17334, acc=0.95312
# [44/100] training 38.1% loss=0.21749, acc=0.87500
# [44/100] training 38.3% loss=0.16290, acc=0.92188
# [44/100] training 38.4% loss=0.07770, acc=0.98438
# [44/100] training 38.6% loss=0.13324, acc=0.95312
# [44/100] training 38.8% loss=0.31066, acc=0.81250
# [44/100] training 38.9% loss=0.20707, acc=0.92188
# [44/100] training 39.1% loss=0.25039, acc=0.92188
# [44/100] training 39.3% loss=0.30575, acc=0.89062
# [44/100] training 39.5% loss=0.30373, acc=0.89062
# [44/100] training 39.6% loss=0.21285, acc=0.92188
# [44/100] training 39.8% loss=0.09372, acc=0.95312
# [44/100] training 40.0% loss=0.20372, acc=0.93750
# [44/100] training 40.1% loss=0.18972, acc=0.93750
# [44/100] training 40.4% loss=0.08569, acc=0.98438
# [44/100] training 40.5% loss=0.25818, acc=0.92188
# [44/100] training 40.7% loss=0.25471, acc=0.89062
# [44/100] training 40.8% loss=0.10660, acc=0.95312
# [44/100] training 41.0% loss=0.14294, acc=0.96875
# [44/100] training 41.1% loss=0.15825, acc=0.95312
# [44/100] training 41.3% loss=0.24233, acc=0.90625
# [44/100] training 41.6% loss=0.33245, acc=0.84375
# [44/100] training 41.7% loss=0.35842, acc=0.82812
# [44/100] training 41.9% loss=0.14328, acc=0.96875
# [44/100] training 42.0% loss=0.28290, acc=0.89062
# [44/100] training 42.2% loss=0.23112, acc=0.90625
# [44/100] training 42.3% loss=0.21981, acc=0.93750
# [44/100] training 42.5% loss=0.12134, acc=0.98438
# [44/100] training 42.8% loss=0.10605, acc=1.00000
# [44/100] training 42.9% loss=0.17797, acc=0.92188
# [44/100] training 43.1% loss=0.19522, acc=0.93750
# [44/100] training 43.2% loss=0.17961, acc=0.93750
# [44/100] training 43.4% loss=0.17172, acc=0.92188
# [44/100] training 43.5% loss=0.22431, acc=0.85938
# [44/100] training 43.8% loss=0.27013, acc=0.89062
# [44/100] training 43.9% loss=0.16595, acc=0.93750
# [44/100] training 44.1% loss=0.11143, acc=0.98438
# [44/100] training 44.3% loss=0.11567, acc=0.95312
# [44/100] training 44.4% loss=0.30043, acc=0.87500
# [44/100] training 44.6% loss=0.22558, acc=0.89062
# [44/100] training 44.7% loss=0.28088, acc=0.89062
# [44/100] training 45.0% loss=0.17393, acc=0.90625
# [44/100] training 45.1% loss=0.18600, acc=0.92188
# [44/100] training 45.3% loss=0.25447, acc=0.84375
# [44/100] training 45.5% loss=0.12041, acc=0.96875
# [44/100] training 45.6% loss=0.23794, acc=0.90625
# [44/100] training 45.8% loss=0.11214, acc=0.92188
# [44/100] training 45.9% loss=0.14298, acc=0.90625
# [44/100] training 46.2% loss=0.10030, acc=0.96875
# [44/100] training 46.3% loss=0.12989, acc=0.95312
# [44/100] training 46.5% loss=0.36766, acc=0.87500
# [44/100] training 46.6% loss=0.21243, acc=0.92188
# [44/100] training 46.8% loss=0.14787, acc=0.95312
# [44/100] training 47.0% loss=0.16497, acc=0.92188
# [44/100] training 47.2% loss=0.19615, acc=0.92188
# [44/100] training 47.4% loss=0.22222, acc=0.92188
# [44/100] training 47.5% loss=0.20224, acc=0.93750
# [44/100] training 47.7% loss=0.16930, acc=0.92188
# [44/100] training 47.8% loss=0.26114, acc=0.89062
# [44/100] training 48.0% loss=0.21407, acc=0.89062
# [44/100] training 48.3% loss=0.09577, acc=0.93750
# [44/100] training 48.4% loss=0.07162, acc=0.98438
# [44/100] training 48.6% loss=0.18050, acc=0.93750
# [44/100] training 48.7% loss=0.25898, acc=0.89062
# [44/100] training 48.9% loss=0.11257, acc=0.96875
# [44/100] training 49.0% loss=0.06164, acc=0.98438
# [44/100] training 49.2% loss=0.22589, acc=0.89062
# [44/100] training 49.3% loss=0.16268, acc=0.92188
# [44/100] training 49.6% loss=0.20093, acc=0.93750
# [44/100] training 49.8% loss=0.21735, acc=0.93750
# [44/100] training 49.9% loss=0.19678, acc=0.90625
# [44/100] training 50.1% loss=0.09260, acc=0.95312
# [44/100] training 50.2% loss=0.22492, acc=0.87500
# [44/100] training 50.4% loss=0.25859, acc=0.89062
# [44/100] training 50.6% loss=0.17634, acc=0.92188
# [44/100] training 50.8% loss=0.29675, acc=0.84375
# [44/100] training 51.0% loss=0.23603, acc=0.90625
# [44/100] training 51.1% loss=0.27637, acc=0.89062
# [44/100] training 51.3% loss=0.19045, acc=0.93750
# [44/100] training 51.4% loss=0.20319, acc=0.93750
# [44/100] training 51.7% loss=0.13019, acc=0.93750
# [44/100] training 51.8% loss=0.29978, acc=0.85938
# [44/100] training 52.0% loss=0.13822, acc=0.95312
# [44/100] training 52.1% loss=0.15819, acc=0.92188
# [44/100] training 52.3% loss=0.37264, acc=0.87500
# [44/100] training 52.5% loss=0.06721, acc=0.96875
# [44/100] training 52.6% loss=0.15484, acc=0.92188
# [44/100] training 52.9% loss=0.37580, acc=0.85938
# [44/100] training 53.0% loss=0.09742, acc=0.96875
# [44/100] training 53.2% loss=0.26679, acc=0.90625
# [44/100] training 53.3% loss=0.12722, acc=0.93750
# [44/100] training 53.5% loss=0.14188, acc=0.95312
# [44/100] training 53.7% loss=0.08876, acc=0.96875
# [44/100] training 53.8% loss=0.27638, acc=0.90625
# [44/100] training 54.1% loss=0.24685, acc=0.90625
# [44/100] training 54.2% loss=0.13140, acc=0.96875
# [44/100] training 54.4% loss=0.15337, acc=0.95312
# [44/100] training 54.5% loss=0.30706, acc=0.93750
# [44/100] training 54.7% loss=0.26920, acc=0.90625
# [44/100] training 54.8% loss=0.09437, acc=0.95312
# [44/100] training 55.1% loss=0.21233, acc=0.90625
# [44/100] training 55.3% loss=0.18282, acc=0.89062
# [44/100] training 55.4% loss=0.18747, acc=0.92188
# [44/100] training 55.6% loss=0.13612, acc=0.95312
# [44/100] training 55.7% loss=0.19371, acc=0.90625
# [44/100] training 55.9% loss=0.12256, acc=0.93750
# [44/100] training 56.0% loss=0.12819, acc=0.95312
# [44/100] training 56.3% loss=0.33809, acc=0.84375
# [44/100] training 56.5% loss=0.19941, acc=0.89062
# [44/100] training 56.6% loss=0.11514, acc=0.96875
# [44/100] training 56.8% loss=0.21746, acc=0.92188
# [44/100] training 56.9% loss=0.12901, acc=0.93750
# [44/100] training 57.1% loss=0.26577, acc=0.89062
# [44/100] training 57.2% loss=0.17270, acc=0.92188
# [44/100] training 57.5% loss=0.21412, acc=0.90625
# [44/100] training 57.6% loss=0.26566, acc=0.89062
# [44/100] training 57.8% loss=0.14328, acc=0.96875
# [44/100] training 58.0% loss=0.19473, acc=0.89062
# [44/100] training 58.1% loss=0.17508, acc=0.93750
# [44/100] training 58.3% loss=0.11597, acc=0.96875
# [44/100] training 58.4% loss=0.25240, acc=0.95312
# [44/100] training 58.7% loss=0.31200, acc=0.92188
# [44/100] training 58.8% loss=0.21667, acc=0.95312
# [44/100] training 59.0% loss=0.12160, acc=0.93750
# [44/100] training 59.2% loss=0.22151, acc=0.90625
# [44/100] training 59.3% loss=0.07787, acc=1.00000
# [44/100] training 59.5% loss=0.16259, acc=0.95312
# [44/100] training 59.7% loss=0.17621, acc=0.96875
# [44/100] training 59.9% loss=0.13121, acc=0.95312
# [44/100] training 60.0% loss=0.22446, acc=0.90625
# [44/100] training 60.2% loss=0.25362, acc=0.93750
# [44/100] training 60.3% loss=0.19765, acc=0.93750
# [44/100] training 60.5% loss=0.25336, acc=0.87500
# [44/100] training 60.8% loss=0.19834, acc=0.90625
# [44/100] training 60.9% loss=0.32567, acc=0.82812
# [44/100] training 61.1% loss=0.23879, acc=0.90625
# [44/100] training 61.2% loss=0.26867, acc=0.87500
# [44/100] training 61.4% loss=0.24191, acc=0.92188
# [44/100] training 61.5% loss=0.35249, acc=0.84375
# [44/100] training 61.7% loss=0.21392, acc=0.90625
# [44/100] training 62.0% loss=0.22006, acc=0.90625
# [44/100] training 62.1% loss=0.30420, acc=0.92188
# [44/100] training 62.3% loss=0.08453, acc=0.98438
# [44/100] training 62.4% loss=0.14834, acc=0.93750
# [44/100] training 62.6% loss=0.18075, acc=0.92188
# [44/100] training 62.7% loss=0.12717, acc=0.93750
# [44/100] training 62.9% loss=0.18315, acc=0.87500
# [44/100] training 63.1% loss=0.17958, acc=0.92188
# [44/100] training 63.3% loss=0.10758, acc=0.93750
# [44/100] training 63.5% loss=0.14082, acc=0.95312
# [44/100] training 63.6% loss=0.24358, acc=0.95312
# [44/100] training 63.8% loss=0.25023, acc=0.87500
# [44/100] training 63.9% loss=0.14703, acc=0.95312
# [44/100] training 64.2% loss=0.13082, acc=0.95312
# [44/100] training 64.3% loss=0.19952, acc=0.90625
# [44/100] training 64.5% loss=0.07944, acc=0.98438
# [44/100] training 64.7% loss=0.15898, acc=0.96875
# [44/100] training 64.8% loss=0.34801, acc=0.85938
# [44/100] training 65.0% loss=0.25181, acc=0.92188
# [44/100] training 65.1% loss=0.22006, acc=0.89062
# [44/100] training 65.4% loss=0.20455, acc=0.89062
# [44/100] training 65.5% loss=0.10185, acc=0.98438
# [44/100] training 65.7% loss=0.18225, acc=0.93750
# [44/100] training 65.8% loss=0.23464, acc=0.90625
# [44/100] training 66.0% loss=0.16444, acc=0.92188
# [44/100] training 66.2% loss=0.10149, acc=0.95312
# [44/100] training 66.3% loss=0.26053, acc=0.89062
# [44/100] training 66.6% loss=0.18648, acc=0.90625
# [44/100] training 66.7% loss=0.14917, acc=0.96875
# [44/100] training 66.9% loss=0.22971, acc=0.90625
# [44/100] training 67.0% loss=0.11104, acc=0.95312
# [44/100] training 67.2% loss=0.10557, acc=0.95312
# [44/100] training 67.4% loss=0.10212, acc=0.96875
# [44/100] training 67.6% loss=0.12797, acc=0.96875
# [44/100] training 67.8% loss=0.07036, acc=1.00000
# [44/100] training 67.9% loss=0.09052, acc=0.96875
# [44/100] training 68.1% loss=0.10831, acc=0.95312
# [44/100] training 68.2% loss=0.07841, acc=0.98438
# [44/100] training 68.4% loss=0.13430, acc=0.93750
# [44/100] training 68.5% loss=0.18290, acc=0.95312
# [44/100] training 68.8% loss=0.12810, acc=0.93750
# [44/100] training 69.0% loss=0.34178, acc=0.89062
# [44/100] training 69.1% loss=0.15772, acc=0.93750
# [44/100] training 69.3% loss=0.22468, acc=0.90625
# [44/100] training 69.4% loss=0.19114, acc=0.92188
# [44/100] training 69.6% loss=0.11602, acc=0.95312
# [44/100] training 69.7% loss=0.14567, acc=0.93750
# [44/100] training 70.0% loss=0.37026, acc=0.84375
# [44/100] training 70.2% loss=0.29274, acc=0.93750
# [44/100] training 70.3% loss=0.13320, acc=0.93750
# [44/100] training 70.5% loss=0.17567, acc=0.93750
# [44/100] training 70.6% loss=0.15073, acc=0.95312
# [44/100] training 70.8% loss=0.22636, acc=0.90625
# [44/100] training 71.0% loss=0.19364, acc=0.93750
# [44/100] training 71.2% loss=0.12682, acc=0.95312
# [44/100] training 71.3% loss=0.14416, acc=0.93750
# [44/100] training 71.5% loss=0.21407, acc=0.93750
# [44/100] training 71.7% loss=0.26709, acc=0.90625
# [44/100] training 71.8% loss=0.25722, acc=0.92188
# [44/100] training 72.0% loss=0.05939, acc=0.98438
# [44/100] training 72.2% loss=0.20119, acc=0.90625
# [44/100] training 72.4% loss=0.16649, acc=0.93750
# [44/100] training 72.5% loss=0.19810, acc=0.90625
# [44/100] training 72.7% loss=0.28347, acc=0.92188
# [44/100] training 72.9% loss=0.10922, acc=0.96875
# [44/100] training 73.0% loss=0.06025, acc=0.98438
# [44/100] training 73.3% loss=0.30053, acc=0.85938
# [44/100] training 73.4% loss=0.10724, acc=0.96875
# [44/100] training 73.6% loss=0.10298, acc=0.96875
# [44/100] training 73.7% loss=0.23924, acc=0.90625
# [44/100] training 73.9% loss=0.09967, acc=0.95312
# [44/100] training 74.0% loss=0.24823, acc=0.89062
# [44/100] training 74.2% loss=0.11438, acc=0.96875
# [44/100] training 74.5% loss=0.17546, acc=0.92188
# [44/100] training 74.6% loss=0.34751, acc=0.84375
# [44/100] training 74.8% loss=0.30253, acc=0.93750
# [44/100] training 74.9% loss=0.28115, acc=0.90625
# [44/100] training 75.1% loss=0.12046, acc=0.95312
# [44/100] training 75.2% loss=0.10993, acc=0.95312
# [44/100] training 75.4% loss=0.19546, acc=0.92188
# [44/100] training 75.7% loss=0.18689, acc=0.93750
# [44/100] training 75.8% loss=0.24606, acc=0.89062
# [44/100] training 76.0% loss=0.11039, acc=0.95312
# [44/100] training 76.1% loss=0.24621, acc=0.92188
# [44/100] training 76.3% loss=0.17185, acc=0.89062
# [44/100] training 76.4% loss=0.28938, acc=0.89062
# [44/100] training 76.7% loss=0.12723, acc=0.95312
# [44/100] training 76.8% loss=0.16673, acc=0.92188
# [44/100] training 77.0% loss=0.11356, acc=0.93750
# [44/100] training 77.2% loss=0.09182, acc=0.95312
# [44/100] training 77.3% loss=0.11311, acc=0.96875
# [44/100] training 77.5% loss=0.38117, acc=0.89062
# [44/100] training 77.6% loss=0.13487, acc=0.93750
# [44/100] training 77.9% loss=0.19265, acc=0.93750
# [44/100] training 78.0% loss=0.18922, acc=0.93750
# [44/100] training 78.2% loss=0.21219, acc=0.93750
# [44/100] training 78.4% loss=0.07907, acc=0.98438
# [44/100] training 78.5% loss=0.22193, acc=0.92188
# [44/100] training 78.7% loss=0.28163, acc=0.90625
# [44/100] training 78.8% loss=0.14444, acc=0.93750
# [44/100] training 79.1% loss=0.09552, acc=1.00000
# [44/100] training 79.2% loss=0.15553, acc=0.95312
# [44/100] training 79.4% loss=0.12931, acc=0.95312
# [44/100] training 79.5% loss=0.17822, acc=0.93750
# [44/100] training 79.7% loss=0.06024, acc=0.98438
# [44/100] training 79.9% loss=0.14872, acc=0.92188
# [44/100] training 80.1% loss=0.10865, acc=0.96875
# [44/100] training 80.3% loss=0.16785, acc=0.90625
# [44/100] training 80.4% loss=0.15884, acc=0.95312
# [44/100] training 80.6% loss=0.28134, acc=0.89062
# [44/100] training 80.7% loss=0.14179, acc=0.95312
# [44/100] training 80.9% loss=0.25437, acc=0.90625
# [44/100] training 81.2% loss=0.17081, acc=0.95312
# [44/100] training 81.3% loss=0.14419, acc=0.93750
# [44/100] training 81.5% loss=0.16854, acc=0.92188
# [44/100] training 81.6% loss=0.29492, acc=0.85938
# [44/100] training 81.8% loss=0.15357, acc=0.93750
# [44/100] training 81.9% loss=0.37512, acc=0.90625
# [44/100] training 82.1% loss=0.11654, acc=0.96875
# [44/100] training 82.2% loss=0.18831, acc=0.90625
# [44/100] training 82.5% loss=0.08072, acc=0.98438
# [44/100] training 82.7% loss=0.17379, acc=0.93750
# [44/100] training 82.8% loss=0.16135, acc=0.96875
# [44/100] training 83.0% loss=0.14650, acc=0.92188
# [44/100] training 83.1% loss=0.12831, acc=0.96875
# [44/100] training 83.3% loss=0.09397, acc=0.96875
# [44/100] training 83.5% loss=0.10496, acc=0.96875
# [44/100] training 83.7% loss=0.37352, acc=0.85938
# [44/100] training 83.9% loss=0.21613, acc=0.90625
# [44/100] training 84.0% loss=0.13991, acc=0.92188
# [44/100] training 84.2% loss=0.07474, acc=0.98438
# [44/100] training 84.3% loss=0.14059, acc=0.92188
# [44/100] training 84.5% loss=0.22689, acc=0.95312
# [44/100] training 84.7% loss=0.17494, acc=0.93750
# [44/100] training 84.9% loss=0.11591, acc=0.93750
# [44/100] training 85.0% loss=0.07331, acc=0.96875
# [44/100] training 85.2% loss=0.13154, acc=0.95312
# [44/100] training 85.4% loss=0.11548, acc=0.92188
# [44/100] training 85.5% loss=0.15911, acc=0.93750
# [44/100] training 85.8% loss=0.17006, acc=0.92188
# [44/100] training 85.9% loss=0.17733, acc=0.89062
# [44/100] training 86.1% loss=0.14111, acc=0.96875
# [44/100] training 86.2% loss=0.08727, acc=0.95312
# [44/100] training 86.4% loss=0.24482, acc=0.89062
# [44/100] training 86.6% loss=0.17301, acc=0.90625
# [44/100] training 86.7% loss=0.13072, acc=0.95312
# [44/100] training 87.0% loss=0.23473, acc=0.89062
# [44/100] training 87.1% loss=0.14338, acc=0.93750
# [44/100] training 87.3% loss=0.12059, acc=0.95312
# [44/100] training 87.4% loss=0.20772, acc=0.89062
# [44/100] training 87.6% loss=0.20016, acc=0.89062
# [44/100] training 87.7% loss=0.28842, acc=0.90625
# [44/100] training 87.9% loss=0.22539, acc=0.92188
# [44/100] training 88.2% loss=0.10986, acc=0.95312
# [44/100] training 88.3% loss=0.20473, acc=0.93750
# [44/100] training 88.5% loss=0.20230, acc=0.92188
# [44/100] training 88.6% loss=0.15314, acc=0.95312
# [44/100] training 88.8% loss=0.12139, acc=0.93750
# [44/100] training 88.9% loss=0.23782, acc=0.89062
# [44/100] training 89.2% loss=0.15365, acc=0.95312
# [44/100] training 89.4% loss=0.13280, acc=0.95312
# [44/100] training 89.5% loss=0.19599, acc=0.92188
# [44/100] training 89.7% loss=0.29351, acc=0.87500
# [44/100] training 89.8% loss=0.11212, acc=0.93750
# [44/100] training 90.0% loss=0.14431, acc=0.95312
# [44/100] training 90.1% loss=0.16005, acc=0.93750
# [44/100] training 90.4% loss=0.22453, acc=0.87500
# [44/100] training 90.5% loss=0.13436, acc=0.93750
# [44/100] training 90.7% loss=0.13027, acc=0.96875
# [44/100] training 90.9% loss=0.16338, acc=0.95312
# [44/100] training 91.0% loss=0.20680, acc=0.93750
# [44/100] training 91.2% loss=0.28043, acc=0.92188
# [44/100] training 91.3% loss=0.23556, acc=0.90625
# [44/100] training 91.6% loss=0.23801, acc=0.92188
# [44/100] training 91.7% loss=0.23130, acc=0.87500
# [44/100] training 91.9% loss=0.20601, acc=0.90625
# [44/100] training 92.1% loss=0.13315, acc=0.93750
# [44/100] training 92.2% loss=0.13470, acc=0.95312
# [44/100] training 92.4% loss=0.16811, acc=0.93750
# [44/100] training 92.6% loss=0.27233, acc=0.89062
# [44/100] training 92.8% loss=0.17599, acc=0.93750
# [44/100] training 92.9% loss=0.22151, acc=0.89062
# [44/100] training 93.1% loss=0.36778, acc=0.85938
# [44/100] training 93.2% loss=0.25001, acc=0.93750
# [44/100] training 93.4% loss=0.24082, acc=0.93750
# [44/100] training 93.7% loss=0.17276, acc=0.92188
# [44/100] training 93.8% loss=0.15873, acc=0.93750
# [44/100] training 94.0% loss=0.15373, acc=0.95312
# [44/100] training 94.1% loss=0.20894, acc=0.92188
# [44/100] training 94.3% loss=0.10531, acc=0.96875
# [44/100] training 94.4% loss=0.19762, acc=0.92188
# [44/100] training 94.6% loss=0.08809, acc=0.96875
# [44/100] training 94.9% loss=0.18691, acc=0.90625
# [44/100] training 95.0% loss=0.38323, acc=0.85938
# [44/100] training 95.2% loss=0.51912, acc=0.84375
# [44/100] training 95.3% loss=0.21088, acc=0.93750
# [44/100] training 95.5% loss=0.16823, acc=0.90625
# [44/100] training 95.6% loss=0.24358, acc=0.92188
# [44/100] training 95.8% loss=0.19535, acc=0.92188
# [44/100] training 96.0% loss=0.24489, acc=0.90625
# [44/100] training 96.2% loss=0.14697, acc=0.93750
# [44/100] training 96.4% loss=0.13922, acc=0.96875
# [44/100] training 96.5% loss=0.25815, acc=0.89062
# [44/100] training 96.7% loss=0.10231, acc=0.96875
# [44/100] training 96.8% loss=0.13886, acc=0.95312
# [44/100] training 97.1% loss=0.17393, acc=0.93750
# [44/100] training 97.2% loss=0.11818, acc=0.96875
# [44/100] training 97.4% loss=0.21474, acc=0.95312
# [44/100] training 97.6% loss=0.30963, acc=0.89062
# [44/100] training 97.7% loss=0.15677, acc=0.93750
# [44/100] training 97.9% loss=0.13331, acc=0.92188
# [44/100] training 98.0% loss=0.16284, acc=0.93750
# [44/100] training 98.3% loss=0.26138, acc=0.89062
# [44/100] training 98.4% loss=0.19816, acc=0.90625
# [44/100] training 98.6% loss=0.35321, acc=0.84375
# [44/100] training 98.7% loss=0.25143, acc=0.92188
# [44/100] training 98.9% loss=0.16569, acc=0.93750
# [44/100] training 99.1% loss=0.14773, acc=0.95312
# [44/100] training 99.2% loss=0.16631, acc=0.93750
# [44/100] training 99.5% loss=0.31293, acc=0.84375
# [44/100] training 99.6% loss=0.22269, acc=0.89062
# [44/100] training 99.8% loss=0.09802, acc=0.95312
# [44/100] training 99.9% loss=0.11485, acc=0.93750
# [44/100] testing 0.9% loss=0.17552, acc=0.87500
# [44/100] testing 1.8% loss=0.30105, acc=0.87500
# [44/100] testing 2.2% loss=0.23100, acc=0.89062
# [44/100] testing 3.1% loss=0.43947, acc=0.87500
# [44/100] testing 3.5% loss=0.18778, acc=0.90625
# [44/100] testing 4.4% loss=0.15683, acc=0.93750
# [44/100] testing 4.8% loss=0.32401, acc=0.85938
# [44/100] testing 5.7% loss=0.16418, acc=0.93750
# [44/100] testing 6.6% loss=0.11733, acc=0.95312
# [44/100] testing 7.0% loss=0.13607, acc=0.92188
# [44/100] testing 7.9% loss=0.23732, acc=0.90625
# [44/100] testing 8.3% loss=0.28284, acc=0.92188
# [44/100] testing 9.2% loss=0.24662, acc=0.90625
# [44/100] testing 9.7% loss=0.13379, acc=0.96875
# [44/100] testing 10.5% loss=0.26289, acc=0.92188
# [44/100] testing 11.0% loss=0.24010, acc=0.85938
# [44/100] testing 11.8% loss=0.29544, acc=0.87500
# [44/100] testing 12.7% loss=0.29765, acc=0.90625
# [44/100] testing 13.2% loss=0.21785, acc=0.87500
# [44/100] testing 14.0% loss=0.31764, acc=0.92188
# [44/100] testing 14.5% loss=0.22664, acc=0.89062
# [44/100] testing 15.4% loss=0.23780, acc=0.89062
# [44/100] testing 15.8% loss=0.24083, acc=0.89062
# [44/100] testing 16.7% loss=0.20956, acc=0.87500
# [44/100] testing 17.5% loss=0.22579, acc=0.89062
# [44/100] testing 18.0% loss=0.26601, acc=0.89062
# [44/100] testing 18.9% loss=0.13464, acc=0.95312
# [44/100] testing 19.3% loss=0.34929, acc=0.85938
# [44/100] testing 20.2% loss=0.29969, acc=0.90625
# [44/100] testing 20.6% loss=0.25476, acc=0.89062
# [44/100] testing 21.5% loss=0.18711, acc=0.92188
# [44/100] testing 21.9% loss=0.37094, acc=0.85938
# [44/100] testing 22.8% loss=0.30724, acc=0.85938
# [44/100] testing 23.7% loss=0.44025, acc=0.87500
# [44/100] testing 24.1% loss=0.14649, acc=0.90625
# [44/100] testing 25.0% loss=0.30738, acc=0.90625
# [44/100] testing 25.4% loss=0.18457, acc=0.90625
# [44/100] testing 26.3% loss=0.27668, acc=0.84375
# [44/100] testing 26.8% loss=0.35287, acc=0.87500
# [44/100] testing 27.6% loss=0.16987, acc=0.89062
# [44/100] testing 28.5% loss=0.18140, acc=0.95312
# [44/100] testing 29.0% loss=0.22312, acc=0.95312
# [44/100] testing 29.8% loss=0.43438, acc=0.85938
# [44/100] testing 30.3% loss=0.20791, acc=0.93750
# [44/100] testing 31.1% loss=0.29026, acc=0.85938
# [44/100] testing 31.6% loss=0.21341, acc=0.93750
# [44/100] testing 32.5% loss=0.29315, acc=0.89062
# [44/100] testing 32.9% loss=0.37404, acc=0.90625
# [44/100] testing 33.8% loss=0.32518, acc=0.84375
# [44/100] testing 34.7% loss=0.34200, acc=0.85938
# [44/100] testing 35.1% loss=0.10592, acc=0.95312
# [44/100] testing 36.0% loss=0.25810, acc=0.89062
# [44/100] testing 36.4% loss=0.16765, acc=0.92188
# [44/100] testing 37.3% loss=0.27802, acc=0.89062
# [44/100] testing 37.7% loss=0.42745, acc=0.84375
# [44/100] testing 38.6% loss=0.25070, acc=0.92188
# [44/100] testing 39.5% loss=0.30990, acc=0.87500
# [44/100] testing 39.9% loss=0.25254, acc=0.90625
# [44/100] testing 40.8% loss=0.25897, acc=0.95312
# [44/100] testing 41.2% loss=0.24834, acc=0.93750
# [44/100] testing 42.1% loss=0.23611, acc=0.95312
# [44/100] testing 42.5% loss=0.14374, acc=0.93750
# [44/100] testing 43.4% loss=0.40939, acc=0.89062
# [44/100] testing 43.9% loss=0.08329, acc=0.98438
# [44/100] testing 44.7% loss=0.31571, acc=0.87500
# [44/100] testing 45.6% loss=0.25819, acc=0.87500
# [44/100] testing 46.1% loss=0.29358, acc=0.87500
# [44/100] testing 46.9% loss=0.22899, acc=0.89062
# [44/100] testing 47.4% loss=0.14170, acc=0.93750
# [44/100] testing 48.3% loss=0.35171, acc=0.89062
# [44/100] testing 48.7% loss=0.32461, acc=0.87500
# [44/100] testing 49.6% loss=0.42666, acc=0.85938
# [44/100] testing 50.4% loss=0.19266, acc=0.90625
# [44/100] testing 50.9% loss=0.31759, acc=0.89062
# [44/100] testing 51.8% loss=0.18245, acc=0.93750
# [44/100] testing 52.2% loss=0.28106, acc=0.89062
# [44/100] testing 53.1% loss=0.29132, acc=0.85938
# [44/100] testing 53.5% loss=0.20439, acc=0.90625
# [44/100] testing 54.4% loss=0.43955, acc=0.79688
# [44/100] testing 54.8% loss=0.23758, acc=0.84375
# [44/100] testing 55.7% loss=0.14407, acc=0.96875
# [44/100] testing 56.6% loss=0.28921, acc=0.89062
# [44/100] testing 57.0% loss=0.32968, acc=0.89062
# [44/100] testing 57.9% loss=0.26202, acc=0.87500
# [44/100] testing 58.3% loss=0.37139, acc=0.87500
# [44/100] testing 59.2% loss=0.21030, acc=0.92188
# [44/100] testing 59.7% loss=0.20490, acc=0.89062
# [44/100] testing 60.5% loss=0.34034, acc=0.84375
# [44/100] testing 61.4% loss=0.15382, acc=0.93750
# [44/100] testing 61.9% loss=0.30871, acc=0.84375
# [44/100] testing 62.7% loss=0.14411, acc=0.93750
# [44/100] testing 63.2% loss=0.40889, acc=0.87500
# [44/100] testing 64.0% loss=0.44804, acc=0.84375
# [44/100] testing 64.5% loss=0.16215, acc=0.92188
# [44/100] testing 65.4% loss=0.15198, acc=0.95312
# [44/100] testing 65.8% loss=0.30270, acc=0.89062
# [44/100] testing 66.7% loss=0.25354, acc=0.89062
# [44/100] testing 67.6% loss=0.41006, acc=0.89062
# [44/100] testing 68.0% loss=0.07668, acc=0.98438
# [44/100] testing 68.9% loss=0.35096, acc=0.89062
# [44/100] testing 69.3% loss=0.35837, acc=0.84375
# [44/100] testing 70.2% loss=0.34195, acc=0.85938
# [44/100] testing 70.6% loss=0.27727, acc=0.89062
# [44/100] testing 71.5% loss=0.25851, acc=0.90625
# [44/100] testing 72.4% loss=0.15019, acc=0.93750
# [44/100] testing 72.8% loss=0.12067, acc=0.95312
# [44/100] testing 73.7% loss=0.12229, acc=0.95312
# [44/100] testing 74.1% loss=0.39696, acc=0.90625
# [44/100] testing 75.0% loss=0.17228, acc=0.90625
# [44/100] testing 75.4% loss=0.51139, acc=0.82812
# [44/100] testing 76.3% loss=0.09967, acc=0.93750
# [44/100] testing 76.8% loss=0.25275, acc=0.85938
# [44/100] testing 77.6% loss=0.24564, acc=0.89062
# [44/100] testing 78.5% loss=0.31720, acc=0.89062
# [44/100] testing 79.0% loss=0.23570, acc=0.87500
# [44/100] testing 79.8% loss=0.32194, acc=0.84375
# [44/100] testing 80.3% loss=0.16893, acc=0.92188
# [44/100] testing 81.2% loss=0.49179, acc=0.82812
# [44/100] testing 81.6% loss=0.21619, acc=0.92188
# [44/100] testing 82.5% loss=0.18968, acc=0.95312
# [44/100] testing 83.3% loss=0.26281, acc=0.92188
# [44/100] testing 83.8% loss=0.23568, acc=0.90625
# [44/100] testing 84.7% loss=0.34396, acc=0.85938
# [44/100] testing 85.1% loss=0.21688, acc=0.90625
# [44/100] testing 86.0% loss=0.30393, acc=0.92188
# [44/100] testing 86.4% loss=0.33049, acc=0.90625
# [44/100] testing 87.3% loss=0.17579, acc=0.92188
# [44/100] testing 87.7% loss=0.14734, acc=0.96875
# [44/100] testing 88.6% loss=0.29888, acc=0.84375
# [44/100] testing 89.5% loss=0.50160, acc=0.84375
# [44/100] testing 89.9% loss=0.20890, acc=0.93750
# [44/100] testing 90.8% loss=0.30332, acc=0.90625
# [44/100] testing 91.2% loss=0.20890, acc=0.92188
# [44/100] testing 92.1% loss=0.32705, acc=0.90625
# [44/100] testing 92.6% loss=0.43007, acc=0.82812
# [44/100] testing 93.4% loss=0.28368, acc=0.87500
# [44/100] testing 94.3% loss=0.14597, acc=0.92188
# [44/100] testing 94.7% loss=0.19075, acc=0.93750
# [44/100] testing 95.6% loss=0.42765, acc=0.84375
# [44/100] testing 96.1% loss=0.17734, acc=0.90625
# [44/100] testing 96.9% loss=0.34285, acc=0.84375
# [44/100] testing 97.4% loss=0.13495, acc=0.95312
# [44/100] testing 98.3% loss=0.18483, acc=0.92188
# [44/100] testing 98.7% loss=0.15756, acc=0.93750
# [44/100] testing 99.6% loss=0.33216, acc=0.90625
# [45/100] training 0.2% loss=0.30665, acc=0.85938
# [45/100] training 0.4% loss=0.24199, acc=0.87500
# [45/100] training 0.5% loss=0.14776, acc=0.95312
# [45/100] training 0.8% loss=0.08822, acc=0.98438
# [45/100] training 0.9% loss=0.17574, acc=0.92188
# [45/100] training 1.1% loss=0.32143, acc=0.93750
# [45/100] training 1.2% loss=0.16119, acc=0.93750
# [45/100] training 1.4% loss=0.19149, acc=0.92188
# [45/100] training 1.6% loss=0.08613, acc=0.96875
# [45/100] training 1.8% loss=0.14710, acc=0.96875
# [45/100] training 2.0% loss=0.26779, acc=0.90625
# [45/100] training 2.1% loss=0.15557, acc=0.93750
# [45/100] training 2.3% loss=0.12890, acc=0.95312
# [45/100] training 2.4% loss=0.30204, acc=0.85938
# [45/100] training 2.6% loss=0.15165, acc=0.92188
# [45/100] training 2.7% loss=0.28404, acc=0.89062
# [45/100] training 3.0% loss=0.20672, acc=0.93750
# [45/100] training 3.2% loss=0.18047, acc=0.90625
# [45/100] training 3.3% loss=0.29262, acc=0.90625
# [45/100] training 3.5% loss=0.14522, acc=0.95312
# [45/100] training 3.6% loss=0.22331, acc=0.92188
# [45/100] training 3.8% loss=0.20024, acc=0.92188
# [45/100] training 3.9% loss=0.13523, acc=0.96875
# [45/100] training 4.2% loss=0.11854, acc=0.96875
# [45/100] training 4.4% loss=0.10302, acc=0.96875
# [45/100] training 4.5% loss=0.15917, acc=0.90625
# [45/100] training 4.7% loss=0.38701, acc=0.89062
# [45/100] training 4.8% loss=0.15651, acc=0.93750
# [45/100] training 5.0% loss=0.07483, acc=0.95312
# [45/100] training 5.2% loss=0.22209, acc=0.90625
# [45/100] training 5.4% loss=0.11861, acc=0.96875
# [45/100] training 5.5% loss=0.21689, acc=0.92188
# [45/100] training 5.7% loss=0.19141, acc=0.93750
# [45/100] training 5.9% loss=0.19439, acc=0.95312
# [45/100] training 6.0% loss=0.20636, acc=0.89062
# [45/100] training 6.3% loss=0.20796, acc=0.87500
# [45/100] training 6.4% loss=0.14476, acc=0.95312
# [45/100] training 6.6% loss=0.15726, acc=0.92188
# [45/100] training 6.7% loss=0.30836, acc=0.84375
# [45/100] training 6.9% loss=0.14526, acc=0.93750
# [45/100] training 7.1% loss=0.18647, acc=0.90625
# [45/100] training 7.2% loss=0.20935, acc=0.87500
# [45/100] training 7.5% loss=0.16174, acc=0.92188
# [45/100] training 7.6% loss=0.23183, acc=0.90625
# [45/100] training 7.8% loss=0.08695, acc=0.96875
# [45/100] training 7.9% loss=0.19189, acc=0.93750
# [45/100] training 8.1% loss=0.09964, acc=0.95312
# [45/100] training 8.2% loss=0.19617, acc=0.93750
# [45/100] training 8.4% loss=0.25379, acc=0.87500
# [45/100] training 8.7% loss=0.23433, acc=0.87500
# [45/100] training 8.8% loss=0.13582, acc=0.93750
# [45/100] training 9.0% loss=0.20421, acc=0.90625
# [45/100] training 9.1% loss=0.20774, acc=0.90625
# [45/100] training 9.3% loss=0.34678, acc=0.89062
# [45/100] training 9.4% loss=0.10893, acc=0.96875
# [45/100] training 9.7% loss=0.15608, acc=0.93750
# [45/100] training 9.9% loss=0.24882, acc=0.89062
# [45/100] training 10.0% loss=0.15670, acc=0.95312
# [45/100] training 10.2% loss=0.14454, acc=0.93750
# [45/100] training 10.3% loss=0.13236, acc=0.93750
# [45/100] training 10.5% loss=0.31275, acc=0.89062
# [45/100] training 10.6% loss=0.21830, acc=0.89062
# [45/100] training 10.9% loss=0.10472, acc=0.95312
# [45/100] training 11.0% loss=0.33084, acc=0.84375
# [45/100] training 11.2% loss=0.13029, acc=0.93750
# [45/100] training 11.4% loss=0.26237, acc=0.89062
# [45/100] training 11.5% loss=0.30971, acc=0.87500
# [45/100] training 11.7% loss=0.14434, acc=0.93750
# [45/100] training 11.8% loss=0.13363, acc=0.95312
# [45/100] training 12.1% loss=0.14907, acc=0.92188
# [45/100] training 12.2% loss=0.10858, acc=0.95312
# [45/100] training 12.4% loss=0.08649, acc=0.95312
# [45/100] training 12.6% loss=0.22848, acc=0.93750
# [45/100] training 12.7% loss=0.32221, acc=0.87500
# [45/100] training 12.9% loss=0.35368, acc=0.84375
# [45/100] training 13.0% loss=0.18119, acc=0.92188
# [45/100] training 13.3% loss=0.13755, acc=0.92188
# [45/100] training 13.4% loss=0.17255, acc=0.89062
# [45/100] training 13.6% loss=0.18183, acc=0.92188
# [45/100] training 13.7% loss=0.27465, acc=0.89062
# [45/100] training 13.9% loss=0.30288, acc=0.89062
# [45/100] training 14.1% loss=0.20694, acc=0.95312
# [45/100] training 14.3% loss=0.21897, acc=0.90625
# [45/100] training 14.5% loss=0.16510, acc=0.93750
# [45/100] training 14.6% loss=0.22881, acc=0.93750
# [45/100] training 14.8% loss=0.21544, acc=0.90625
# [45/100] training 14.9% loss=0.14549, acc=0.95312
# [45/100] training 15.1% loss=0.35213, acc=0.87500
# [45/100] training 15.4% loss=0.23826, acc=0.90625
# [45/100] training 15.5% loss=0.14834, acc=0.95312
# [45/100] training 15.7% loss=0.30787, acc=0.84375
# [45/100] training 15.8% loss=0.12583, acc=0.96875
# [45/100] training 16.0% loss=0.22440, acc=0.90625
# [45/100] training 16.1% loss=0.26667, acc=0.92188
# [45/100] training 16.3% loss=0.28796, acc=0.87500
# [45/100] training 16.4% loss=0.18340, acc=0.95312
# [45/100] training 16.7% loss=0.25218, acc=0.85938
# [45/100] training 16.9% loss=0.18551, acc=0.93750
# [45/100] training 17.0% loss=0.19852, acc=0.90625
# [45/100] training 17.2% loss=0.19923, acc=0.90625
# [45/100] training 17.3% loss=0.16721, acc=0.95312
# [45/100] training 17.5% loss=0.16029, acc=0.93750
# [45/100] training 17.7% loss=0.16933, acc=0.92188
# [45/100] training 17.9% loss=0.23639, acc=0.92188
# [45/100] training 18.1% loss=0.23131, acc=0.90625
# [45/100] training 18.2% loss=0.17176, acc=0.93750
# [45/100] training 18.4% loss=0.23768, acc=0.85938
# [45/100] training 18.5% loss=0.23952, acc=0.89062
# [45/100] training 18.8% loss=0.17586, acc=0.95312
# [45/100] training 18.9% loss=0.17734, acc=0.95312
# [45/100] training 19.1% loss=0.24880, acc=0.89062
# [45/100] training 19.2% loss=0.08720, acc=0.98438
# [45/100] training 19.4% loss=0.15080, acc=0.93750
# [45/100] training 19.6% loss=0.17034, acc=0.93750
# [45/100] training 19.7% loss=0.19417, acc=0.90625
# [45/100] training 20.0% loss=0.13682, acc=0.93750
# [45/100] training 20.1% loss=0.25333, acc=0.90625
# [45/100] training 20.3% loss=0.20365, acc=0.90625
# [45/100] training 20.4% loss=0.22489, acc=0.90625
# [45/100] training 20.6% loss=0.20575, acc=0.90625
# [45/100] training 20.8% loss=0.13245, acc=0.93750
# [45/100] training 20.9% loss=0.15041, acc=0.92188
# [45/100] training 21.2% loss=0.18370, acc=0.93750
# [45/100] training 21.3% loss=0.20356, acc=0.95312
# [45/100] training 21.5% loss=0.31383, acc=0.90625
# [45/100] training 21.6% loss=0.07340, acc=0.96875
# [45/100] training 21.8% loss=0.16490, acc=0.92188
# [45/100] training 21.9% loss=0.22062, acc=0.93750
# [45/100] training 22.2% loss=0.22973, acc=0.90625
# [45/100] training 22.4% loss=0.22309, acc=0.89062
# [45/100] training 22.5% loss=0.17701, acc=0.93750
# [45/100] training 22.7% loss=0.19305, acc=0.93750
# [45/100] training 22.8% loss=0.16753, acc=0.95312
# [45/100] training 23.0% loss=0.09349, acc=0.96875
# [45/100] training 23.1% loss=0.30534, acc=0.89062
# [45/100] training 23.4% loss=0.27294, acc=0.87500
# [45/100] training 23.6% loss=0.20327, acc=0.92188
# [45/100] training 23.7% loss=0.14435, acc=0.96875
# [45/100] training 23.9% loss=0.17030, acc=0.92188
# [45/100] training 24.0% loss=0.12415, acc=0.95312
# [45/100] training 24.2% loss=0.14048, acc=0.92188
# [45/100] training 24.3% loss=0.20855, acc=0.92188
# [45/100] training 24.6% loss=0.22823, acc=0.90625
# [45/100] training 24.7% loss=0.24909, acc=0.92188
# [45/100] training 24.9% loss=0.19729, acc=0.92188
# [45/100] training 25.1% loss=0.24605, acc=0.92188
# [45/100] training 25.2% loss=0.13256, acc=0.95312
# [45/100] training 25.4% loss=0.21761, acc=0.90625
# [45/100] training 25.6% loss=0.17161, acc=0.93750
# [45/100] training 25.8% loss=0.26468, acc=0.95312
# [45/100] training 25.9% loss=0.13867, acc=0.93750
# [45/100] training 26.1% loss=0.22495, acc=0.92188
# [45/100] training 26.3% loss=0.20261, acc=0.95312
# [45/100] training 26.4% loss=0.17634, acc=0.92188
# [45/100] training 26.6% loss=0.08638, acc=0.95312
# [45/100] training 26.8% loss=0.14155, acc=0.93750
# [45/100] training 27.0% loss=0.11850, acc=0.95312
# [45/100] training 27.1% loss=0.14215, acc=0.95312
# [45/100] training 27.3% loss=0.12022, acc=0.92188
# [45/100] training 27.4% loss=0.09735, acc=0.95312
# [45/100] training 27.6% loss=0.23385, acc=0.93750
# [45/100] training 27.9% loss=0.15910, acc=0.93750
# [45/100] training 28.0% loss=0.30034, acc=0.90625
# [45/100] training 28.2% loss=0.09888, acc=0.96875
# [45/100] training 28.3% loss=0.05564, acc=1.00000
# [45/100] training 28.5% loss=0.18675, acc=0.90625
# [45/100] training 28.6% loss=0.19508, acc=0.93750
# [45/100] training 28.8% loss=0.16519, acc=0.92188
# [45/100] training 29.1% loss=0.14662, acc=0.93750
# [45/100] training 29.2% loss=0.13289, acc=0.96875
# [45/100] training 29.4% loss=0.22797, acc=0.87500
# [45/100] training 29.5% loss=0.12917, acc=0.92188
# [45/100] training 29.7% loss=0.25683, acc=0.90625
# [45/100] training 29.8% loss=0.13082, acc=0.95312
# [45/100] training 30.0% loss=0.21513, acc=0.90625
# [45/100] training 30.2% loss=0.11760, acc=0.93750
# [45/100] training 30.4% loss=0.14428, acc=0.95312
# [45/100] training 30.6% loss=0.26756, acc=0.87500
# [45/100] training 30.7% loss=0.15430, acc=0.93750
# [45/100] training 30.9% loss=0.31033, acc=0.92188
# [45/100] training 31.0% loss=0.09031, acc=0.98438
# [45/100] training 31.3% loss=0.17448, acc=0.92188
# [45/100] training 31.4% loss=0.21177, acc=0.89062
# [45/100] training 31.6% loss=0.21182, acc=0.90625
# [45/100] training 31.8% loss=0.14450, acc=0.93750
# [45/100] training 31.9% loss=0.14614, acc=0.95312
# [45/100] training 32.1% loss=0.13208, acc=0.93750
# [45/100] training 32.2% loss=0.32002, acc=0.89062
# [45/100] training 32.5% loss=0.09760, acc=0.95312
# [45/100] training 32.6% loss=0.21942, acc=0.92188
# [45/100] training 32.8% loss=0.13510, acc=0.95312
# [45/100] training 32.9% loss=0.22056, acc=0.90625
# [45/100] training 33.1% loss=0.12514, acc=0.93750
# [45/100] training 33.3% loss=0.21342, acc=0.90625
# [45/100] training 33.4% loss=0.11424, acc=0.95312
# [45/100] training 33.7% loss=0.15489, acc=0.92188
# [45/100] training 33.8% loss=0.21849, acc=0.92188
# [45/100] training 34.0% loss=0.20153, acc=0.89062
# [45/100] training 34.1% loss=0.23656, acc=0.90625
# [45/100] training 34.3% loss=0.20087, acc=0.90625
# [45/100] training 34.5% loss=0.22090, acc=0.90625
# [45/100] training 34.7% loss=0.11703, acc=0.96875
# [45/100] training 34.9% loss=0.15303, acc=0.90625
# [45/100] training 35.0% loss=0.16407, acc=0.95312
# [45/100] training 35.2% loss=0.27381, acc=0.87500
# [45/100] training 35.3% loss=0.19530, acc=0.90625
# [45/100] training 35.5% loss=0.12352, acc=0.95312
# [45/100] training 35.6% loss=0.24853, acc=0.93750
# [45/100] training 35.9% loss=0.13259, acc=0.95312
# [45/100] training 36.1% loss=0.28779, acc=0.90625
# [45/100] training 36.2% loss=0.26337, acc=0.87500
# [45/100] training 36.4% loss=0.22992, acc=0.92188
# [45/100] training 36.5% loss=0.21130, acc=0.90625
# [45/100] training 36.7% loss=0.27659, acc=0.87500
# [45/100] training 36.8% loss=0.11133, acc=0.96875
# [45/100] training 37.1% loss=0.27588, acc=0.85938
# [45/100] training 37.3% loss=0.16489, acc=0.93750
# [45/100] training 37.4% loss=0.10490, acc=0.95312
# [45/100] training 37.6% loss=0.09206, acc=0.96875
# [45/100] training 37.7% loss=0.27199, acc=0.90625
# [45/100] training 37.9% loss=0.21106, acc=0.92188
# [45/100] training 38.1% loss=0.16887, acc=0.90625
# [45/100] training 38.3% loss=0.15893, acc=0.90625
# [45/100] training 38.4% loss=0.06557, acc=0.98438
# [45/100] training 38.6% loss=0.19070, acc=0.92188
# [45/100] training 38.8% loss=0.24215, acc=0.89062
# [45/100] training 38.9% loss=0.17179, acc=0.93750
# [45/100] training 39.1% loss=0.21654, acc=0.90625
# [45/100] training 39.3% loss=0.19981, acc=0.93750
# [45/100] training 39.5% loss=0.23421, acc=0.90625
# [45/100] training 39.6% loss=0.08899, acc=0.98438
# [45/100] training 39.8% loss=0.11028, acc=0.95312
# [45/100] training 40.0% loss=0.21323, acc=0.95312
# [45/100] training 40.1% loss=0.34685, acc=0.89062
# [45/100] training 40.4% loss=0.11913, acc=0.93750
# [45/100] training 40.5% loss=0.27247, acc=0.92188
# [45/100] training 40.7% loss=0.17926, acc=0.93750
# [45/100] training 40.8% loss=0.09670, acc=0.96875
# [45/100] training 41.0% loss=0.15788, acc=0.92188
# [45/100] training 41.1% loss=0.28113, acc=0.89062
# [45/100] training 41.3% loss=0.14038, acc=0.96875
# [45/100] training 41.6% loss=0.16470, acc=0.93750
# [45/100] training 41.7% loss=0.22452, acc=0.89062
# [45/100] training 41.9% loss=0.12554, acc=0.96875
# [45/100] training 42.0% loss=0.22688, acc=0.90625
# [45/100] training 42.2% loss=0.20616, acc=0.92188
# [45/100] training 42.3% loss=0.19249, acc=0.93750
# [45/100] training 42.5% loss=0.12002, acc=0.95312
# [45/100] training 42.8% loss=0.17614, acc=0.95312
# [45/100] training 42.9% loss=0.14819, acc=0.93750
# [45/100] training 43.1% loss=0.16411, acc=0.95312
# [45/100] training 43.2% loss=0.10317, acc=0.93750
# [45/100] training 43.4% loss=0.20164, acc=0.93750
# [45/100] training 43.5% loss=0.24667, acc=0.89062
# [45/100] training 43.8% loss=0.29006, acc=0.89062
# [45/100] training 43.9% loss=0.18907, acc=0.92188
# [45/100] training 44.1% loss=0.11094, acc=0.98438
# [45/100] training 44.3% loss=0.12934, acc=0.98438
# [45/100] training 44.4% loss=0.22407, acc=0.90625
# [45/100] training 44.6% loss=0.21745, acc=0.90625
# [45/100] training 44.7% loss=0.28927, acc=0.89062
# [45/100] training 45.0% loss=0.12748, acc=0.95312
# [45/100] training 45.1% loss=0.26432, acc=0.89062
# [45/100] training 45.3% loss=0.12282, acc=0.96875
# [45/100] training 45.5% loss=0.04166, acc=1.00000
# [45/100] training 45.6% loss=0.27780, acc=0.89062
# [45/100] training 45.8% loss=0.08506, acc=0.96875
# [45/100] training 45.9% loss=0.15317, acc=0.89062
# [45/100] training 46.2% loss=0.03603, acc=1.00000
# [45/100] training 46.3% loss=0.15198, acc=0.95312
# [45/100] training 46.5% loss=0.27325, acc=0.89062
# [45/100] training 46.6% loss=0.21968, acc=0.92188
# [45/100] training 46.8% loss=0.09868, acc=0.96875
# [45/100] training 47.0% loss=0.14188, acc=0.93750
# [45/100] training 47.2% loss=0.18108, acc=0.93750
# [45/100] training 47.4% loss=0.22317, acc=0.92188
# [45/100] training 47.5% loss=0.31508, acc=0.87500
# [45/100] training 47.7% loss=0.12131, acc=0.95312
# [45/100] training 47.8% loss=0.30109, acc=0.87500
# [45/100] training 48.0% loss=0.18755, acc=0.90625
# [45/100] training 48.3% loss=0.07264, acc=0.96875
# [45/100] training 48.4% loss=0.09164, acc=0.95312
# [45/100] training 48.6% loss=0.21384, acc=0.90625
# [45/100] training 48.7% loss=0.14360, acc=0.93750
# [45/100] training 48.9% loss=0.15991, acc=0.90625
# [45/100] training 49.0% loss=0.10928, acc=0.96875
# [45/100] training 49.2% loss=0.22842, acc=0.93750
# [45/100] training 49.3% loss=0.15083, acc=0.95312
# [45/100] training 49.6% loss=0.22392, acc=0.93750
# [45/100] training 49.8% loss=0.19303, acc=0.93750
# [45/100] training 49.9% loss=0.22037, acc=0.89062
# [45/100] training 50.1% loss=0.11429, acc=0.96875
# [45/100] training 50.2% loss=0.16369, acc=0.93750
# [45/100] training 50.4% loss=0.37432, acc=0.87500
# [45/100] training 50.6% loss=0.13977, acc=0.92188
# [45/100] training 50.8% loss=0.26927, acc=0.87500
# [45/100] training 51.0% loss=0.33810, acc=0.87500
# [45/100] training 51.1% loss=0.20036, acc=0.95312
# [45/100] training 51.3% loss=0.21894, acc=0.89062
# [45/100] training 51.4% loss=0.22683, acc=0.92188
# [45/100] training 51.7% loss=0.12954, acc=0.95312
# [45/100] training 51.8% loss=0.16185, acc=0.93750
# [45/100] training 52.0% loss=0.21262, acc=0.90625
# [45/100] training 52.1% loss=0.20482, acc=0.89062
# [45/100] training 52.3% loss=0.26523, acc=0.92188
# [45/100] training 52.5% loss=0.11108, acc=0.96875
# [45/100] training 52.6% loss=0.15518, acc=0.90625
# [45/100] training 52.9% loss=0.35355, acc=0.90625
# [45/100] training 53.0% loss=0.14093, acc=0.95312
# [45/100] training 53.2% loss=0.19424, acc=0.95312
# [45/100] training 53.3% loss=0.17164, acc=0.93750
# [45/100] training 53.5% loss=0.32553, acc=0.90625
# [45/100] training 53.7% loss=0.04598, acc=1.00000
# [45/100] training 53.8% loss=0.18833, acc=0.90625
# [45/100] training 54.1% loss=0.27797, acc=0.89062
# [45/100] training 54.2% loss=0.10767, acc=0.98438
# [45/100] training 54.4% loss=0.14347, acc=0.93750
# [45/100] training 54.5% loss=0.30368, acc=0.89062
# [45/100] training 54.7% loss=0.25137, acc=0.92188
# [45/100] training 54.8% loss=0.08474, acc=0.98438
# [45/100] training 55.1% loss=0.20835, acc=0.90625
# [45/100] training 55.3% loss=0.16297, acc=0.93750
# [45/100] training 55.4% loss=0.09601, acc=0.96875
# [45/100] training 55.6% loss=0.11602, acc=0.96875
# [45/100] training 55.7% loss=0.08652, acc=0.96875
# [45/100] training 55.9% loss=0.18200, acc=0.90625
# [45/100] training 56.0% loss=0.12991, acc=0.96875
# [45/100] training 56.3% loss=0.41979, acc=0.82812
# [45/100] training 56.5% loss=0.13828, acc=0.93750
# [45/100] training 56.6% loss=0.10982, acc=0.95312
# [45/100] training 56.8% loss=0.22683, acc=0.92188
# [45/100] training 56.9% loss=0.14709, acc=0.92188
# [45/100] training 57.1% loss=0.25044, acc=0.90625
# [45/100] training 57.2% loss=0.18379, acc=0.93750
# [45/100] training 57.5% loss=0.20258, acc=0.89062
# [45/100] training 57.6% loss=0.12724, acc=0.92188
# [45/100] training 57.8% loss=0.08639, acc=0.98438
# [45/100] training 58.0% loss=0.21572, acc=0.93750
# [45/100] training 58.1% loss=0.11963, acc=0.95312
# [45/100] training 58.3% loss=0.13153, acc=0.93750
# [45/100] training 58.4% loss=0.19605, acc=0.93750
# [45/100] training 58.7% loss=0.21712, acc=0.93750
# [45/100] training 58.8% loss=0.17266, acc=0.92188
# [45/100] training 59.0% loss=0.10387, acc=0.96875
# [45/100] training 59.2% loss=0.28280, acc=0.89062
# [45/100] training 59.3% loss=0.16542, acc=0.93750
# [45/100] training 59.5% loss=0.20490, acc=0.93750
# [45/100] training 59.7% loss=0.19117, acc=0.90625
# [45/100] training 59.9% loss=0.15582, acc=0.93750
# [45/100] training 60.0% loss=0.24121, acc=0.89062
# [45/100] training 60.2% loss=0.14674, acc=0.92188
# [45/100] training 60.3% loss=0.16396, acc=0.93750
# [45/100] training 60.5% loss=0.25674, acc=0.87500
# [45/100] training 60.8% loss=0.15194, acc=0.92188
# [45/100] training 60.9% loss=0.37578, acc=0.82812
# [45/100] training 61.1% loss=0.20645, acc=0.90625
# [45/100] training 61.2% loss=0.16921, acc=0.93750
# [45/100] training 61.4% loss=0.26005, acc=0.87500
# [45/100] training 61.5% loss=0.29631, acc=0.87500
# [45/100] training 61.7% loss=0.22424, acc=0.90625
# [45/100] training 62.0% loss=0.31769, acc=0.89062
# [45/100] training 62.1% loss=0.23093, acc=0.89062
# [45/100] training 62.3% loss=0.09683, acc=0.96875
# [45/100] training 62.4% loss=0.18662, acc=0.92188
# [45/100] training 62.6% loss=0.26984, acc=0.87500
# [45/100] training 62.7% loss=0.12724, acc=0.95312
# [45/100] training 62.9% loss=0.23415, acc=0.85938
# [45/100] training 63.1% loss=0.15873, acc=0.93750
# [45/100] training 63.3% loss=0.16339, acc=0.92188
# [45/100] training 63.5% loss=0.18440, acc=0.95312
# [45/100] training 63.6% loss=0.26240, acc=0.89062
# [45/100] training 63.8% loss=0.26149, acc=0.87500
# [45/100] training 63.9% loss=0.15729, acc=0.90625
# [45/100] training 64.2% loss=0.18872, acc=0.89062
# [45/100] training 64.3% loss=0.26535, acc=0.89062
# [45/100] training 64.5% loss=0.16841, acc=0.93750
# [45/100] training 64.7% loss=0.15141, acc=0.93750
# [45/100] training 64.8% loss=0.32094, acc=0.87500
# [45/100] training 65.0% loss=0.24023, acc=0.89062
# [45/100] training 65.1% loss=0.22556, acc=0.92188
# [45/100] training 65.4% loss=0.10122, acc=0.96875
# [45/100] training 65.5% loss=0.13040, acc=0.95312
# [45/100] training 65.7% loss=0.17072, acc=0.93750
# [45/100] training 65.8% loss=0.22647, acc=0.89062
# [45/100] training 66.0% loss=0.13815, acc=0.96875
# [45/100] training 66.2% loss=0.08360, acc=0.96875
# [45/100] training 66.3% loss=0.20668, acc=0.92188
# [45/100] training 66.6% loss=0.15395, acc=0.95312
# [45/100] training 66.7% loss=0.16199, acc=0.93750
# [45/100] training 66.9% loss=0.21654, acc=0.93750
# [45/100] training 67.0% loss=0.23256, acc=0.90625
# [45/100] training 67.2% loss=0.16200, acc=0.90625
# [45/100] training 67.4% loss=0.08040, acc=0.96875
# [45/100] training 67.6% loss=0.12477, acc=0.93750
# [45/100] training 67.8% loss=0.11054, acc=0.96875
# [45/100] training 67.9% loss=0.12781, acc=0.92188
# [45/100] training 68.1% loss=0.17407, acc=0.93750
# [45/100] training 68.2% loss=0.22211, acc=0.90625
# [45/100] training 68.4% loss=0.12840, acc=0.95312
# [45/100] training 68.5% loss=0.24433, acc=0.90625
# [45/100] training 68.8% loss=0.17008, acc=0.92188
# [45/100] training 69.0% loss=0.27980, acc=0.92188
# [45/100] training 69.1% loss=0.16157, acc=0.92188
# [45/100] training 69.3% loss=0.15679, acc=0.92188
# [45/100] training 69.4% loss=0.22290, acc=0.90625
# [45/100] training 69.6% loss=0.11168, acc=0.95312
# [45/100] training 69.7% loss=0.13470, acc=0.96875
# [45/100] training 70.0% loss=0.20719, acc=0.87500
# [45/100] training 70.2% loss=0.22919, acc=0.92188
# [45/100] training 70.3% loss=0.22119, acc=0.92188
# [45/100] training 70.5% loss=0.23161, acc=0.93750
# [45/100] training 70.6% loss=0.12786, acc=0.96875
# [45/100] training 70.8% loss=0.16070, acc=0.93750
# [45/100] training 71.0% loss=0.20345, acc=0.90625
# [45/100] training 71.2% loss=0.16714, acc=0.89062
# [45/100] training 71.3% loss=0.08929, acc=0.96875
# [45/100] training 71.5% loss=0.22314, acc=0.90625
# [45/100] training 71.7% loss=0.17241, acc=0.89062
# [45/100] training 71.8% loss=0.22420, acc=0.93750
# [45/100] training 72.0% loss=0.08399, acc=0.96875
# [45/100] training 72.2% loss=0.18046, acc=0.90625
# [45/100] training 72.4% loss=0.26323, acc=0.87500
# [45/100] training 72.5% loss=0.30397, acc=0.84375
# [45/100] training 72.7% loss=0.27299, acc=0.92188
# [45/100] training 72.9% loss=0.16293, acc=0.95312
# [45/100] training 73.0% loss=0.07575, acc=0.98438
# [45/100] training 73.3% loss=0.25794, acc=0.92188
# [45/100] training 73.4% loss=0.10046, acc=0.93750
# [45/100] training 73.6% loss=0.13780, acc=0.96875
# [45/100] training 73.7% loss=0.16010, acc=0.90625
# [45/100] training 73.9% loss=0.08341, acc=0.96875
# [45/100] training 74.0% loss=0.25578, acc=0.92188
# [45/100] training 74.2% loss=0.10569, acc=0.96875
# [45/100] training 74.5% loss=0.13357, acc=0.95312
# [45/100] training 74.6% loss=0.41623, acc=0.84375
# [45/100] training 74.8% loss=0.33235, acc=0.87500
# [45/100] training 74.9% loss=0.30591, acc=0.89062
# [45/100] training 75.1% loss=0.16933, acc=0.95312
# [45/100] training 75.2% loss=0.17824, acc=0.90625
# [45/100] training 75.4% loss=0.17390, acc=0.92188
# [45/100] training 75.7% loss=0.18011, acc=0.93750
# [45/100] training 75.8% loss=0.20209, acc=0.92188
# [45/100] training 76.0% loss=0.16078, acc=0.92188
# [45/100] training 76.1% loss=0.20432, acc=0.93750
# [45/100] training 76.3% loss=0.07108, acc=0.98438
# [45/100] training 76.4% loss=0.22265, acc=0.90625
# [45/100] training 76.7% loss=0.15929, acc=0.95312
# [45/100] training 76.8% loss=0.08149, acc=0.95312
# [45/100] training 77.0% loss=0.08387, acc=0.96875
# [45/100] training 77.2% loss=0.13803, acc=0.95312
# [45/100] training 77.3% loss=0.09318, acc=0.96875
# [45/100] training 77.5% loss=0.19781, acc=0.93750
# [45/100] training 77.6% loss=0.19528, acc=0.93750
# [45/100] training 77.9% loss=0.13363, acc=0.93750
# [45/100] training 78.0% loss=0.29696, acc=0.87500
# [45/100] training 78.2% loss=0.28413, acc=0.92188
# [45/100] training 78.4% loss=0.09621, acc=0.95312
# [45/100] training 78.5% loss=0.20324, acc=0.89062
# [45/100] training 78.7% loss=0.21074, acc=0.92188
# [45/100] training 78.8% loss=0.09956, acc=0.96875
# [45/100] training 79.1% loss=0.12045, acc=0.96875
# [45/100] training 79.2% loss=0.16779, acc=0.95312
# [45/100] training 79.4% loss=0.21216, acc=0.90625
# [45/100] training 79.5% loss=0.14375, acc=0.93750
# [45/100] training 79.7% loss=0.08214, acc=1.00000
# [45/100] training 79.9% loss=0.13338, acc=0.93750
# [45/100] training 80.1% loss=0.10049, acc=0.95312
# [45/100] training 80.3% loss=0.23141, acc=0.89062
# [45/100] training 80.4% loss=0.15286, acc=0.96875
# [45/100] training 80.6% loss=0.25196, acc=0.89062
# [45/100] training 80.7% loss=0.10430, acc=1.00000
# [45/100] training 80.9% loss=0.21372, acc=0.93750
# [45/100] training 81.2% loss=0.20954, acc=0.92188
# [45/100] training 81.3% loss=0.25506, acc=0.90625
# [45/100] training 81.5% loss=0.17938, acc=0.90625
# [45/100] training 81.6% loss=0.30375, acc=0.87500
# [45/100] training 81.8% loss=0.16476, acc=0.92188
# [45/100] training 81.9% loss=0.26993, acc=0.92188
# [45/100] training 82.1% loss=0.15121, acc=0.93750
# [45/100] training 82.2% loss=0.20376, acc=0.92188
# [45/100] training 82.5% loss=0.13856, acc=0.93750
# [45/100] training 82.7% loss=0.21905, acc=0.93750
# [45/100] training 82.8% loss=0.24698, acc=0.92188
# [45/100] training 83.0% loss=0.18607, acc=0.93750
# [45/100] training 83.1% loss=0.17010, acc=0.93750
# [45/100] training 83.3% loss=0.25481, acc=0.96875
# [45/100] training 83.5% loss=0.16806, acc=0.95312
# [45/100] training 83.7% loss=0.29853, acc=0.90625
# [45/100] training 83.9% loss=0.18625, acc=0.93750
# [45/100] training 84.0% loss=0.07220, acc=1.00000
# [45/100] training 84.2% loss=0.10604, acc=0.95312
# [45/100] training 84.3% loss=0.08876, acc=0.93750
# [45/100] training 84.5% loss=0.18278, acc=0.95312
# [45/100] training 84.7% loss=0.15643, acc=0.95312
# [45/100] training 84.9% loss=0.19480, acc=0.90625
# [45/100] training 85.0% loss=0.16464, acc=0.92188
# [45/100] training 85.2% loss=0.10494, acc=0.95312
# [45/100] training 85.4% loss=0.07489, acc=0.96875
# [45/100] training 85.5% loss=0.18529, acc=0.93750
# [45/100] training 85.8% loss=0.27058, acc=0.90625
# [45/100] training 85.9% loss=0.20926, acc=0.92188
# [45/100] training 86.1% loss=0.22267, acc=0.90625
# [45/100] training 86.2% loss=0.08182, acc=0.96875
# [45/100] training 86.4% loss=0.26673, acc=0.89062
# [45/100] training 86.6% loss=0.24637, acc=0.92188
# [45/100] training 86.7% loss=0.19591, acc=0.93750
# [45/100] training 87.0% loss=0.22686, acc=0.92188
# [45/100] training 87.1% loss=0.20675, acc=0.92188
# [45/100] training 87.3% loss=0.15903, acc=0.93750
# [45/100] training 87.4% loss=0.18104, acc=0.92188
# [45/100] training 87.6% loss=0.20372, acc=0.93750
# [45/100] training 87.7% loss=0.24990, acc=0.92188
# [45/100] training 87.9% loss=0.23245, acc=0.90625
# [45/100] training 88.2% loss=0.11854, acc=0.93750
# [45/100] training 88.3% loss=0.24108, acc=0.89062
# [45/100] training 88.5% loss=0.19625, acc=0.95312
# [45/100] training 88.6% loss=0.16586, acc=0.93750
# [45/100] training 88.8% loss=0.14297, acc=0.93750
# [45/100] training 88.9% loss=0.28891, acc=0.87500
# [45/100] training 89.2% loss=0.22045, acc=0.90625
# [45/100] training 89.4% loss=0.17367, acc=0.93750
# [45/100] training 89.5% loss=0.21213, acc=0.92188
# [45/100] training 89.7% loss=0.26725, acc=0.92188
# [45/100] training 89.8% loss=0.15427, acc=0.93750
# [45/100] training 90.0% loss=0.13652, acc=0.95312
# [45/100] training 90.1% loss=0.12476, acc=0.93750
# [45/100] training 90.4% loss=0.17253, acc=0.93750
# [45/100] training 90.5% loss=0.17864, acc=0.90625
# [45/100] training 90.7% loss=0.13055, acc=0.92188
# [45/100] training 90.9% loss=0.11820, acc=0.96875
# [45/100] training 91.0% loss=0.20545, acc=0.90625
# [45/100] training 91.2% loss=0.22719, acc=0.90625
# [45/100] training 91.3% loss=0.35875, acc=0.87500
# [45/100] training 91.6% loss=0.23254, acc=0.89062
# [45/100] training 91.7% loss=0.15971, acc=0.95312
# [45/100] training 91.9% loss=0.28770, acc=0.87500
# [45/100] training 92.1% loss=0.16231, acc=0.93750
# [45/100] training 92.2% loss=0.08665, acc=0.95312
# [45/100] training 92.4% loss=0.24541, acc=0.89062
# [45/100] training 92.6% loss=0.36091, acc=0.85938
# [45/100] training 92.8% loss=0.19137, acc=0.90625
# [45/100] training 92.9% loss=0.28282, acc=0.90625
# [45/100] training 93.1% loss=0.36477, acc=0.87500
# [45/100] training 93.2% loss=0.26869, acc=0.93750
# [45/100] training 93.4% loss=0.24539, acc=0.89062
# [45/100] training 93.7% loss=0.15233, acc=0.93750
# [45/100] training 93.8% loss=0.16337, acc=0.92188
# [45/100] training 94.0% loss=0.17495, acc=0.93750
# [45/100] training 94.1% loss=0.21222, acc=0.89062
# [45/100] training 94.3% loss=0.12272, acc=0.95312
# [45/100] training 94.4% loss=0.24165, acc=0.93750
# [45/100] training 94.6% loss=0.10880, acc=0.93750
# [45/100] training 94.9% loss=0.13893, acc=0.93750
# [45/100] training 95.0% loss=0.23260, acc=0.90625
# [45/100] training 95.2% loss=0.31375, acc=0.90625
# [45/100] training 95.3% loss=0.22971, acc=0.93750
# [45/100] training 95.5% loss=0.10581, acc=0.96875
# [45/100] training 95.6% loss=0.19724, acc=0.93750
# [45/100] training 95.8% loss=0.20840, acc=0.89062
# [45/100] training 96.0% loss=0.25072, acc=0.92188
# [45/100] training 96.2% loss=0.09302, acc=0.98438
# [45/100] training 96.4% loss=0.12407, acc=0.96875
# [45/100] training 96.5% loss=0.22745, acc=0.89062
# [45/100] training 96.7% loss=0.14783, acc=0.93750
# [45/100] training 96.8% loss=0.28133, acc=0.92188
# [45/100] training 97.1% loss=0.20163, acc=0.90625
# [45/100] training 97.2% loss=0.10762, acc=0.95312
# [45/100] training 97.4% loss=0.19612, acc=0.92188
# [45/100] training 97.6% loss=0.18683, acc=0.90625
# [45/100] training 97.7% loss=0.21166, acc=0.92188
# [45/100] training 97.9% loss=0.08281, acc=0.98438
# [45/100] training 98.0% loss=0.23853, acc=0.89062
# [45/100] training 98.3% loss=0.17154, acc=0.93750
# [45/100] training 98.4% loss=0.13966, acc=0.93750
# [45/100] training 98.6% loss=0.23383, acc=0.85938
# [45/100] training 98.7% loss=0.24860, acc=0.93750
# [45/100] training 98.9% loss=0.15856, acc=0.93750
# [45/100] training 99.1% loss=0.19336, acc=0.93750
# [45/100] training 99.2% loss=0.14779, acc=0.93750
# [45/100] training 99.5% loss=0.24631, acc=0.93750
# [45/100] training 99.6% loss=0.16292, acc=0.92188
# [45/100] training 99.8% loss=0.13456, acc=0.92188
# [45/100] training 99.9% loss=0.06039, acc=0.98438
# [45/100] testing 0.9% loss=0.20800, acc=0.90625
# [45/100] testing 1.8% loss=0.25065, acc=0.90625
# [45/100] testing 2.2% loss=0.31023, acc=0.90625
# [45/100] testing 3.1% loss=0.39520, acc=0.87500
# [45/100] testing 3.5% loss=0.14066, acc=0.95312
# [45/100] testing 4.4% loss=0.24019, acc=0.89062
# [45/100] testing 4.8% loss=0.35756, acc=0.81250
# [45/100] testing 5.7% loss=0.18513, acc=0.92188
# [45/100] testing 6.6% loss=0.09902, acc=0.95312
# [45/100] testing 7.0% loss=0.10188, acc=0.98438
# [45/100] testing 7.9% loss=0.29941, acc=0.82812
# [45/100] testing 8.3% loss=0.34894, acc=0.89062
# [45/100] testing 9.2% loss=0.20448, acc=0.93750
# [45/100] testing 9.7% loss=0.12804, acc=0.96875
# [45/100] testing 10.5% loss=0.21084, acc=0.90625
# [45/100] testing 11.0% loss=0.34612, acc=0.85938
# [45/100] testing 11.8% loss=0.18463, acc=0.92188
# [45/100] testing 12.7% loss=0.28554, acc=0.87500
# [45/100] testing 13.2% loss=0.19331, acc=0.92188
# [45/100] testing 14.0% loss=0.39314, acc=0.89062
# [45/100] testing 14.5% loss=0.18957, acc=0.93750
# [45/100] testing 15.4% loss=0.25454, acc=0.93750
# [45/100] testing 15.8% loss=0.19109, acc=0.93750
# [45/100] testing 16.7% loss=0.25666, acc=0.90625
# [45/100] testing 17.5% loss=0.24862, acc=0.89062
# [45/100] testing 18.0% loss=0.23218, acc=0.92188
# [45/100] testing 18.9% loss=0.16537, acc=0.90625
# [45/100] testing 19.3% loss=0.31948, acc=0.85938
# [45/100] testing 20.2% loss=0.27793, acc=0.92188
# [45/100] testing 20.6% loss=0.39896, acc=0.90625
# [45/100] testing 21.5% loss=0.16776, acc=0.93750
# [45/100] testing 21.9% loss=0.38899, acc=0.84375
# [45/100] testing 22.8% loss=0.28989, acc=0.89062
# [45/100] testing 23.7% loss=0.38732, acc=0.84375
# [45/100] testing 24.1% loss=0.20447, acc=0.90625
# [45/100] testing 25.0% loss=0.30550, acc=0.90625
# [45/100] testing 25.4% loss=0.14616, acc=0.92188
# [45/100] testing 26.3% loss=0.33305, acc=0.85938
# [45/100] testing 26.8% loss=0.26591, acc=0.90625
# [45/100] testing 27.6% loss=0.16903, acc=0.95312
# [45/100] testing 28.5% loss=0.12397, acc=0.92188
# [45/100] testing 29.0% loss=0.26543, acc=0.89062
# [45/100] testing 29.8% loss=0.37140, acc=0.90625
# [45/100] testing 30.3% loss=0.21010, acc=0.92188
# [45/100] testing 31.1% loss=0.24894, acc=0.90625
# [45/100] testing 31.6% loss=0.21208, acc=0.92188
# [45/100] testing 32.5% loss=0.24145, acc=0.93750
# [45/100] testing 32.9% loss=0.35340, acc=0.92188
# [45/100] testing 33.8% loss=0.36899, acc=0.85938
# [45/100] testing 34.7% loss=0.37633, acc=0.87500
# [45/100] testing 35.1% loss=0.12421, acc=0.95312
# [45/100] testing 36.0% loss=0.25050, acc=0.90625
# [45/100] testing 36.4% loss=0.27208, acc=0.87500
# [45/100] testing 37.3% loss=0.22299, acc=0.96875
# [45/100] testing 37.7% loss=0.46111, acc=0.84375
# [45/100] testing 38.6% loss=0.19835, acc=0.93750
# [45/100] testing 39.5% loss=0.20406, acc=0.96875
# [45/100] testing 39.9% loss=0.20375, acc=0.90625
# [45/100] testing 40.8% loss=0.32448, acc=0.90625
# [45/100] testing 41.2% loss=0.23131, acc=0.96875
# [45/100] testing 42.1% loss=0.19732, acc=0.89062
# [45/100] testing 42.5% loss=0.19697, acc=0.92188
# [45/100] testing 43.4% loss=0.29184, acc=0.90625
# [45/100] testing 43.9% loss=0.09372, acc=0.98438
# [45/100] testing 44.7% loss=0.29522, acc=0.87500
# [45/100] testing 45.6% loss=0.16403, acc=0.93750
# [45/100] testing 46.1% loss=0.31627, acc=0.87500
# [45/100] testing 46.9% loss=0.19282, acc=0.93750
# [45/100] testing 47.4% loss=0.11181, acc=0.95312
# [45/100] testing 48.3% loss=0.38765, acc=0.89062
# [45/100] testing 48.7% loss=0.37597, acc=0.85938
# [45/100] testing 49.6% loss=0.46192, acc=0.82812
# [45/100] testing 50.4% loss=0.16031, acc=0.90625
# [45/100] testing 50.9% loss=0.30312, acc=0.90625
# [45/100] testing 51.8% loss=0.26233, acc=0.89062
# [45/100] testing 52.2% loss=0.20488, acc=0.89062
# [45/100] testing 53.1% loss=0.20022, acc=0.92188
# [45/100] testing 53.5% loss=0.18457, acc=0.93750
# [45/100] testing 54.4% loss=0.26358, acc=0.89062
# [45/100] testing 54.8% loss=0.33564, acc=0.90625
# [45/100] testing 55.7% loss=0.14835, acc=0.95312
# [45/100] testing 56.6% loss=0.29461, acc=0.85938
# [45/100] testing 57.0% loss=0.33676, acc=0.87500
# [45/100] testing 57.9% loss=0.24790, acc=0.89062
# [45/100] testing 58.3% loss=0.34989, acc=0.87500
# [45/100] testing 59.2% loss=0.27067, acc=0.89062
# [45/100] testing 59.7% loss=0.23621, acc=0.87500
# [45/100] testing 60.5% loss=0.40072, acc=0.85938
# [45/100] testing 61.4% loss=0.12847, acc=0.95312
# [45/100] testing 61.9% loss=0.18236, acc=0.93750
# [45/100] testing 62.7% loss=0.18184, acc=0.90625
# [45/100] testing 63.2% loss=0.30289, acc=0.87500
# [45/100] testing 64.0% loss=0.35207, acc=0.89062
# [45/100] testing 64.5% loss=0.22302, acc=0.89062
# [45/100] testing 65.4% loss=0.14032, acc=0.95312
# [45/100] testing 65.8% loss=0.30392, acc=0.85938
# [45/100] testing 66.7% loss=0.15195, acc=0.92188
# [45/100] testing 67.6% loss=0.44637, acc=0.89062
# [45/100] testing 68.0% loss=0.10300, acc=0.95312
# [45/100] testing 68.9% loss=0.25250, acc=0.95312
# [45/100] testing 69.3% loss=0.27750, acc=0.87500
# [45/100] testing 70.2% loss=0.26701, acc=0.85938
# [45/100] testing 70.6% loss=0.33116, acc=0.89062
# [45/100] testing 71.5% loss=0.27814, acc=0.90625
# [45/100] testing 72.4% loss=0.17037, acc=0.92188
# [45/100] testing 72.8% loss=0.23094, acc=0.87500
# [45/100] testing 73.7% loss=0.14403, acc=0.95312
# [45/100] testing 74.1% loss=0.38818, acc=0.87500
# [45/100] testing 75.0% loss=0.19648, acc=0.90625
# [45/100] testing 75.4% loss=0.42624, acc=0.85938
# [45/100] testing 76.3% loss=0.06131, acc=1.00000
# [45/100] testing 76.8% loss=0.21109, acc=0.90625
# [45/100] testing 77.6% loss=0.21151, acc=0.89062
# [45/100] testing 78.5% loss=0.31673, acc=0.89062
# [45/100] testing 79.0% loss=0.21670, acc=0.92188
# [45/100] testing 79.8% loss=0.24045, acc=0.89062
# [45/100] testing 80.3% loss=0.27709, acc=0.89062
# [45/100] testing 81.2% loss=0.39283, acc=0.85938
# [45/100] testing 81.6% loss=0.22038, acc=0.92188
# [45/100] testing 82.5% loss=0.16552, acc=0.90625
# [45/100] testing 83.3% loss=0.21738, acc=0.92188
# [45/100] testing 83.8% loss=0.17888, acc=0.96875
# [45/100] testing 84.7% loss=0.33877, acc=0.82812
# [45/100] testing 85.1% loss=0.23930, acc=0.90625
# [45/100] testing 86.0% loss=0.24691, acc=0.90625
# [45/100] testing 86.4% loss=0.42748, acc=0.84375
# [45/100] testing 87.3% loss=0.26735, acc=0.89062
# [45/100] testing 87.7% loss=0.19402, acc=0.89062
# [45/100] testing 88.6% loss=0.21681, acc=0.90625
# [45/100] testing 89.5% loss=0.51479, acc=0.76562
# [45/100] testing 89.9% loss=0.15911, acc=0.92188
# [45/100] testing 90.8% loss=0.33507, acc=0.92188
# [45/100] testing 91.2% loss=0.16858, acc=0.93750
# [45/100] testing 92.1% loss=0.31540, acc=0.89062
# [45/100] testing 92.6% loss=0.46198, acc=0.82812
# [45/100] testing 93.4% loss=0.43349, acc=0.81250
# [45/100] testing 94.3% loss=0.08546, acc=0.98438
# [45/100] testing 94.7% loss=0.22270, acc=0.90625
# [45/100] testing 95.6% loss=0.43598, acc=0.81250
# [45/100] testing 96.1% loss=0.21226, acc=0.90625
# [45/100] testing 96.9% loss=0.24445, acc=0.87500
# [45/100] testing 97.4% loss=0.11549, acc=0.95312
# [45/100] testing 98.3% loss=0.17710, acc=0.89062
# [45/100] testing 98.7% loss=0.22793, acc=0.90625
# [45/100] testing 99.6% loss=0.29381, acc=0.89062
# [46/100] training 0.2% loss=0.28825, acc=0.84375
# [46/100] training 0.4% loss=0.34441, acc=0.85938
# [46/100] training 0.5% loss=0.08563, acc=0.98438
# [46/100] training 0.8% loss=0.14723, acc=0.90625
# [46/100] training 0.9% loss=0.16932, acc=0.92188
# [46/100] training 1.1% loss=0.21702, acc=0.95312
# [46/100] training 1.2% loss=0.17729, acc=0.95312
# [46/100] training 1.4% loss=0.23605, acc=0.89062
# [46/100] training 1.6% loss=0.09954, acc=0.98438
# [46/100] training 1.8% loss=0.22877, acc=0.87500
# [46/100] training 2.0% loss=0.27263, acc=0.85938
# [46/100] training 2.1% loss=0.16231, acc=0.90625
# [46/100] training 2.3% loss=0.15129, acc=0.92188
# [46/100] training 2.4% loss=0.23182, acc=0.90625
# [46/100] training 2.6% loss=0.06494, acc=1.00000
# [46/100] training 2.7% loss=0.30696, acc=0.90625
# [46/100] training 3.0% loss=0.23337, acc=0.89062
# [46/100] training 3.2% loss=0.11499, acc=0.93750
# [46/100] training 3.3% loss=0.22644, acc=0.90625
# [46/100] training 3.5% loss=0.13579, acc=0.93750
# [46/100] training 3.6% loss=0.27570, acc=0.87500
# [46/100] training 3.8% loss=0.13536, acc=0.95312
# [46/100] training 3.9% loss=0.17020, acc=0.95312
# [46/100] training 4.2% loss=0.10090, acc=0.95312
# [46/100] training 4.4% loss=0.16812, acc=0.93750
# [46/100] training 4.5% loss=0.11068, acc=0.93750
# [46/100] training 4.7% loss=0.26416, acc=0.87500
# [46/100] training 4.8% loss=0.24434, acc=0.92188
# [46/100] training 5.0% loss=0.04838, acc=0.98438
# [46/100] training 5.2% loss=0.15576, acc=0.92188
# [46/100] training 5.4% loss=0.10629, acc=0.95312
# [46/100] training 5.5% loss=0.19494, acc=0.93750
# [46/100] training 5.7% loss=0.18125, acc=0.92188
# [46/100] training 5.9% loss=0.13430, acc=0.95312
# [46/100] training 6.0% loss=0.28466, acc=0.89062
# [46/100] training 6.3% loss=0.17783, acc=0.92188
# [46/100] training 6.4% loss=0.12569, acc=0.96875
# [46/100] training 6.6% loss=0.24573, acc=0.89062
# [46/100] training 6.7% loss=0.29541, acc=0.87500
# [46/100] training 6.9% loss=0.16422, acc=0.90625
# [46/100] training 7.1% loss=0.10734, acc=0.96875
# [46/100] training 7.2% loss=0.25332, acc=0.90625
# [46/100] training 7.5% loss=0.17646, acc=0.93750
# [46/100] training 7.6% loss=0.25065, acc=0.89062
# [46/100] training 7.8% loss=0.20044, acc=0.90625
# [46/100] training 7.9% loss=0.16655, acc=0.92188
# [46/100] training 8.1% loss=0.09312, acc=0.98438
# [46/100] training 8.2% loss=0.26354, acc=0.89062
# [46/100] training 8.4% loss=0.12901, acc=0.95312
# [46/100] training 8.7% loss=0.13509, acc=0.93750
# [46/100] training 8.8% loss=0.22844, acc=0.89062
# [46/100] training 9.0% loss=0.13182, acc=0.95312
# [46/100] training 9.1% loss=0.21063, acc=0.93750
# [46/100] training 9.3% loss=0.33473, acc=0.87500
# [46/100] training 9.4% loss=0.13498, acc=0.93750
# [46/100] training 9.7% loss=0.13773, acc=0.96875
# [46/100] training 9.9% loss=0.34135, acc=0.78125
# [46/100] training 10.0% loss=0.28851, acc=0.89062
# [46/100] training 10.2% loss=0.15011, acc=0.93750
# [46/100] training 10.3% loss=0.15057, acc=0.95312
# [46/100] training 10.5% loss=0.24621, acc=0.93750
# [46/100] training 10.6% loss=0.21153, acc=0.93750
# [46/100] training 10.9% loss=0.17225, acc=0.93750
# [46/100] training 11.0% loss=0.31142, acc=0.87500
# [46/100] training 11.2% loss=0.13199, acc=0.95312
# [46/100] training 11.4% loss=0.24187, acc=0.87500
# [46/100] training 11.5% loss=0.34065, acc=0.84375
# [46/100] training 11.7% loss=0.13442, acc=0.98438
# [46/100] training 11.8% loss=0.14946, acc=0.93750
# [46/100] training 12.1% loss=0.18781, acc=0.90625
# [46/100] training 12.2% loss=0.07460, acc=0.96875
# [46/100] training 12.4% loss=0.13702, acc=0.93750
# [46/100] training 12.6% loss=0.28828, acc=0.93750
# [46/100] training 12.7% loss=0.14601, acc=0.93750
# [46/100] training 12.9% loss=0.25985, acc=0.89062
# [46/100] training 13.0% loss=0.15755, acc=0.93750
# [46/100] training 13.3% loss=0.25125, acc=0.90625
# [46/100] training 13.4% loss=0.23060, acc=0.92188
# [46/100] training 13.6% loss=0.17512, acc=0.92188
# [46/100] training 13.7% loss=0.32097, acc=0.89062
# [46/100] training 13.9% loss=0.23333, acc=0.92188
# [46/100] training 14.1% loss=0.22878, acc=0.92188
# [46/100] training 14.3% loss=0.18562, acc=0.90625
# [46/100] training 14.5% loss=0.26607, acc=0.90625
# [46/100] training 14.6% loss=0.16495, acc=0.93750
# [46/100] training 14.8% loss=0.23552, acc=0.89062
# [46/100] training 14.9% loss=0.15378, acc=0.92188
# [46/100] training 15.1% loss=0.37522, acc=0.87500
# [46/100] training 15.4% loss=0.25013, acc=0.87500
# [46/100] training 15.5% loss=0.21968, acc=0.89062
# [46/100] training 15.7% loss=0.24828, acc=0.90625
# [46/100] training 15.8% loss=0.17784, acc=0.92188
# [46/100] training 16.0% loss=0.27001, acc=0.85938
# [46/100] training 16.1% loss=0.25017, acc=0.90625
# [46/100] training 16.3% loss=0.21894, acc=0.90625
# [46/100] training 16.4% loss=0.14438, acc=0.93750
# [46/100] training 16.7% loss=0.20426, acc=0.89062
# [46/100] training 16.9% loss=0.17700, acc=0.93750
# [46/100] training 17.0% loss=0.19331, acc=0.90625
# [46/100] training 17.2% loss=0.23499, acc=0.92188
# [46/100] training 17.3% loss=0.18006, acc=0.92188
# [46/100] training 17.5% loss=0.23922, acc=0.90625
# [46/100] training 17.7% loss=0.15232, acc=0.92188
# [46/100] training 17.9% loss=0.26459, acc=0.90625
# [46/100] training 18.1% loss=0.22947, acc=0.90625
# [46/100] training 18.2% loss=0.15045, acc=0.92188
# [46/100] training 18.4% loss=0.32881, acc=0.84375
# [46/100] training 18.5% loss=0.19219, acc=0.93750
# [46/100] training 18.8% loss=0.16760, acc=0.95312
# [46/100] training 18.9% loss=0.12961, acc=0.93750
# [46/100] training 19.1% loss=0.26425, acc=0.90625
# [46/100] training 19.2% loss=0.17134, acc=0.93750
# [46/100] training 19.4% loss=0.13420, acc=0.93750
# [46/100] training 19.6% loss=0.21994, acc=0.89062
# [46/100] training 19.7% loss=0.29148, acc=0.85938
# [46/100] training 20.0% loss=0.14459, acc=0.95312
# [46/100] training 20.1% loss=0.24166, acc=0.92188
# [46/100] training 20.3% loss=0.28046, acc=0.89062
# [46/100] training 20.4% loss=0.17724, acc=0.90625
# [46/100] training 20.6% loss=0.25162, acc=0.89062
# [46/100] training 20.8% loss=0.14537, acc=0.93750
# [46/100] training 20.9% loss=0.17596, acc=0.93750
# [46/100] training 21.2% loss=0.24159, acc=0.90625
# [46/100] training 21.3% loss=0.21708, acc=0.89062
# [46/100] training 21.5% loss=0.23317, acc=0.93750
# [46/100] training 21.6% loss=0.10661, acc=0.95312
# [46/100] training 21.8% loss=0.11639, acc=0.96875
# [46/100] training 21.9% loss=0.16792, acc=0.93750
# [46/100] training 22.2% loss=0.12707, acc=0.93750
# [46/100] training 22.4% loss=0.19567, acc=0.89062
# [46/100] training 22.5% loss=0.17742, acc=0.92188
# [46/100] training 22.7% loss=0.12015, acc=0.96875
# [46/100] training 22.8% loss=0.17234, acc=0.90625
# [46/100] training 23.0% loss=0.11398, acc=0.96875
# [46/100] training 23.1% loss=0.32477, acc=0.89062
# [46/100] training 23.4% loss=0.26517, acc=0.87500
# [46/100] training 23.6% loss=0.21191, acc=0.93750
# [46/100] training 23.7% loss=0.08691, acc=0.98438
# [46/100] training 23.9% loss=0.17049, acc=0.92188
# [46/100] training 24.0% loss=0.11060, acc=0.93750
# [46/100] training 24.2% loss=0.18580, acc=0.92188
# [46/100] training 24.3% loss=0.26161, acc=0.92188
# [46/100] training 24.6% loss=0.24823, acc=0.92188
# [46/100] training 24.7% loss=0.30488, acc=0.90625
# [46/100] training 24.9% loss=0.16453, acc=0.92188
# [46/100] training 25.1% loss=0.16884, acc=0.92188
# [46/100] training 25.2% loss=0.13052, acc=0.93750
# [46/100] training 25.4% loss=0.18066, acc=0.92188
# [46/100] training 25.6% loss=0.18876, acc=0.92188
# [46/100] training 25.8% loss=0.23159, acc=0.93750
# [46/100] training 25.9% loss=0.16485, acc=0.93750
# [46/100] training 26.1% loss=0.13529, acc=0.95312
# [46/100] training 26.3% loss=0.14862, acc=0.93750
# [46/100] training 26.4% loss=0.14547, acc=0.95312
# [46/100] training 26.6% loss=0.08609, acc=0.96875
# [46/100] training 26.8% loss=0.09655, acc=0.95312
# [46/100] training 27.0% loss=0.09963, acc=0.95312
# [46/100] training 27.1% loss=0.16027, acc=0.93750
# [46/100] training 27.3% loss=0.12745, acc=0.95312
# [46/100] training 27.4% loss=0.03779, acc=1.00000
# [46/100] training 27.6% loss=0.27003, acc=0.93750
# [46/100] training 27.9% loss=0.09645, acc=0.95312
# [46/100] training 28.0% loss=0.31756, acc=0.85938
# [46/100] training 28.2% loss=0.17418, acc=0.93750
# [46/100] training 28.3% loss=0.06702, acc=0.96875
# [46/100] training 28.5% loss=0.20889, acc=0.92188
# [46/100] training 28.6% loss=0.19179, acc=0.95312
# [46/100] training 28.8% loss=0.12042, acc=0.96875
# [46/100] training 29.1% loss=0.17305, acc=0.95312
# [46/100] training 29.2% loss=0.16859, acc=0.93750
# [46/100] training 29.4% loss=0.23018, acc=0.89062
# [46/100] training 29.5% loss=0.14486, acc=0.93750
# [46/100] training 29.7% loss=0.30360, acc=0.87500
# [46/100] training 29.8% loss=0.17403, acc=0.93750
# [46/100] training 30.0% loss=0.19040, acc=0.93750
# [46/100] training 30.2% loss=0.20839, acc=0.92188
# [46/100] training 30.4% loss=0.07893, acc=0.96875
# [46/100] training 30.6% loss=0.20717, acc=0.92188
# [46/100] training 30.7% loss=0.21216, acc=0.93750
# [46/100] training 30.9% loss=0.28968, acc=0.92188
# [46/100] training 31.0% loss=0.10607, acc=0.96875
# [46/100] training 31.3% loss=0.12682, acc=0.95312
# [46/100] training 31.4% loss=0.29287, acc=0.81250
# [46/100] training 31.6% loss=0.21255, acc=0.90625
# [46/100] training 31.8% loss=0.21133, acc=0.90625
# [46/100] training 31.9% loss=0.22626, acc=0.93750
# [46/100] training 32.1% loss=0.14676, acc=0.95312
# [46/100] training 32.2% loss=0.32551, acc=0.90625
# [46/100] training 32.5% loss=0.13632, acc=0.93750
# [46/100] training 32.6% loss=0.17960, acc=0.92188
# [46/100] training 32.8% loss=0.15082, acc=0.95312
# [46/100] training 32.9% loss=0.19310, acc=0.93750
# [46/100] training 33.1% loss=0.12763, acc=0.96875
# [46/100] training 33.3% loss=0.24750, acc=0.90625
# [46/100] training 33.4% loss=0.16465, acc=0.93750
# [46/100] training 33.7% loss=0.14151, acc=0.96875
# [46/100] training 33.8% loss=0.17918, acc=0.92188
# [46/100] training 34.0% loss=0.14927, acc=0.93750
# [46/100] training 34.1% loss=0.18550, acc=0.95312
# [46/100] training 34.3% loss=0.20624, acc=0.95312
# [46/100] training 34.5% loss=0.35796, acc=0.87500
# [46/100] training 34.7% loss=0.17804, acc=0.95312
# [46/100] training 34.9% loss=0.13320, acc=0.92188
# [46/100] training 35.0% loss=0.12633, acc=0.95312
# [46/100] training 35.2% loss=0.16038, acc=0.92188
# [46/100] training 35.3% loss=0.19189, acc=0.87500
# [46/100] training 35.5% loss=0.17022, acc=0.95312
# [46/100] training 35.6% loss=0.51623, acc=0.85938
# [46/100] training 35.9% loss=0.17270, acc=0.92188
# [46/100] training 36.1% loss=0.34669, acc=0.84375
# [46/100] training 36.2% loss=0.14195, acc=0.96875
# [46/100] training 36.4% loss=0.18923, acc=0.95312
# [46/100] training 36.5% loss=0.26457, acc=0.90625
# [46/100] training 36.7% loss=0.19373, acc=0.90625
# [46/100] training 36.8% loss=0.13596, acc=0.93750
# [46/100] training 37.1% loss=0.21478, acc=0.92188
# [46/100] training 37.3% loss=0.18489, acc=0.89062
# [46/100] training 37.4% loss=0.12916, acc=0.95312
# [46/100] training 37.6% loss=0.14692, acc=0.96875
# [46/100] training 37.7% loss=0.18380, acc=0.92188
# [46/100] training 37.9% loss=0.20580, acc=0.92188
# [46/100] training 38.1% loss=0.24755, acc=0.90625
# [46/100] training 38.3% loss=0.14437, acc=0.96875
# [46/100] training 38.4% loss=0.08154, acc=0.96875
# [46/100] training 38.6% loss=0.14001, acc=0.96875
# [46/100] training 38.8% loss=0.19402, acc=0.90625
# [46/100] training 38.9% loss=0.14246, acc=0.93750
# [46/100] training 39.1% loss=0.21709, acc=0.92188
# [46/100] training 39.3% loss=0.27779, acc=0.89062
# [46/100] training 39.5% loss=0.28711, acc=0.89062
# [46/100] training 39.6% loss=0.14088, acc=0.95312
# [46/100] training 39.8% loss=0.09857, acc=0.96875
# [46/100] training 40.0% loss=0.20384, acc=0.93750
# [46/100] training 40.1% loss=0.28285, acc=0.89062
# [46/100] training 40.4% loss=0.12815, acc=0.95312
# [46/100] training 40.5% loss=0.21580, acc=0.90625
# [46/100] training 40.7% loss=0.20579, acc=0.93750
# [46/100] training 40.8% loss=0.14818, acc=0.93750
# [46/100] training 41.0% loss=0.12075, acc=0.93750
# [46/100] training 41.1% loss=0.21212, acc=0.89062
# [46/100] training 41.3% loss=0.19812, acc=0.92188
# [46/100] training 41.6% loss=0.16871, acc=0.92188
# [46/100] training 41.7% loss=0.21490, acc=0.90625
# [46/100] training 41.9% loss=0.17304, acc=0.92188
# [46/100] training 42.0% loss=0.18354, acc=0.95312
# [46/100] training 42.2% loss=0.18344, acc=0.92188
# [46/100] training 42.3% loss=0.16298, acc=0.93750
# [46/100] training 42.5% loss=0.12126, acc=0.95312
# [46/100] training 42.8% loss=0.09783, acc=0.95312
# [46/100] training 42.9% loss=0.15806, acc=0.90625
# [46/100] training 43.1% loss=0.13076, acc=0.96875
# [46/100] training 43.2% loss=0.10812, acc=0.95312
# [46/100] training 43.4% loss=0.18697, acc=0.93750
# [46/100] training 43.5% loss=0.22027, acc=0.92188
# [46/100] training 43.8% loss=0.25214, acc=0.89062
# [46/100] training 43.9% loss=0.17089, acc=0.93750
# [46/100] training 44.1% loss=0.16375, acc=0.98438
# [46/100] training 44.3% loss=0.20316, acc=0.93750
# [46/100] training 44.4% loss=0.26190, acc=0.84375
# [46/100] training 44.6% loss=0.19387, acc=0.90625
# [46/100] training 44.7% loss=0.21715, acc=0.90625
# [46/100] training 45.0% loss=0.09363, acc=0.96875
# [46/100] training 45.1% loss=0.30548, acc=0.85938
# [46/100] training 45.3% loss=0.24862, acc=0.85938
# [46/100] training 45.5% loss=0.08762, acc=0.98438
# [46/100] training 45.6% loss=0.20987, acc=0.92188
# [46/100] training 45.8% loss=0.11643, acc=0.96875
# [46/100] training 45.9% loss=0.15993, acc=0.92188
# [46/100] training 46.2% loss=0.12547, acc=0.95312
# [46/100] training 46.3% loss=0.11395, acc=0.96875
# [46/100] training 46.5% loss=0.31101, acc=0.89062
# [46/100] training 46.6% loss=0.19996, acc=0.93750
# [46/100] training 46.8% loss=0.20679, acc=0.90625
# [46/100] training 47.0% loss=0.18082, acc=0.92188
# [46/100] training 47.2% loss=0.19689, acc=0.93750
# [46/100] training 47.4% loss=0.13614, acc=0.96875
# [46/100] training 47.5% loss=0.31811, acc=0.87500
# [46/100] training 47.7% loss=0.13837, acc=0.92188
# [46/100] training 47.8% loss=0.23654, acc=0.90625
# [46/100] training 48.0% loss=0.16267, acc=0.92188
# [46/100] training 48.3% loss=0.11542, acc=0.95312
# [46/100] training 48.4% loss=0.08478, acc=0.96875
# [46/100] training 48.6% loss=0.13797, acc=0.96875
# [46/100] training 48.7% loss=0.27736, acc=0.90625
# [46/100] training 48.9% loss=0.17693, acc=0.93750
# [46/100] training 49.0% loss=0.09590, acc=0.98438
# [46/100] training 49.2% loss=0.28018, acc=0.84375
# [46/100] training 49.3% loss=0.14432, acc=0.95312
# [46/100] training 49.6% loss=0.22465, acc=0.90625
# [46/100] training 49.8% loss=0.22741, acc=0.92188
# [46/100] training 49.9% loss=0.23206, acc=0.92188
# [46/100] training 50.1% loss=0.10834, acc=0.95312
# [46/100] training 50.2% loss=0.23514, acc=0.95312
# [46/100] training 50.4% loss=0.31166, acc=0.87500
# [46/100] training 50.6% loss=0.18266, acc=0.90625
# [46/100] training 50.8% loss=0.28482, acc=0.89062
# [46/100] training 51.0% loss=0.29500, acc=0.89062
# [46/100] training 51.1% loss=0.16925, acc=0.93750
# [46/100] training 51.3% loss=0.23192, acc=0.90625
# [46/100] training 51.4% loss=0.17004, acc=0.95312
# [46/100] training 51.7% loss=0.22484, acc=0.89062
# [46/100] training 51.8% loss=0.27893, acc=0.85938
# [46/100] training 52.0% loss=0.23584, acc=0.89062
# [46/100] training 52.1% loss=0.15297, acc=0.95312
# [46/100] training 52.3% loss=0.24527, acc=0.90625
# [46/100] training 52.5% loss=0.07141, acc=1.00000
# [46/100] training 52.6% loss=0.23032, acc=0.92188
# [46/100] training 52.9% loss=0.25119, acc=0.90625
# [46/100] training 53.0% loss=0.15779, acc=0.95312
# [46/100] training 53.2% loss=0.27099, acc=0.92188
# [46/100] training 53.3% loss=0.15353, acc=0.95312
# [46/100] training 53.5% loss=0.14513, acc=0.90625
# [46/100] training 53.7% loss=0.06626, acc=0.98438
# [46/100] training 53.8% loss=0.28012, acc=0.89062
# [46/100] training 54.1% loss=0.24879, acc=0.89062
# [46/100] training 54.2% loss=0.08572, acc=0.98438
# [46/100] training 54.4% loss=0.17681, acc=0.95312
# [46/100] training 54.5% loss=0.24604, acc=0.89062
# [46/100] training 54.7% loss=0.36615, acc=0.81250
# [46/100] training 54.8% loss=0.12104, acc=0.95312
# [46/100] training 55.1% loss=0.13553, acc=0.95312
# [46/100] training 55.3% loss=0.17404, acc=0.92188
# [46/100] training 55.4% loss=0.12716, acc=0.95312
# [46/100] training 55.6% loss=0.13591, acc=0.93750
# [46/100] training 55.7% loss=0.15789, acc=0.93750
# [46/100] training 55.9% loss=0.16132, acc=0.93750
# [46/100] training 56.0% loss=0.05924, acc=0.98438
# [46/100] training 56.3% loss=0.26873, acc=0.92188
# [46/100] training 56.5% loss=0.19925, acc=0.90625
# [46/100] training 56.6% loss=0.15736, acc=0.95312
# [46/100] training 56.8% loss=0.18359, acc=0.90625
# [46/100] training 56.9% loss=0.11706, acc=0.95312
# [46/100] training 57.1% loss=0.23791, acc=0.92188
# [46/100] training 57.2% loss=0.20424, acc=0.89062
# [46/100] training 57.5% loss=0.16693, acc=0.90625
# [46/100] training 57.6% loss=0.14738, acc=0.92188
# [46/100] training 57.8% loss=0.11279, acc=0.96875
# [46/100] training 58.0% loss=0.18340, acc=0.95312
# [46/100] training 58.1% loss=0.10915, acc=0.96875
# [46/100] training 58.3% loss=0.13557, acc=0.93750
# [46/100] training 58.4% loss=0.27480, acc=0.90625
# [46/100] training 58.7% loss=0.24996, acc=0.93750
# [46/100] training 58.8% loss=0.25465, acc=0.90625
# [46/100] training 59.0% loss=0.13318, acc=0.92188
# [46/100] training 59.2% loss=0.18880, acc=0.92188
# [46/100] training 59.3% loss=0.19167, acc=0.93750
# [46/100] training 59.5% loss=0.22080, acc=0.95312
# [46/100] training 59.7% loss=0.18282, acc=0.96875
# [46/100] training 59.9% loss=0.15310, acc=0.96875
# [46/100] training 60.0% loss=0.18480, acc=0.90625
# [46/100] training 60.2% loss=0.20540, acc=0.92188
# [46/100] training 60.3% loss=0.09508, acc=0.96875
# [46/100] training 60.5% loss=0.19193, acc=0.92188
# [46/100] training 60.8% loss=0.30528, acc=0.87500
# [46/100] training 60.9% loss=0.21346, acc=0.89062
# [46/100] training 61.1% loss=0.15752, acc=0.92188
# [46/100] training 61.2% loss=0.15946, acc=0.95312
# [46/100] training 61.4% loss=0.32101, acc=0.90625
# [46/100] training 61.5% loss=0.50297, acc=0.82812
# [46/100] training 61.7% loss=0.20650, acc=0.89062
# [46/100] training 62.0% loss=0.33425, acc=0.84375
# [46/100] training 62.1% loss=0.25168, acc=0.89062
# [46/100] training 62.3% loss=0.14115, acc=0.95312
# [46/100] training 62.4% loss=0.23950, acc=0.90625
# [46/100] training 62.6% loss=0.28496, acc=0.82812
# [46/100] training 62.7% loss=0.12252, acc=0.96875
# [46/100] training 62.9% loss=0.15213, acc=0.95312
# [46/100] training 63.1% loss=0.19472, acc=0.93750
# [46/100] training 63.3% loss=0.15983, acc=0.93750
# [46/100] training 63.5% loss=0.23307, acc=0.92188
# [46/100] training 63.6% loss=0.21732, acc=0.93750
# [46/100] training 63.8% loss=0.27752, acc=0.89062
# [46/100] training 63.9% loss=0.10561, acc=0.96875
# [46/100] training 64.2% loss=0.14977, acc=0.92188
# [46/100] training 64.3% loss=0.18170, acc=0.90625
# [46/100] training 64.5% loss=0.15684, acc=0.89062
# [46/100] training 64.7% loss=0.10221, acc=0.96875
# [46/100] training 64.8% loss=0.34865, acc=0.90625
# [46/100] training 65.0% loss=0.19784, acc=0.90625
# [46/100] training 65.1% loss=0.17317, acc=0.90625
# [46/100] training 65.4% loss=0.14565, acc=0.95312
# [46/100] training 65.5% loss=0.09368, acc=0.96875
# [46/100] training 65.7% loss=0.15735, acc=0.93750
# [46/100] training 65.8% loss=0.22741, acc=0.92188
# [46/100] training 66.0% loss=0.19832, acc=0.93750
# [46/100] training 66.2% loss=0.09182, acc=0.96875
# [46/100] training 66.3% loss=0.16986, acc=0.92188
# [46/100] training 66.6% loss=0.16434, acc=0.95312
# [46/100] training 66.7% loss=0.14104, acc=0.95312
# [46/100] training 66.9% loss=0.15348, acc=0.92188
# [46/100] training 67.0% loss=0.11253, acc=0.95312
# [46/100] training 67.2% loss=0.10726, acc=0.96875
# [46/100] training 67.4% loss=0.12752, acc=0.95312
# [46/100] training 67.6% loss=0.09225, acc=0.95312
# [46/100] training 67.8% loss=0.11039, acc=0.98438
# [46/100] training 67.9% loss=0.09956, acc=0.93750
# [46/100] training 68.1% loss=0.14412, acc=0.93750
# [46/100] training 68.2% loss=0.09629, acc=0.93750
# [46/100] training 68.4% loss=0.14286, acc=0.93750
# [46/100] training 68.5% loss=0.23878, acc=0.92188
# [46/100] training 68.8% loss=0.07928, acc=0.95312
# [46/100] training 69.0% loss=0.26665, acc=0.90625
# [46/100] training 69.1% loss=0.22089, acc=0.93750
# [46/100] training 69.3% loss=0.12473, acc=0.93750
# [46/100] training 69.4% loss=0.28494, acc=0.92188
# [46/100] training 69.6% loss=0.15340, acc=0.95312
# [46/100] training 69.7% loss=0.19406, acc=0.93750
# [46/100] training 70.0% loss=0.28827, acc=0.85938
# [46/100] training 70.2% loss=0.22174, acc=0.89062
# [46/100] training 70.3% loss=0.20762, acc=0.89062
# [46/100] training 70.5% loss=0.23730, acc=0.89062
# [46/100] training 70.6% loss=0.14500, acc=0.93750
# [46/100] training 70.8% loss=0.25242, acc=0.87500
# [46/100] training 71.0% loss=0.11912, acc=0.93750
# [46/100] training 71.2% loss=0.13815, acc=0.95312
# [46/100] training 71.3% loss=0.07429, acc=0.98438
# [46/100] training 71.5% loss=0.22300, acc=0.93750
# [46/100] training 71.7% loss=0.22498, acc=0.87500
# [46/100] training 71.8% loss=0.19330, acc=0.92188
# [46/100] training 72.0% loss=0.06980, acc=0.96875
# [46/100] training 72.2% loss=0.20048, acc=0.90625
# [46/100] training 72.4% loss=0.24837, acc=0.89062
# [46/100] training 72.5% loss=0.18539, acc=0.90625
# [46/100] training 72.7% loss=0.40736, acc=0.84375
# [46/100] training 72.9% loss=0.14471, acc=0.93750
# [46/100] training 73.0% loss=0.10209, acc=0.93750
# [46/100] training 73.3% loss=0.32284, acc=0.84375
# [46/100] training 73.4% loss=0.15082, acc=0.92188
# [46/100] training 73.6% loss=0.12296, acc=0.95312
# [46/100] training 73.7% loss=0.13109, acc=0.92188
# [46/100] training 73.9% loss=0.16686, acc=0.93750
# [46/100] training 74.0% loss=0.28139, acc=0.89062
# [46/100] training 74.2% loss=0.12077, acc=0.96875
# [46/100] training 74.5% loss=0.24820, acc=0.87500
# [46/100] training 74.6% loss=0.27407, acc=0.89062
# [46/100] training 74.8% loss=0.29148, acc=0.92188
# [46/100] training 74.9% loss=0.23719, acc=0.90625
# [46/100] training 75.1% loss=0.16574, acc=0.95312
# [46/100] training 75.2% loss=0.16755, acc=0.96875
# [46/100] training 75.4% loss=0.12731, acc=0.93750
# [46/100] training 75.7% loss=0.22133, acc=0.87500
# [46/100] training 75.8% loss=0.14146, acc=0.92188
# [46/100] training 76.0% loss=0.07654, acc=0.96875
# [46/100] training 76.1% loss=0.24420, acc=0.92188
# [46/100] training 76.3% loss=0.07800, acc=0.98438
# [46/100] training 76.4% loss=0.25505, acc=0.85938
# [46/100] training 76.7% loss=0.14284, acc=0.93750
# [46/100] training 76.8% loss=0.16374, acc=0.96875
# [46/100] training 77.0% loss=0.15034, acc=0.96875
# [46/100] training 77.2% loss=0.10686, acc=0.95312
# [46/100] training 77.3% loss=0.08791, acc=0.96875
# [46/100] training 77.5% loss=0.16095, acc=0.92188
# [46/100] training 77.6% loss=0.24366, acc=0.90625
# [46/100] training 77.9% loss=0.15701, acc=0.93750
# [46/100] training 78.0% loss=0.23509, acc=0.87500
# [46/100] training 78.2% loss=0.21777, acc=0.92188
# [46/100] training 78.4% loss=0.11036, acc=0.95312
# [46/100] training 78.5% loss=0.21498, acc=0.89062
# [46/100] training 78.7% loss=0.28037, acc=0.90625
# [46/100] training 78.8% loss=0.12144, acc=0.96875
# [46/100] training 79.1% loss=0.17509, acc=0.92188
# [46/100] training 79.2% loss=0.17548, acc=0.93750
# [46/100] training 79.4% loss=0.19693, acc=0.92188
# [46/100] training 79.5% loss=0.13692, acc=0.96875
# [46/100] training 79.7% loss=0.09853, acc=0.96875
# [46/100] training 79.9% loss=0.13088, acc=0.95312
# [46/100] training 80.1% loss=0.06472, acc=0.96875
# [46/100] training 80.3% loss=0.17619, acc=0.95312
# [46/100] training 80.4% loss=0.20756, acc=0.92188
# [46/100] training 80.6% loss=0.22297, acc=0.90625
# [46/100] training 80.7% loss=0.14002, acc=0.93750
# [46/100] training 80.9% loss=0.24394, acc=0.92188
# [46/100] training 81.2% loss=0.26682, acc=0.87500
# [46/100] training 81.3% loss=0.16178, acc=0.92188
# [46/100] training 81.5% loss=0.17327, acc=0.90625
# [46/100] training 81.6% loss=0.29088, acc=0.87500
# [46/100] training 81.8% loss=0.21388, acc=0.92188
# [46/100] training 81.9% loss=0.30966, acc=0.90625
# [46/100] training 82.1% loss=0.14001, acc=0.95312
# [46/100] training 82.2% loss=0.18070, acc=0.92188
# [46/100] training 82.5% loss=0.10458, acc=0.96875
# [46/100] training 82.7% loss=0.24247, acc=0.89062
# [46/100] training 82.8% loss=0.14051, acc=0.96875
# [46/100] training 83.0% loss=0.18716, acc=0.93750
# [46/100] training 83.1% loss=0.19316, acc=0.93750
# [46/100] training 83.3% loss=0.13265, acc=0.95312
# [46/100] training 83.5% loss=0.15540, acc=0.92188
# [46/100] training 83.7% loss=0.33902, acc=0.89062
# [46/100] training 83.9% loss=0.21030, acc=0.89062
# [46/100] training 84.0% loss=0.09555, acc=0.98438
# [46/100] training 84.2% loss=0.16918, acc=0.93750
# [46/100] training 84.3% loss=0.15991, acc=0.93750
# [46/100] training 84.5% loss=0.23875, acc=0.85938
# [46/100] training 84.7% loss=0.19919, acc=0.95312
# [46/100] training 84.9% loss=0.11767, acc=0.95312
# [46/100] training 85.0% loss=0.20030, acc=0.95312
# [46/100] training 85.2% loss=0.16014, acc=0.92188
# [46/100] training 85.4% loss=0.12911, acc=0.93750
# [46/100] training 85.5% loss=0.16766, acc=0.93750
# [46/100] training 85.8% loss=0.16189, acc=0.93750
# [46/100] training 85.9% loss=0.13675, acc=0.92188
# [46/100] training 86.1% loss=0.18611, acc=0.90625
# [46/100] training 86.2% loss=0.21086, acc=0.93750
# [46/100] training 86.4% loss=0.31778, acc=0.90625
# [46/100] training 86.6% loss=0.17902, acc=0.93750
# [46/100] training 86.7% loss=0.10348, acc=0.96875
# [46/100] training 87.0% loss=0.15373, acc=0.92188
# [46/100] training 87.1% loss=0.14962, acc=0.93750
# [46/100] training 87.3% loss=0.14659, acc=0.95312
# [46/100] training 87.4% loss=0.22138, acc=0.92188
# [46/100] training 87.6% loss=0.19148, acc=0.92188
# [46/100] training 87.7% loss=0.24060, acc=0.93750
# [46/100] training 87.9% loss=0.22140, acc=0.89062
# [46/100] training 88.2% loss=0.16531, acc=0.92188
# [46/100] training 88.3% loss=0.20399, acc=0.92188
# [46/100] training 88.5% loss=0.24358, acc=0.90625
# [46/100] training 88.6% loss=0.13882, acc=0.93750
# [46/100] training 88.8% loss=0.12274, acc=0.96875
# [46/100] training 88.9% loss=0.34217, acc=0.85938
# [46/100] training 89.2% loss=0.22366, acc=0.90625
# [46/100] training 89.4% loss=0.20019, acc=0.90625
# [46/100] training 89.5% loss=0.21626, acc=0.87500
# [46/100] training 89.7% loss=0.24432, acc=0.89062
# [46/100] training 89.8% loss=0.16020, acc=0.93750
# [46/100] training 90.0% loss=0.12962, acc=0.96875
# [46/100] training 90.1% loss=0.17247, acc=0.92188
# [46/100] training 90.4% loss=0.20470, acc=0.92188
# [46/100] training 90.5% loss=0.17667, acc=0.92188
# [46/100] training 90.7% loss=0.16669, acc=0.92188
# [46/100] training 90.9% loss=0.15899, acc=0.95312
# [46/100] training 91.0% loss=0.18343, acc=0.95312
# [46/100] training 91.2% loss=0.29682, acc=0.90625
# [46/100] training 91.3% loss=0.39679, acc=0.89062
# [46/100] training 91.6% loss=0.23968, acc=0.90625
# [46/100] training 91.7% loss=0.17854, acc=0.92188
# [46/100] training 91.9% loss=0.28915, acc=0.89062
# [46/100] training 92.1% loss=0.23051, acc=0.90625
# [46/100] training 92.2% loss=0.13878, acc=0.93750
# [46/100] training 92.4% loss=0.18923, acc=0.93750
# [46/100] training 92.6% loss=0.30331, acc=0.84375
# [46/100] training 92.8% loss=0.14736, acc=0.90625
# [46/100] training 92.9% loss=0.34689, acc=0.85938
# [46/100] training 93.1% loss=0.49270, acc=0.82812
# [46/100] training 93.2% loss=0.37097, acc=0.87500
# [46/100] training 93.4% loss=0.22488, acc=0.92188
# [46/100] training 93.7% loss=0.16315, acc=0.95312
# [46/100] training 93.8% loss=0.15714, acc=0.93750
# [46/100] training 94.0% loss=0.14409, acc=0.95312
# [46/100] training 94.1% loss=0.16224, acc=0.95312
# [46/100] training 94.3% loss=0.08502, acc=0.95312
# [46/100] training 94.4% loss=0.19937, acc=0.93750
# [46/100] training 94.6% loss=0.09834, acc=0.96875
# [46/100] training 94.9% loss=0.11895, acc=0.96875
# [46/100] training 95.0% loss=0.29315, acc=0.87500
# [46/100] training 95.2% loss=0.29145, acc=0.90625
# [46/100] training 95.3% loss=0.27377, acc=0.92188
# [46/100] training 95.5% loss=0.11870, acc=0.95312
# [46/100] training 95.6% loss=0.27536, acc=0.89062
# [46/100] training 95.8% loss=0.19564, acc=0.90625
# [46/100] training 96.0% loss=0.25947, acc=0.89062
# [46/100] training 96.2% loss=0.13651, acc=0.93750
# [46/100] training 96.4% loss=0.16909, acc=0.95312
# [46/100] training 96.5% loss=0.19144, acc=0.93750
# [46/100] training 96.7% loss=0.10293, acc=0.93750
# [46/100] training 96.8% loss=0.16644, acc=0.90625
# [46/100] training 97.1% loss=0.12054, acc=0.93750
# [46/100] training 97.2% loss=0.19577, acc=0.95312
# [46/100] training 97.4% loss=0.24596, acc=0.90625
# [46/100] training 97.6% loss=0.23422, acc=0.89062
# [46/100] training 97.7% loss=0.19797, acc=0.89062
# [46/100] training 97.9% loss=0.11798, acc=0.95312
# [46/100] training 98.0% loss=0.17475, acc=0.92188
# [46/100] training 98.3% loss=0.21433, acc=0.95312
# [46/100] training 98.4% loss=0.23967, acc=0.90625
# [46/100] training 98.6% loss=0.26185, acc=0.92188
# [46/100] training 98.7% loss=0.25671, acc=0.93750
# [46/100] training 98.9% loss=0.14941, acc=0.93750
# [46/100] training 99.1% loss=0.13324, acc=0.95312
# [46/100] training 99.2% loss=0.13050, acc=0.93750
# [46/100] training 99.5% loss=0.21239, acc=0.93750
# [46/100] training 99.6% loss=0.15666, acc=0.92188
# [46/100] training 99.8% loss=0.11498, acc=0.95312
# [46/100] training 99.9% loss=0.07893, acc=0.96875
# [46/100] testing 0.9% loss=0.15566, acc=0.95312
# [46/100] testing 1.8% loss=0.40788, acc=0.87500
# [46/100] testing 2.2% loss=0.19301, acc=0.93750
# [46/100] testing 3.1% loss=0.33438, acc=0.89062
# [46/100] testing 3.5% loss=0.07933, acc=0.98438
# [46/100] testing 4.4% loss=0.15739, acc=0.93750
# [46/100] testing 4.8% loss=0.26785, acc=0.87500
# [46/100] testing 5.7% loss=0.23447, acc=0.85938
# [46/100] testing 6.6% loss=0.14575, acc=0.92188
# [46/100] testing 7.0% loss=0.06790, acc=1.00000
# [46/100] testing 7.9% loss=0.31358, acc=0.87500
# [46/100] testing 8.3% loss=0.29102, acc=0.92188
# [46/100] testing 9.2% loss=0.40282, acc=0.87500
# [46/100] testing 9.7% loss=0.04686, acc=1.00000
# [46/100] testing 10.5% loss=0.22477, acc=0.90625
# [46/100] testing 11.0% loss=0.22687, acc=0.89062
# [46/100] testing 11.8% loss=0.19210, acc=0.93750
# [46/100] testing 12.7% loss=0.42118, acc=0.85938
# [46/100] testing 13.2% loss=0.16952, acc=0.92188
# [46/100] testing 14.0% loss=0.41993, acc=0.89062
# [46/100] testing 14.5% loss=0.22611, acc=0.92188
# [46/100] testing 15.4% loss=0.35914, acc=0.87500
# [46/100] testing 15.8% loss=0.16838, acc=0.95312
# [46/100] testing 16.7% loss=0.29270, acc=0.89062
# [46/100] testing 17.5% loss=0.22490, acc=0.92188
# [46/100] testing 18.0% loss=0.24478, acc=0.90625
# [46/100] testing 18.9% loss=0.06562, acc=0.98438
# [46/100] testing 19.3% loss=0.44195, acc=0.87500
# [46/100] testing 20.2% loss=0.36789, acc=0.87500
# [46/100] testing 20.6% loss=0.40251, acc=0.89062
# [46/100] testing 21.5% loss=0.20063, acc=0.92188
# [46/100] testing 21.9% loss=0.36380, acc=0.85938
# [46/100] testing 22.8% loss=0.33022, acc=0.87500
# [46/100] testing 23.7% loss=0.37489, acc=0.89062
# [46/100] testing 24.1% loss=0.22250, acc=0.92188
# [46/100] testing 25.0% loss=0.38532, acc=0.87500
# [46/100] testing 25.4% loss=0.16146, acc=0.93750
# [46/100] testing 26.3% loss=0.30814, acc=0.87500
# [46/100] testing 26.8% loss=0.37951, acc=0.89062
# [46/100] testing 27.6% loss=0.09235, acc=0.96875
# [46/100] testing 28.5% loss=0.19663, acc=0.93750
# [46/100] testing 29.0% loss=0.22866, acc=0.93750
# [46/100] testing 29.8% loss=0.42209, acc=0.92188
# [46/100] testing 30.3% loss=0.22855, acc=0.95312
# [46/100] testing 31.1% loss=0.29758, acc=0.90625
# [46/100] testing 31.6% loss=0.22569, acc=0.95312
# [46/100] testing 32.5% loss=0.33417, acc=0.90625
# [46/100] testing 32.9% loss=0.45356, acc=0.89062
# [46/100] testing 33.8% loss=0.33333, acc=0.90625
# [46/100] testing 34.7% loss=0.33000, acc=0.89062
# [46/100] testing 35.1% loss=0.12342, acc=0.92188
# [46/100] testing 36.0% loss=0.37598, acc=0.92188
# [46/100] testing 36.4% loss=0.26270, acc=0.92188
# [46/100] testing 37.3% loss=0.26812, acc=0.93750
# [46/100] testing 37.7% loss=0.47614, acc=0.82812
# [46/100] testing 38.6% loss=0.24674, acc=0.96875
# [46/100] testing 39.5% loss=0.26823, acc=0.92188
# [46/100] testing 39.9% loss=0.23327, acc=0.90625
# [46/100] testing 40.8% loss=0.27602, acc=0.93750
# [46/100] testing 41.2% loss=0.29843, acc=0.93750
# [46/100] testing 42.1% loss=0.24249, acc=0.92188
# [46/100] testing 42.5% loss=0.14830, acc=0.93750
# [46/100] testing 43.4% loss=0.35093, acc=0.90625
# [46/100] testing 43.9% loss=0.08695, acc=0.96875
# [46/100] testing 44.7% loss=0.23030, acc=0.93750
# [46/100] testing 45.6% loss=0.15179, acc=0.95312
# [46/100] testing 46.1% loss=0.29155, acc=0.89062
# [46/100] testing 46.9% loss=0.22725, acc=0.95312
# [46/100] testing 47.4% loss=0.17533, acc=0.90625
# [46/100] testing 48.3% loss=0.42318, acc=0.87500
# [46/100] testing 48.7% loss=0.31864, acc=0.87500
# [46/100] testing 49.6% loss=0.51133, acc=0.84375
# [46/100] testing 50.4% loss=0.19831, acc=0.90625
# [46/100] testing 50.9% loss=0.34351, acc=0.89062
# [46/100] testing 51.8% loss=0.36447, acc=0.89062
# [46/100] testing 52.2% loss=0.20639, acc=0.90625
# [46/100] testing 53.1% loss=0.34951, acc=0.90625
# [46/100] testing 53.5% loss=0.21198, acc=0.92188
# [46/100] testing 54.4% loss=0.36343, acc=0.82812
# [46/100] testing 54.8% loss=0.39036, acc=0.85938
# [46/100] testing 55.7% loss=0.08710, acc=0.96875
# [46/100] testing 56.6% loss=0.24201, acc=0.89062
# [46/100] testing 57.0% loss=0.39128, acc=0.90625
# [46/100] testing 57.9% loss=0.25938, acc=0.92188
# [46/100] testing 58.3% loss=0.41054, acc=0.85938
# [46/100] testing 59.2% loss=0.20712, acc=0.90625
# [46/100] testing 59.7% loss=0.27822, acc=0.89062
# [46/100] testing 60.5% loss=0.42092, acc=0.84375
# [46/100] testing 61.4% loss=0.14018, acc=0.95312
# [46/100] testing 61.9% loss=0.21766, acc=0.89062
# [46/100] testing 62.7% loss=0.17843, acc=0.92188
# [46/100] testing 63.2% loss=0.42410, acc=0.87500
# [46/100] testing 64.0% loss=0.49943, acc=0.89062
# [46/100] testing 64.5% loss=0.20134, acc=0.92188
# [46/100] testing 65.4% loss=0.17732, acc=0.95312
# [46/100] testing 65.8% loss=0.37039, acc=0.90625
# [46/100] testing 66.7% loss=0.20386, acc=0.92188
# [46/100] testing 67.6% loss=0.48315, acc=0.89062
# [46/100] testing 68.0% loss=0.07655, acc=0.98438
# [46/100] testing 68.9% loss=0.38386, acc=0.85938
# [46/100] testing 69.3% loss=0.28811, acc=0.87500
# [46/100] testing 70.2% loss=0.33174, acc=0.92188
# [46/100] testing 70.6% loss=0.31529, acc=0.90625
# [46/100] testing 71.5% loss=0.35269, acc=0.90625
# [46/100] testing 72.4% loss=0.18070, acc=0.93750
# [46/100] testing 72.8% loss=0.13829, acc=0.95312
# [46/100] testing 73.7% loss=0.10882, acc=0.96875
# [46/100] testing 74.1% loss=0.44094, acc=0.87500
# [46/100] testing 75.0% loss=0.18589, acc=0.87500
# [46/100] testing 75.4% loss=0.46179, acc=0.84375
# [46/100] testing 76.3% loss=0.06138, acc=0.98438
# [46/100] testing 76.8% loss=0.24308, acc=0.90625
# [46/100] testing 77.6% loss=0.16357, acc=0.89062
# [46/100] testing 78.5% loss=0.32667, acc=0.87500
# [46/100] testing 79.0% loss=0.32622, acc=0.90625
# [46/100] testing 79.8% loss=0.39478, acc=0.85938
# [46/100] testing 80.3% loss=0.34442, acc=0.89062
# [46/100] testing 81.2% loss=0.48644, acc=0.87500
# [46/100] testing 81.6% loss=0.25520, acc=0.90625
# [46/100] testing 82.5% loss=0.20965, acc=0.90625
# [46/100] testing 83.3% loss=0.16042, acc=0.96875
# [46/100] testing 83.8% loss=0.16476, acc=0.93750
# [46/100] testing 84.7% loss=0.38529, acc=0.84375
# [46/100] testing 85.1% loss=0.17525, acc=0.92188
# [46/100] testing 86.0% loss=0.31018, acc=0.90625
# [46/100] testing 86.4% loss=0.43057, acc=0.87500
# [46/100] testing 87.3% loss=0.29366, acc=0.85938
# [46/100] testing 87.7% loss=0.24558, acc=0.92188
# [46/100] testing 88.6% loss=0.26721, acc=0.87500
# [46/100] testing 89.5% loss=0.58624, acc=0.84375
# [46/100] testing 89.9% loss=0.12124, acc=0.93750
# [46/100] testing 90.8% loss=0.33703, acc=0.92188
# [46/100] testing 91.2% loss=0.23673, acc=0.92188
# [46/100] testing 92.1% loss=0.35971, acc=0.92188
# [46/100] testing 92.6% loss=0.39386, acc=0.85938
# [46/100] testing 93.4% loss=0.29722, acc=0.90625
# [46/100] testing 94.3% loss=0.12679, acc=0.92188
# [46/100] testing 94.7% loss=0.21545, acc=0.89062
# [46/100] testing 95.6% loss=0.40264, acc=0.82812
# [46/100] testing 96.1% loss=0.15739, acc=0.92188
# [46/100] testing 96.9% loss=0.28180, acc=0.92188
# [46/100] testing 97.4% loss=0.18789, acc=0.96875
# [46/100] testing 98.3% loss=0.20951, acc=0.90625
# [46/100] testing 98.7% loss=0.26535, acc=0.90625
# [46/100] testing 99.6% loss=0.29346, acc=0.89062
# [47/100] training 0.2% loss=0.33334, acc=0.87500
# [47/100] training 0.4% loss=0.33673, acc=0.82812
# [47/100] training 0.5% loss=0.16517, acc=0.95312
# [47/100] training 0.8% loss=0.16700, acc=0.95312
# [47/100] training 0.9% loss=0.15122, acc=0.96875
# [47/100] training 1.1% loss=0.20115, acc=0.95312
# [47/100] training 1.2% loss=0.19623, acc=0.89062
# [47/100] training 1.4% loss=0.08041, acc=0.96875
# [47/100] training 1.6% loss=0.12016, acc=0.95312
# [47/100] training 1.8% loss=0.15645, acc=0.92188
# [47/100] training 2.0% loss=0.26425, acc=0.87500
# [47/100] training 2.1% loss=0.22435, acc=0.85938
# [47/100] training 2.3% loss=0.11112, acc=0.95312
# [47/100] training 2.4% loss=0.16267, acc=0.90625
# [47/100] training 2.6% loss=0.10565, acc=0.95312
# [47/100] training 2.7% loss=0.27925, acc=0.90625
# [47/100] training 3.0% loss=0.24200, acc=0.93750
# [47/100] training 3.2% loss=0.08663, acc=0.96875
# [47/100] training 3.3% loss=0.33885, acc=0.90625
# [47/100] training 3.5% loss=0.19485, acc=0.90625
# [47/100] training 3.6% loss=0.24598, acc=0.90625
# [47/100] training 3.8% loss=0.14898, acc=0.93750
# [47/100] training 3.9% loss=0.19620, acc=0.89062
# [47/100] training 4.2% loss=0.14443, acc=0.92188
# [47/100] training 4.4% loss=0.19924, acc=0.95312
# [47/100] training 4.5% loss=0.12352, acc=0.92188
# [47/100] training 4.7% loss=0.31946, acc=0.90625
# [47/100] training 4.8% loss=0.15104, acc=0.93750
# [47/100] training 5.0% loss=0.13139, acc=0.92188
# [47/100] training 5.2% loss=0.18754, acc=0.93750
# [47/100] training 5.4% loss=0.08331, acc=0.98438
# [47/100] training 5.5% loss=0.14289, acc=0.95312
# [47/100] training 5.7% loss=0.15103, acc=0.93750
# [47/100] training 5.9% loss=0.15902, acc=0.93750
# [47/100] training 6.0% loss=0.22339, acc=0.89062
# [47/100] training 6.3% loss=0.18844, acc=0.89062
# [47/100] training 6.4% loss=0.11369, acc=0.96875
# [47/100] training 6.6% loss=0.16183, acc=0.95312
# [47/100] training 6.7% loss=0.30825, acc=0.85938
# [47/100] training 6.9% loss=0.18801, acc=0.92188
# [47/100] training 7.1% loss=0.08642, acc=0.95312
# [47/100] training 7.2% loss=0.28109, acc=0.89062
# [47/100] training 7.5% loss=0.17619, acc=0.95312
# [47/100] training 7.6% loss=0.30028, acc=0.87500
# [47/100] training 7.8% loss=0.14457, acc=0.93750
# [47/100] training 7.9% loss=0.15877, acc=0.93750
# [47/100] training 8.1% loss=0.12109, acc=0.93750
# [47/100] training 8.2% loss=0.18344, acc=0.90625
# [47/100] training 8.4% loss=0.19907, acc=0.90625
# [47/100] training 8.7% loss=0.16491, acc=0.92188
# [47/100] training 8.8% loss=0.21722, acc=0.89062
# [47/100] training 9.0% loss=0.23935, acc=0.92188
# [47/100] training 9.1% loss=0.21897, acc=0.92188
# [47/100] training 9.3% loss=0.24550, acc=0.92188
# [47/100] training 9.4% loss=0.13493, acc=0.93750
# [47/100] training 9.7% loss=0.22973, acc=0.89062
# [47/100] training 9.9% loss=0.23086, acc=0.93750
# [47/100] training 10.0% loss=0.25168, acc=0.89062
# [47/100] training 10.2% loss=0.17223, acc=0.95312
# [47/100] training 10.3% loss=0.17811, acc=0.92188
# [47/100] training 10.5% loss=0.21850, acc=0.90625
# [47/100] training 10.6% loss=0.22548, acc=0.90625
# [47/100] training 10.9% loss=0.10597, acc=0.93750
# [47/100] training 11.0% loss=0.25524, acc=0.92188
# [47/100] training 11.2% loss=0.11230, acc=0.95312
# [47/100] training 11.4% loss=0.17521, acc=0.95312
# [47/100] training 11.5% loss=0.39283, acc=0.84375
# [47/100] training 11.7% loss=0.08192, acc=0.98438
# [47/100] training 11.8% loss=0.14796, acc=0.95312
# [47/100] training 12.1% loss=0.16733, acc=0.89062
# [47/100] training 12.2% loss=0.05693, acc=0.98438
# [47/100] training 12.4% loss=0.16597, acc=0.89062
# [47/100] training 12.6% loss=0.29020, acc=0.96875
# [47/100] training 12.7% loss=0.15618, acc=0.92188
# [47/100] training 12.9% loss=0.29882, acc=0.89062
# [47/100] training 13.0% loss=0.18465, acc=0.92188
# [47/100] training 13.3% loss=0.22792, acc=0.93750
# [47/100] training 13.4% loss=0.20736, acc=0.89062
# [47/100] training 13.6% loss=0.18987, acc=0.93750
# [47/100] training 13.7% loss=0.33575, acc=0.87500
# [47/100] training 13.9% loss=0.32269, acc=0.92188
# [47/100] training 14.1% loss=0.16997, acc=0.95312
# [47/100] training 14.3% loss=0.18929, acc=0.93750
# [47/100] training 14.5% loss=0.23142, acc=0.89062
# [47/100] training 14.6% loss=0.22287, acc=0.93750
# [47/100] training 14.8% loss=0.19997, acc=0.92188
# [47/100] training 14.9% loss=0.11877, acc=0.93750
# [47/100] training 15.1% loss=0.34234, acc=0.84375
# [47/100] training 15.4% loss=0.27682, acc=0.84375
# [47/100] training 15.5% loss=0.14948, acc=0.95312
# [47/100] training 15.7% loss=0.29697, acc=0.90625
# [47/100] training 15.8% loss=0.16679, acc=0.93750
# [47/100] training 16.0% loss=0.27618, acc=0.87500
# [47/100] training 16.1% loss=0.28606, acc=0.89062
# [47/100] training 16.3% loss=0.20097, acc=0.93750
# [47/100] training 16.4% loss=0.13478, acc=0.93750
# [47/100] training 16.7% loss=0.21446, acc=0.90625
# [47/100] training 16.9% loss=0.15529, acc=0.92188
# [47/100] training 17.0% loss=0.13556, acc=0.92188
# [47/100] training 17.2% loss=0.16181, acc=0.95312
# [47/100] training 17.3% loss=0.13887, acc=0.96875
# [47/100] training 17.5% loss=0.14403, acc=0.92188
# [47/100] training 17.7% loss=0.12423, acc=0.93750
# [47/100] training 17.9% loss=0.22687, acc=0.90625
# [47/100] training 18.1% loss=0.18963, acc=0.92188
# [47/100] training 18.2% loss=0.17269, acc=0.92188
# [47/100] training 18.4% loss=0.26879, acc=0.87500
# [47/100] training 18.5% loss=0.20476, acc=0.93750
# [47/100] training 18.8% loss=0.15503, acc=0.90625
# [47/100] training 18.9% loss=0.12753, acc=0.95312
# [47/100] training 19.1% loss=0.28391, acc=0.84375
# [47/100] training 19.2% loss=0.11250, acc=0.96875
# [47/100] training 19.4% loss=0.14845, acc=0.92188
# [47/100] training 19.6% loss=0.21815, acc=0.90625
# [47/100] training 19.7% loss=0.34487, acc=0.84375
# [47/100] training 20.0% loss=0.08804, acc=0.93750
# [47/100] training 20.1% loss=0.23425, acc=0.90625
# [47/100] training 20.3% loss=0.20535, acc=0.90625
# [47/100] training 20.4% loss=0.14466, acc=0.93750
# [47/100] training 20.6% loss=0.29873, acc=0.90625
# [47/100] training 20.8% loss=0.14831, acc=0.93750
# [47/100] training 20.9% loss=0.10890, acc=0.95312
# [47/100] training 21.2% loss=0.26459, acc=0.92188
# [47/100] training 21.3% loss=0.23885, acc=0.92188
# [47/100] training 21.5% loss=0.24083, acc=0.92188
# [47/100] training 21.6% loss=0.06291, acc=0.98438
# [47/100] training 21.8% loss=0.26209, acc=0.90625
# [47/100] training 21.9% loss=0.23738, acc=0.90625
# [47/100] training 22.2% loss=0.16673, acc=0.92188
# [47/100] training 22.4% loss=0.26058, acc=0.87500
# [47/100] training 22.5% loss=0.14037, acc=0.95312
# [47/100] training 22.7% loss=0.15092, acc=0.93750
# [47/100] training 22.8% loss=0.20183, acc=0.93750
# [47/100] training 23.0% loss=0.12746, acc=0.92188
# [47/100] training 23.1% loss=0.25669, acc=0.89062
# [47/100] training 23.4% loss=0.30282, acc=0.87500
# [47/100] training 23.6% loss=0.21047, acc=0.93750
# [47/100] training 23.7% loss=0.17188, acc=0.93750
# [47/100] training 23.9% loss=0.11812, acc=0.95312
# [47/100] training 24.0% loss=0.14360, acc=0.93750
# [47/100] training 24.2% loss=0.13303, acc=0.93750
# [47/100] training 24.3% loss=0.19453, acc=0.92188
# [47/100] training 24.6% loss=0.17055, acc=0.93750
# [47/100] training 24.7% loss=0.32914, acc=0.90625
# [47/100] training 24.9% loss=0.21339, acc=0.92188
# [47/100] training 25.1% loss=0.25098, acc=0.87500
# [47/100] training 25.2% loss=0.10586, acc=0.96875
# [47/100] training 25.4% loss=0.18351, acc=0.93750
# [47/100] training 25.6% loss=0.14423, acc=0.92188
# [47/100] training 25.8% loss=0.19427, acc=0.92188
# [47/100] training 25.9% loss=0.12033, acc=0.96875
# [47/100] training 26.1% loss=0.17996, acc=0.92188
# [47/100] training 26.3% loss=0.10592, acc=0.95312
# [47/100] training 26.4% loss=0.12559, acc=0.93750
# [47/100] training 26.6% loss=0.09667, acc=0.96875
# [47/100] training 26.8% loss=0.07035, acc=0.98438
# [47/100] training 27.0% loss=0.23111, acc=0.92188
# [47/100] training 27.1% loss=0.14458, acc=0.90625
# [47/100] training 27.3% loss=0.09931, acc=0.96875
# [47/100] training 27.4% loss=0.10968, acc=0.96875
# [47/100] training 27.6% loss=0.37495, acc=0.92188
# [47/100] training 27.9% loss=0.22205, acc=0.93750
# [47/100] training 28.0% loss=0.38458, acc=0.89062
# [47/100] training 28.2% loss=0.15624, acc=0.93750
# [47/100] training 28.3% loss=0.08585, acc=0.96875
# [47/100] training 28.5% loss=0.19777, acc=0.90625
# [47/100] training 28.6% loss=0.19770, acc=0.95312
# [47/100] training 28.8% loss=0.09073, acc=0.95312
# [47/100] training 29.1% loss=0.09322, acc=0.95312
# [47/100] training 29.2% loss=0.11226, acc=0.93750
# [47/100] training 29.4% loss=0.28729, acc=0.89062
# [47/100] training 29.5% loss=0.05955, acc=0.98438
# [47/100] training 29.7% loss=0.20491, acc=0.89062
# [47/100] training 29.8% loss=0.13438, acc=0.96875
# [47/100] training 30.0% loss=0.16017, acc=0.95312
# [47/100] training 30.2% loss=0.12907, acc=0.96875
# [47/100] training 30.4% loss=0.03201, acc=1.00000
# [47/100] training 30.6% loss=0.26347, acc=0.87500
# [47/100] training 30.7% loss=0.20851, acc=0.92188
# [47/100] training 30.9% loss=0.36741, acc=0.90625
# [47/100] training 31.0% loss=0.15050, acc=0.95312
# [47/100] training 31.3% loss=0.25558, acc=0.87500
# [47/100] training 31.4% loss=0.32871, acc=0.85938
# [47/100] training 31.6% loss=0.27714, acc=0.90625
# [47/100] training 31.8% loss=0.19881, acc=0.90625
# [47/100] training 31.9% loss=0.21649, acc=0.90625
# [47/100] training 32.1% loss=0.18328, acc=0.95312
# [47/100] training 32.2% loss=0.28428, acc=0.87500
# [47/100] training 32.5% loss=0.11097, acc=0.96875
# [47/100] training 32.6% loss=0.22945, acc=0.92188
# [47/100] training 32.8% loss=0.12944, acc=0.93750
# [47/100] training 32.9% loss=0.22934, acc=0.93750
# [47/100] training 33.1% loss=0.16667, acc=0.90625
# [47/100] training 33.3% loss=0.21399, acc=0.90625
# [47/100] training 33.4% loss=0.16161, acc=0.93750
# [47/100] training 33.7% loss=0.14372, acc=0.96875
# [47/100] training 33.8% loss=0.16352, acc=0.92188
# [47/100] training 34.0% loss=0.20337, acc=0.92188
# [47/100] training 34.1% loss=0.15062, acc=0.90625
# [47/100] training 34.3% loss=0.20708, acc=0.92188
# [47/100] training 34.5% loss=0.28099, acc=0.92188
# [47/100] training 34.7% loss=0.16282, acc=0.95312
# [47/100] training 34.9% loss=0.14543, acc=0.95312
# [47/100] training 35.0% loss=0.16008, acc=0.95312
# [47/100] training 35.2% loss=0.19264, acc=0.90625
# [47/100] training 35.3% loss=0.20006, acc=0.93750
# [47/100] training 35.5% loss=0.15766, acc=0.93750
# [47/100] training 35.6% loss=0.31489, acc=0.90625
# [47/100] training 35.9% loss=0.12236, acc=0.93750
# [47/100] training 36.1% loss=0.26067, acc=0.85938
# [47/100] training 36.2% loss=0.16415, acc=0.93750
# [47/100] training 36.4% loss=0.26022, acc=0.90625
# [47/100] training 36.5% loss=0.16997, acc=0.93750
# [47/100] training 36.7% loss=0.23915, acc=0.89062
# [47/100] training 36.8% loss=0.11212, acc=0.95312
# [47/100] training 37.1% loss=0.21565, acc=0.92188
# [47/100] training 37.3% loss=0.18498, acc=0.92188
# [47/100] training 37.4% loss=0.16015, acc=0.95312
# [47/100] training 37.6% loss=0.17806, acc=0.92188
# [47/100] training 37.7% loss=0.21846, acc=0.93750
# [47/100] training 37.9% loss=0.14653, acc=0.90625
# [47/100] training 38.1% loss=0.29679, acc=0.90625
# [47/100] training 38.3% loss=0.13175, acc=0.92188
# [47/100] training 38.4% loss=0.07778, acc=0.98438
# [47/100] training 38.6% loss=0.15679, acc=0.92188
# [47/100] training 38.8% loss=0.24582, acc=0.89062
# [47/100] training 38.9% loss=0.15388, acc=0.93750
# [47/100] training 39.1% loss=0.21573, acc=0.92188
# [47/100] training 39.3% loss=0.22188, acc=0.87500
# [47/100] training 39.5% loss=0.21511, acc=0.93750
# [47/100] training 39.6% loss=0.17418, acc=0.95312
# [47/100] training 39.8% loss=0.09595, acc=0.96875
# [47/100] training 40.0% loss=0.17803, acc=0.92188
# [47/100] training 40.1% loss=0.21371, acc=0.92188
# [47/100] training 40.4% loss=0.17529, acc=0.93750
# [47/100] training 40.5% loss=0.29809, acc=0.92188
# [47/100] training 40.7% loss=0.23006, acc=0.90625
# [47/100] training 40.8% loss=0.10515, acc=0.95312
# [47/100] training 41.0% loss=0.19203, acc=0.90625
# [47/100] training 41.1% loss=0.25324, acc=0.87500
# [47/100] training 41.3% loss=0.21327, acc=0.85938
# [47/100] training 41.6% loss=0.19402, acc=0.90625
# [47/100] training 41.7% loss=0.17563, acc=0.93750
# [47/100] training 41.9% loss=0.16295, acc=0.93750
# [47/100] training 42.0% loss=0.24093, acc=0.92188
# [47/100] training 42.2% loss=0.22045, acc=0.93750
# [47/100] training 42.3% loss=0.15653, acc=0.93750
# [47/100] training 42.5% loss=0.20682, acc=0.92188
# [47/100] training 42.8% loss=0.17574, acc=0.92188
# [47/100] training 42.9% loss=0.12921, acc=0.95312
# [47/100] training 43.1% loss=0.21174, acc=0.93750
# [47/100] training 43.2% loss=0.11178, acc=0.95312
# [47/100] training 43.4% loss=0.14414, acc=0.95312
# [47/100] training 43.5% loss=0.26111, acc=0.89062
# [47/100] training 43.8% loss=0.29376, acc=0.87500
# [47/100] training 43.9% loss=0.17009, acc=0.95312
# [47/100] training 44.1% loss=0.20221, acc=0.95312
# [47/100] training 44.3% loss=0.09794, acc=0.95312
# [47/100] training 44.4% loss=0.24999, acc=0.93750
# [47/100] training 44.6% loss=0.18952, acc=0.93750
# [47/100] training 44.7% loss=0.27943, acc=0.89062
# [47/100] training 45.0% loss=0.11413, acc=0.95312
# [47/100] training 45.1% loss=0.30798, acc=0.90625
# [47/100] training 45.3% loss=0.16823, acc=0.93750
# [47/100] training 45.5% loss=0.06199, acc=0.98438
# [47/100] training 45.6% loss=0.24294, acc=0.89062
# [47/100] training 45.8% loss=0.12602, acc=0.95312
# [47/100] training 45.9% loss=0.20644, acc=0.90625
# [47/100] training 46.2% loss=0.06268, acc=0.96875
# [47/100] training 46.3% loss=0.15212, acc=0.93750
# [47/100] training 46.5% loss=0.30624, acc=0.87500
# [47/100] training 46.6% loss=0.14169, acc=0.95312
# [47/100] training 46.8% loss=0.13879, acc=0.95312
# [47/100] training 47.0% loss=0.21386, acc=0.89062
# [47/100] training 47.2% loss=0.27941, acc=0.90625
# [47/100] training 47.4% loss=0.16010, acc=0.92188
# [47/100] training 47.5% loss=0.22438, acc=0.92188
# [47/100] training 47.7% loss=0.11708, acc=0.95312
# [47/100] training 47.8% loss=0.22782, acc=0.92188
# [47/100] training 48.0% loss=0.13945, acc=0.90625
# [47/100] training 48.3% loss=0.10979, acc=0.98438
# [47/100] training 48.4% loss=0.05500, acc=0.98438
# [47/100] training 48.6% loss=0.18026, acc=0.95312
# [47/100] training 48.7% loss=0.15280, acc=0.92188
# [47/100] training 48.9% loss=0.11834, acc=0.95312
# [47/100] training 49.0% loss=0.11437, acc=0.93750
# [47/100] training 49.2% loss=0.26382, acc=0.85938
# [47/100] training 49.3% loss=0.17613, acc=0.92188
# [47/100] training 49.6% loss=0.22402, acc=0.93750
# [47/100] training 49.8% loss=0.19556, acc=0.93750
# [47/100] training 49.9% loss=0.20411, acc=0.93750
# [47/100] training 50.1% loss=0.13571, acc=0.93750
# [47/100] training 50.2% loss=0.24830, acc=0.89062
# [47/100] training 50.4% loss=0.37699, acc=0.85938
# [47/100] training 50.6% loss=0.17421, acc=0.90625
# [47/100] training 50.8% loss=0.18170, acc=0.92188
# [47/100] training 51.0% loss=0.24015, acc=0.92188
# [47/100] training 51.1% loss=0.27280, acc=0.89062
# [47/100] training 51.3% loss=0.25035, acc=0.90625
# [47/100] training 51.4% loss=0.17773, acc=0.93750
# [47/100] training 51.7% loss=0.21907, acc=0.89062
# [47/100] training 51.8% loss=0.22638, acc=0.85938
# [47/100] training 52.0% loss=0.22106, acc=0.93750
# [47/100] training 52.1% loss=0.15071, acc=0.95312
# [47/100] training 52.3% loss=0.28677, acc=0.89062
# [47/100] training 52.5% loss=0.07172, acc=0.96875
# [47/100] training 52.6% loss=0.20556, acc=0.93750
# [47/100] training 52.9% loss=0.20492, acc=0.90625
# [47/100] training 53.0% loss=0.11356, acc=0.95312
# [47/100] training 53.2% loss=0.19953, acc=0.95312
# [47/100] training 53.3% loss=0.17437, acc=0.95312
# [47/100] training 53.5% loss=0.19437, acc=0.89062
# [47/100] training 53.7% loss=0.13078, acc=0.93750
# [47/100] training 53.8% loss=0.31957, acc=0.89062
# [47/100] training 54.1% loss=0.28954, acc=0.85938
# [47/100] training 54.2% loss=0.06822, acc=0.96875
# [47/100] training 54.4% loss=0.18449, acc=0.89062
# [47/100] training 54.5% loss=0.22633, acc=0.92188
# [47/100] training 54.7% loss=0.30474, acc=0.87500
# [47/100] training 54.8% loss=0.13790, acc=0.93750
# [47/100] training 55.1% loss=0.23953, acc=0.90625
# [47/100] training 55.3% loss=0.18141, acc=0.93750
# [47/100] training 55.4% loss=0.12420, acc=0.95312
# [47/100] training 55.6% loss=0.16020, acc=0.93750
# [47/100] training 55.7% loss=0.07532, acc=0.96875
# [47/100] training 55.9% loss=0.16943, acc=0.96875
# [47/100] training 56.0% loss=0.14276, acc=0.92188
# [47/100] training 56.3% loss=0.36338, acc=0.82812
# [47/100] training 56.5% loss=0.19966, acc=0.93750
# [47/100] training 56.6% loss=0.23096, acc=0.92188
# [47/100] training 56.8% loss=0.22658, acc=0.89062
# [47/100] training 56.9% loss=0.10650, acc=0.96875
# [47/100] training 57.1% loss=0.25405, acc=0.89062
# [47/100] training 57.2% loss=0.14785, acc=0.95312
# [47/100] training 57.5% loss=0.17347, acc=0.92188
# [47/100] training 57.6% loss=0.22640, acc=0.93750
# [47/100] training 57.8% loss=0.14160, acc=0.96875
# [47/100] training 58.0% loss=0.17709, acc=0.95312
# [47/100] training 58.1% loss=0.16026, acc=0.93750
# [47/100] training 58.3% loss=0.32735, acc=0.90625
# [47/100] training 58.4% loss=0.19555, acc=0.93750
# [47/100] training 58.7% loss=0.26145, acc=0.89062
# [47/100] training 58.8% loss=0.18462, acc=0.92188
# [47/100] training 59.0% loss=0.22583, acc=0.92188
# [47/100] training 59.2% loss=0.25002, acc=0.87500
# [47/100] training 59.3% loss=0.16943, acc=0.95312
# [47/100] training 59.5% loss=0.14743, acc=0.95312
# [47/100] training 59.7% loss=0.23898, acc=0.89062
# [47/100] training 59.9% loss=0.19527, acc=0.93750
# [47/100] training 60.0% loss=0.13808, acc=0.93750
# [47/100] training 60.2% loss=0.36959, acc=0.87500
# [47/100] training 60.3% loss=0.11049, acc=0.96875
# [47/100] training 60.5% loss=0.29794, acc=0.87500
# [47/100] training 60.8% loss=0.22433, acc=0.89062
# [47/100] training 60.9% loss=0.22132, acc=0.90625
# [47/100] training 61.1% loss=0.21161, acc=0.95312
# [47/100] training 61.2% loss=0.13472, acc=0.92188
# [47/100] training 61.4% loss=0.28654, acc=0.87500
# [47/100] training 61.5% loss=0.46048, acc=0.84375
# [47/100] training 61.7% loss=0.24088, acc=0.89062
# [47/100] training 62.0% loss=0.29517, acc=0.87500
# [47/100] training 62.1% loss=0.28241, acc=0.90625
# [47/100] training 62.3% loss=0.11064, acc=0.98438
# [47/100] training 62.4% loss=0.16884, acc=0.92188
# [47/100] training 62.6% loss=0.20365, acc=0.89062
# [47/100] training 62.7% loss=0.12270, acc=0.96875
# [47/100] training 62.9% loss=0.15695, acc=0.92188
# [47/100] training 63.1% loss=0.19433, acc=0.92188
# [47/100] training 63.3% loss=0.14964, acc=0.93750
# [47/100] training 63.5% loss=0.18691, acc=0.92188
# [47/100] training 63.6% loss=0.31044, acc=0.89062
# [47/100] training 63.8% loss=0.27454, acc=0.87500
# [47/100] training 63.9% loss=0.15538, acc=0.96875
# [47/100] training 64.2% loss=0.15001, acc=0.93750
# [47/100] training 64.3% loss=0.24288, acc=0.90625
# [47/100] training 64.5% loss=0.16707, acc=0.92188
# [47/100] training 64.7% loss=0.18575, acc=0.93750
# [47/100] training 64.8% loss=0.27564, acc=0.93750
# [47/100] training 65.0% loss=0.18393, acc=0.90625
# [47/100] training 65.1% loss=0.30173, acc=0.85938
# [47/100] training 65.4% loss=0.15078, acc=0.93750
# [47/100] training 65.5% loss=0.11195, acc=0.95312
# [47/100] training 65.7% loss=0.12035, acc=0.96875
# [47/100] training 65.8% loss=0.29383, acc=0.85938
# [47/100] training 66.0% loss=0.16974, acc=0.95312
# [47/100] training 66.2% loss=0.08242, acc=0.96875
# [47/100] training 66.3% loss=0.21863, acc=0.89062
# [47/100] training 66.6% loss=0.17829, acc=0.95312
# [47/100] training 66.7% loss=0.19054, acc=0.93750
# [47/100] training 66.9% loss=0.21874, acc=0.92188
# [47/100] training 67.0% loss=0.16102, acc=0.93750
# [47/100] training 67.2% loss=0.16963, acc=0.93750
# [47/100] training 67.4% loss=0.15427, acc=0.95312
# [47/100] training 67.6% loss=0.14457, acc=0.93750
# [47/100] training 67.8% loss=0.14980, acc=0.95312
# [47/100] training 67.9% loss=0.15516, acc=0.95312
# [47/100] training 68.1% loss=0.13268, acc=0.96875
# [47/100] training 68.2% loss=0.07325, acc=0.96875
# [47/100] training 68.4% loss=0.05751, acc=0.96875
# [47/100] training 68.5% loss=0.20778, acc=0.92188
# [47/100] training 68.8% loss=0.12882, acc=0.93750
# [47/100] training 69.0% loss=0.20995, acc=0.93750
# [47/100] training 69.1% loss=0.15920, acc=0.93750
# [47/100] training 69.3% loss=0.09837, acc=0.96875
# [47/100] training 69.4% loss=0.21265, acc=0.89062
# [47/100] training 69.6% loss=0.19733, acc=0.92188
# [47/100] training 69.7% loss=0.28174, acc=0.89062
# [47/100] training 70.0% loss=0.28947, acc=0.85938
# [47/100] training 70.2% loss=0.21589, acc=0.92188
# [47/100] training 70.3% loss=0.16261, acc=0.95312
# [47/100] training 70.5% loss=0.24786, acc=0.89062
# [47/100] training 70.6% loss=0.14806, acc=0.95312
# [47/100] training 70.8% loss=0.21779, acc=0.90625
# [47/100] training 71.0% loss=0.25611, acc=0.89062
# [47/100] training 71.2% loss=0.14359, acc=0.95312
# [47/100] training 71.3% loss=0.07207, acc=0.98438
# [47/100] training 71.5% loss=0.27445, acc=0.96875
# [47/100] training 71.7% loss=0.21771, acc=0.92188
# [47/100] training 71.8% loss=0.26028, acc=0.93750
# [47/100] training 72.0% loss=0.10417, acc=0.96875
# [47/100] training 72.2% loss=0.18334, acc=0.92188
# [47/100] training 72.4% loss=0.19623, acc=0.89062
# [47/100] training 72.5% loss=0.23412, acc=0.89062
# [47/100] training 72.7% loss=0.36414, acc=0.85938
# [47/100] training 72.9% loss=0.08449, acc=0.98438
# [47/100] training 73.0% loss=0.13201, acc=0.95312
# [47/100] training 73.3% loss=0.30701, acc=0.90625
# [47/100] training 73.4% loss=0.09865, acc=0.95312
# [47/100] training 73.6% loss=0.13731, acc=0.95312
# [47/100] training 73.7% loss=0.24268, acc=0.93750
# [47/100] training 73.9% loss=0.08379, acc=0.98438
# [47/100] training 74.0% loss=0.19974, acc=0.92188
# [47/100] training 74.2% loss=0.15254, acc=0.98438
# [47/100] training 74.5% loss=0.17041, acc=0.95312
# [47/100] training 74.6% loss=0.27705, acc=0.90625
# [47/100] training 74.8% loss=0.21892, acc=0.90625
# [47/100] training 74.9% loss=0.25131, acc=0.84375
# [47/100] training 75.1% loss=0.16163, acc=0.93750
# [47/100] training 75.2% loss=0.11817, acc=0.95312
# [47/100] training 75.4% loss=0.14067, acc=0.93750
# [47/100] training 75.7% loss=0.16968, acc=0.92188
# [47/100] training 75.8% loss=0.21371, acc=0.89062
# [47/100] training 76.0% loss=0.11666, acc=0.93750
# [47/100] training 76.1% loss=0.24094, acc=0.93750
# [47/100] training 76.3% loss=0.12820, acc=0.93750
# [47/100] training 76.4% loss=0.23314, acc=0.92188
# [47/100] training 76.7% loss=0.20890, acc=0.93750
# [47/100] training 76.8% loss=0.08425, acc=0.98438
# [47/100] training 77.0% loss=0.12228, acc=0.95312
# [47/100] training 77.2% loss=0.12302, acc=0.95312
# [47/100] training 77.3% loss=0.05266, acc=1.00000
# [47/100] training 77.5% loss=0.25608, acc=0.87500
# [47/100] training 77.6% loss=0.13276, acc=0.92188
# [47/100] training 77.9% loss=0.20417, acc=0.89062
# [47/100] training 78.0% loss=0.21488, acc=0.89062
# [47/100] training 78.2% loss=0.26409, acc=0.90625
# [47/100] training 78.4% loss=0.12159, acc=0.95312
# [47/100] training 78.5% loss=0.25587, acc=0.92188
# [47/100] training 78.7% loss=0.19312, acc=0.93750
# [47/100] training 78.8% loss=0.08815, acc=0.96875
# [47/100] training 79.1% loss=0.09707, acc=0.98438
# [47/100] training 79.2% loss=0.13745, acc=0.96875
# [47/100] training 79.4% loss=0.16329, acc=0.93750
# [47/100] training 79.5% loss=0.13728, acc=0.93750
# [47/100] training 79.7% loss=0.06356, acc=0.96875
# [47/100] training 79.9% loss=0.16102, acc=0.92188
# [47/100] training 80.1% loss=0.12558, acc=0.95312
# [47/100] training 80.3% loss=0.16173, acc=0.95312
# [47/100] training 80.4% loss=0.16956, acc=0.95312
# [47/100] training 80.6% loss=0.20412, acc=0.92188
# [47/100] training 80.7% loss=0.19840, acc=0.92188
# [47/100] training 80.9% loss=0.21823, acc=0.90625
# [47/100] training 81.2% loss=0.27980, acc=0.89062
# [47/100] training 81.3% loss=0.19111, acc=0.92188
# [47/100] training 81.5% loss=0.18461, acc=0.92188
# [47/100] training 81.6% loss=0.24463, acc=0.87500
# [47/100] training 81.8% loss=0.22364, acc=0.93750
# [47/100] training 81.9% loss=0.39810, acc=0.87500
# [47/100] training 82.1% loss=0.13791, acc=0.93750
# [47/100] training 82.2% loss=0.20956, acc=0.89062
# [47/100] training 82.5% loss=0.12666, acc=0.96875
# [47/100] training 82.7% loss=0.13980, acc=0.92188
# [47/100] training 82.8% loss=0.14512, acc=0.95312
# [47/100] training 83.0% loss=0.14700, acc=0.95312
# [47/100] training 83.1% loss=0.23680, acc=0.93750
# [47/100] training 83.3% loss=0.20000, acc=0.90625
# [47/100] training 83.5% loss=0.13054, acc=0.95312
# [47/100] training 83.7% loss=0.33056, acc=0.89062
# [47/100] training 83.9% loss=0.24742, acc=0.90625
# [47/100] training 84.0% loss=0.06035, acc=1.00000
# [47/100] training 84.2% loss=0.06388, acc=1.00000
# [47/100] training 84.3% loss=0.11499, acc=0.95312
# [47/100] training 84.5% loss=0.15756, acc=0.95312
# [47/100] training 84.7% loss=0.18550, acc=0.90625
# [47/100] training 84.9% loss=0.14551, acc=0.95312
# [47/100] training 85.0% loss=0.18232, acc=0.93750
# [47/100] training 85.2% loss=0.11029, acc=0.93750
# [47/100] training 85.4% loss=0.17941, acc=0.92188
# [47/100] training 85.5% loss=0.22151, acc=0.90625
# [47/100] training 85.8% loss=0.29281, acc=0.87500
# [47/100] training 85.9% loss=0.17368, acc=0.90625
# [47/100] training 86.1% loss=0.12154, acc=0.98438
# [47/100] training 86.2% loss=0.11319, acc=0.95312
# [47/100] training 86.4% loss=0.34454, acc=0.89062
# [47/100] training 86.6% loss=0.21723, acc=0.92188
# [47/100] training 86.7% loss=0.12275, acc=0.95312
# [47/100] training 87.0% loss=0.30799, acc=0.85938
# [47/100] training 87.1% loss=0.15414, acc=0.93750
# [47/100] training 87.3% loss=0.18225, acc=0.90625
# [47/100] training 87.4% loss=0.15464, acc=0.92188
# [47/100] training 87.6% loss=0.17779, acc=0.93750
# [47/100] training 87.7% loss=0.29168, acc=0.89062
# [47/100] training 87.9% loss=0.23257, acc=0.93750
# [47/100] training 88.2% loss=0.23088, acc=0.93750
# [47/100] training 88.3% loss=0.19949, acc=0.95312
# [47/100] training 88.5% loss=0.22672, acc=0.85938
# [47/100] training 88.6% loss=0.13943, acc=0.95312
# [47/100] training 88.8% loss=0.13181, acc=0.95312
# [47/100] training 88.9% loss=0.21811, acc=0.90625
# [47/100] training 89.2% loss=0.14368, acc=0.93750
# [47/100] training 89.4% loss=0.17865, acc=0.93750
# [47/100] training 89.5% loss=0.18833, acc=0.93750
# [47/100] training 89.7% loss=0.23300, acc=0.89062
# [47/100] training 89.8% loss=0.10649, acc=0.96875
# [47/100] training 90.0% loss=0.14980, acc=0.93750
# [47/100] training 90.1% loss=0.17618, acc=0.95312
# [47/100] training 90.4% loss=0.26247, acc=0.89062
# [47/100] training 90.5% loss=0.10985, acc=0.93750
# [47/100] training 90.7% loss=0.09976, acc=0.95312
# [47/100] training 90.9% loss=0.09139, acc=0.95312
# [47/100] training 91.0% loss=0.21732, acc=0.93750
# [47/100] training 91.2% loss=0.10930, acc=0.96875
# [47/100] training 91.3% loss=0.23510, acc=0.93750
# [47/100] training 91.6% loss=0.23933, acc=0.90625
# [47/100] training 91.7% loss=0.14135, acc=0.92188
# [47/100] training 91.9% loss=0.32048, acc=0.89062
# [47/100] training 92.1% loss=0.22797, acc=0.87500
# [47/100] training 92.2% loss=0.13598, acc=0.95312
# [47/100] training 92.4% loss=0.24192, acc=0.87500
# [47/100] training 92.6% loss=0.24495, acc=0.92188
# [47/100] training 92.8% loss=0.24382, acc=0.87500
# [47/100] training 92.9% loss=0.19444, acc=0.92188
# [47/100] training 93.1% loss=0.30967, acc=0.90625
# [47/100] training 93.2% loss=0.19272, acc=0.92188
# [47/100] training 93.4% loss=0.31906, acc=0.89062
# [47/100] training 93.7% loss=0.13191, acc=0.93750
# [47/100] training 93.8% loss=0.11615, acc=0.98438
# [47/100] training 94.0% loss=0.18437, acc=0.93750
# [47/100] training 94.1% loss=0.15984, acc=0.95312
# [47/100] training 94.3% loss=0.10435, acc=0.95312
# [47/100] training 94.4% loss=0.17085, acc=0.93750
# [47/100] training 94.6% loss=0.08846, acc=0.93750
# [47/100] training 94.9% loss=0.12329, acc=0.93750
# [47/100] training 95.0% loss=0.30192, acc=0.89062
# [47/100] training 95.2% loss=0.45647, acc=0.87500
# [47/100] training 95.3% loss=0.16381, acc=0.93750
# [47/100] training 95.5% loss=0.09969, acc=0.96875
# [47/100] training 95.6% loss=0.25112, acc=0.92188
# [47/100] training 95.8% loss=0.24714, acc=0.89062
# [47/100] training 96.0% loss=0.21111, acc=0.93750
# [47/100] training 96.2% loss=0.14786, acc=0.95312
# [47/100] training 96.4% loss=0.14486, acc=0.96875
# [47/100] training 96.5% loss=0.24194, acc=0.92188
# [47/100] training 96.7% loss=0.13206, acc=0.93750
# [47/100] training 96.8% loss=0.18018, acc=0.92188
# [47/100] training 97.1% loss=0.13350, acc=0.93750
# [47/100] training 97.2% loss=0.19137, acc=0.93750
# [47/100] training 97.4% loss=0.24696, acc=0.93750
# [47/100] training 97.6% loss=0.20694, acc=0.87500
# [47/100] training 97.7% loss=0.13755, acc=0.90625
# [47/100] training 97.9% loss=0.17067, acc=0.92188
# [47/100] training 98.0% loss=0.16267, acc=0.93750
# [47/100] training 98.3% loss=0.19741, acc=0.92188
# [47/100] training 98.4% loss=0.17065, acc=0.92188
# [47/100] training 98.6% loss=0.32390, acc=0.85938
# [47/100] training 98.7% loss=0.27055, acc=0.90625
# [47/100] training 98.9% loss=0.18792, acc=0.87500
# [47/100] training 99.1% loss=0.19947, acc=0.92188
# [47/100] training 99.2% loss=0.17446, acc=0.92188
# [47/100] training 99.5% loss=0.27027, acc=0.90625
# [47/100] training 99.6% loss=0.17988, acc=0.92188
# [47/100] training 99.8% loss=0.18175, acc=0.92188
# [47/100] training 99.9% loss=0.11804, acc=0.93750
# [47/100] testing 0.9% loss=0.10980, acc=0.95312
# [47/100] testing 1.8% loss=0.44512, acc=0.81250
# [47/100] testing 2.2% loss=0.19330, acc=0.90625
# [47/100] testing 3.1% loss=0.27512, acc=0.87500
# [47/100] testing 3.5% loss=0.13453, acc=0.96875
# [47/100] testing 4.4% loss=0.15021, acc=0.92188
# [47/100] testing 4.8% loss=0.33491, acc=0.85938
# [47/100] testing 5.7% loss=0.25377, acc=0.85938
# [47/100] testing 6.6% loss=0.23691, acc=0.89062
# [47/100] testing 7.0% loss=0.10251, acc=0.96875
# [47/100] testing 7.9% loss=0.35540, acc=0.84375
# [47/100] testing 8.3% loss=0.25019, acc=0.92188
# [47/100] testing 9.2% loss=0.29742, acc=0.90625
# [47/100] testing 9.7% loss=0.10921, acc=0.95312
# [47/100] testing 10.5% loss=0.27686, acc=0.90625
# [47/100] testing 11.0% loss=0.30681, acc=0.89062
# [47/100] testing 11.8% loss=0.14105, acc=0.96875
# [47/100] testing 12.7% loss=0.41852, acc=0.81250
# [47/100] testing 13.2% loss=0.20300, acc=0.90625
# [47/100] testing 14.0% loss=0.47829, acc=0.84375
# [47/100] testing 14.5% loss=0.34256, acc=0.89062
# [47/100] testing 15.4% loss=0.32176, acc=0.90625
# [47/100] testing 15.8% loss=0.13158, acc=0.95312
# [47/100] testing 16.7% loss=0.24637, acc=0.95312
# [47/100] testing 17.5% loss=0.15863, acc=0.90625
# [47/100] testing 18.0% loss=0.20838, acc=0.90625
# [47/100] testing 18.9% loss=0.12834, acc=0.95312
# [47/100] testing 19.3% loss=0.39856, acc=0.87500
# [47/100] testing 20.2% loss=0.43405, acc=0.85938
# [47/100] testing 20.6% loss=0.34080, acc=0.87500
# [47/100] testing 21.5% loss=0.21164, acc=0.89062
# [47/100] testing 21.9% loss=0.52968, acc=0.85938
# [47/100] testing 22.8% loss=0.31405, acc=0.92188
# [47/100] testing 23.7% loss=0.45726, acc=0.85938
# [47/100] testing 24.1% loss=0.24580, acc=0.89062
# [47/100] testing 25.0% loss=0.41165, acc=0.85938
# [47/100] testing 25.4% loss=0.16555, acc=0.95312
# [47/100] testing 26.3% loss=0.24094, acc=0.85938
# [47/100] testing 26.8% loss=0.32894, acc=0.89062
# [47/100] testing 27.6% loss=0.16521, acc=0.90625
# [47/100] testing 28.5% loss=0.20667, acc=0.92188
# [47/100] testing 29.0% loss=0.23196, acc=0.92188
# [47/100] testing 29.8% loss=0.34664, acc=0.90625
# [47/100] testing 30.3% loss=0.43801, acc=0.87500
# [47/100] testing 31.1% loss=0.33699, acc=0.84375
# [47/100] testing 31.6% loss=0.24910, acc=0.92188
# [47/100] testing 32.5% loss=0.42626, acc=0.84375
# [47/100] testing 32.9% loss=0.47736, acc=0.85938
# [47/100] testing 33.8% loss=0.31940, acc=0.85938
# [47/100] testing 34.7% loss=0.34775, acc=0.87500
# [47/100] testing 35.1% loss=0.14925, acc=0.93750
# [47/100] testing 36.0% loss=0.31295, acc=0.87500
# [47/100] testing 36.4% loss=0.22633, acc=0.95312
# [47/100] testing 37.3% loss=0.31493, acc=0.89062
# [47/100] testing 37.7% loss=0.48395, acc=0.81250
# [47/100] testing 38.6% loss=0.14000, acc=0.93750
# [47/100] testing 39.5% loss=0.24718, acc=0.95312
# [47/100] testing 39.9% loss=0.25356, acc=0.89062
# [47/100] testing 40.8% loss=0.34581, acc=0.92188
# [47/100] testing 41.2% loss=0.18932, acc=0.93750
# [47/100] testing 42.1% loss=0.25152, acc=0.87500
# [47/100] testing 42.5% loss=0.12579, acc=0.96875
# [47/100] testing 43.4% loss=0.39757, acc=0.87500
# [47/100] testing 43.9% loss=0.15699, acc=0.96875
# [47/100] testing 44.7% loss=0.28607, acc=0.92188
# [47/100] testing 45.6% loss=0.28218, acc=0.89062
# [47/100] testing 46.1% loss=0.23526, acc=0.87500
# [47/100] testing 46.9% loss=0.20241, acc=0.93750
# [47/100] testing 47.4% loss=0.14455, acc=0.96875
# [47/100] testing 48.3% loss=0.38975, acc=0.89062
# [47/100] testing 48.7% loss=0.35413, acc=0.87500
# [47/100] testing 49.6% loss=0.51810, acc=0.82812
# [47/100] testing 50.4% loss=0.16407, acc=0.92188
# [47/100] testing 50.9% loss=0.35861, acc=0.89062
# [47/100] testing 51.8% loss=0.25205, acc=0.89062
# [47/100] testing 52.2% loss=0.18932, acc=0.93750
# [47/100] testing 53.1% loss=0.26118, acc=0.90625
# [47/100] testing 53.5% loss=0.17219, acc=0.93750
# [47/100] testing 54.4% loss=0.27613, acc=0.92188
# [47/100] testing 54.8% loss=0.42299, acc=0.84375
# [47/100] testing 55.7% loss=0.10846, acc=0.95312
# [47/100] testing 56.6% loss=0.32693, acc=0.87500
# [47/100] testing 57.0% loss=0.48834, acc=0.89062
# [47/100] testing 57.9% loss=0.34185, acc=0.82812
# [47/100] testing 58.3% loss=0.40511, acc=0.84375
# [47/100] testing 59.2% loss=0.26348, acc=0.89062
# [47/100] testing 59.7% loss=0.21940, acc=0.89062
# [47/100] testing 60.5% loss=0.37328, acc=0.84375
# [47/100] testing 61.4% loss=0.18436, acc=0.90625
# [47/100] testing 61.9% loss=0.25131, acc=0.90625
# [47/100] testing 62.7% loss=0.17336, acc=0.93750
# [47/100] testing 63.2% loss=0.55382, acc=0.79688
# [47/100] testing 64.0% loss=0.35324, acc=0.85938
# [47/100] testing 64.5% loss=0.13602, acc=0.98438
# [47/100] testing 65.4% loss=0.25134, acc=0.90625
# [47/100] testing 65.8% loss=0.31053, acc=0.87500
# [47/100] testing 66.7% loss=0.24253, acc=0.92188
# [47/100] testing 67.6% loss=0.48748, acc=0.85938
# [47/100] testing 68.0% loss=0.13617, acc=0.95312
# [47/100] testing 68.9% loss=0.29051, acc=0.87500
# [47/100] testing 69.3% loss=0.28789, acc=0.87500
# [47/100] testing 70.2% loss=0.38378, acc=0.85938
# [47/100] testing 70.6% loss=0.29776, acc=0.87500
# [47/100] testing 71.5% loss=0.40918, acc=0.89062
# [47/100] testing 72.4% loss=0.25818, acc=0.90625
# [47/100] testing 72.8% loss=0.25064, acc=0.87500
# [47/100] testing 73.7% loss=0.19414, acc=0.93750
# [47/100] testing 74.1% loss=0.53959, acc=0.82812
# [47/100] testing 75.0% loss=0.17815, acc=0.89062
# [47/100] testing 75.4% loss=0.34079, acc=0.89062
# [47/100] testing 76.3% loss=0.05863, acc=0.98438
# [47/100] testing 76.8% loss=0.22333, acc=0.93750
# [47/100] testing 77.6% loss=0.18644, acc=0.92188
# [47/100] testing 78.5% loss=0.26252, acc=0.87500
# [47/100] testing 79.0% loss=0.40585, acc=0.89062
# [47/100] testing 79.8% loss=0.23478, acc=0.85938
# [47/100] testing 80.3% loss=0.39196, acc=0.85938
# [47/100] testing 81.2% loss=0.48978, acc=0.87500
# [47/100] testing 81.6% loss=0.28740, acc=0.89062
# [47/100] testing 82.5% loss=0.17317, acc=0.93750
# [47/100] testing 83.3% loss=0.12144, acc=0.96875
# [47/100] testing 83.8% loss=0.18769, acc=0.93750
# [47/100] testing 84.7% loss=0.22050, acc=0.89062
# [47/100] testing 85.1% loss=0.29295, acc=0.89062
# [47/100] testing 86.0% loss=0.25117, acc=0.87500
# [47/100] testing 86.4% loss=0.43207, acc=0.85938
# [47/100] testing 87.3% loss=0.37618, acc=0.78125
# [47/100] testing 87.7% loss=0.28914, acc=0.93750
# [47/100] testing 88.6% loss=0.24032, acc=0.90625
# [47/100] testing 89.5% loss=0.60594, acc=0.75000
# [47/100] testing 89.9% loss=0.20862, acc=0.92188
# [47/100] testing 90.8% loss=0.32012, acc=0.85938
# [47/100] testing 91.2% loss=0.13890, acc=0.95312
# [47/100] testing 92.1% loss=0.23151, acc=0.90625
# [47/100] testing 92.6% loss=0.31818, acc=0.89062
# [47/100] testing 93.4% loss=0.35798, acc=0.84375
# [47/100] testing 94.3% loss=0.15783, acc=0.95312
# [47/100] testing 94.7% loss=0.24800, acc=0.87500
# [47/100] testing 95.6% loss=0.54220, acc=0.79688
# [47/100] testing 96.1% loss=0.22558, acc=0.90625
# [47/100] testing 96.9% loss=0.28351, acc=0.92188
# [47/100] testing 97.4% loss=0.14673, acc=0.95312
# [47/100] testing 98.3% loss=0.27183, acc=0.90625
# [47/100] testing 98.7% loss=0.38312, acc=0.87500
# [47/100] testing 99.6% loss=0.27861, acc=0.92188
# [48/100] training 0.2% loss=0.34354, acc=0.87500
# [48/100] training 0.4% loss=0.27703, acc=0.87500
# [48/100] training 0.5% loss=0.16363, acc=0.93750
# [48/100] training 0.8% loss=0.21913, acc=0.89062
# [48/100] training 0.9% loss=0.20818, acc=0.92188
# [48/100] training 1.1% loss=0.22328, acc=0.93750
# [48/100] training 1.2% loss=0.22092, acc=0.90625
# [48/100] training 1.4% loss=0.17314, acc=0.90625
# [48/100] training 1.6% loss=0.13754, acc=0.93750
# [48/100] training 1.8% loss=0.20345, acc=0.95312
# [48/100] training 2.0% loss=0.27612, acc=0.89062
# [48/100] training 2.1% loss=0.16789, acc=0.90625
# [48/100] training 2.3% loss=0.16503, acc=0.92188
# [48/100] training 2.4% loss=0.25510, acc=0.90625
# [48/100] training 2.6% loss=0.11538, acc=0.95312
# [48/100] training 2.7% loss=0.15057, acc=0.95312
# [48/100] training 3.0% loss=0.22772, acc=0.92188
# [48/100] training 3.2% loss=0.11386, acc=0.96875
# [48/100] training 3.3% loss=0.33692, acc=0.87500
# [48/100] training 3.5% loss=0.19183, acc=0.95312
# [48/100] training 3.6% loss=0.17560, acc=0.92188
# [48/100] training 3.8% loss=0.12985, acc=0.92188
# [48/100] training 3.9% loss=0.23479, acc=0.90625
# [48/100] training 4.2% loss=0.08365, acc=0.96875
# [48/100] training 4.4% loss=0.12730, acc=0.93750
# [48/100] training 4.5% loss=0.11556, acc=0.95312
# [48/100] training 4.7% loss=0.25522, acc=0.92188
# [48/100] training 4.8% loss=0.17356, acc=0.92188
# [48/100] training 5.0% loss=0.05015, acc=1.00000
# [48/100] training 5.2% loss=0.21094, acc=0.90625
# [48/100] training 5.4% loss=0.11590, acc=0.96875
# [48/100] training 5.5% loss=0.18736, acc=0.89062
# [48/100] training 5.7% loss=0.20269, acc=0.93750
# [48/100] training 5.9% loss=0.18371, acc=0.93750
# [48/100] training 6.0% loss=0.14297, acc=0.93750
# [48/100] training 6.3% loss=0.12848, acc=0.96875
# [48/100] training 6.4% loss=0.14942, acc=0.95312
# [48/100] training 6.6% loss=0.14921, acc=0.95312
# [48/100] training 6.7% loss=0.22055, acc=0.89062
# [48/100] training 6.9% loss=0.22195, acc=0.90625
# [48/100] training 7.1% loss=0.07981, acc=0.95312
# [48/100] training 7.2% loss=0.21404, acc=0.93750
# [48/100] training 7.5% loss=0.14651, acc=0.96875
# [48/100] training 7.6% loss=0.28914, acc=0.90625
# [48/100] training 7.8% loss=0.17341, acc=0.92188
# [48/100] training 7.9% loss=0.18562, acc=0.90625
# [48/100] training 8.1% loss=0.17159, acc=0.92188
# [48/100] training 8.2% loss=0.17803, acc=0.92188
# [48/100] training 8.4% loss=0.19186, acc=0.92188
# [48/100] training 8.7% loss=0.17063, acc=0.93750
# [48/100] training 8.8% loss=0.27406, acc=0.87500
# [48/100] training 9.0% loss=0.19488, acc=0.90625
# [48/100] training 9.1% loss=0.26090, acc=0.89062
# [48/100] training 9.3% loss=0.29731, acc=0.89062
# [48/100] training 9.4% loss=0.16835, acc=0.95312
# [48/100] training 9.7% loss=0.15585, acc=0.93750
# [48/100] training 9.9% loss=0.28997, acc=0.89062
# [48/100] training 10.0% loss=0.21357, acc=0.89062
# [48/100] training 10.2% loss=0.13947, acc=0.93750
# [48/100] training 10.3% loss=0.12948, acc=0.93750
# [48/100] training 10.5% loss=0.23835, acc=0.90625
# [48/100] training 10.6% loss=0.15410, acc=0.93750
# [48/100] training 10.9% loss=0.13213, acc=0.93750
# [48/100] training 11.0% loss=0.27224, acc=0.93750
# [48/100] training 11.2% loss=0.12308, acc=0.98438
# [48/100] training 11.4% loss=0.29256, acc=0.92188
# [48/100] training 11.5% loss=0.27746, acc=0.87500
# [48/100] training 11.7% loss=0.09484, acc=0.98438
# [48/100] training 11.8% loss=0.16811, acc=0.95312
# [48/100] training 12.1% loss=0.23898, acc=0.90625
# [48/100] training 12.2% loss=0.15196, acc=0.93750
# [48/100] training 12.4% loss=0.12412, acc=0.93750
# [48/100] training 12.6% loss=0.24457, acc=0.92188
# [48/100] training 12.7% loss=0.16115, acc=0.90625
# [48/100] training 12.9% loss=0.31450, acc=0.84375
# [48/100] training 13.0% loss=0.14195, acc=0.96875
# [48/100] training 13.3% loss=0.10720, acc=0.95312
# [48/100] training 13.4% loss=0.22591, acc=0.89062
# [48/100] training 13.6% loss=0.17403, acc=0.90625
# [48/100] training 13.7% loss=0.24707, acc=0.89062
# [48/100] training 13.9% loss=0.21859, acc=0.95312
# [48/100] training 14.1% loss=0.25556, acc=0.95312
# [48/100] training 14.3% loss=0.11786, acc=0.95312
# [48/100] training 14.5% loss=0.29813, acc=0.90625
# [48/100] training 14.6% loss=0.13835, acc=0.96875
# [48/100] training 14.8% loss=0.15561, acc=0.95312
# [48/100] training 14.9% loss=0.07737, acc=0.98438
# [48/100] training 15.1% loss=0.33884, acc=0.87500
# [48/100] training 15.4% loss=0.19195, acc=0.92188
# [48/100] training 15.5% loss=0.11864, acc=0.95312
# [48/100] training 15.7% loss=0.31047, acc=0.89062
# [48/100] training 15.8% loss=0.13551, acc=0.92188
# [48/100] training 16.0% loss=0.16321, acc=0.95312
# [48/100] training 16.1% loss=0.27665, acc=0.89062
# [48/100] training 16.3% loss=0.28005, acc=0.89062
# [48/100] training 16.4% loss=0.19733, acc=0.92188
# [48/100] training 16.7% loss=0.22342, acc=0.90625
# [48/100] training 16.9% loss=0.26536, acc=0.90625
# [48/100] training 17.0% loss=0.16190, acc=0.93750
# [48/100] training 17.2% loss=0.15456, acc=0.90625
# [48/100] training 17.3% loss=0.21333, acc=0.90625
# [48/100] training 17.5% loss=0.22854, acc=0.93750
# [48/100] training 17.7% loss=0.16665, acc=0.92188
# [48/100] training 17.9% loss=0.23093, acc=0.92188
# [48/100] training 18.1% loss=0.28891, acc=0.85938
# [48/100] training 18.2% loss=0.23826, acc=0.93750
# [48/100] training 18.4% loss=0.21816, acc=0.89062
# [48/100] training 18.5% loss=0.16798, acc=0.93750
# [48/100] training 18.8% loss=0.13161, acc=0.93750
# [48/100] training 18.9% loss=0.10322, acc=0.96875
# [48/100] training 19.1% loss=0.20940, acc=0.89062
# [48/100] training 19.2% loss=0.08766, acc=0.95312
# [48/100] training 19.4% loss=0.16375, acc=0.93750
# [48/100] training 19.6% loss=0.16236, acc=0.92188
# [48/100] training 19.7% loss=0.22611, acc=0.89062
# [48/100] training 20.0% loss=0.07104, acc=0.98438
# [48/100] training 20.1% loss=0.21151, acc=0.93750
# [48/100] training 20.3% loss=0.18269, acc=0.93750
# [48/100] training 20.4% loss=0.19286, acc=0.92188
# [48/100] training 20.6% loss=0.28714, acc=0.85938
# [48/100] training 20.8% loss=0.14329, acc=0.93750
# [48/100] training 20.9% loss=0.14895, acc=0.90625
# [48/100] training 21.2% loss=0.33295, acc=0.89062
# [48/100] training 21.3% loss=0.27265, acc=0.90625
# [48/100] training 21.5% loss=0.31713, acc=0.92188
# [48/100] training 21.6% loss=0.10111, acc=0.95312
# [48/100] training 21.8% loss=0.13907, acc=0.93750
# [48/100] training 21.9% loss=0.25645, acc=0.89062
# [48/100] training 22.2% loss=0.20196, acc=0.89062
# [48/100] training 22.4% loss=0.21783, acc=0.95312
# [48/100] training 22.5% loss=0.14971, acc=0.93750
# [48/100] training 22.7% loss=0.16751, acc=0.93750
# [48/100] training 22.8% loss=0.20719, acc=0.92188
# [48/100] training 23.0% loss=0.11749, acc=0.95312
# [48/100] training 23.1% loss=0.29154, acc=0.87500
# [48/100] training 23.4% loss=0.20250, acc=0.92188
# [48/100] training 23.6% loss=0.26117, acc=0.90625
# [48/100] training 23.7% loss=0.19779, acc=0.92188
# [48/100] training 23.9% loss=0.10615, acc=0.93750
# [48/100] training 24.0% loss=0.17776, acc=0.92188
# [48/100] training 24.2% loss=0.12633, acc=0.93750
# [48/100] training 24.3% loss=0.16958, acc=0.92188
# [48/100] training 24.6% loss=0.24813, acc=0.90625
# [48/100] training 24.7% loss=0.22732, acc=0.92188
# [48/100] training 24.9% loss=0.28384, acc=0.92188
# [48/100] training 25.1% loss=0.30896, acc=0.87500
# [48/100] training 25.2% loss=0.22153, acc=0.90625
# [48/100] training 25.4% loss=0.21911, acc=0.85938
# [48/100] training 25.6% loss=0.25247, acc=0.89062
# [48/100] training 25.8% loss=0.26709, acc=0.85938
# [48/100] training 25.9% loss=0.19711, acc=0.92188
# [48/100] training 26.1% loss=0.24154, acc=0.90625
# [48/100] training 26.3% loss=0.17757, acc=0.90625
# [48/100] training 26.4% loss=0.12841, acc=0.95312
# [48/100] training 26.6% loss=0.14371, acc=0.93750
# [48/100] training 26.8% loss=0.17561, acc=0.92188
# [48/100] training 27.0% loss=0.21441, acc=0.89062
# [48/100] training 27.1% loss=0.13717, acc=0.95312
# [48/100] training 27.3% loss=0.20572, acc=0.92188
# [48/100] training 27.4% loss=0.13091, acc=0.93750
# [48/100] training 27.6% loss=0.25067, acc=0.89062
# [48/100] training 27.9% loss=0.14648, acc=0.93750
# [48/100] training 28.0% loss=0.27293, acc=0.87500
# [48/100] training 28.2% loss=0.09023, acc=0.95312
# [48/100] training 28.3% loss=0.07412, acc=0.98438
# [48/100] training 28.5% loss=0.30108, acc=0.89062
# [48/100] training 28.6% loss=0.18939, acc=0.93750
# [48/100] training 28.8% loss=0.14972, acc=0.95312
# [48/100] training 29.1% loss=0.11125, acc=0.95312
# [48/100] training 29.2% loss=0.11441, acc=0.93750
# [48/100] training 29.4% loss=0.23692, acc=0.84375
# [48/100] training 29.5% loss=0.19107, acc=0.90625
# [48/100] training 29.7% loss=0.22649, acc=0.89062
# [48/100] training 29.8% loss=0.17305, acc=0.93750
# [48/100] training 30.0% loss=0.22960, acc=0.89062
# [48/100] training 30.2% loss=0.13226, acc=0.92188
# [48/100] training 30.4% loss=0.13844, acc=0.93750
# [48/100] training 30.6% loss=0.20037, acc=0.92188
# [48/100] training 30.7% loss=0.19086, acc=0.95312
# [48/100] training 30.9% loss=0.36424, acc=0.92188
# [48/100] training 31.0% loss=0.16333, acc=0.93750
# [48/100] training 31.3% loss=0.21666, acc=0.90625
# [48/100] training 31.4% loss=0.23975, acc=0.87500
# [48/100] training 31.6% loss=0.24910, acc=0.93750
# [48/100] training 31.8% loss=0.20822, acc=0.90625
# [48/100] training 31.9% loss=0.15533, acc=0.92188
# [48/100] training 32.1% loss=0.13051, acc=0.96875
# [48/100] training 32.2% loss=0.25151, acc=0.89062
# [48/100] training 32.5% loss=0.07500, acc=0.96875
# [48/100] training 32.6% loss=0.20798, acc=0.90625
# [48/100] training 32.8% loss=0.13677, acc=0.95312
# [48/100] training 32.9% loss=0.22106, acc=0.92188
# [48/100] training 33.1% loss=0.09253, acc=0.95312
# [48/100] training 33.3% loss=0.11997, acc=0.95312
# [48/100] training 33.4% loss=0.13180, acc=0.96875
# [48/100] training 33.7% loss=0.12420, acc=0.95312
# [48/100] training 33.8% loss=0.20840, acc=0.92188
# [48/100] training 34.0% loss=0.21545, acc=0.90625
# [48/100] training 34.1% loss=0.20769, acc=0.92188
# [48/100] training 34.3% loss=0.14931, acc=0.95312
# [48/100] training 34.5% loss=0.20827, acc=0.90625
# [48/100] training 34.7% loss=0.10479, acc=0.95312
# [48/100] training 34.9% loss=0.13360, acc=0.93750
# [48/100] training 35.0% loss=0.13809, acc=0.96875
# [48/100] training 35.2% loss=0.30270, acc=0.87500
# [48/100] training 35.3% loss=0.20835, acc=0.93750
# [48/100] training 35.5% loss=0.16668, acc=0.95312
# [48/100] training 35.6% loss=0.30621, acc=0.90625
# [48/100] training 35.9% loss=0.14909, acc=0.92188
# [48/100] training 36.1% loss=0.31981, acc=0.89062
# [48/100] training 36.2% loss=0.20675, acc=0.93750
# [48/100] training 36.4% loss=0.29422, acc=0.90625
# [48/100] training 36.5% loss=0.34649, acc=0.85938
# [48/100] training 36.7% loss=0.18915, acc=0.93750
# [48/100] training 36.8% loss=0.09145, acc=0.98438
# [48/100] training 37.1% loss=0.38928, acc=0.85938
# [48/100] training 37.3% loss=0.19804, acc=0.95312
# [48/100] training 37.4% loss=0.13897, acc=0.96875
# [48/100] training 37.6% loss=0.13697, acc=0.98438
# [48/100] training 37.7% loss=0.40184, acc=0.84375
# [48/100] training 37.9% loss=0.12112, acc=0.96875
# [48/100] training 38.1% loss=0.22803, acc=0.92188
# [48/100] training 38.3% loss=0.19893, acc=0.90625
# [48/100] training 38.4% loss=0.08103, acc=0.95312
# [48/100] training 38.6% loss=0.14954, acc=0.95312
# [48/100] training 38.8% loss=0.33481, acc=0.89062
# [48/100] training 38.9% loss=0.17908, acc=0.89062
# [48/100] training 39.1% loss=0.18675, acc=0.92188
# [48/100] training 39.3% loss=0.21867, acc=0.90625
# [48/100] training 39.5% loss=0.24727, acc=0.92188
# [48/100] training 39.6% loss=0.15191, acc=0.93750
# [48/100] training 39.8% loss=0.08591, acc=0.98438
# [48/100] training 40.0% loss=0.26036, acc=0.87500
# [48/100] training 40.1% loss=0.25862, acc=0.87500
# [48/100] training 40.4% loss=0.12421, acc=0.93750
# [48/100] training 40.5% loss=0.23014, acc=0.90625
# [48/100] training 40.7% loss=0.18953, acc=0.92188
# [48/100] training 40.8% loss=0.11424, acc=0.95312
# [48/100] training 41.0% loss=0.07846, acc=0.98438
# [48/100] training 41.1% loss=0.11676, acc=0.95312
# [48/100] training 41.3% loss=0.11420, acc=0.95312
# [48/100] training 41.6% loss=0.32438, acc=0.89062
# [48/100] training 41.7% loss=0.22414, acc=0.90625
# [48/100] training 41.9% loss=0.19381, acc=0.93750
# [48/100] training 42.0% loss=0.32447, acc=0.85938
# [48/100] training 42.2% loss=0.25754, acc=0.92188
# [48/100] training 42.3% loss=0.14295, acc=0.92188
# [48/100] training 42.5% loss=0.16084, acc=0.93750
# [48/100] training 42.8% loss=0.11714, acc=0.96875
# [48/100] training 42.9% loss=0.12726, acc=0.96875
# [48/100] training 43.1% loss=0.16564, acc=0.95312
# [48/100] training 43.2% loss=0.10952, acc=0.96875
# [48/100] training 43.4% loss=0.19793, acc=0.93750
# [48/100] training 43.5% loss=0.13012, acc=0.95312
# [48/100] training 43.8% loss=0.27222, acc=0.90625
# [48/100] training 43.9% loss=0.14385, acc=0.92188
# [48/100] training 44.1% loss=0.21804, acc=0.95312
# [48/100] training 44.3% loss=0.11072, acc=0.96875
# [48/100] training 44.4% loss=0.29972, acc=0.89062
# [48/100] training 44.6% loss=0.15862, acc=0.93750
# [48/100] training 44.7% loss=0.35982, acc=0.90625
# [48/100] training 45.0% loss=0.10220, acc=0.95312
# [48/100] training 45.1% loss=0.23401, acc=0.93750
# [48/100] training 45.3% loss=0.18446, acc=0.93750
# [48/100] training 45.5% loss=0.07290, acc=0.98438
# [48/100] training 45.6% loss=0.21861, acc=0.90625
# [48/100] training 45.8% loss=0.11999, acc=0.95312
# [48/100] training 45.9% loss=0.11736, acc=0.96875
# [48/100] training 46.2% loss=0.05549, acc=1.00000
# [48/100] training 46.3% loss=0.10061, acc=0.95312
# [48/100] training 46.5% loss=0.23897, acc=0.92188
# [48/100] training 46.6% loss=0.13686, acc=0.95312
# [48/100] training 46.8% loss=0.16909, acc=0.90625
# [48/100] training 47.0% loss=0.21245, acc=0.90625
# [48/100] training 47.2% loss=0.16654, acc=0.93750
# [48/100] training 47.4% loss=0.20421, acc=0.90625
# [48/100] training 47.5% loss=0.25475, acc=0.90625
# [48/100] training 47.7% loss=0.20097, acc=0.92188
# [48/100] training 47.8% loss=0.27940, acc=0.93750
# [48/100] training 48.0% loss=0.16202, acc=0.92188
# [48/100] training 48.3% loss=0.07521, acc=0.98438
# [48/100] training 48.4% loss=0.05469, acc=0.98438
# [48/100] training 48.6% loss=0.09007, acc=0.95312
# [48/100] training 48.7% loss=0.24013, acc=0.89062
# [48/100] training 48.9% loss=0.11362, acc=0.95312
# [48/100] training 49.0% loss=0.08320, acc=0.98438
# [48/100] training 49.2% loss=0.25865, acc=0.90625
# [48/100] training 49.3% loss=0.24694, acc=0.92188
# [48/100] training 49.6% loss=0.23798, acc=0.90625
# [48/100] training 49.8% loss=0.13247, acc=0.93750
# [48/100] training 49.9% loss=0.22691, acc=0.92188
# [48/100] training 50.1% loss=0.09827, acc=0.95312
# [48/100] training 50.2% loss=0.20845, acc=0.89062
# [48/100] training 50.4% loss=0.25828, acc=0.89062
# [48/100] training 50.6% loss=0.12939, acc=0.93750
# [48/100] training 50.8% loss=0.23956, acc=0.90625
# [48/100] training 51.0% loss=0.24278, acc=0.87500
# [48/100] training 51.1% loss=0.23378, acc=0.90625
# [48/100] training 51.3% loss=0.22615, acc=0.90625
# [48/100] training 51.4% loss=0.19622, acc=0.95312
# [48/100] training 51.7% loss=0.21353, acc=0.87500
# [48/100] training 51.8% loss=0.23818, acc=0.87500
# [48/100] training 52.0% loss=0.20992, acc=0.92188
# [48/100] training 52.1% loss=0.14604, acc=0.96875
# [48/100] training 52.3% loss=0.25452, acc=0.87500
# [48/100] training 52.5% loss=0.11756, acc=0.95312
# [48/100] training 52.6% loss=0.17040, acc=0.92188
# [48/100] training 52.9% loss=0.27634, acc=0.89062
# [48/100] training 53.0% loss=0.08709, acc=0.96875
# [48/100] training 53.2% loss=0.20168, acc=0.95312
# [48/100] training 53.3% loss=0.16657, acc=0.96875
# [48/100] training 53.5% loss=0.25810, acc=0.89062
# [48/100] training 53.7% loss=0.08438, acc=0.95312
# [48/100] training 53.8% loss=0.22138, acc=0.92188
# [48/100] training 54.1% loss=0.27512, acc=0.84375
# [48/100] training 54.2% loss=0.09680, acc=0.96875
# [48/100] training 54.4% loss=0.11453, acc=0.95312
# [48/100] training 54.5% loss=0.22439, acc=0.90625
# [48/100] training 54.7% loss=0.26777, acc=0.90625
# [48/100] training 54.8% loss=0.09773, acc=0.98438
# [48/100] training 55.1% loss=0.17402, acc=0.93750
# [48/100] training 55.3% loss=0.12033, acc=0.96875
# [48/100] training 55.4% loss=0.11263, acc=0.96875
# [48/100] training 55.6% loss=0.14498, acc=0.93750
# [48/100] training 55.7% loss=0.07125, acc=0.98438
# [48/100] training 55.9% loss=0.13873, acc=0.93750
# [48/100] training 56.0% loss=0.14345, acc=0.93750
# [48/100] training 56.3% loss=0.37156, acc=0.87500
# [48/100] training 56.5% loss=0.13372, acc=0.92188
# [48/100] training 56.6% loss=0.18702, acc=0.92188
# [48/100] training 56.8% loss=0.22157, acc=0.90625
# [48/100] training 56.9% loss=0.11038, acc=0.96875
# [48/100] training 57.1% loss=0.23056, acc=0.92188
# [48/100] training 57.2% loss=0.15834, acc=0.95312
# [48/100] training 57.5% loss=0.14120, acc=0.93750
# [48/100] training 57.6% loss=0.09759, acc=0.95312
# [48/100] training 57.8% loss=0.14879, acc=0.93750
# [48/100] training 58.0% loss=0.07132, acc=0.98438
# [48/100] training 58.1% loss=0.16653, acc=0.95312
# [48/100] training 58.3% loss=0.23900, acc=0.92188
# [48/100] training 58.4% loss=0.30445, acc=0.90625
# [48/100] training 58.7% loss=0.25175, acc=0.90625
# [48/100] training 58.8% loss=0.32727, acc=0.87500
# [48/100] training 59.0% loss=0.12265, acc=1.00000
# [48/100] training 59.2% loss=0.25706, acc=0.92188
# [48/100] training 59.3% loss=0.16098, acc=0.92188
# [48/100] training 59.5% loss=0.19129, acc=0.92188
# [48/100] training 59.7% loss=0.18589, acc=0.93750
# [48/100] training 59.9% loss=0.13287, acc=0.98438
# [48/100] training 60.0% loss=0.14580, acc=0.95312
# [48/100] training 60.2% loss=0.13312, acc=0.92188
# [48/100] training 60.3% loss=0.26436, acc=0.90625
# [48/100] training 60.5% loss=0.23348, acc=0.90625
# [48/100] training 60.8% loss=0.26378, acc=0.84375
# [48/100] training 60.9% loss=0.25860, acc=0.89062
# [48/100] training 61.1% loss=0.19775, acc=0.92188
# [48/100] training 61.2% loss=0.21130, acc=0.90625
# [48/100] training 61.4% loss=0.33540, acc=0.89062
# [48/100] training 61.5% loss=0.36673, acc=0.81250
# [48/100] training 61.7% loss=0.20434, acc=0.89062
# [48/100] training 62.0% loss=0.33861, acc=0.90625
# [48/100] training 62.1% loss=0.35380, acc=0.81250
# [48/100] training 62.3% loss=0.12097, acc=0.96875
# [48/100] training 62.4% loss=0.12580, acc=0.93750
# [48/100] training 62.6% loss=0.24846, acc=0.90625
# [48/100] training 62.7% loss=0.15432, acc=0.92188
# [48/100] training 62.9% loss=0.15414, acc=0.93750
# [48/100] training 63.1% loss=0.23282, acc=0.92188
# [48/100] training 63.3% loss=0.18193, acc=0.89062
# [48/100] training 63.5% loss=0.18219, acc=0.95312
# [48/100] training 63.6% loss=0.31027, acc=0.87500
# [48/100] training 63.8% loss=0.35860, acc=0.82812
# [48/100] training 63.9% loss=0.18157, acc=0.92188
# [48/100] training 64.2% loss=0.12400, acc=0.93750
# [48/100] training 64.3% loss=0.18020, acc=0.93750
# [48/100] training 64.5% loss=0.16627, acc=0.90625
# [48/100] training 64.7% loss=0.15411, acc=0.92188
# [48/100] training 64.8% loss=0.39354, acc=0.85938
# [48/100] training 65.0% loss=0.15052, acc=0.93750
# [48/100] training 65.1% loss=0.28981, acc=0.87500
# [48/100] training 65.4% loss=0.11907, acc=0.96875
# [48/100] training 65.5% loss=0.08552, acc=1.00000
# [48/100] training 65.7% loss=0.18030, acc=0.92188
# [48/100] training 65.8% loss=0.22231, acc=0.87500
# [48/100] training 66.0% loss=0.16169, acc=0.95312
# [48/100] training 66.2% loss=0.08839, acc=0.98438
# [48/100] training 66.3% loss=0.23140, acc=0.87500
# [48/100] training 66.6% loss=0.18586, acc=0.92188
# [48/100] training 66.7% loss=0.15295, acc=0.96875
# [48/100] training 66.9% loss=0.20449, acc=0.92188
# [48/100] training 67.0% loss=0.13684, acc=0.96875
# [48/100] training 67.2% loss=0.11245, acc=0.95312
# [48/100] training 67.4% loss=0.12486, acc=0.95312
# [48/100] training 67.6% loss=0.10823, acc=0.95312
# [48/100] training 67.8% loss=0.14619, acc=0.98438
# [48/100] training 67.9% loss=0.06361, acc=0.98438
# [48/100] training 68.1% loss=0.17131, acc=0.92188
# [48/100] training 68.2% loss=0.08104, acc=0.95312
# [48/100] training 68.4% loss=0.11665, acc=0.95312
# [48/100] training 68.5% loss=0.10017, acc=0.93750
# [48/100] training 68.8% loss=0.05494, acc=0.98438
# [48/100] training 69.0% loss=0.19177, acc=0.93750
# [48/100] training 69.1% loss=0.17303, acc=0.93750
# [48/100] training 69.3% loss=0.17206, acc=0.92188
# [48/100] training 69.4% loss=0.16744, acc=0.90625
# [48/100] training 69.6% loss=0.13952, acc=0.96875
# [48/100] training 69.7% loss=0.27605, acc=0.92188
# [48/100] training 70.0% loss=0.12958, acc=0.92188
# [48/100] training 70.2% loss=0.33784, acc=0.92188
# [48/100] training 70.3% loss=0.16483, acc=0.93750
# [48/100] training 70.5% loss=0.14683, acc=0.96875
# [48/100] training 70.6% loss=0.15246, acc=0.93750
# [48/100] training 70.8% loss=0.22985, acc=0.93750
# [48/100] training 71.0% loss=0.14194, acc=0.95312
# [48/100] training 71.2% loss=0.15937, acc=0.93750
# [48/100] training 71.3% loss=0.09003, acc=0.98438
# [48/100] training 71.5% loss=0.24759, acc=0.92188
# [48/100] training 71.7% loss=0.21326, acc=0.90625
# [48/100] training 71.8% loss=0.19619, acc=0.92188
# [48/100] training 72.0% loss=0.12915, acc=0.93750
# [48/100] training 72.2% loss=0.20487, acc=0.89062
# [48/100] training 72.4% loss=0.16026, acc=0.95312
# [48/100] training 72.5% loss=0.20654, acc=0.93750
# [48/100] training 72.7% loss=0.27841, acc=0.93750
# [48/100] training 72.9% loss=0.09938, acc=0.95312
# [48/100] training 73.0% loss=0.12264, acc=0.96875
# [48/100] training 73.3% loss=0.20391, acc=0.93750
# [48/100] training 73.4% loss=0.15817, acc=0.93750
# [48/100] training 73.6% loss=0.14436, acc=0.95312
# [48/100] training 73.7% loss=0.20022, acc=0.93750
# [48/100] training 73.9% loss=0.19357, acc=0.90625
# [48/100] training 74.0% loss=0.19890, acc=0.93750
# [48/100] training 74.2% loss=0.12076, acc=0.96875
# [48/100] training 74.5% loss=0.18971, acc=0.89062
# [48/100] training 74.6% loss=0.37085, acc=0.85938
# [48/100] training 74.8% loss=0.30912, acc=0.87500
# [48/100] training 74.9% loss=0.21377, acc=0.92188
# [48/100] training 75.1% loss=0.21623, acc=0.93750
# [48/100] training 75.2% loss=0.13857, acc=0.93750
# [48/100] training 75.4% loss=0.18612, acc=0.90625
# [48/100] training 75.7% loss=0.16978, acc=0.98438
# [48/100] training 75.8% loss=0.29598, acc=0.90625
# [48/100] training 76.0% loss=0.09494, acc=0.96875
# [48/100] training 76.1% loss=0.24331, acc=0.92188
# [48/100] training 76.3% loss=0.13655, acc=0.96875
# [48/100] training 76.4% loss=0.22087, acc=0.90625
# [48/100] training 76.7% loss=0.24329, acc=0.92188
# [48/100] training 76.8% loss=0.17535, acc=0.96875
# [48/100] training 77.0% loss=0.08995, acc=0.95312
# [48/100] training 77.2% loss=0.19163, acc=0.93750
# [48/100] training 77.3% loss=0.17147, acc=0.90625
# [48/100] training 77.5% loss=0.18803, acc=0.92188
# [48/100] training 77.6% loss=0.20878, acc=0.93750
# [48/100] training 77.9% loss=0.20437, acc=0.92188
# [48/100] training 78.0% loss=0.14273, acc=0.92188
# [48/100] training 78.2% loss=0.21288, acc=0.90625
# [48/100] training 78.4% loss=0.09925, acc=0.98438
# [48/100] training 78.5% loss=0.21990, acc=0.92188
# [48/100] training 78.7% loss=0.20323, acc=0.92188
# [48/100] training 78.8% loss=0.05983, acc=0.98438
# [48/100] training 79.1% loss=0.09609, acc=0.96875
# [48/100] training 79.2% loss=0.12774, acc=0.98438
# [48/100] training 79.4% loss=0.16590, acc=0.92188
# [48/100] training 79.5% loss=0.12536, acc=0.96875
# [48/100] training 79.7% loss=0.06630, acc=0.98438
# [48/100] training 79.9% loss=0.13065, acc=0.93750
# [48/100] training 80.1% loss=0.09956, acc=0.96875
# [48/100] training 80.3% loss=0.25070, acc=0.89062
# [48/100] training 80.4% loss=0.18650, acc=0.93750
# [48/100] training 80.6% loss=0.22250, acc=0.92188
# [48/100] training 80.7% loss=0.16231, acc=0.93750
# [48/100] training 80.9% loss=0.22537, acc=0.92188
# [48/100] training 81.2% loss=0.23938, acc=0.89062
# [48/100] training 81.3% loss=0.23933, acc=0.90625
# [48/100] training 81.5% loss=0.17405, acc=0.92188
# [48/100] training 81.6% loss=0.16105, acc=0.90625
# [48/100] training 81.8% loss=0.23243, acc=0.90625
# [48/100] training 81.9% loss=0.25436, acc=0.93750
# [48/100] training 82.1% loss=0.11686, acc=0.95312
# [48/100] training 82.2% loss=0.20032, acc=0.93750
# [48/100] training 82.5% loss=0.14335, acc=0.93750
# [48/100] training 82.7% loss=0.13524, acc=0.95312
# [48/100] training 82.8% loss=0.18198, acc=0.89062
# [48/100] training 83.0% loss=0.14340, acc=0.96875
# [48/100] training 83.1% loss=0.11154, acc=0.96875
# [48/100] training 83.3% loss=0.08901, acc=0.98438
# [48/100] training 83.5% loss=0.19624, acc=0.93750
# [48/100] training 83.7% loss=0.29698, acc=0.93750
# [48/100] training 83.9% loss=0.22888, acc=0.93750
# [48/100] training 84.0% loss=0.08314, acc=0.98438
# [48/100] training 84.2% loss=0.10826, acc=0.95312
# [48/100] training 84.3% loss=0.14553, acc=0.95312
# [48/100] training 84.5% loss=0.20536, acc=0.89062
# [48/100] training 84.7% loss=0.17994, acc=0.90625
# [48/100] training 84.9% loss=0.12111, acc=0.96875
# [48/100] training 85.0% loss=0.24032, acc=0.92188
# [48/100] training 85.2% loss=0.17123, acc=0.90625
# [48/100] training 85.4% loss=0.16866, acc=0.90625
# [48/100] training 85.5% loss=0.15987, acc=0.93750
# [48/100] training 85.8% loss=0.19488, acc=0.93750
# [48/100] training 85.9% loss=0.14076, acc=0.93750
# [48/100] training 86.1% loss=0.12354, acc=0.95312
# [48/100] training 86.2% loss=0.07750, acc=0.96875
# [48/100] training 86.4% loss=0.23566, acc=0.90625
# [48/100] training 86.6% loss=0.18820, acc=0.92188
# [48/100] training 86.7% loss=0.16647, acc=0.93750
# [48/100] training 87.0% loss=0.24466, acc=0.89062
# [48/100] training 87.1% loss=0.11693, acc=0.93750
# [48/100] training 87.3% loss=0.18234, acc=0.90625
# [48/100] training 87.4% loss=0.20100, acc=0.95312
# [48/100] training 87.6% loss=0.20814, acc=0.92188
# [48/100] training 87.7% loss=0.20322, acc=0.93750
# [48/100] training 87.9% loss=0.21105, acc=0.90625
# [48/100] training 88.2% loss=0.14055, acc=0.93750
# [48/100] training 88.3% loss=0.31282, acc=0.89062
# [48/100] training 88.5% loss=0.20393, acc=0.92188
# [48/100] training 88.6% loss=0.13039, acc=0.96875
# [48/100] training 88.8% loss=0.21042, acc=0.89062
# [48/100] training 88.9% loss=0.26500, acc=0.92188
# [48/100] training 89.2% loss=0.22362, acc=0.87500
# [48/100] training 89.4% loss=0.15967, acc=0.93750
# [48/100] training 89.5% loss=0.17634, acc=0.93750
# [48/100] training 89.7% loss=0.23512, acc=0.90625
# [48/100] training 89.8% loss=0.21407, acc=0.95312
# [48/100] training 90.0% loss=0.12040, acc=0.96875
# [48/100] training 90.1% loss=0.23681, acc=0.87500
# [48/100] training 90.4% loss=0.21141, acc=0.90625
# [48/100] training 90.5% loss=0.13677, acc=0.95312
# [48/100] training 90.7% loss=0.16347, acc=0.92188
# [48/100] training 90.9% loss=0.18440, acc=0.92188
# [48/100] training 91.0% loss=0.22483, acc=0.93750
# [48/100] training 91.2% loss=0.19487, acc=0.89062
# [48/100] training 91.3% loss=0.25417, acc=0.90625
# [48/100] training 91.6% loss=0.27252, acc=0.90625
# [48/100] training 91.7% loss=0.15557, acc=0.93750
# [48/100] training 91.9% loss=0.25951, acc=0.90625
# [48/100] training 92.1% loss=0.18506, acc=0.89062
# [48/100] training 92.2% loss=0.12721, acc=0.95312
# [48/100] training 92.4% loss=0.14037, acc=0.95312
# [48/100] training 92.6% loss=0.27065, acc=0.89062
# [48/100] training 92.8% loss=0.15408, acc=0.92188
# [48/100] training 92.9% loss=0.28775, acc=0.87500
# [48/100] training 93.1% loss=0.39973, acc=0.87500
# [48/100] training 93.2% loss=0.17418, acc=0.93750
# [48/100] training 93.4% loss=0.24409, acc=0.92188
# [48/100] training 93.7% loss=0.10576, acc=0.98438
# [48/100] training 93.8% loss=0.19400, acc=0.87500
# [48/100] training 94.0% loss=0.15165, acc=0.95312
# [48/100] training 94.1% loss=0.23732, acc=0.92188
# [48/100] training 94.3% loss=0.14565, acc=0.95312
# [48/100] training 94.4% loss=0.13782, acc=0.96875
# [48/100] training 94.6% loss=0.14734, acc=0.92188
# [48/100] training 94.9% loss=0.11279, acc=0.95312
# [48/100] training 95.0% loss=0.19108, acc=0.90625
# [48/100] training 95.2% loss=0.34144, acc=0.85938
# [48/100] training 95.3% loss=0.26545, acc=0.90625
# [48/100] training 95.5% loss=0.09761, acc=0.98438
# [48/100] training 95.6% loss=0.17904, acc=0.90625
# [48/100] training 95.8% loss=0.10526, acc=0.96875
# [48/100] training 96.0% loss=0.24290, acc=0.92188
# [48/100] training 96.2% loss=0.08548, acc=0.98438
# [48/100] training 96.4% loss=0.15537, acc=0.96875
# [48/100] training 96.5% loss=0.14181, acc=0.95312
# [48/100] training 96.7% loss=0.11706, acc=0.96875
# [48/100] training 96.8% loss=0.26678, acc=0.90625
# [48/100] training 97.1% loss=0.14653, acc=0.95312
# [48/100] training 97.2% loss=0.22548, acc=0.92188
# [48/100] training 97.4% loss=0.21440, acc=0.90625
# [48/100] training 97.6% loss=0.25367, acc=0.92188
# [48/100] training 97.7% loss=0.14955, acc=0.93750
# [48/100] training 97.9% loss=0.15009, acc=0.95312
# [48/100] training 98.0% loss=0.16731, acc=0.92188
# [48/100] training 98.3% loss=0.30460, acc=0.84375
# [48/100] training 98.4% loss=0.09956, acc=0.95312
# [48/100] training 98.6% loss=0.23687, acc=0.89062
# [48/100] training 98.7% loss=0.22303, acc=0.93750
# [48/100] training 98.9% loss=0.17105, acc=0.93750
# [48/100] training 99.1% loss=0.19645, acc=0.93750
# [48/100] training 99.2% loss=0.16994, acc=0.92188
# [48/100] training 99.5% loss=0.29653, acc=0.89062
# [48/100] training 99.6% loss=0.13674, acc=0.93750
# [48/100] training 99.8% loss=0.15414, acc=0.96875
# [48/100] training 99.9% loss=0.06230, acc=0.96875
# [48/100] testing 0.9% loss=0.15241, acc=0.93750
# [48/100] testing 1.8% loss=0.46596, acc=0.82812
# [48/100] testing 2.2% loss=0.38629, acc=0.89062
# [48/100] testing 3.1% loss=0.31656, acc=0.90625
# [48/100] testing 3.5% loss=0.10865, acc=0.96875
# [48/100] testing 4.4% loss=0.20615, acc=0.90625
# [48/100] testing 4.8% loss=0.22871, acc=0.92188
# [48/100] testing 5.7% loss=0.23624, acc=0.87500
# [48/100] testing 6.6% loss=0.09876, acc=0.96875
# [48/100] testing 7.0% loss=0.11413, acc=0.98438
# [48/100] testing 7.9% loss=0.21873, acc=0.89062
# [48/100] testing 8.3% loss=0.34297, acc=0.85938
# [48/100] testing 9.2% loss=0.28244, acc=0.89062
# [48/100] testing 9.7% loss=0.07698, acc=0.98438
# [48/100] testing 10.5% loss=0.16765, acc=0.90625
# [48/100] testing 11.0% loss=0.29288, acc=0.89062
# [48/100] testing 11.8% loss=0.20813, acc=0.93750
# [48/100] testing 12.7% loss=0.37027, acc=0.89062
# [48/100] testing 13.2% loss=0.18412, acc=0.90625
# [48/100] testing 14.0% loss=0.47613, acc=0.89062
# [48/100] testing 14.5% loss=0.21259, acc=0.92188
# [48/100] testing 15.4% loss=0.32557, acc=0.89062
# [48/100] testing 15.8% loss=0.22111, acc=0.90625
# [48/100] testing 16.7% loss=0.24058, acc=0.90625
# [48/100] testing 17.5% loss=0.14963, acc=0.93750
# [48/100] testing 18.0% loss=0.21220, acc=0.89062
# [48/100] testing 18.9% loss=0.10528, acc=0.93750
# [48/100] testing 19.3% loss=0.38236, acc=0.85938
# [48/100] testing 20.2% loss=0.40929, acc=0.89062
# [48/100] testing 20.6% loss=0.29133, acc=0.87500
# [48/100] testing 21.5% loss=0.17641, acc=0.92188
# [48/100] testing 21.9% loss=0.44538, acc=0.87500
# [48/100] testing 22.8% loss=0.27658, acc=0.90625
# [48/100] testing 23.7% loss=0.40212, acc=0.90625
# [48/100] testing 24.1% loss=0.19217, acc=0.93750
# [48/100] testing 25.0% loss=0.40384, acc=0.87500
# [48/100] testing 25.4% loss=0.18685, acc=0.92188
# [48/100] testing 26.3% loss=0.24061, acc=0.90625
# [48/100] testing 26.8% loss=0.30933, acc=0.92188
# [48/100] testing 27.6% loss=0.28113, acc=0.87500
# [48/100] testing 28.5% loss=0.25820, acc=0.92188
# [48/100] testing 29.0% loss=0.26373, acc=0.92188
# [48/100] testing 29.8% loss=0.39534, acc=0.89062
# [48/100] testing 30.3% loss=0.27787, acc=0.90625
# [48/100] testing 31.1% loss=0.26295, acc=0.89062
# [48/100] testing 31.6% loss=0.23131, acc=0.92188
# [48/100] testing 32.5% loss=0.25343, acc=0.89062
# [48/100] testing 32.9% loss=0.50186, acc=0.90625
# [48/100] testing 33.8% loss=0.25984, acc=0.90625
# [48/100] testing 34.7% loss=0.44528, acc=0.87500
# [48/100] testing 35.1% loss=0.12133, acc=0.95312
# [48/100] testing 36.0% loss=0.38669, acc=0.89062
# [48/100] testing 36.4% loss=0.26170, acc=0.92188
# [48/100] testing 37.3% loss=0.31702, acc=0.92188
# [48/100] testing 37.7% loss=0.44199, acc=0.84375
# [48/100] testing 38.6% loss=0.23029, acc=0.93750
# [48/100] testing 39.5% loss=0.28314, acc=0.93750
# [48/100] testing 39.9% loss=0.30766, acc=0.92188
# [48/100] testing 40.8% loss=0.38049, acc=0.93750
# [48/100] testing 41.2% loss=0.28787, acc=0.93750
# [48/100] testing 42.1% loss=0.23322, acc=0.90625
# [48/100] testing 42.5% loss=0.13747, acc=0.95312
# [48/100] testing 43.4% loss=0.27991, acc=0.92188
# [48/100] testing 43.9% loss=0.12855, acc=0.96875
# [48/100] testing 44.7% loss=0.22642, acc=0.89062
# [48/100] testing 45.6% loss=0.24553, acc=0.93750
# [48/100] testing 46.1% loss=0.25061, acc=0.85938
# [48/100] testing 46.9% loss=0.15461, acc=0.92188
# [48/100] testing 47.4% loss=0.11237, acc=0.96875
# [48/100] testing 48.3% loss=0.38469, acc=0.89062
# [48/100] testing 48.7% loss=0.30190, acc=0.90625
# [48/100] testing 49.6% loss=0.48548, acc=0.82812
# [48/100] testing 50.4% loss=0.22103, acc=0.92188
# [48/100] testing 50.9% loss=0.34525, acc=0.87500
# [48/100] testing 51.8% loss=0.23882, acc=0.84375
# [48/100] testing 52.2% loss=0.25843, acc=0.89062
# [48/100] testing 53.1% loss=0.19281, acc=0.93750
# [48/100] testing 53.5% loss=0.22524, acc=0.92188
# [48/100] testing 54.4% loss=0.36048, acc=0.82812
# [48/100] testing 54.8% loss=0.30575, acc=0.82812
# [48/100] testing 55.7% loss=0.11937, acc=0.96875
# [48/100] testing 56.6% loss=0.28518, acc=0.87500
# [48/100] testing 57.0% loss=0.36938, acc=0.92188
# [48/100] testing 57.9% loss=0.27123, acc=0.90625
# [48/100] testing 58.3% loss=0.37116, acc=0.87500
# [48/100] testing 59.2% loss=0.22890, acc=0.89062
# [48/100] testing 59.7% loss=0.25029, acc=0.92188
# [48/100] testing 60.5% loss=0.44148, acc=0.87500
# [48/100] testing 61.4% loss=0.24680, acc=0.92188
# [48/100] testing 61.9% loss=0.22152, acc=0.89062
# [48/100] testing 62.7% loss=0.22602, acc=0.90625
# [48/100] testing 63.2% loss=0.39958, acc=0.85938
# [48/100] testing 64.0% loss=0.47403, acc=0.87500
# [48/100] testing 64.5% loss=0.15696, acc=0.92188
# [48/100] testing 65.4% loss=0.16408, acc=0.93750
# [48/100] testing 65.8% loss=0.30437, acc=0.89062
# [48/100] testing 66.7% loss=0.22526, acc=0.90625
# [48/100] testing 67.6% loss=0.30352, acc=0.92188
# [48/100] testing 68.0% loss=0.09017, acc=0.95312
# [48/100] testing 68.9% loss=0.30634, acc=0.93750
# [48/100] testing 69.3% loss=0.29990, acc=0.87500
# [48/100] testing 70.2% loss=0.39904, acc=0.81250
# [48/100] testing 70.6% loss=0.27902, acc=0.89062
# [48/100] testing 71.5% loss=0.34350, acc=0.89062
# [48/100] testing 72.4% loss=0.15086, acc=0.90625
# [48/100] testing 72.8% loss=0.14217, acc=0.95312
# [48/100] testing 73.7% loss=0.10528, acc=0.95312
# [48/100] testing 74.1% loss=0.45276, acc=0.85938
# [48/100] testing 75.0% loss=0.29813, acc=0.90625
# [48/100] testing 75.4% loss=0.34845, acc=0.82812
# [48/100] testing 76.3% loss=0.08087, acc=0.96875
# [48/100] testing 76.8% loss=0.27056, acc=0.92188
# [48/100] testing 77.6% loss=0.29186, acc=0.90625
# [48/100] testing 78.5% loss=0.31833, acc=0.84375
# [48/100] testing 79.0% loss=0.27848, acc=0.89062
# [48/100] testing 79.8% loss=0.41935, acc=0.85938
# [48/100] testing 80.3% loss=0.27475, acc=0.89062
# [48/100] testing 81.2% loss=0.45719, acc=0.87500
# [48/100] testing 81.6% loss=0.17950, acc=0.89062
# [48/100] testing 82.5% loss=0.19356, acc=0.92188
# [48/100] testing 83.3% loss=0.17461, acc=0.93750
# [48/100] testing 83.8% loss=0.24593, acc=0.93750
# [48/100] testing 84.7% loss=0.31173, acc=0.87500
# [48/100] testing 85.1% loss=0.18391, acc=0.89062
# [48/100] testing 86.0% loss=0.25483, acc=0.90625
# [48/100] testing 86.4% loss=0.36276, acc=0.85938
# [48/100] testing 87.3% loss=0.22001, acc=0.89062
# [48/100] testing 87.7% loss=0.23590, acc=0.93750
# [48/100] testing 88.6% loss=0.24490, acc=0.95312
# [48/100] testing 89.5% loss=0.53146, acc=0.79688
# [48/100] testing 89.9% loss=0.14634, acc=0.89062
# [48/100] testing 90.8% loss=0.25036, acc=0.95312
# [48/100] testing 91.2% loss=0.22813, acc=0.93750
# [48/100] testing 92.1% loss=0.23435, acc=0.92188
# [48/100] testing 92.6% loss=0.45687, acc=0.85938
# [48/100] testing 93.4% loss=0.28312, acc=0.85938
# [48/100] testing 94.3% loss=0.10874, acc=0.96875
# [48/100] testing 94.7% loss=0.17091, acc=0.93750
# [48/100] testing 95.6% loss=0.32575, acc=0.89062
# [48/100] testing 96.1% loss=0.20774, acc=0.92188
# [48/100] testing 96.9% loss=0.36458, acc=0.89062
# [48/100] testing 97.4% loss=0.13105, acc=0.96875
# [48/100] testing 98.3% loss=0.26163, acc=0.89062
# [48/100] testing 98.7% loss=0.21831, acc=0.90625
# [48/100] testing 99.6% loss=0.40592, acc=0.87500
# [49/100] training 0.2% loss=0.32265, acc=0.85938
# [49/100] training 0.4% loss=0.33594, acc=0.85938
# [49/100] training 0.5% loss=0.12575, acc=0.96875
# [49/100] training 0.8% loss=0.17567, acc=0.92188
# [49/100] training 0.9% loss=0.21200, acc=0.93750
# [49/100] training 1.1% loss=0.24259, acc=0.93750
# [49/100] training 1.2% loss=0.20954, acc=0.89062
# [49/100] training 1.4% loss=0.09274, acc=0.96875
# [49/100] training 1.6% loss=0.12105, acc=0.93750
# [49/100] training 1.8% loss=0.18122, acc=0.92188
# [49/100] training 2.0% loss=0.21457, acc=0.93750
# [49/100] training 2.1% loss=0.20345, acc=0.85938
# [49/100] training 2.3% loss=0.14557, acc=0.90625
# [49/100] training 2.4% loss=0.19065, acc=0.93750
# [49/100] training 2.6% loss=0.11415, acc=0.95312
# [49/100] training 2.7% loss=0.25823, acc=0.87500
# [49/100] training 3.0% loss=0.20878, acc=0.95312
# [49/100] training 3.2% loss=0.11823, acc=0.95312
# [49/100] training 3.3% loss=0.42442, acc=0.87500
# [49/100] training 3.5% loss=0.20480, acc=0.87500
# [49/100] training 3.6% loss=0.26342, acc=0.89062
# [49/100] training 3.8% loss=0.17440, acc=0.93750
# [49/100] training 3.9% loss=0.23127, acc=0.85938
# [49/100] training 4.2% loss=0.14935, acc=0.92188
# [49/100] training 4.4% loss=0.14116, acc=0.95312
# [49/100] training 4.5% loss=0.12969, acc=0.95312
# [49/100] training 4.7% loss=0.32284, acc=0.89062
# [49/100] training 4.8% loss=0.12579, acc=0.96875
# [49/100] training 5.0% loss=0.10134, acc=0.96875
# [49/100] training 5.2% loss=0.23908, acc=0.87500
# [49/100] training 5.4% loss=0.07345, acc=1.00000
# [49/100] training 5.5% loss=0.19802, acc=0.90625
# [49/100] training 5.7% loss=0.11549, acc=0.95312
# [49/100] training 5.9% loss=0.21442, acc=0.92188
# [49/100] training 6.0% loss=0.20193, acc=0.92188
# [49/100] training 6.3% loss=0.13104, acc=0.96875
# [49/100] training 6.4% loss=0.16813, acc=0.93750
# [49/100] training 6.6% loss=0.16307, acc=0.92188
# [49/100] training 6.7% loss=0.26879, acc=0.89062
# [49/100] training 6.9% loss=0.20026, acc=0.92188
# [49/100] training 7.1% loss=0.13395, acc=0.93750
# [49/100] training 7.2% loss=0.24183, acc=0.89062
# [49/100] training 7.5% loss=0.19172, acc=0.93750
# [49/100] training 7.6% loss=0.23846, acc=0.89062
# [49/100] training 7.8% loss=0.10844, acc=0.96875
# [49/100] training 7.9% loss=0.29185, acc=0.85938
# [49/100] training 8.1% loss=0.09852, acc=0.98438
# [49/100] training 8.2% loss=0.17519, acc=0.95312
# [49/100] training 8.4% loss=0.19963, acc=0.90625
# [49/100] training 8.7% loss=0.19380, acc=0.93750
# [49/100] training 8.8% loss=0.25038, acc=0.89062
# [49/100] training 9.0% loss=0.26315, acc=0.89062
# [49/100] training 9.1% loss=0.25951, acc=0.92188
# [49/100] training 9.3% loss=0.25776, acc=0.92188
# [49/100] training 9.4% loss=0.19341, acc=0.95312
# [49/100] training 9.7% loss=0.26396, acc=0.89062
# [49/100] training 9.9% loss=0.23472, acc=0.90625
# [49/100] training 10.0% loss=0.24115, acc=0.89062
# [49/100] training 10.2% loss=0.18584, acc=0.93750
# [49/100] training 10.3% loss=0.20022, acc=0.90625
# [49/100] training 10.5% loss=0.21779, acc=0.90625
# [49/100] training 10.6% loss=0.15907, acc=0.95312
# [49/100] training 10.9% loss=0.11798, acc=0.96875
# [49/100] training 11.0% loss=0.32377, acc=0.87500
# [49/100] training 11.2% loss=0.09580, acc=0.98438
# [49/100] training 11.4% loss=0.20108, acc=0.90625
# [49/100] training 11.5% loss=0.31828, acc=0.89062
# [49/100] training 11.7% loss=0.10646, acc=0.98438
# [49/100] training 11.8% loss=0.19576, acc=0.92188
# [49/100] training 12.1% loss=0.16405, acc=0.93750
# [49/100] training 12.2% loss=0.12825, acc=0.98438
# [49/100] training 12.4% loss=0.25899, acc=0.92188
# [49/100] training 12.6% loss=0.27200, acc=0.90625
# [49/100] training 12.7% loss=0.19673, acc=0.90625
# [49/100] training 12.9% loss=0.16866, acc=0.95312
# [49/100] training 13.0% loss=0.15549, acc=0.93750
# [49/100] training 13.3% loss=0.18834, acc=0.92188
# [49/100] training 13.4% loss=0.29122, acc=0.90625
# [49/100] training 13.6% loss=0.22197, acc=0.90625
# [49/100] training 13.7% loss=0.33510, acc=0.85938
# [49/100] training 13.9% loss=0.31906, acc=0.92188
# [49/100] training 14.1% loss=0.18446, acc=0.95312
# [49/100] training 14.3% loss=0.11621, acc=0.98438
# [49/100] training 14.5% loss=0.17466, acc=0.93750
# [49/100] training 14.6% loss=0.14085, acc=0.96875
# [49/100] training 14.8% loss=0.22767, acc=0.87500
# [49/100] training 14.9% loss=0.18080, acc=0.92188
# [49/100] training 15.1% loss=0.30574, acc=0.87500
# [49/100] training 15.4% loss=0.19882, acc=0.92188
# [49/100] training 15.5% loss=0.13515, acc=0.95312
# [49/100] training 15.7% loss=0.34751, acc=0.84375
# [49/100] training 15.8% loss=0.15638, acc=0.95312
# [49/100] training 16.0% loss=0.26450, acc=0.89062
# [49/100] training 16.1% loss=0.31824, acc=0.90625
# [49/100] training 16.3% loss=0.23387, acc=0.90625
# [49/100] training 16.4% loss=0.15296, acc=0.95312
# [49/100] training 16.7% loss=0.24076, acc=0.87500
# [49/100] training 16.9% loss=0.19923, acc=0.90625
# [49/100] training 17.0% loss=0.18043, acc=0.92188
# [49/100] training 17.2% loss=0.19285, acc=0.92188
# [49/100] training 17.3% loss=0.21486, acc=0.90625
# [49/100] training 17.5% loss=0.20322, acc=0.90625
# [49/100] training 17.7% loss=0.17493, acc=0.92188
# [49/100] training 17.9% loss=0.20076, acc=0.93750
# [49/100] training 18.1% loss=0.26484, acc=0.90625
# [49/100] training 18.2% loss=0.21541, acc=0.93750
# [49/100] training 18.4% loss=0.34574, acc=0.81250
# [49/100] training 18.5% loss=0.17946, acc=0.95312
# [49/100] training 18.8% loss=0.16053, acc=0.93750
# [49/100] training 18.9% loss=0.13985, acc=0.92188
# [49/100] training 19.1% loss=0.24024, acc=0.87500
# [49/100] training 19.2% loss=0.08825, acc=0.98438
# [49/100] training 19.4% loss=0.13932, acc=0.93750
# [49/100] training 19.6% loss=0.27800, acc=0.89062
# [49/100] training 19.7% loss=0.24084, acc=0.93750
# [49/100] training 20.0% loss=0.19895, acc=0.92188
# [49/100] training 20.1% loss=0.19116, acc=0.93750
# [49/100] training 20.3% loss=0.18351, acc=0.90625
# [49/100] training 20.4% loss=0.20792, acc=0.92188
# [49/100] training 20.6% loss=0.24491, acc=0.92188
# [49/100] training 20.8% loss=0.18620, acc=0.90625
# [49/100] training 20.9% loss=0.20115, acc=0.92188
# [49/100] training 21.2% loss=0.19026, acc=0.95312
# [49/100] training 21.3% loss=0.19038, acc=0.95312
# [49/100] training 21.5% loss=0.20342, acc=0.95312
# [49/100] training 21.6% loss=0.11380, acc=0.93750
# [49/100] training 21.8% loss=0.18746, acc=0.90625
# [49/100] training 21.9% loss=0.20591, acc=0.92188
# [49/100] training 22.2% loss=0.19040, acc=0.92188
# [49/100] training 22.4% loss=0.24874, acc=0.87500
# [49/100] training 22.5% loss=0.15362, acc=0.92188
# [49/100] training 22.7% loss=0.16432, acc=0.95312
# [49/100] training 22.8% loss=0.13508, acc=0.96875
# [49/100] training 23.0% loss=0.08071, acc=0.96875
# [49/100] training 23.1% loss=0.24357, acc=0.90625
# [49/100] training 23.4% loss=0.24712, acc=0.87500
# [49/100] training 23.6% loss=0.23358, acc=0.90625
# [49/100] training 23.7% loss=0.19184, acc=0.90625
# [49/100] training 23.9% loss=0.19423, acc=0.92188
# [49/100] training 24.0% loss=0.09848, acc=0.96875
# [49/100] training 24.2% loss=0.13420, acc=0.92188
# [49/100] training 24.3% loss=0.24397, acc=0.93750
# [49/100] training 24.6% loss=0.21114, acc=0.90625
# [49/100] training 24.7% loss=0.26017, acc=0.89062
# [49/100] training 24.9% loss=0.15858, acc=0.93750
# [49/100] training 25.1% loss=0.20128, acc=0.89062
# [49/100] training 25.2% loss=0.13913, acc=0.95312
# [49/100] training 25.4% loss=0.22024, acc=0.90625
# [49/100] training 25.6% loss=0.14619, acc=0.93750
# [49/100] training 25.8% loss=0.21185, acc=0.92188
# [49/100] training 25.9% loss=0.11747, acc=0.93750
# [49/100] training 26.1% loss=0.14344, acc=0.92188
# [49/100] training 26.3% loss=0.14446, acc=0.90625
# [49/100] training 26.4% loss=0.08978, acc=0.98438
# [49/100] training 26.6% loss=0.04774, acc=1.00000
# [49/100] training 26.8% loss=0.11067, acc=0.96875
# [49/100] training 27.0% loss=0.14981, acc=0.93750
# [49/100] training 27.1% loss=0.19658, acc=0.90625
# [49/100] training 27.3% loss=0.15470, acc=0.92188
# [49/100] training 27.4% loss=0.05193, acc=0.96875
# [49/100] training 27.6% loss=0.38680, acc=0.89062
# [49/100] training 27.9% loss=0.11546, acc=0.93750
# [49/100] training 28.0% loss=0.25267, acc=0.89062
# [49/100] training 28.2% loss=0.10727, acc=0.95312
# [49/100] training 28.3% loss=0.12584, acc=0.95312
# [49/100] training 28.5% loss=0.23333, acc=0.87500
# [49/100] training 28.6% loss=0.14236, acc=0.95312
# [49/100] training 28.8% loss=0.10088, acc=0.98438
# [49/100] training 29.1% loss=0.13245, acc=0.96875
# [49/100] training 29.2% loss=0.10247, acc=0.93750
# [49/100] training 29.4% loss=0.22715, acc=0.89062
# [49/100] training 29.5% loss=0.07154, acc=0.95312
# [49/100] training 29.7% loss=0.21593, acc=0.92188
# [49/100] training 29.8% loss=0.14683, acc=0.93750
# [49/100] training 30.0% loss=0.21821, acc=0.87500
# [49/100] training 30.2% loss=0.10970, acc=0.92188
# [49/100] training 30.4% loss=0.11582, acc=0.96875
# [49/100] training 30.6% loss=0.20845, acc=0.89062
# [49/100] training 30.7% loss=0.20889, acc=0.90625
# [49/100] training 30.9% loss=0.42646, acc=0.92188
# [49/100] training 31.0% loss=0.13104, acc=0.95312
# [49/100] training 31.3% loss=0.14018, acc=0.95312
# [49/100] training 31.4% loss=0.26506, acc=0.87500
# [49/100] training 31.6% loss=0.23565, acc=0.93750
# [49/100] training 31.8% loss=0.16500, acc=0.92188
# [49/100] training 31.9% loss=0.22659, acc=0.90625
# [49/100] training 32.1% loss=0.19252, acc=0.92188
# [49/100] training 32.2% loss=0.28418, acc=0.92188
# [49/100] training 32.5% loss=0.12652, acc=0.96875
# [49/100] training 32.6% loss=0.23859, acc=0.87500
# [49/100] training 32.8% loss=0.17489, acc=0.92188
# [49/100] training 32.9% loss=0.19625, acc=0.93750
# [49/100] training 33.1% loss=0.12564, acc=0.96875
# [49/100] training 33.3% loss=0.19230, acc=0.89062
# [49/100] training 33.4% loss=0.17501, acc=0.93750
# [49/100] training 33.7% loss=0.16398, acc=0.93750
# [49/100] training 33.8% loss=0.19775, acc=0.92188
# [49/100] training 34.0% loss=0.15165, acc=0.95312
# [49/100] training 34.1% loss=0.20270, acc=0.92188
# [49/100] training 34.3% loss=0.19641, acc=0.92188
# [49/100] training 34.5% loss=0.30981, acc=0.85938
# [49/100] training 34.7% loss=0.14078, acc=0.93750
# [49/100] training 34.9% loss=0.13671, acc=0.93750
# [49/100] training 35.0% loss=0.13826, acc=0.93750
# [49/100] training 35.2% loss=0.26353, acc=0.89062
# [49/100] training 35.3% loss=0.27905, acc=0.89062
# [49/100] training 35.5% loss=0.14249, acc=0.92188
# [49/100] training 35.6% loss=0.41809, acc=0.78125
# [49/100] training 35.9% loss=0.14025, acc=0.95312
# [49/100] training 36.1% loss=0.21115, acc=0.92188
# [49/100] training 36.2% loss=0.20439, acc=0.90625
# [49/100] training 36.4% loss=0.15605, acc=0.96875
# [49/100] training 36.5% loss=0.16698, acc=0.93750
# [49/100] training 36.7% loss=0.27333, acc=0.90625
# [49/100] training 36.8% loss=0.10798, acc=0.95312
# [49/100] training 37.1% loss=0.21427, acc=0.90625
# [49/100] training 37.3% loss=0.20830, acc=0.89062
# [49/100] training 37.4% loss=0.15858, acc=0.93750
# [49/100] training 37.6% loss=0.08459, acc=0.96875
# [49/100] training 37.7% loss=0.20750, acc=0.92188
# [49/100] training 37.9% loss=0.17264, acc=0.92188
# [49/100] training 38.1% loss=0.22178, acc=0.89062
# [49/100] training 38.3% loss=0.08894, acc=0.93750
# [49/100] training 38.4% loss=0.07196, acc=0.98438
# [49/100] training 38.6% loss=0.14809, acc=0.92188
# [49/100] training 38.8% loss=0.35889, acc=0.85938
# [49/100] training 38.9% loss=0.23924, acc=0.89062
# [49/100] training 39.1% loss=0.30111, acc=0.85938
# [49/100] training 39.3% loss=0.26540, acc=0.89062
# [49/100] training 39.5% loss=0.21624, acc=0.93750
# [49/100] training 39.6% loss=0.12889, acc=0.96875
# [49/100] training 39.8% loss=0.13748, acc=0.93750
# [49/100] training 40.0% loss=0.11968, acc=0.93750
# [49/100] training 40.1% loss=0.29819, acc=0.82812
# [49/100] training 40.4% loss=0.14313, acc=0.92188
# [49/100] training 40.5% loss=0.21640, acc=0.92188
# [49/100] training 40.7% loss=0.18388, acc=0.89062
# [49/100] training 40.8% loss=0.09914, acc=0.95312
# [49/100] training 41.0% loss=0.14023, acc=0.93750
# [49/100] training 41.1% loss=0.29197, acc=0.87500
# [49/100] training 41.3% loss=0.20442, acc=0.92188
# [49/100] training 41.6% loss=0.31983, acc=0.85938
# [49/100] training 41.7% loss=0.19489, acc=0.92188
# [49/100] training 41.9% loss=0.14727, acc=0.98438
# [49/100] training 42.0% loss=0.26136, acc=0.90625
# [49/100] training 42.2% loss=0.30707, acc=0.90625
# [49/100] training 42.3% loss=0.13463, acc=0.95312
# [49/100] training 42.5% loss=0.19983, acc=0.92188
# [49/100] training 42.8% loss=0.16552, acc=0.93750
# [49/100] training 42.9% loss=0.13022, acc=0.96875
# [49/100] training 43.1% loss=0.24176, acc=0.90625
# [49/100] training 43.2% loss=0.15768, acc=0.93750
# [49/100] training 43.4% loss=0.16049, acc=0.93750
# [49/100] training 43.5% loss=0.23220, acc=0.89062
# [49/100] training 43.8% loss=0.19296, acc=0.92188
# [49/100] training 43.9% loss=0.22055, acc=0.90625
# [49/100] training 44.1% loss=0.15656, acc=0.95312
# [49/100] training 44.3% loss=0.14797, acc=0.93750
# [49/100] training 44.4% loss=0.26098, acc=0.87500
# [49/100] training 44.6% loss=0.19514, acc=0.90625
# [49/100] training 44.7% loss=0.26189, acc=0.90625
# [49/100] training 45.0% loss=0.09781, acc=0.96875
# [49/100] training 45.1% loss=0.22549, acc=0.93750
# [49/100] training 45.3% loss=0.10840, acc=0.95312
# [49/100] training 45.5% loss=0.10070, acc=0.96875
# [49/100] training 45.6% loss=0.16997, acc=0.93750
# [49/100] training 45.8% loss=0.12510, acc=0.92188
# [49/100] training 45.9% loss=0.12934, acc=0.93750
# [49/100] training 46.2% loss=0.11905, acc=0.95312
# [49/100] training 46.3% loss=0.10741, acc=0.92188
# [49/100] training 46.5% loss=0.34470, acc=0.90625
# [49/100] training 46.6% loss=0.17207, acc=0.90625
# [49/100] training 46.8% loss=0.11516, acc=0.96875
# [49/100] training 47.0% loss=0.25107, acc=0.90625
# [49/100] training 47.2% loss=0.15523, acc=0.90625
# [49/100] training 47.4% loss=0.22779, acc=0.92188
# [49/100] training 47.5% loss=0.32790, acc=0.89062
# [49/100] training 47.7% loss=0.16470, acc=0.92188
# [49/100] training 47.8% loss=0.30537, acc=0.90625
# [49/100] training 48.0% loss=0.18115, acc=0.95312
# [49/100] training 48.3% loss=0.09014, acc=0.96875
# [49/100] training 48.4% loss=0.08775, acc=0.98438
# [49/100] training 48.6% loss=0.10411, acc=0.95312
# [49/100] training 48.7% loss=0.29614, acc=0.90625
# [49/100] training 48.9% loss=0.15969, acc=0.93750
# [49/100] training 49.0% loss=0.14400, acc=0.93750
# [49/100] training 49.2% loss=0.23029, acc=0.92188
# [49/100] training 49.3% loss=0.15088, acc=0.93750
# [49/100] training 49.6% loss=0.20526, acc=0.93750
# [49/100] training 49.8% loss=0.27883, acc=0.92188
# [49/100] training 49.9% loss=0.25470, acc=0.89062
# [49/100] training 50.1% loss=0.13349, acc=0.92188
# [49/100] training 50.2% loss=0.27655, acc=0.89062
# [49/100] training 50.4% loss=0.36748, acc=0.89062
# [49/100] training 50.6% loss=0.21773, acc=0.89062
# [49/100] training 50.8% loss=0.27418, acc=0.89062
# [49/100] training 51.0% loss=0.24422, acc=0.92188
# [49/100] training 51.1% loss=0.25965, acc=0.87500
# [49/100] training 51.3% loss=0.23100, acc=0.92188
# [49/100] training 51.4% loss=0.13700, acc=0.95312
# [49/100] training 51.7% loss=0.24102, acc=0.89062
# [49/100] training 51.8% loss=0.25690, acc=0.89062
# [49/100] training 52.0% loss=0.18794, acc=0.92188
# [49/100] training 52.1% loss=0.18828, acc=0.89062
# [49/100] training 52.3% loss=0.19767, acc=0.89062
# [49/100] training 52.5% loss=0.09630, acc=0.96875
# [49/100] training 52.6% loss=0.15683, acc=0.93750
# [49/100] training 52.9% loss=0.22265, acc=0.92188
# [49/100] training 53.0% loss=0.08650, acc=0.96875
# [49/100] training 53.2% loss=0.26679, acc=0.95312
# [49/100] training 53.3% loss=0.22288, acc=0.93750
# [49/100] training 53.5% loss=0.30108, acc=0.87500
# [49/100] training 53.7% loss=0.06209, acc=0.98438
# [49/100] training 53.8% loss=0.23591, acc=0.87500
# [49/100] training 54.1% loss=0.30172, acc=0.85938
# [49/100] training 54.2% loss=0.11748, acc=0.95312
# [49/100] training 54.4% loss=0.17435, acc=0.90625
# [49/100] training 54.5% loss=0.22281, acc=0.92188
# [49/100] training 54.7% loss=0.29709, acc=0.90625
# [49/100] training 54.8% loss=0.16605, acc=0.92188
# [49/100] training 55.1% loss=0.16831, acc=0.92188
# [49/100] training 55.3% loss=0.14108, acc=0.95312
# [49/100] training 55.4% loss=0.12652, acc=0.93750
# [49/100] training 55.6% loss=0.19342, acc=0.95312
# [49/100] training 55.7% loss=0.08325, acc=0.95312
# [49/100] training 55.9% loss=0.16202, acc=0.92188
# [49/100] training 56.0% loss=0.14616, acc=0.90625
# [49/100] training 56.3% loss=0.24375, acc=0.85938
# [49/100] training 56.5% loss=0.16019, acc=0.92188
# [49/100] training 56.6% loss=0.23347, acc=0.90625
# [49/100] training 56.8% loss=0.25430, acc=0.89062
# [49/100] training 56.9% loss=0.18059, acc=0.90625
# [49/100] training 57.1% loss=0.27083, acc=0.85938
# [49/100] training 57.2% loss=0.07872, acc=0.95312
# [49/100] training 57.5% loss=0.11774, acc=0.95312
# [49/100] training 57.6% loss=0.15792, acc=0.92188
# [49/100] training 57.8% loss=0.13053, acc=0.93750
# [49/100] training 58.0% loss=0.21889, acc=0.87500
# [49/100] training 58.1% loss=0.12251, acc=0.92188
# [49/100] training 58.3% loss=0.10208, acc=0.98438
# [49/100] training 58.4% loss=0.28245, acc=0.92188
# [49/100] training 58.7% loss=0.18241, acc=0.93750
# [49/100] training 58.8% loss=0.23770, acc=0.90625
# [49/100] training 59.0% loss=0.12365, acc=0.93750
# [49/100] training 59.2% loss=0.16318, acc=0.92188
# [49/100] training 59.3% loss=0.17654, acc=0.92188
# [49/100] training 59.5% loss=0.20403, acc=0.92188
# [49/100] training 59.7% loss=0.18547, acc=0.93750
# [49/100] training 59.9% loss=0.14417, acc=0.98438
# [49/100] training 60.0% loss=0.09605, acc=0.96875
# [49/100] training 60.2% loss=0.11900, acc=0.92188
# [49/100] training 60.3% loss=0.09517, acc=0.96875
# [49/100] training 60.5% loss=0.13344, acc=0.93750
# [49/100] training 60.8% loss=0.17311, acc=0.93750
# [49/100] training 60.9% loss=0.17230, acc=0.98438
# [49/100] training 61.1% loss=0.19283, acc=0.92188
# [49/100] training 61.2% loss=0.27974, acc=0.89062
# [49/100] training 61.4% loss=0.35507, acc=0.87500
# [49/100] training 61.5% loss=0.36339, acc=0.87500
# [49/100] training 61.7% loss=0.14222, acc=0.93750
# [49/100] training 62.0% loss=0.23701, acc=0.89062
# [49/100] training 62.1% loss=0.19592, acc=0.89062
# [49/100] training 62.3% loss=0.13667, acc=0.95312
# [49/100] training 62.4% loss=0.16369, acc=0.95312
# [49/100] training 62.6% loss=0.24446, acc=0.84375
# [49/100] training 62.7% loss=0.11676, acc=0.95312
# [49/100] training 62.9% loss=0.18806, acc=0.90625
# [49/100] training 63.1% loss=0.15780, acc=0.95312
# [49/100] training 63.3% loss=0.12689, acc=0.96875
# [49/100] training 63.5% loss=0.19957, acc=0.89062
# [49/100] training 63.6% loss=0.23722, acc=0.90625
# [49/100] training 63.8% loss=0.26396, acc=0.84375
# [49/100] training 63.9% loss=0.15504, acc=0.93750
# [49/100] training 64.2% loss=0.15538, acc=0.95312
# [49/100] training 64.3% loss=0.18662, acc=0.93750
# [49/100] training 64.5% loss=0.19463, acc=0.92188
# [49/100] training 64.7% loss=0.14793, acc=0.93750
# [49/100] training 64.8% loss=0.37143, acc=0.85938
# [49/100] training 65.0% loss=0.22993, acc=0.90625
# [49/100] training 65.1% loss=0.19351, acc=0.87500
# [49/100] training 65.4% loss=0.14330, acc=0.95312
# [49/100] training 65.5% loss=0.13431, acc=0.92188
# [49/100] training 65.7% loss=0.13920, acc=0.93750
# [49/100] training 65.8% loss=0.21809, acc=0.89062
# [49/100] training 66.0% loss=0.17393, acc=0.90625
# [49/100] training 66.2% loss=0.09191, acc=0.95312
# [49/100] training 66.3% loss=0.18792, acc=0.90625
# [49/100] training 66.6% loss=0.13351, acc=0.93750
# [49/100] training 66.7% loss=0.15288, acc=0.96875
# [49/100] training 66.9% loss=0.25133, acc=0.89062
# [49/100] training 67.0% loss=0.13702, acc=0.93750
# [49/100] training 67.2% loss=0.08330, acc=0.95312
# [49/100] training 67.4% loss=0.13235, acc=0.92188
# [49/100] training 67.6% loss=0.11466, acc=0.95312
# [49/100] training 67.8% loss=0.11412, acc=0.95312
# [49/100] training 67.9% loss=0.16839, acc=0.90625
# [49/100] training 68.1% loss=0.14187, acc=0.95312
# [49/100] training 68.2% loss=0.09004, acc=0.96875
# [49/100] training 68.4% loss=0.12062, acc=0.98438
# [49/100] training 68.5% loss=0.15443, acc=0.90625
# [49/100] training 68.8% loss=0.11173, acc=0.96875
# [49/100] training 69.0% loss=0.28994, acc=0.93750
# [49/100] training 69.1% loss=0.16197, acc=0.92188
# [49/100] training 69.3% loss=0.23528, acc=0.90625
# [49/100] training 69.4% loss=0.11447, acc=0.98438
# [49/100] training 69.6% loss=0.27328, acc=0.93750
# [49/100] training 69.7% loss=0.20853, acc=0.95312
# [49/100] training 70.0% loss=0.17514, acc=0.95312
# [49/100] training 70.2% loss=0.22061, acc=0.92188
# [49/100] training 70.3% loss=0.18624, acc=0.92188
# [49/100] training 70.5% loss=0.12487, acc=0.98438
# [49/100] training 70.6% loss=0.13369, acc=0.96875
# [49/100] training 70.8% loss=0.21034, acc=0.87500
# [49/100] training 71.0% loss=0.22289, acc=0.90625
# [49/100] training 71.2% loss=0.14693, acc=0.93750
# [49/100] training 71.3% loss=0.09717, acc=0.96875
# [49/100] training 71.5% loss=0.17332, acc=0.93750
# [49/100] training 71.7% loss=0.33210, acc=0.90625
# [49/100] training 71.8% loss=0.20904, acc=0.92188
# [49/100] training 72.0% loss=0.08867, acc=0.96875
# [49/100] training 72.2% loss=0.18773, acc=0.92188
# [49/100] training 72.4% loss=0.27012, acc=0.92188
# [49/100] training 72.5% loss=0.18632, acc=0.92188
# [49/100] training 72.7% loss=0.24514, acc=0.90625
# [49/100] training 72.9% loss=0.14265, acc=0.93750
# [49/100] training 73.0% loss=0.08215, acc=0.96875
# [49/100] training 73.3% loss=0.28426, acc=0.92188
# [49/100] training 73.4% loss=0.15909, acc=0.93750
# [49/100] training 73.6% loss=0.13457, acc=0.95312
# [49/100] training 73.7% loss=0.18307, acc=0.92188
# [49/100] training 73.9% loss=0.07489, acc=0.96875
# [49/100] training 74.0% loss=0.27421, acc=0.85938
# [49/100] training 74.2% loss=0.15135, acc=0.92188
# [49/100] training 74.5% loss=0.10452, acc=0.95312
# [49/100] training 74.6% loss=0.29969, acc=0.90625
# [49/100] training 74.8% loss=0.26842, acc=0.90625
# [49/100] training 74.9% loss=0.24860, acc=0.85938
# [49/100] training 75.1% loss=0.12631, acc=0.93750
# [49/100] training 75.2% loss=0.14522, acc=0.96875
# [49/100] training 75.4% loss=0.11848, acc=0.93750
# [49/100] training 75.7% loss=0.18533, acc=0.93750
# [49/100] training 75.8% loss=0.24032, acc=0.90625
# [49/100] training 76.0% loss=0.12066, acc=0.95312
# [49/100] training 76.1% loss=0.24594, acc=0.87500
# [49/100] training 76.3% loss=0.15458, acc=0.92188
# [49/100] training 76.4% loss=0.16963, acc=0.92188
# [49/100] training 76.7% loss=0.18834, acc=0.95312
# [49/100] training 76.8% loss=0.20912, acc=0.95312
# [49/100] training 77.0% loss=0.09494, acc=0.96875
# [49/100] training 77.2% loss=0.21067, acc=0.90625
# [49/100] training 77.3% loss=0.13524, acc=0.93750
# [49/100] training 77.5% loss=0.31715, acc=0.89062
# [49/100] training 77.6% loss=0.17460, acc=0.92188
# [49/100] training 77.9% loss=0.20980, acc=0.87500
# [49/100] training 78.0% loss=0.20981, acc=0.89062
# [49/100] training 78.2% loss=0.16537, acc=0.92188
# [49/100] training 78.4% loss=0.08256, acc=0.96875
# [49/100] training 78.5% loss=0.20159, acc=0.93750
# [49/100] training 78.7% loss=0.26126, acc=0.85938
# [49/100] training 78.8% loss=0.09354, acc=0.96875
# [49/100] training 79.1% loss=0.09464, acc=0.96875
# [49/100] training 79.2% loss=0.14910, acc=0.92188
# [49/100] training 79.4% loss=0.14985, acc=0.95312
# [49/100] training 79.5% loss=0.14287, acc=0.95312
# [49/100] training 79.7% loss=0.05011, acc=0.98438
# [49/100] training 79.9% loss=0.16407, acc=0.95312
# [49/100] training 80.1% loss=0.15753, acc=0.93750
# [49/100] training 80.3% loss=0.21820, acc=0.92188
# [49/100] training 80.4% loss=0.21488, acc=0.93750
# [49/100] training 80.6% loss=0.18621, acc=0.93750
# [49/100] training 80.7% loss=0.15098, acc=0.93750
# [49/100] training 80.9% loss=0.18640, acc=0.93750
# [49/100] training 81.2% loss=0.14950, acc=0.93750
# [49/100] training 81.3% loss=0.15900, acc=0.93750
# [49/100] training 81.5% loss=0.16965, acc=0.92188
# [49/100] training 81.6% loss=0.28632, acc=0.89062
# [49/100] training 81.8% loss=0.19131, acc=0.92188
# [49/100] training 81.9% loss=0.43422, acc=0.89062
# [49/100] training 82.1% loss=0.13712, acc=0.93750
# [49/100] training 82.2% loss=0.28645, acc=0.87500
# [49/100] training 82.5% loss=0.12512, acc=0.96875
# [49/100] training 82.7% loss=0.18172, acc=0.92188
# [49/100] training 82.8% loss=0.18615, acc=0.93750
# [49/100] training 83.0% loss=0.16448, acc=0.95312
# [49/100] training 83.1% loss=0.14375, acc=0.95312
# [49/100] training 83.3% loss=0.12098, acc=0.95312
# [49/100] training 83.5% loss=0.14641, acc=0.95312
# [49/100] training 83.7% loss=0.33199, acc=0.81250
# [49/100] training 83.9% loss=0.27615, acc=0.87500
# [49/100] training 84.0% loss=0.08210, acc=0.96875
# [49/100] training 84.2% loss=0.09273, acc=0.98438
# [49/100] training 84.3% loss=0.13627, acc=0.93750
# [49/100] training 84.5% loss=0.15467, acc=0.95312
# [49/100] training 84.7% loss=0.17930, acc=0.90625
# [49/100] training 84.9% loss=0.14143, acc=0.95312
# [49/100] training 85.0% loss=0.13995, acc=0.95312
# [49/100] training 85.2% loss=0.16568, acc=0.95312
# [49/100] training 85.4% loss=0.09686, acc=0.98438
# [49/100] training 85.5% loss=0.11303, acc=0.96875
# [49/100] training 85.8% loss=0.21244, acc=0.92188
# [49/100] training 85.9% loss=0.12055, acc=0.93750
# [49/100] training 86.1% loss=0.13231, acc=0.93750
# [49/100] training 86.2% loss=0.05178, acc=0.98438
# [49/100] training 86.4% loss=0.23287, acc=0.92188
# [49/100] training 86.6% loss=0.12596, acc=0.95312
# [49/100] training 86.7% loss=0.18166, acc=0.90625
# [49/100] training 87.0% loss=0.22864, acc=0.95312
# [49/100] training 87.1% loss=0.16572, acc=0.95312
# [49/100] training 87.3% loss=0.10557, acc=0.98438
# [49/100] training 87.4% loss=0.19004, acc=0.92188
# [49/100] training 87.6% loss=0.15644, acc=0.92188
# [49/100] training 87.7% loss=0.31488, acc=0.89062
# [49/100] training 87.9% loss=0.17860, acc=0.89062
# [49/100] training 88.2% loss=0.09692, acc=0.98438
# [49/100] training 88.3% loss=0.23806, acc=0.93750
# [49/100] training 88.5% loss=0.26698, acc=0.89062
# [49/100] training 88.6% loss=0.08820, acc=0.98438
# [49/100] training 88.8% loss=0.17014, acc=0.92188
# [49/100] training 88.9% loss=0.26211, acc=0.87500
# [49/100] training 89.2% loss=0.13277, acc=0.93750
# [49/100] training 89.4% loss=0.14286, acc=0.93750
# [49/100] training 89.5% loss=0.14248, acc=0.95312
# [49/100] training 89.7% loss=0.24537, acc=0.92188
# [49/100] training 89.8% loss=0.17099, acc=0.96875
# [49/100] training 90.0% loss=0.14609, acc=0.93750
# [49/100] training 90.1% loss=0.21417, acc=0.90625
# [49/100] training 90.4% loss=0.24200, acc=0.90625
# [49/100] training 90.5% loss=0.19832, acc=0.90625
# [49/100] training 90.7% loss=0.20576, acc=0.93750
# [49/100] training 90.9% loss=0.14352, acc=0.93750
# [49/100] training 91.0% loss=0.21544, acc=0.92188
# [49/100] training 91.2% loss=0.26673, acc=0.85938
# [49/100] training 91.3% loss=0.18147, acc=0.92188
# [49/100] training 91.6% loss=0.26295, acc=0.87500
# [49/100] training 91.7% loss=0.17775, acc=0.90625
# [49/100] training 91.9% loss=0.26782, acc=0.89062
# [49/100] training 92.1% loss=0.16303, acc=0.93750
# [49/100] training 92.2% loss=0.05109, acc=1.00000
# [49/100] training 92.4% loss=0.28251, acc=0.89062
# [49/100] training 92.6% loss=0.35720, acc=0.82812
# [49/100] training 92.8% loss=0.19228, acc=0.92188
# [49/100] training 92.9% loss=0.26419, acc=0.90625
# [49/100] training 93.1% loss=0.39483, acc=0.82812
# [49/100] training 93.2% loss=0.25901, acc=0.92188
# [49/100] training 93.4% loss=0.22256, acc=0.92188
# [49/100] training 93.7% loss=0.18164, acc=0.93750
# [49/100] training 93.8% loss=0.18497, acc=0.92188
# [49/100] training 94.0% loss=0.13912, acc=0.96875
# [49/100] training 94.1% loss=0.17531, acc=0.93750
# [49/100] training 94.3% loss=0.06746, acc=1.00000
# [49/100] training 94.4% loss=0.24053, acc=0.95312
# [49/100] training 94.6% loss=0.09697, acc=0.95312
# [49/100] training 94.9% loss=0.12527, acc=0.98438
# [49/100] training 95.0% loss=0.13728, acc=0.93750
# [49/100] training 95.2% loss=0.37968, acc=0.85938
# [49/100] training 95.3% loss=0.27744, acc=0.93750
# [49/100] training 95.5% loss=0.17254, acc=0.93750
# [49/100] training 95.6% loss=0.28667, acc=0.89062
# [49/100] training 95.8% loss=0.24732, acc=0.85938
# [49/100] training 96.0% loss=0.18169, acc=0.95312
# [49/100] training 96.2% loss=0.08475, acc=0.96875
# [49/100] training 96.4% loss=0.18715, acc=0.98438
# [49/100] training 96.5% loss=0.21031, acc=0.89062
# [49/100] training 96.7% loss=0.09925, acc=0.96875
# [49/100] training 96.8% loss=0.23460, acc=0.90625
# [49/100] training 97.1% loss=0.13089, acc=0.93750
# [49/100] training 97.2% loss=0.20366, acc=0.92188
# [49/100] training 97.4% loss=0.18554, acc=0.90625
# [49/100] training 97.6% loss=0.24938, acc=0.90625
# [49/100] training 97.7% loss=0.17387, acc=0.92188
# [49/100] training 97.9% loss=0.23808, acc=0.85938
# [49/100] training 98.0% loss=0.16494, acc=0.90625
# [49/100] training 98.3% loss=0.27880, acc=0.87500
# [49/100] training 98.4% loss=0.19747, acc=0.92188
# [49/100] training 98.6% loss=0.31127, acc=0.87500
# [49/100] training 98.7% loss=0.26214, acc=0.89062
# [49/100] training 98.9% loss=0.16024, acc=0.93750
# [49/100] training 99.1% loss=0.19421, acc=0.89062
# [49/100] training 99.2% loss=0.15532, acc=0.93750
# [49/100] training 99.5% loss=0.31616, acc=0.89062
# [49/100] training 99.6% loss=0.16864, acc=0.92188
# [49/100] training 99.8% loss=0.13593, acc=0.95312
# [49/100] training 99.9% loss=0.05300, acc=0.98438
# [49/100] testing 0.9% loss=0.13054, acc=0.95312
# [49/100] testing 1.8% loss=0.37023, acc=0.85938
# [49/100] testing 2.2% loss=0.30051, acc=0.84375
# [49/100] testing 3.1% loss=0.29844, acc=0.90625
# [49/100] testing 3.5% loss=0.09892, acc=0.95312
# [49/100] testing 4.4% loss=0.21506, acc=0.92188
# [49/100] testing 4.8% loss=0.31883, acc=0.85938
# [49/100] testing 5.7% loss=0.20957, acc=0.89062
# [49/100] testing 6.6% loss=0.12717, acc=0.95312
# [49/100] testing 7.0% loss=0.10003, acc=0.93750
# [49/100] testing 7.9% loss=0.34215, acc=0.85938
# [49/100] testing 8.3% loss=0.26040, acc=0.87500
# [49/100] testing 9.2% loss=0.27011, acc=0.92188
# [49/100] testing 9.7% loss=0.10541, acc=0.95312
# [49/100] testing 10.5% loss=0.22816, acc=0.92188
# [49/100] testing 11.0% loss=0.35048, acc=0.85938
# [49/100] testing 11.8% loss=0.12192, acc=0.92188
# [49/100] testing 12.7% loss=0.32667, acc=0.90625
# [49/100] testing 13.2% loss=0.23423, acc=0.87500
# [49/100] testing 14.0% loss=0.37391, acc=0.92188
# [49/100] testing 14.5% loss=0.22783, acc=0.85938
# [49/100] testing 15.4% loss=0.29007, acc=0.92188
# [49/100] testing 15.8% loss=0.19261, acc=0.90625
# [49/100] testing 16.7% loss=0.29430, acc=0.85938
# [49/100] testing 17.5% loss=0.15620, acc=0.92188
# [49/100] testing 18.0% loss=0.20477, acc=0.92188
# [49/100] testing 18.9% loss=0.15748, acc=0.93750
# [49/100] testing 19.3% loss=0.36080, acc=0.89062
# [49/100] testing 20.2% loss=0.35240, acc=0.89062
# [49/100] testing 20.6% loss=0.29270, acc=0.89062
# [49/100] testing 21.5% loss=0.20695, acc=0.95312
# [49/100] testing 21.9% loss=0.47179, acc=0.87500
# [49/100] testing 22.8% loss=0.34402, acc=0.89062
# [49/100] testing 23.7% loss=0.47797, acc=0.84375
# [49/100] testing 24.1% loss=0.20627, acc=0.92188
# [49/100] testing 25.0% loss=0.44380, acc=0.89062
# [49/100] testing 25.4% loss=0.16684, acc=0.95312
# [49/100] testing 26.3% loss=0.30327, acc=0.90625
# [49/100] testing 26.8% loss=0.33410, acc=0.90625
# [49/100] testing 27.6% loss=0.18461, acc=0.95312
# [49/100] testing 28.5% loss=0.30121, acc=0.89062
# [49/100] testing 29.0% loss=0.20422, acc=0.93750
# [49/100] testing 29.8% loss=0.42683, acc=0.90625
# [49/100] testing 30.3% loss=0.30059, acc=0.93750
# [49/100] testing 31.1% loss=0.31111, acc=0.90625
# [49/100] testing 31.6% loss=0.22532, acc=0.92188
# [49/100] testing 32.5% loss=0.24827, acc=0.93750
# [49/100] testing 32.9% loss=0.48797, acc=0.87500
# [49/100] testing 33.8% loss=0.28872, acc=0.89062
# [49/100] testing 34.7% loss=0.31480, acc=0.90625
# [49/100] testing 35.1% loss=0.15233, acc=0.92188
# [49/100] testing 36.0% loss=0.32220, acc=0.89062
# [49/100] testing 36.4% loss=0.30343, acc=0.87500
# [49/100] testing 37.3% loss=0.29892, acc=0.92188
# [49/100] testing 37.7% loss=0.48872, acc=0.84375
# [49/100] testing 38.6% loss=0.16074, acc=0.96875
# [49/100] testing 39.5% loss=0.21854, acc=0.93750
# [49/100] testing 39.9% loss=0.24483, acc=0.93750
# [49/100] testing 40.8% loss=0.29437, acc=0.92188
# [49/100] testing 41.2% loss=0.23359, acc=0.95312
# [49/100] testing 42.1% loss=0.25056, acc=0.90625
# [49/100] testing 42.5% loss=0.16955, acc=0.89062
# [49/100] testing 43.4% loss=0.26570, acc=0.90625
# [49/100] testing 43.9% loss=0.09760, acc=0.96875
# [49/100] testing 44.7% loss=0.35148, acc=0.85938
# [49/100] testing 45.6% loss=0.20696, acc=0.89062
# [49/100] testing 46.1% loss=0.20766, acc=0.93750
# [49/100] testing 46.9% loss=0.23139, acc=0.89062
# [49/100] testing 47.4% loss=0.16304, acc=0.93750
# [49/100] testing 48.3% loss=0.42630, acc=0.87500
# [49/100] testing 48.7% loss=0.31703, acc=0.89062
# [49/100] testing 49.6% loss=0.46893, acc=0.81250
# [49/100] testing 50.4% loss=0.20306, acc=0.93750
# [49/100] testing 50.9% loss=0.33634, acc=0.89062
# [49/100] testing 51.8% loss=0.24577, acc=0.90625
# [49/100] testing 52.2% loss=0.28525, acc=0.90625
# [49/100] testing 53.1% loss=0.14622, acc=0.95312
# [49/100] testing 53.5% loss=0.15809, acc=0.95312
# [49/100] testing 54.4% loss=0.35985, acc=0.85938
# [49/100] testing 54.8% loss=0.32729, acc=0.85938
# [49/100] testing 55.7% loss=0.14811, acc=0.95312
# [49/100] testing 56.6% loss=0.30991, acc=0.89062
# [49/100] testing 57.0% loss=0.30369, acc=0.92188
# [49/100] testing 57.9% loss=0.22295, acc=0.92188
# [49/100] testing 58.3% loss=0.38678, acc=0.87500
# [49/100] testing 59.2% loss=0.25038, acc=0.87500
# [49/100] testing 59.7% loss=0.29123, acc=0.92188
# [49/100] testing 60.5% loss=0.35392, acc=0.82812
# [49/100] testing 61.4% loss=0.09942, acc=0.96875
# [49/100] testing 61.9% loss=0.18590, acc=0.89062
# [49/100] testing 62.7% loss=0.13650, acc=0.92188
# [49/100] testing 63.2% loss=0.43771, acc=0.87500
# [49/100] testing 64.0% loss=0.42210, acc=0.87500
# [49/100] testing 64.5% loss=0.26365, acc=0.90625
# [49/100] testing 65.4% loss=0.21984, acc=0.89062
# [49/100] testing 65.8% loss=0.27942, acc=0.90625
# [49/100] testing 66.7% loss=0.16906, acc=0.90625
# [49/100] testing 67.6% loss=0.30754, acc=0.90625
# [49/100] testing 68.0% loss=0.11760, acc=0.95312
# [49/100] testing 68.9% loss=0.30882, acc=0.93750
# [49/100] testing 69.3% loss=0.25167, acc=0.89062
# [49/100] testing 70.2% loss=0.40923, acc=0.85938
# [49/100] testing 70.6% loss=0.27057, acc=0.90625
# [49/100] testing 71.5% loss=0.30719, acc=0.90625
# [49/100] testing 72.4% loss=0.09057, acc=0.98438
# [49/100] testing 72.8% loss=0.16227, acc=0.93750
# [49/100] testing 73.7% loss=0.08540, acc=0.96875
# [49/100] testing 74.1% loss=0.32227, acc=0.90625
# [49/100] testing 75.0% loss=0.23939, acc=0.87500
# [49/100] testing 75.4% loss=0.34409, acc=0.87500
# [49/100] testing 76.3% loss=0.09901, acc=0.96875
# [49/100] testing 76.8% loss=0.32042, acc=0.84375
# [49/100] testing 77.6% loss=0.15524, acc=0.93750
# [49/100] testing 78.5% loss=0.38317, acc=0.85938
# [49/100] testing 79.0% loss=0.25228, acc=0.90625
# [49/100] testing 79.8% loss=0.31639, acc=0.87500
# [49/100] testing 80.3% loss=0.24269, acc=0.92188
# [49/100] testing 81.2% loss=0.39583, acc=0.89062
# [49/100] testing 81.6% loss=0.23144, acc=0.92188
# [49/100] testing 82.5% loss=0.14405, acc=0.90625
# [49/100] testing 83.3% loss=0.19147, acc=0.93750
# [49/100] testing 83.8% loss=0.14936, acc=0.96875
# [49/100] testing 84.7% loss=0.24650, acc=0.90625
# [49/100] testing 85.1% loss=0.21695, acc=0.87500
# [49/100] testing 86.0% loss=0.28356, acc=0.92188
# [49/100] testing 86.4% loss=0.40401, acc=0.87500
# [49/100] testing 87.3% loss=0.29494, acc=0.87500
# [49/100] testing 87.7% loss=0.27267, acc=0.93750
# [49/100] testing 88.6% loss=0.25562, acc=0.87500
# [49/100] testing 89.5% loss=0.53971, acc=0.81250
# [49/100] testing 89.9% loss=0.15641, acc=0.90625
# [49/100] testing 90.8% loss=0.34411, acc=0.92188
# [49/100] testing 91.2% loss=0.12344, acc=0.95312
# [49/100] testing 92.1% loss=0.35458, acc=0.89062
# [49/100] testing 92.6% loss=0.37399, acc=0.87500
# [49/100] testing 93.4% loss=0.26907, acc=0.89062
# [49/100] testing 94.3% loss=0.13101, acc=0.96875
# [49/100] testing 94.7% loss=0.23358, acc=0.90625
# [49/100] testing 95.6% loss=0.44757, acc=0.82812
# [49/100] testing 96.1% loss=0.19005, acc=0.92188
# [49/100] testing 96.9% loss=0.27076, acc=0.90625
# [49/100] testing 97.4% loss=0.14218, acc=0.93750
# [49/100] testing 98.3% loss=0.17751, acc=0.90625
# [49/100] testing 98.7% loss=0.26229, acc=0.90625
# [49/100] testing 99.6% loss=0.32896, acc=0.90625
# [50/100] training 0.2% loss=0.21997, acc=0.93750
# [50/100] training 0.4% loss=0.37591, acc=0.89062
# [50/100] training 0.5% loss=0.07419, acc=0.96875
# [50/100] training 0.8% loss=0.10061, acc=0.96875
# [50/100] training 0.9% loss=0.13173, acc=0.95312
# [50/100] training 1.1% loss=0.20862, acc=0.93750
# [50/100] training 1.2% loss=0.28225, acc=0.89062
# [50/100] training 1.4% loss=0.14693, acc=0.92188
# [50/100] training 1.6% loss=0.13739, acc=0.92188
# [50/100] training 1.8% loss=0.18649, acc=0.92188
# [50/100] training 2.0% loss=0.23028, acc=0.89062
# [50/100] training 2.1% loss=0.15238, acc=0.95312
# [50/100] training 2.3% loss=0.15111, acc=0.90625
# [50/100] training 2.4% loss=0.20105, acc=0.87500
# [50/100] training 2.6% loss=0.07368, acc=0.96875
# [50/100] training 2.7% loss=0.26816, acc=0.92188
# [50/100] training 3.0% loss=0.14683, acc=0.93750
# [50/100] training 3.2% loss=0.11590, acc=0.95312
# [50/100] training 3.3% loss=0.31711, acc=0.89062
# [50/100] training 3.5% loss=0.10438, acc=0.95312
# [50/100] training 3.6% loss=0.21703, acc=0.90625
# [50/100] training 3.8% loss=0.17519, acc=0.93750
# [50/100] training 3.9% loss=0.12541, acc=0.95312
# [50/100] training 4.2% loss=0.11298, acc=0.95312
# [50/100] training 4.4% loss=0.14277, acc=0.95312
# [50/100] training 4.5% loss=0.15436, acc=0.93750
# [50/100] training 4.7% loss=0.35519, acc=0.89062
# [50/100] training 4.8% loss=0.12600, acc=0.96875
# [50/100] training 5.0% loss=0.11751, acc=0.96875
# [50/100] training 5.2% loss=0.23223, acc=0.90625
# [50/100] training 5.4% loss=0.12165, acc=0.93750
# [50/100] training 5.5% loss=0.23127, acc=0.90625
# [50/100] training 5.7% loss=0.16325, acc=0.92188
# [50/100] training 5.9% loss=0.18315, acc=0.95312
# [50/100] training 6.0% loss=0.25277, acc=0.89062
# [50/100] training 6.3% loss=0.19551, acc=0.93750
# [50/100] training 6.4% loss=0.16603, acc=0.95312
# [50/100] training 6.6% loss=0.22754, acc=0.87500
# [50/100] training 6.7% loss=0.28479, acc=0.82812
# [50/100] training 6.9% loss=0.14391, acc=0.92188
# [50/100] training 7.1% loss=0.15759, acc=0.92188
# [50/100] training 7.2% loss=0.21957, acc=0.90625
# [50/100] training 7.5% loss=0.15382, acc=0.93750
# [50/100] training 7.6% loss=0.24851, acc=0.92188
# [50/100] training 7.8% loss=0.23157, acc=0.85938
# [50/100] training 7.9% loss=0.14233, acc=0.95312
# [50/100] training 8.1% loss=0.14415, acc=0.95312
# [50/100] training 8.2% loss=0.16709, acc=0.92188
# [50/100] training 8.4% loss=0.22112, acc=0.87500
# [50/100] training 8.7% loss=0.18930, acc=0.93750
# [50/100] training 8.8% loss=0.25202, acc=0.89062
# [50/100] training 9.0% loss=0.13721, acc=0.93750
# [50/100] training 9.1% loss=0.22490, acc=0.90625
# [50/100] training 9.3% loss=0.35625, acc=0.89062
# [50/100] training 9.4% loss=0.15208, acc=0.89062
# [50/100] training 9.7% loss=0.15572, acc=0.95312
# [50/100] training 9.9% loss=0.35573, acc=0.79688
# [50/100] training 10.0% loss=0.19632, acc=0.92188
# [50/100] training 10.2% loss=0.16078, acc=0.95312
# [50/100] training 10.3% loss=0.13451, acc=0.93750
# [50/100] training 10.5% loss=0.17923, acc=0.95312
# [50/100] training 10.6% loss=0.15523, acc=0.92188
# [50/100] training 10.9% loss=0.12771, acc=0.95312
# [50/100] training 11.0% loss=0.24890, acc=0.90625
# [50/100] training 11.2% loss=0.09367, acc=0.96875
# [50/100] training 11.4% loss=0.27906, acc=0.87500
# [50/100] training 11.5% loss=0.33185, acc=0.89062
# [50/100] training 11.7% loss=0.09566, acc=1.00000
# [50/100] training 11.8% loss=0.12023, acc=0.98438
# [50/100] training 12.1% loss=0.19069, acc=0.92188
# [50/100] training 12.2% loss=0.14325, acc=0.95312
# [50/100] training 12.4% loss=0.13838, acc=0.95312
# [50/100] training 12.6% loss=0.28862, acc=0.92188
# [50/100] training 12.7% loss=0.15021, acc=0.95312
# [50/100] training 12.9% loss=0.18197, acc=0.95312
# [50/100] training 13.0% loss=0.12792, acc=0.96875
# [50/100] training 13.3% loss=0.21174, acc=0.93750
# [50/100] training 13.4% loss=0.27201, acc=0.89062
# [50/100] training 13.6% loss=0.11094, acc=0.96875
# [50/100] training 13.7% loss=0.23902, acc=0.90625
# [50/100] training 13.9% loss=0.38990, acc=0.89062
# [50/100] training 14.1% loss=0.23059, acc=0.93750
# [50/100] training 14.3% loss=0.08264, acc=0.98438
# [50/100] training 14.5% loss=0.28404, acc=0.89062
# [50/100] training 14.6% loss=0.24757, acc=0.90625
# [50/100] training 14.8% loss=0.22250, acc=0.90625
# [50/100] training 14.9% loss=0.13156, acc=0.93750
# [50/100] training 15.1% loss=0.31647, acc=0.87500
# [50/100] training 15.4% loss=0.30585, acc=0.84375
# [50/100] training 15.5% loss=0.14645, acc=0.96875
# [50/100] training 15.7% loss=0.33259, acc=0.84375
# [50/100] training 15.8% loss=0.09796, acc=0.95312
# [50/100] training 16.0% loss=0.22216, acc=0.89062
# [50/100] training 16.1% loss=0.33397, acc=0.87500
# [50/100] training 16.3% loss=0.16759, acc=0.92188
# [50/100] training 16.4% loss=0.11866, acc=0.96875
# [50/100] training 16.7% loss=0.22342, acc=0.90625
# [50/100] training 16.9% loss=0.17502, acc=0.95312
# [50/100] training 17.0% loss=0.15633, acc=0.92188
# [50/100] training 17.2% loss=0.13137, acc=0.93750
# [50/100] training 17.3% loss=0.16393, acc=0.95312
# [50/100] training 17.5% loss=0.20649, acc=0.93750
# [50/100] training 17.7% loss=0.12852, acc=0.95312
# [50/100] training 17.9% loss=0.22699, acc=0.90625
# [50/100] training 18.1% loss=0.19830, acc=0.90625
# [50/100] training 18.2% loss=0.28243, acc=0.90625
# [50/100] training 18.4% loss=0.30730, acc=0.89062
# [50/100] training 18.5% loss=0.19306, acc=0.95312
# [50/100] training 18.8% loss=0.11802, acc=0.95312
# [50/100] training 18.9% loss=0.11719, acc=0.96875
# [50/100] training 19.1% loss=0.21824, acc=0.89062
# [50/100] training 19.2% loss=0.10869, acc=0.96875
# [50/100] training 19.4% loss=0.12509, acc=0.92188
# [50/100] training 19.6% loss=0.22273, acc=0.92188
# [50/100] training 19.7% loss=0.25238, acc=0.89062
# [50/100] training 20.0% loss=0.14767, acc=0.92188
# [50/100] training 20.1% loss=0.17392, acc=0.93750
# [50/100] training 20.3% loss=0.18147, acc=0.95312
# [50/100] training 20.4% loss=0.19009, acc=0.92188
# [50/100] training 20.6% loss=0.26922, acc=0.89062
# [50/100] training 20.8% loss=0.13986, acc=0.93750
# [50/100] training 20.9% loss=0.19224, acc=0.92188
# [50/100] training 21.2% loss=0.26304, acc=0.89062
# [50/100] training 21.3% loss=0.32006, acc=0.87500
# [50/100] training 21.5% loss=0.31059, acc=0.85938
# [50/100] training 21.6% loss=0.06107, acc=0.98438
# [50/100] training 21.8% loss=0.13479, acc=0.92188
# [50/100] training 21.9% loss=0.23503, acc=0.87500
# [50/100] training 22.2% loss=0.17468, acc=0.92188
# [50/100] training 22.4% loss=0.21257, acc=0.90625
# [50/100] training 22.5% loss=0.17102, acc=0.90625
# [50/100] training 22.7% loss=0.20672, acc=0.95312
# [50/100] training 22.8% loss=0.19970, acc=0.93750
# [50/100] training 23.0% loss=0.12037, acc=0.95312
# [50/100] training 23.1% loss=0.31041, acc=0.87500
# [50/100] training 23.4% loss=0.20933, acc=0.90625
# [50/100] training 23.6% loss=0.25603, acc=0.89062
# [50/100] training 23.7% loss=0.14809, acc=0.93750
# [50/100] training 23.9% loss=0.15709, acc=0.92188
# [50/100] training 24.0% loss=0.09017, acc=0.96875
# [50/100] training 24.2% loss=0.06631, acc=0.98438
# [50/100] training 24.3% loss=0.14687, acc=0.92188
# [50/100] training 24.6% loss=0.30168, acc=0.92188
# [50/100] training 24.7% loss=0.30359, acc=0.90625
# [50/100] training 24.9% loss=0.25739, acc=0.90625
# [50/100] training 25.1% loss=0.28616, acc=0.89062
# [50/100] training 25.2% loss=0.14424, acc=0.93750
# [50/100] training 25.4% loss=0.17969, acc=0.89062
# [50/100] training 25.6% loss=0.15732, acc=0.95312
# [50/100] training 25.8% loss=0.19451, acc=0.93750
# [50/100] training 25.9% loss=0.23841, acc=0.89062
# [50/100] training 26.1% loss=0.16614, acc=0.93750
# [50/100] training 26.3% loss=0.16502, acc=0.90625
# [50/100] training 26.4% loss=0.16960, acc=0.92188
# [50/100] training 26.6% loss=0.10295, acc=0.96875
# [50/100] training 26.8% loss=0.20292, acc=0.93750
# [50/100] training 27.0% loss=0.20221, acc=0.90625
# [50/100] training 27.1% loss=0.12988, acc=0.92188
# [50/100] training 27.3% loss=0.11743, acc=0.95312
# [50/100] training 27.4% loss=0.13437, acc=0.93750
# [50/100] training 27.6% loss=0.33385, acc=0.85938
# [50/100] training 27.9% loss=0.19485, acc=0.95312
# [50/100] training 28.0% loss=0.26951, acc=0.89062
# [50/100] training 28.2% loss=0.19197, acc=0.92188
# [50/100] training 28.3% loss=0.12648, acc=0.93750
# [50/100] training 28.5% loss=0.17214, acc=0.90625
# [50/100] training 28.6% loss=0.17907, acc=0.95312
# [50/100] training 28.8% loss=0.13080, acc=0.96875
# [50/100] training 29.1% loss=0.11889, acc=0.98438
# [50/100] training 29.2% loss=0.14112, acc=0.93750
# [50/100] training 29.4% loss=0.21444, acc=0.89062
# [50/100] training 29.5% loss=0.08750, acc=0.96875
# [50/100] training 29.7% loss=0.17965, acc=0.93750
# [50/100] training 29.8% loss=0.15473, acc=0.95312
# [50/100] training 30.0% loss=0.18168, acc=0.89062
# [50/100] training 30.2% loss=0.17969, acc=0.92188
# [50/100] training 30.4% loss=0.12112, acc=0.95312
# [50/100] training 30.6% loss=0.15001, acc=0.92188
# [50/100] training 30.7% loss=0.15077, acc=0.95312
# [50/100] training 30.9% loss=0.36643, acc=0.89062
# [50/100] training 31.0% loss=0.16219, acc=0.92188
# [50/100] training 31.3% loss=0.18377, acc=0.95312
# [50/100] training 31.4% loss=0.27193, acc=0.90625
# [50/100] training 31.6% loss=0.31406, acc=0.92188
# [50/100] training 31.8% loss=0.22098, acc=0.90625
# [50/100] training 31.9% loss=0.17803, acc=0.92188
# [50/100] training 32.1% loss=0.18104, acc=0.93750
# [50/100] training 32.2% loss=0.28608, acc=0.90625
# [50/100] training 32.5% loss=0.12862, acc=0.93750
# [50/100] training 32.6% loss=0.15752, acc=0.95312
# [50/100] training 32.8% loss=0.12467, acc=0.96875
# [50/100] training 32.9% loss=0.23489, acc=0.93750
# [50/100] training 33.1% loss=0.06639, acc=0.98438
# [50/100] training 33.3% loss=0.19523, acc=0.92188
# [50/100] training 33.4% loss=0.13062, acc=0.96875
# [50/100] training 33.7% loss=0.17976, acc=0.93750
# [50/100] training 33.8% loss=0.25676, acc=0.92188
# [50/100] training 34.0% loss=0.23848, acc=0.92188
# [50/100] training 34.1% loss=0.16802, acc=0.93750
# [50/100] training 34.3% loss=0.17157, acc=0.92188
# [50/100] training 34.5% loss=0.23410, acc=0.90625
# [50/100] training 34.7% loss=0.11170, acc=0.95312
# [50/100] training 34.9% loss=0.14093, acc=0.95312
# [50/100] training 35.0% loss=0.10296, acc=0.96875
# [50/100] training 35.2% loss=0.23110, acc=0.84375
# [50/100] training 35.3% loss=0.22902, acc=0.87500
# [50/100] training 35.5% loss=0.16383, acc=0.90625
# [50/100] training 35.6% loss=0.27586, acc=0.92188
# [50/100] training 35.9% loss=0.13180, acc=0.93750
# [50/100] training 36.1% loss=0.24858, acc=0.85938
# [50/100] training 36.2% loss=0.14648, acc=0.95312
# [50/100] training 36.4% loss=0.24299, acc=0.93750
# [50/100] training 36.5% loss=0.21948, acc=0.92188
# [50/100] training 36.7% loss=0.24919, acc=0.87500
# [50/100] training 36.8% loss=0.12619, acc=0.96875
# [50/100] training 37.1% loss=0.31357, acc=0.87500
# [50/100] training 37.3% loss=0.19901, acc=0.92188
# [50/100] training 37.4% loss=0.13947, acc=0.95312
# [50/100] training 37.6% loss=0.09821, acc=1.00000
# [50/100] training 37.7% loss=0.20960, acc=0.90625
# [50/100] training 37.9% loss=0.15658, acc=0.93750
# [50/100] training 38.1% loss=0.13671, acc=0.95312
# [50/100] training 38.3% loss=0.12958, acc=0.93750
# [50/100] training 38.4% loss=0.04727, acc=1.00000
# [50/100] training 38.6% loss=0.20965, acc=0.92188
# [50/100] training 38.8% loss=0.29312, acc=0.89062
# [50/100] training 38.9% loss=0.17109, acc=0.95312
# [50/100] training 39.1% loss=0.20482, acc=0.90625
# [50/100] training 39.3% loss=0.19980, acc=0.92188
# [50/100] training 39.5% loss=0.29095, acc=0.90625
# [50/100] training 39.6% loss=0.12032, acc=0.96875
# [50/100] training 39.8% loss=0.07894, acc=0.96875
# [50/100] training 40.0% loss=0.13126, acc=0.93750
# [50/100] training 40.1% loss=0.20856, acc=0.90625
# [50/100] training 40.4% loss=0.16290, acc=0.89062
# [50/100] training 40.5% loss=0.25562, acc=0.92188
# [50/100] training 40.7% loss=0.18949, acc=0.95312
# [50/100] training 40.8% loss=0.13086, acc=0.96875
# [50/100] training 41.0% loss=0.12754, acc=0.95312
# [50/100] training 41.1% loss=0.29740, acc=0.85938
# [50/100] training 41.3% loss=0.28895, acc=0.90625
# [50/100] training 41.6% loss=0.28067, acc=0.85938
# [50/100] training 41.7% loss=0.19577, acc=0.93750
# [50/100] training 41.9% loss=0.16120, acc=0.96875
# [50/100] training 42.0% loss=0.19640, acc=0.92188
# [50/100] training 42.2% loss=0.22027, acc=0.92188
# [50/100] training 42.3% loss=0.09919, acc=0.96875
# [50/100] training 42.5% loss=0.15488, acc=0.95312
# [50/100] training 42.8% loss=0.15378, acc=0.92188
# [50/100] training 42.9% loss=0.13640, acc=0.92188
# [50/100] training 43.1% loss=0.22315, acc=0.95312
# [50/100] training 43.2% loss=0.11962, acc=0.95312
# [50/100] training 43.4% loss=0.27064, acc=0.90625
# [50/100] training 43.5% loss=0.24395, acc=0.89062
# [50/100] training 43.8% loss=0.23577, acc=0.90625
# [50/100] training 43.9% loss=0.12442, acc=0.93750
# [50/100] training 44.1% loss=0.17253, acc=0.96875
# [50/100] training 44.3% loss=0.16444, acc=0.92188
# [50/100] training 44.4% loss=0.34852, acc=0.81250
# [50/100] training 44.6% loss=0.24987, acc=0.90625
# [50/100] training 44.7% loss=0.37111, acc=0.82812
# [50/100] training 45.0% loss=0.11422, acc=0.95312
# [50/100] training 45.1% loss=0.22370, acc=0.89062
# [50/100] training 45.3% loss=0.19911, acc=0.89062
# [50/100] training 45.5% loss=0.07151, acc=0.98438
# [50/100] training 45.6% loss=0.19626, acc=0.92188
# [50/100] training 45.8% loss=0.15482, acc=0.95312
# [50/100] training 45.9% loss=0.14758, acc=0.93750
# [50/100] training 46.2% loss=0.09174, acc=0.98438
# [50/100] training 46.3% loss=0.08966, acc=0.95312
# [50/100] training 46.5% loss=0.41051, acc=0.84375
# [50/100] training 46.6% loss=0.11738, acc=0.96875
# [50/100] training 46.8% loss=0.11982, acc=0.95312
# [50/100] training 47.0% loss=0.12022, acc=0.93750
# [50/100] training 47.2% loss=0.14983, acc=0.89062
# [50/100] training 47.4% loss=0.14830, acc=0.96875
# [50/100] training 47.5% loss=0.33731, acc=0.85938
# [50/100] training 47.7% loss=0.15579, acc=0.92188
# [50/100] training 47.8% loss=0.27692, acc=0.92188
# [50/100] training 48.0% loss=0.11877, acc=0.95312
# [50/100] training 48.3% loss=0.05892, acc=0.98438
# [50/100] training 48.4% loss=0.07960, acc=0.96875
# [50/100] training 48.6% loss=0.12288, acc=0.96875
# [50/100] training 48.7% loss=0.14990, acc=0.93750
# [50/100] training 48.9% loss=0.11638, acc=0.96875
# [50/100] training 49.0% loss=0.12127, acc=0.95312
# [50/100] training 49.2% loss=0.16273, acc=0.92188
# [50/100] training 49.3% loss=0.15453, acc=0.95312
# [50/100] training 49.6% loss=0.15619, acc=0.93750
# [50/100] training 49.8% loss=0.26814, acc=0.92188
# [50/100] training 49.9% loss=0.18164, acc=0.93750
# [50/100] training 50.1% loss=0.13206, acc=0.95312
# [50/100] training 50.2% loss=0.17390, acc=0.93750
# [50/100] training 50.4% loss=0.29064, acc=0.92188
# [50/100] training 50.6% loss=0.18869, acc=0.93750
# [50/100] training 50.8% loss=0.22684, acc=0.90625
# [50/100] training 51.0% loss=0.28174, acc=0.85938
# [50/100] training 51.1% loss=0.23832, acc=0.90625
# [50/100] training 51.3% loss=0.21336, acc=0.95312
# [50/100] training 51.4% loss=0.15034, acc=0.96875
# [50/100] training 51.7% loss=0.14118, acc=0.93750
# [50/100] training 51.8% loss=0.25841, acc=0.89062
# [50/100] training 52.0% loss=0.20621, acc=0.92188
# [50/100] training 52.1% loss=0.12506, acc=0.95312
# [50/100] training 52.3% loss=0.20840, acc=0.92188
# [50/100] training 52.5% loss=0.06270, acc=1.00000
# [50/100] training 52.6% loss=0.22268, acc=0.90625
# [50/100] training 52.9% loss=0.29608, acc=0.89062
# [50/100] training 53.0% loss=0.09222, acc=0.96875
# [50/100] training 53.2% loss=0.24271, acc=0.90625
# [50/100] training 53.3% loss=0.12012, acc=0.93750
# [50/100] training 53.5% loss=0.22108, acc=0.90625
# [50/100] training 53.7% loss=0.09239, acc=0.96875
# [50/100] training 53.8% loss=0.34364, acc=0.87500
# [50/100] training 54.1% loss=0.22345, acc=0.85938
# [50/100] training 54.2% loss=0.10631, acc=0.96875
# [50/100] training 54.4% loss=0.12773, acc=0.95312
# [50/100] training 54.5% loss=0.21622, acc=0.93750
# [50/100] training 54.7% loss=0.26225, acc=0.89062
# [50/100] training 54.8% loss=0.11921, acc=0.93750
# [50/100] training 55.1% loss=0.18279, acc=0.92188
# [50/100] training 55.3% loss=0.12489, acc=0.96875
# [50/100] training 55.4% loss=0.12738, acc=0.93750
# [50/100] training 55.6% loss=0.10352, acc=0.96875
# [50/100] training 55.7% loss=0.13975, acc=0.95312
# [50/100] training 55.9% loss=0.14380, acc=0.92188
# [50/100] training 56.0% loss=0.11436, acc=0.95312
# [50/100] training 56.3% loss=0.22348, acc=0.89062
# [50/100] training 56.5% loss=0.15368, acc=0.92188
# [50/100] training 56.6% loss=0.18681, acc=0.93750
# [50/100] training 56.8% loss=0.11819, acc=0.95312
# [50/100] training 56.9% loss=0.25789, acc=0.92188
# [50/100] training 57.1% loss=0.42746, acc=0.85938
# [50/100] training 57.2% loss=0.11675, acc=0.95312
# [50/100] training 57.5% loss=0.23770, acc=0.90625
# [50/100] training 57.6% loss=0.12452, acc=0.93750
# [50/100] training 57.8% loss=0.16817, acc=0.92188
# [50/100] training 58.0% loss=0.10547, acc=0.98438
# [50/100] training 58.1% loss=0.13010, acc=0.95312
# [50/100] training 58.3% loss=0.15268, acc=0.90625
# [50/100] training 58.4% loss=0.12159, acc=0.95312
# [50/100] training 58.7% loss=0.22582, acc=0.92188
# [50/100] training 58.8% loss=0.26760, acc=0.90625
# [50/100] training 59.0% loss=0.21545, acc=0.93750
# [50/100] training 59.2% loss=0.09873, acc=0.96875
# [50/100] training 59.3% loss=0.15348, acc=0.92188
# [50/100] training 59.5% loss=0.25023, acc=0.89062
# [50/100] training 59.7% loss=0.21400, acc=0.93750
# [50/100] training 59.9% loss=0.16389, acc=0.93750
# [50/100] training 60.0% loss=0.18629, acc=0.90625
# [50/100] training 60.2% loss=0.10290, acc=0.95312
# [50/100] training 60.3% loss=0.14632, acc=0.95312
# [50/100] training 60.5% loss=0.22036, acc=0.90625
# [50/100] training 60.8% loss=0.17513, acc=0.93750
# [50/100] training 60.9% loss=0.22789, acc=0.93750
# [50/100] training 61.1% loss=0.29705, acc=0.89062
# [50/100] training 61.2% loss=0.12470, acc=0.93750
# [50/100] training 61.4% loss=0.30061, acc=0.90625
# [50/100] training 61.5% loss=0.34364, acc=0.87500
# [50/100] training 61.7% loss=0.20582, acc=0.90625
# [50/100] training 62.0% loss=0.34933, acc=0.92188
# [50/100] training 62.1% loss=0.36927, acc=0.85938
# [50/100] training 62.3% loss=0.11598, acc=0.96875
# [50/100] training 62.4% loss=0.17470, acc=0.92188
# [50/100] training 62.6% loss=0.25687, acc=0.89062
# [50/100] training 62.7% loss=0.10060, acc=0.95312
# [50/100] training 62.9% loss=0.21085, acc=0.92188
# [50/100] training 63.1% loss=0.24546, acc=0.90625
# [50/100] training 63.3% loss=0.17452, acc=0.89062
# [50/100] training 63.5% loss=0.16335, acc=0.95312
# [50/100] training 63.6% loss=0.20825, acc=0.95312
# [50/100] training 63.8% loss=0.30959, acc=0.85938
# [50/100] training 63.9% loss=0.16256, acc=0.90625
# [50/100] training 64.2% loss=0.16753, acc=0.90625
# [50/100] training 64.3% loss=0.22109, acc=0.89062
# [50/100] training 64.5% loss=0.17522, acc=0.92188
# [50/100] training 64.7% loss=0.13518, acc=0.95312
# [50/100] training 64.8% loss=0.37637, acc=0.85938
# [50/100] training 65.0% loss=0.13714, acc=0.95312
# [50/100] training 65.1% loss=0.12833, acc=0.90625
# [50/100] training 65.4% loss=0.20432, acc=0.95312
# [50/100] training 65.5% loss=0.18352, acc=0.90625
# [50/100] training 65.7% loss=0.13689, acc=0.93750
# [50/100] training 65.8% loss=0.19504, acc=0.92188
# [50/100] training 66.0% loss=0.12769, acc=0.95312
# [50/100] training 66.2% loss=0.08380, acc=0.98438
# [50/100] training 66.3% loss=0.16402, acc=0.90625
# [50/100] training 66.6% loss=0.13280, acc=0.92188
# [50/100] training 66.7% loss=0.14028, acc=0.96875
# [50/100] training 66.9% loss=0.18018, acc=0.93750
# [50/100] training 67.0% loss=0.19833, acc=0.92188
# [50/100] training 67.2% loss=0.22725, acc=0.93750
# [50/100] training 67.4% loss=0.12404, acc=0.93750
# [50/100] training 67.6% loss=0.17484, acc=0.93750
# [50/100] training 67.8% loss=0.18253, acc=0.93750
# [50/100] training 67.9% loss=0.11069, acc=0.93750
# [50/100] training 68.1% loss=0.13550, acc=0.90625
# [50/100] training 68.2% loss=0.14144, acc=0.96875
# [50/100] training 68.4% loss=0.13376, acc=0.95312
# [50/100] training 68.5% loss=0.15874, acc=0.93750
# [50/100] training 68.8% loss=0.12105, acc=0.95312
# [50/100] training 69.0% loss=0.21418, acc=0.96875
# [50/100] training 69.1% loss=0.11825, acc=0.95312
# [50/100] training 69.3% loss=0.17430, acc=0.90625
# [50/100] training 69.4% loss=0.11814, acc=0.96875
# [50/100] training 69.6% loss=0.10270, acc=0.93750
# [50/100] training 69.7% loss=0.15790, acc=0.95312
# [50/100] training 70.0% loss=0.30725, acc=0.87500
# [50/100] training 70.2% loss=0.32420, acc=0.92188
# [50/100] training 70.3% loss=0.18687, acc=0.90625
# [50/100] training 70.5% loss=0.20115, acc=0.92188
# [50/100] training 70.6% loss=0.16036, acc=0.96875
# [50/100] training 70.8% loss=0.23893, acc=0.90625
# [50/100] training 71.0% loss=0.25034, acc=0.90625
# [50/100] training 71.2% loss=0.11703, acc=0.96875
# [50/100] training 71.3% loss=0.11370, acc=0.98438
# [50/100] training 71.5% loss=0.23818, acc=0.90625
# [50/100] training 71.7% loss=0.16211, acc=0.92188
# [50/100] training 71.8% loss=0.19818, acc=0.90625
# [50/100] training 72.0% loss=0.06331, acc=0.98438
# [50/100] training 72.2% loss=0.16485, acc=0.89062
# [50/100] training 72.4% loss=0.17218, acc=0.96875
# [50/100] training 72.5% loss=0.26459, acc=0.89062
# [50/100] training 72.7% loss=0.26253, acc=0.93750
# [50/100] training 72.9% loss=0.14189, acc=0.93750
# [50/100] training 73.0% loss=0.13204, acc=0.95312
# [50/100] training 73.3% loss=0.26115, acc=0.90625
# [50/100] training 73.4% loss=0.12914, acc=0.93750
# [50/100] training 73.6% loss=0.14532, acc=0.98438
# [50/100] training 73.7% loss=0.15394, acc=0.92188
# [50/100] training 73.9% loss=0.12308, acc=0.95312
# [50/100] training 74.0% loss=0.28810, acc=0.89062
# [50/100] training 74.2% loss=0.13917, acc=0.93750
# [50/100] training 74.5% loss=0.16933, acc=0.90625
# [50/100] training 74.6% loss=0.26357, acc=0.85938
# [50/100] training 74.8% loss=0.26971, acc=0.89062
# [50/100] training 74.9% loss=0.26455, acc=0.89062
# [50/100] training 75.1% loss=0.11181, acc=0.98438
# [50/100] training 75.2% loss=0.11476, acc=0.93750
# [50/100] training 75.4% loss=0.19525, acc=0.90625
# [50/100] training 75.7% loss=0.19677, acc=0.90625
# [50/100] training 75.8% loss=0.23492, acc=0.89062
# [50/100] training 76.0% loss=0.17406, acc=0.92188
# [50/100] training 76.1% loss=0.18057, acc=0.93750
# [50/100] training 76.3% loss=0.09119, acc=0.98438
# [50/100] training 76.4% loss=0.26859, acc=0.87500
# [50/100] training 76.7% loss=0.10651, acc=0.95312
# [50/100] training 76.8% loss=0.11672, acc=0.95312
# [50/100] training 77.0% loss=0.08994, acc=0.95312
# [50/100] training 77.2% loss=0.17341, acc=0.95312
# [50/100] training 77.3% loss=0.07078, acc=0.98438
# [50/100] training 77.5% loss=0.27494, acc=0.89062
# [50/100] training 77.6% loss=0.16986, acc=0.89062
# [50/100] training 77.9% loss=0.18347, acc=0.95312
# [50/100] training 78.0% loss=0.18359, acc=0.93750
# [50/100] training 78.2% loss=0.20071, acc=0.90625
# [50/100] training 78.4% loss=0.10082, acc=0.95312
# [50/100] training 78.5% loss=0.19444, acc=0.95312
# [50/100] training 78.7% loss=0.14238, acc=0.93750
# [50/100] training 78.8% loss=0.10812, acc=0.96875
# [50/100] training 79.1% loss=0.15016, acc=0.95312
# [50/100] training 79.2% loss=0.15296, acc=0.95312
# [50/100] training 79.4% loss=0.17019, acc=0.93750
# [50/100] training 79.5% loss=0.18122, acc=0.96875
# [50/100] training 79.7% loss=0.05288, acc=1.00000
# [50/100] training 79.9% loss=0.08994, acc=0.98438
# [50/100] training 80.1% loss=0.06065, acc=1.00000
# [50/100] training 80.3% loss=0.20448, acc=0.93750
# [50/100] training 80.4% loss=0.28469, acc=0.90625
# [50/100] training 80.6% loss=0.26273, acc=0.90625
# [50/100] training 80.7% loss=0.12733, acc=0.93750
# [50/100] training 80.9% loss=0.20937, acc=0.93750
# [50/100] training 81.2% loss=0.37426, acc=0.85938
# [50/100] training 81.3% loss=0.10868, acc=0.98438
# [50/100] training 81.5% loss=0.17693, acc=0.92188
# [50/100] training 81.6% loss=0.19400, acc=0.87500
# [50/100] training 81.8% loss=0.18667, acc=0.93750
# [50/100] training 81.9% loss=0.27635, acc=0.93750
# [50/100] training 82.1% loss=0.08396, acc=0.98438
# [50/100] training 82.2% loss=0.17636, acc=0.90625
# [50/100] training 82.5% loss=0.13801, acc=0.96875
# [50/100] training 82.7% loss=0.19520, acc=0.93750
# [50/100] training 82.8% loss=0.18190, acc=0.93750
# [50/100] training 83.0% loss=0.12792, acc=0.93750
# [50/100] training 83.1% loss=0.23869, acc=0.89062
# [50/100] training 83.3% loss=0.08764, acc=0.98438
# [50/100] training 83.5% loss=0.13708, acc=0.96875
# [50/100] training 83.7% loss=0.31624, acc=0.89062
# [50/100] training 83.9% loss=0.20322, acc=0.89062
# [50/100] training 84.0% loss=0.13289, acc=0.93750
# [50/100] training 84.2% loss=0.09189, acc=0.93750
# [50/100] training 84.3% loss=0.18506, acc=0.92188
# [50/100] training 84.5% loss=0.23891, acc=0.90625
# [50/100] training 84.7% loss=0.12115, acc=0.95312
# [50/100] training 84.9% loss=0.13466, acc=0.96875
# [50/100] training 85.0% loss=0.14943, acc=0.92188
# [50/100] training 85.2% loss=0.17181, acc=0.90625
# [50/100] training 85.4% loss=0.07365, acc=0.96875
# [50/100] training 85.5% loss=0.19071, acc=0.92188
# [50/100] training 85.8% loss=0.17565, acc=0.93750
# [50/100] training 85.9% loss=0.15097, acc=0.92188
# [50/100] training 86.1% loss=0.14998, acc=0.95312
# [50/100] training 86.2% loss=0.15099, acc=0.93750
# [50/100] training 86.4% loss=0.19411, acc=0.89062
# [50/100] training 86.6% loss=0.15550, acc=0.93750
# [50/100] training 86.7% loss=0.08717, acc=0.95312
# [50/100] training 87.0% loss=0.21336, acc=0.93750
# [50/100] training 87.1% loss=0.10126, acc=0.95312
# [50/100] training 87.3% loss=0.09433, acc=0.96875
# [50/100] training 87.4% loss=0.15983, acc=0.89062
# [50/100] training 87.6% loss=0.19608, acc=0.90625
# [50/100] training 87.7% loss=0.31062, acc=0.92188
# [50/100] training 87.9% loss=0.20281, acc=0.92188
# [50/100] training 88.2% loss=0.12501, acc=0.95312
# [50/100] training 88.3% loss=0.18862, acc=0.95312
# [50/100] training 88.5% loss=0.22266, acc=0.89062
# [50/100] training 88.6% loss=0.13866, acc=0.95312
# [50/100] training 88.8% loss=0.15807, acc=0.92188
# [50/100] training 88.9% loss=0.22853, acc=0.89062
# [50/100] training 89.2% loss=0.12165, acc=0.90625
# [50/100] training 89.4% loss=0.15753, acc=0.90625
# [50/100] training 89.5% loss=0.12605, acc=0.96875
# [50/100] training 89.7% loss=0.22679, acc=0.90625
# [50/100] training 89.8% loss=0.14097, acc=0.93750
# [50/100] training 90.0% loss=0.11554, acc=0.95312
# [50/100] training 90.1% loss=0.24739, acc=0.90625
# [50/100] training 90.4% loss=0.22449, acc=0.92188
# [50/100] training 90.5% loss=0.12643, acc=0.96875
# [50/100] training 90.7% loss=0.17337, acc=0.93750
# [50/100] training 90.9% loss=0.16430, acc=0.95312
# [50/100] training 91.0% loss=0.17417, acc=0.92188
# [50/100] training 91.2% loss=0.20213, acc=0.90625
# [50/100] training 91.3% loss=0.25192, acc=0.92188
# [50/100] training 91.6% loss=0.25745, acc=0.87500
# [50/100] training 91.7% loss=0.14008, acc=0.95312
# [50/100] training 91.9% loss=0.34345, acc=0.90625
# [50/100] training 92.1% loss=0.21591, acc=0.90625
# [50/100] training 92.2% loss=0.11967, acc=0.95312
# [50/100] training 92.4% loss=0.17539, acc=0.90625
# [50/100] training 92.6% loss=0.25082, acc=0.89062
# [50/100] training 92.8% loss=0.23688, acc=0.93750
# [50/100] training 92.9% loss=0.28562, acc=0.90625
# [50/100] training 93.1% loss=0.40328, acc=0.87500
# [50/100] training 93.2% loss=0.18638, acc=0.93750
# [50/100] training 93.4% loss=0.23836, acc=0.89062
# [50/100] training 93.7% loss=0.19082, acc=0.92188
# [50/100] training 93.8% loss=0.13118, acc=0.95312
# [50/100] training 94.0% loss=0.14226, acc=0.93750
# [50/100] training 94.1% loss=0.21124, acc=0.89062
# [50/100] training 94.3% loss=0.10506, acc=0.93750
# [50/100] training 94.4% loss=0.11879, acc=0.96875
# [50/100] training 94.6% loss=0.06115, acc=0.98438
# [50/100] training 94.9% loss=0.12294, acc=0.93750
# [50/100] training 95.0% loss=0.21635, acc=0.89062
# [50/100] training 95.2% loss=0.38555, acc=0.85938
# [50/100] training 95.3% loss=0.21473, acc=0.92188
# [50/100] training 95.5% loss=0.18716, acc=0.95312
# [50/100] training 95.6% loss=0.30209, acc=0.89062
# [50/100] training 95.8% loss=0.27093, acc=0.87500
# [50/100] training 96.0% loss=0.38282, acc=0.87500
# [50/100] training 96.2% loss=0.09465, acc=0.96875
# [50/100] training 96.4% loss=0.15958, acc=0.93750
# [50/100] training 96.5% loss=0.20171, acc=0.90625
# [50/100] training 96.7% loss=0.11221, acc=0.96875
# [50/100] training 96.8% loss=0.17585, acc=0.92188
# [50/100] training 97.1% loss=0.11337, acc=0.93750
# [50/100] training 97.2% loss=0.24931, acc=0.90625
# [50/100] training 97.4% loss=0.15178, acc=0.93750
# [50/100] training 97.6% loss=0.28192, acc=0.89062
# [50/100] training 97.7% loss=0.15581, acc=0.95312
# [50/100] training 97.9% loss=0.13485, acc=0.92188
# [50/100] training 98.0% loss=0.20304, acc=0.90625
# [50/100] training 98.3% loss=0.21255, acc=0.95312
# [50/100] training 98.4% loss=0.24244, acc=0.89062
# [50/100] training 98.6% loss=0.29375, acc=0.89062
# [50/100] training 98.7% loss=0.24561, acc=0.90625
# [50/100] training 98.9% loss=0.18617, acc=0.95312
# [50/100] training 99.1% loss=0.20421, acc=0.90625
# [50/100] training 99.2% loss=0.14880, acc=0.93750
# [50/100] training 99.5% loss=0.26341, acc=0.89062
# [50/100] training 99.6% loss=0.12235, acc=0.95312
# [50/100] training 99.8% loss=0.15121, acc=0.93750
# [50/100] training 99.9% loss=0.09540, acc=0.95312
# [50/100] testing 0.9% loss=0.19845, acc=0.89062
# [50/100] testing 1.8% loss=0.34713, acc=0.85938
# [50/100] testing 2.2% loss=0.28791, acc=0.90625
# [50/100] testing 3.1% loss=0.33931, acc=0.87500
# [50/100] testing 3.5% loss=0.15632, acc=0.90625
# [50/100] testing 4.4% loss=0.13321, acc=0.93750
# [50/100] testing 4.8% loss=0.35083, acc=0.87500
# [50/100] testing 5.7% loss=0.22862, acc=0.90625
# [50/100] testing 6.6% loss=0.11344, acc=0.95312
# [50/100] testing 7.0% loss=0.08142, acc=0.98438
# [50/100] testing 7.9% loss=0.29512, acc=0.90625
# [50/100] testing 8.3% loss=0.27806, acc=0.89062
# [50/100] testing 9.2% loss=0.29202, acc=0.92188
# [50/100] testing 9.7% loss=0.09184, acc=0.96875
# [50/100] testing 10.5% loss=0.16698, acc=0.89062
# [50/100] testing 11.0% loss=0.26066, acc=0.87500
# [50/100] testing 11.8% loss=0.14392, acc=0.96875
# [50/100] testing 12.7% loss=0.34510, acc=0.89062
# [50/100] testing 13.2% loss=0.16547, acc=0.92188
# [50/100] testing 14.0% loss=0.40351, acc=0.90625
# [50/100] testing 14.5% loss=0.23717, acc=0.87500
# [50/100] testing 15.4% loss=0.31853, acc=0.89062
# [50/100] testing 15.8% loss=0.12427, acc=0.95312
# [50/100] testing 16.7% loss=0.19897, acc=0.93750
# [50/100] testing 17.5% loss=0.15995, acc=0.93750
# [50/100] testing 18.0% loss=0.24729, acc=0.92188
# [50/100] testing 18.9% loss=0.09940, acc=0.96875
# [50/100] testing 19.3% loss=0.42424, acc=0.85938
# [50/100] testing 20.2% loss=0.29853, acc=0.89062
# [50/100] testing 20.6% loss=0.32245, acc=0.90625
# [50/100] testing 21.5% loss=0.20766, acc=0.93750
# [50/100] testing 21.9% loss=0.38822, acc=0.87500
# [50/100] testing 22.8% loss=0.35920, acc=0.92188
# [50/100] testing 23.7% loss=0.41600, acc=0.84375
# [50/100] testing 24.1% loss=0.13878, acc=0.93750
# [50/100] testing 25.0% loss=0.36621, acc=0.89062
# [50/100] testing 25.4% loss=0.17181, acc=0.93750
# [50/100] testing 26.3% loss=0.23464, acc=0.89062
# [50/100] testing 26.8% loss=0.22740, acc=0.92188
# [50/100] testing 27.6% loss=0.22528, acc=0.92188
# [50/100] testing 28.5% loss=0.19006, acc=0.93750
# [50/100] testing 29.0% loss=0.20962, acc=0.93750
# [50/100] testing 29.8% loss=0.35565, acc=0.90625
# [50/100] testing 30.3% loss=0.22899, acc=0.95312
# [50/100] testing 31.1% loss=0.30016, acc=0.89062
# [50/100] testing 31.6% loss=0.23261, acc=0.92188
# [50/100] testing 32.5% loss=0.28104, acc=0.92188
# [50/100] testing 32.9% loss=0.39662, acc=0.90625
# [50/100] testing 33.8% loss=0.31442, acc=0.87500
# [50/100] testing 34.7% loss=0.37202, acc=0.89062
# [50/100] testing 35.1% loss=0.16155, acc=0.93750
# [50/100] testing 36.0% loss=0.21110, acc=0.92188
# [50/100] testing 36.4% loss=0.21679, acc=0.93750
# [50/100] testing 37.3% loss=0.26122, acc=0.95312
# [50/100] testing 37.7% loss=0.42015, acc=0.87500
# [50/100] testing 38.6% loss=0.18255, acc=0.93750
# [50/100] testing 39.5% loss=0.23649, acc=0.96875
# [50/100] testing 39.9% loss=0.23742, acc=0.93750
# [50/100] testing 40.8% loss=0.33927, acc=0.90625
# [50/100] testing 41.2% loss=0.20822, acc=0.96875
# [50/100] testing 42.1% loss=0.37940, acc=0.87500
# [50/100] testing 42.5% loss=0.17607, acc=0.92188
# [50/100] testing 43.4% loss=0.28058, acc=0.92188
# [50/100] testing 43.9% loss=0.14013, acc=0.93750
# [50/100] testing 44.7% loss=0.29456, acc=0.84375
# [50/100] testing 45.6% loss=0.22856, acc=0.93750
# [50/100] testing 46.1% loss=0.24925, acc=0.90625
# [50/100] testing 46.9% loss=0.26568, acc=0.89062
# [50/100] testing 47.4% loss=0.12920, acc=0.92188
# [50/100] testing 48.3% loss=0.31091, acc=0.90625
# [50/100] testing 48.7% loss=0.28130, acc=0.89062
# [50/100] testing 49.6% loss=0.48477, acc=0.85938
# [50/100] testing 50.4% loss=0.16811, acc=0.93750
# [50/100] testing 50.9% loss=0.29502, acc=0.89062
# [50/100] testing 51.8% loss=0.18272, acc=0.93750
# [50/100] testing 52.2% loss=0.19959, acc=0.93750
# [50/100] testing 53.1% loss=0.21376, acc=0.95312
# [50/100] testing 53.5% loss=0.27330, acc=0.92188
# [50/100] testing 54.4% loss=0.31343, acc=0.89062
# [50/100] testing 54.8% loss=0.32530, acc=0.89062
# [50/100] testing 55.7% loss=0.15919, acc=0.93750
# [50/100] testing 56.6% loss=0.31006, acc=0.84375
# [50/100] testing 57.0% loss=0.34805, acc=0.92188
# [50/100] testing 57.9% loss=0.34110, acc=0.87500
# [50/100] testing 58.3% loss=0.39341, acc=0.85938
# [50/100] testing 59.2% loss=0.24912, acc=0.90625
# [50/100] testing 59.7% loss=0.15144, acc=0.93750
# [50/100] testing 60.5% loss=0.34416, acc=0.85938
# [50/100] testing 61.4% loss=0.09705, acc=0.96875
# [50/100] testing 61.9% loss=0.19733, acc=0.92188
# [50/100] testing 62.7% loss=0.11731, acc=0.95312
# [50/100] testing 63.2% loss=0.33310, acc=0.90625
# [50/100] testing 64.0% loss=0.42879, acc=0.89062
# [50/100] testing 64.5% loss=0.15102, acc=0.92188
# [50/100] testing 65.4% loss=0.17648, acc=0.93750
# [50/100] testing 65.8% loss=0.26952, acc=0.89062
# [50/100] testing 66.7% loss=0.21627, acc=0.92188
# [50/100] testing 67.6% loss=0.36535, acc=0.92188
# [50/100] testing 68.0% loss=0.07997, acc=0.96875
# [50/100] testing 68.9% loss=0.26680, acc=0.93750
# [50/100] testing 69.3% loss=0.26194, acc=0.87500
# [50/100] testing 70.2% loss=0.33409, acc=0.89062
# [50/100] testing 70.6% loss=0.29172, acc=0.90625
# [50/100] testing 71.5% loss=0.29310, acc=0.90625
# [50/100] testing 72.4% loss=0.14159, acc=0.96875
# [50/100] testing 72.8% loss=0.16663, acc=0.93750
# [50/100] testing 73.7% loss=0.11805, acc=0.98438
# [50/100] testing 74.1% loss=0.31832, acc=0.90625
# [50/100] testing 75.0% loss=0.19361, acc=0.90625
# [50/100] testing 75.4% loss=0.42208, acc=0.84375
# [50/100] testing 76.3% loss=0.09440, acc=0.96875
# [50/100] testing 76.8% loss=0.23663, acc=0.90625
# [50/100] testing 77.6% loss=0.17330, acc=0.92188
# [50/100] testing 78.5% loss=0.31126, acc=0.85938
# [50/100] testing 79.0% loss=0.26808, acc=0.87500
# [50/100] testing 79.8% loss=0.34337, acc=0.84375
# [50/100] testing 80.3% loss=0.27207, acc=0.90625
# [50/100] testing 81.2% loss=0.53405, acc=0.87500
# [50/100] testing 81.6% loss=0.21879, acc=0.92188
# [50/100] testing 82.5% loss=0.23365, acc=0.89062
# [50/100] testing 83.3% loss=0.22894, acc=0.95312
# [50/100] testing 83.8% loss=0.14241, acc=0.93750
# [50/100] testing 84.7% loss=0.23466, acc=0.92188
# [50/100] testing 85.1% loss=0.19958, acc=0.89062
# [50/100] testing 86.0% loss=0.34125, acc=0.92188
# [50/100] testing 86.4% loss=0.29879, acc=0.89062
# [50/100] testing 87.3% loss=0.24224, acc=0.92188
# [50/100] testing 87.7% loss=0.24117, acc=0.89062
# [50/100] testing 88.6% loss=0.29309, acc=0.89062
# [50/100] testing 89.5% loss=0.38534, acc=0.84375
# [50/100] testing 89.9% loss=0.14095, acc=0.90625
# [50/100] testing 90.8% loss=0.31880, acc=0.95312
# [50/100] testing 91.2% loss=0.19952, acc=0.93750
# [50/100] testing 92.1% loss=0.25185, acc=0.92188
# [50/100] testing 92.6% loss=0.30456, acc=0.90625
# [50/100] testing 93.4% loss=0.28592, acc=0.89062
# [50/100] testing 94.3% loss=0.13662, acc=0.93750
# [50/100] testing 94.7% loss=0.22379, acc=0.90625
# [50/100] testing 95.6% loss=0.40067, acc=0.84375
# [50/100] testing 96.1% loss=0.16210, acc=0.92188
# [50/100] testing 96.9% loss=0.25344, acc=0.92188
# [50/100] testing 97.4% loss=0.12957, acc=0.96875
# [50/100] testing 98.3% loss=0.25417, acc=0.87500
# [50/100] testing 98.7% loss=0.25283, acc=0.89062
# [50/100] testing 99.6% loss=0.33562, acc=0.87500
# [51/100] training 0.2% loss=0.25649, acc=0.90625
# [51/100] training 0.4% loss=0.29040, acc=0.85938
# [51/100] training 0.5% loss=0.09416, acc=0.95312
# [51/100] training 0.8% loss=0.11811, acc=0.93750
# [51/100] training 0.9% loss=0.18209, acc=0.92188
# [51/100] training 1.1% loss=0.26549, acc=0.92188
# [51/100] training 1.2% loss=0.19921, acc=0.90625
# [51/100] training 1.4% loss=0.15281, acc=0.93750
# [51/100] training 1.6% loss=0.17850, acc=0.93750
# [51/100] training 1.8% loss=0.19213, acc=0.90625
# [51/100] training 2.0% loss=0.22618, acc=0.89062
# [51/100] training 2.1% loss=0.22202, acc=0.90625
# [51/100] training 2.3% loss=0.17677, acc=0.95312
# [51/100] training 2.4% loss=0.13629, acc=0.93750
# [51/100] training 2.6% loss=0.18134, acc=0.93750
# [51/100] training 2.7% loss=0.21129, acc=0.93750
# [51/100] training 3.0% loss=0.15785, acc=0.93750
# [51/100] training 3.2% loss=0.13636, acc=0.93750
# [51/100] training 3.3% loss=0.33210, acc=0.90625
# [51/100] training 3.5% loss=0.11232, acc=0.95312
# [51/100] training 3.6% loss=0.17285, acc=0.90625
# [51/100] training 3.8% loss=0.17059, acc=0.95312
# [51/100] training 3.9% loss=0.12282, acc=0.95312
# [51/100] training 4.2% loss=0.11141, acc=0.95312
# [51/100] training 4.4% loss=0.17874, acc=0.93750
# [51/100] training 4.5% loss=0.14975, acc=0.95312
# [51/100] training 4.7% loss=0.30815, acc=0.89062
# [51/100] training 4.8% loss=0.15133, acc=0.92188
# [51/100] training 5.0% loss=0.17274, acc=0.90625
# [51/100] training 5.2% loss=0.20531, acc=0.92188
# [51/100] training 5.4% loss=0.12472, acc=0.93750
# [51/100] training 5.5% loss=0.19416, acc=0.92188
# [51/100] training 5.7% loss=0.13614, acc=0.92188
# [51/100] training 5.9% loss=0.18148, acc=0.95312
# [51/100] training 6.0% loss=0.20565, acc=0.87500
# [51/100] training 6.3% loss=0.16973, acc=0.90625
# [51/100] training 6.4% loss=0.19235, acc=0.93750
# [51/100] training 6.6% loss=0.17270, acc=0.92188
# [51/100] training 6.7% loss=0.24980, acc=0.89062
# [51/100] training 6.9% loss=0.17021, acc=0.93750
# [51/100] training 7.1% loss=0.11393, acc=0.96875
# [51/100] training 7.2% loss=0.31740, acc=0.85938
# [51/100] training 7.5% loss=0.20764, acc=0.90625
# [51/100] training 7.6% loss=0.21485, acc=0.95312
# [51/100] training 7.8% loss=0.17559, acc=0.95312
# [51/100] training 7.9% loss=0.22208, acc=0.90625
# [51/100] training 8.1% loss=0.11389, acc=0.95312
# [51/100] training 8.2% loss=0.21484, acc=0.92188
# [51/100] training 8.4% loss=0.22599, acc=0.93750
# [51/100] training 8.7% loss=0.14195, acc=0.93750
# [51/100] training 8.8% loss=0.14615, acc=0.95312
# [51/100] training 9.0% loss=0.19112, acc=0.93750
# [51/100] training 9.1% loss=0.12619, acc=0.98438
# [51/100] training 9.3% loss=0.34654, acc=0.85938
# [51/100] training 9.4% loss=0.08470, acc=1.00000
# [51/100] training 9.7% loss=0.16098, acc=0.93750
# [51/100] training 9.9% loss=0.25654, acc=0.89062
# [51/100] training 10.0% loss=0.29828, acc=0.87500
# [51/100] training 10.2% loss=0.13614, acc=0.95312
# [51/100] training 10.3% loss=0.15473, acc=0.90625
# [51/100] training 10.5% loss=0.17964, acc=0.93750
# [51/100] training 10.6% loss=0.20225, acc=0.95312
# [51/100] training 10.9% loss=0.13062, acc=0.95312
# [51/100] training 11.0% loss=0.33355, acc=0.84375
# [51/100] training 11.2% loss=0.13758, acc=0.93750
# [51/100] training 11.4% loss=0.15189, acc=0.96875
# [51/100] training 11.5% loss=0.33930, acc=0.89062
# [51/100] training 11.7% loss=0.09930, acc=0.96875
# [51/100] training 11.8% loss=0.19643, acc=0.90625
# [51/100] training 12.1% loss=0.20874, acc=0.87500
# [51/100] training 12.2% loss=0.09415, acc=0.98438
# [51/100] training 12.4% loss=0.21714, acc=0.93750
# [51/100] training 12.6% loss=0.19975, acc=0.92188
# [51/100] training 12.7% loss=0.18245, acc=0.93750
# [51/100] training 12.9% loss=0.20391, acc=0.90625
# [51/100] training 13.0% loss=0.12683, acc=0.93750
# [51/100] training 13.3% loss=0.22649, acc=0.90625
# [51/100] training 13.4% loss=0.21049, acc=0.85938
# [51/100] training 13.6% loss=0.12747, acc=0.93750
# [51/100] training 13.7% loss=0.33512, acc=0.85938
# [51/100] training 13.9% loss=0.14291, acc=0.95312
# [51/100] training 14.1% loss=0.19388, acc=0.93750
# [51/100] training 14.3% loss=0.23110, acc=0.90625
# [51/100] training 14.5% loss=0.24538, acc=0.95312
# [51/100] training 14.6% loss=0.19379, acc=0.92188
# [51/100] training 14.8% loss=0.22750, acc=0.92188
# [51/100] training 14.9% loss=0.07301, acc=0.98438
# [51/100] training 15.1% loss=0.34685, acc=0.87500
# [51/100] training 15.4% loss=0.19011, acc=0.89062
# [51/100] training 15.5% loss=0.17670, acc=0.90625
# [51/100] training 15.7% loss=0.33369, acc=0.89062
# [51/100] training 15.8% loss=0.11465, acc=0.95312
# [51/100] training 16.0% loss=0.18367, acc=0.92188
# [51/100] training 16.1% loss=0.32656, acc=0.89062
# [51/100] training 16.3% loss=0.15789, acc=0.92188
# [51/100] training 16.4% loss=0.14978, acc=0.93750
# [51/100] training 16.7% loss=0.24166, acc=0.89062
# [51/100] training 16.9% loss=0.16190, acc=0.92188
# [51/100] training 17.0% loss=0.16853, acc=0.93750
# [51/100] training 17.2% loss=0.17128, acc=0.93750
# [51/100] training 17.3% loss=0.13062, acc=0.95312
# [51/100] training 17.5% loss=0.23049, acc=0.87500
# [51/100] training 17.7% loss=0.14397, acc=0.93750
# [51/100] training 17.9% loss=0.19587, acc=0.90625
# [51/100] training 18.1% loss=0.28282, acc=0.87500
# [51/100] training 18.2% loss=0.20926, acc=0.92188
# [51/100] training 18.4% loss=0.34017, acc=0.89062
# [51/100] training 18.5% loss=0.20258, acc=0.90625
# [51/100] training 18.8% loss=0.16176, acc=0.90625
# [51/100] training 18.9% loss=0.08794, acc=0.98438
# [51/100] training 19.1% loss=0.29321, acc=0.85938
# [51/100] training 19.2% loss=0.10937, acc=0.96875
# [51/100] training 19.4% loss=0.12748, acc=0.93750
# [51/100] training 19.6% loss=0.22953, acc=0.90625
# [51/100] training 19.7% loss=0.19964, acc=0.93750
# [51/100] training 20.0% loss=0.10793, acc=0.95312
# [51/100] training 20.1% loss=0.22189, acc=0.90625
# [51/100] training 20.3% loss=0.15951, acc=0.95312
# [51/100] training 20.4% loss=0.18250, acc=0.93750
# [51/100] training 20.6% loss=0.26713, acc=0.89062
# [51/100] training 20.8% loss=0.13500, acc=0.93750
# [51/100] training 20.9% loss=0.22390, acc=0.90625
# [51/100] training 21.2% loss=0.25333, acc=0.93750
# [51/100] training 21.3% loss=0.22631, acc=0.92188
# [51/100] training 21.5% loss=0.32679, acc=0.89062
# [51/100] training 21.6% loss=0.06881, acc=0.96875
# [51/100] training 21.8% loss=0.06397, acc=0.98438
# [51/100] training 21.9% loss=0.27036, acc=0.90625
# [51/100] training 22.2% loss=0.16375, acc=0.92188
# [51/100] training 22.4% loss=0.21862, acc=0.90625
# [51/100] training 22.5% loss=0.11698, acc=0.95312
# [51/100] training 22.7% loss=0.16278, acc=0.93750
# [51/100] training 22.8% loss=0.18039, acc=0.92188
# [51/100] training 23.0% loss=0.08431, acc=0.96875
# [51/100] training 23.1% loss=0.23438, acc=0.92188
# [51/100] training 23.4% loss=0.13409, acc=0.93750
# [51/100] training 23.6% loss=0.26988, acc=0.92188
# [51/100] training 23.7% loss=0.18776, acc=0.92188
# [51/100] training 23.9% loss=0.13141, acc=0.96875
# [51/100] training 24.0% loss=0.15992, acc=0.93750
# [51/100] training 24.2% loss=0.14748, acc=0.93750
# [51/100] training 24.3% loss=0.20855, acc=0.92188
# [51/100] training 24.6% loss=0.14787, acc=0.92188
# [51/100] training 24.7% loss=0.23967, acc=0.90625
# [51/100] training 24.9% loss=0.24692, acc=0.90625
# [51/100] training 25.1% loss=0.23402, acc=0.90625
# [51/100] training 25.2% loss=0.21946, acc=0.89062
# [51/100] training 25.4% loss=0.21497, acc=0.90625
# [51/100] training 25.6% loss=0.14099, acc=0.96875
# [51/100] training 25.8% loss=0.24002, acc=0.85938
# [51/100] training 25.9% loss=0.16069, acc=0.92188
# [51/100] training 26.1% loss=0.31049, acc=0.90625
# [51/100] training 26.3% loss=0.08964, acc=0.95312
# [51/100] training 26.4% loss=0.17036, acc=0.90625
# [51/100] training 26.6% loss=0.08061, acc=0.96875
# [51/100] training 26.8% loss=0.13820, acc=0.95312
# [51/100] training 27.0% loss=0.28283, acc=0.93750
# [51/100] training 27.1% loss=0.18432, acc=0.92188
# [51/100] training 27.3% loss=0.14040, acc=0.93750
# [51/100] training 27.4% loss=0.04828, acc=1.00000
# [51/100] training 27.6% loss=0.32801, acc=0.90625
# [51/100] training 27.9% loss=0.08449, acc=0.96875
# [51/100] training 28.0% loss=0.26863, acc=0.87500
# [51/100] training 28.2% loss=0.18703, acc=0.95312
# [51/100] training 28.3% loss=0.07493, acc=0.96875
# [51/100] training 28.5% loss=0.24789, acc=0.92188
# [51/100] training 28.6% loss=0.52083, acc=0.87500
# [51/100] training 28.8% loss=0.10966, acc=0.95312
# [51/100] training 29.1% loss=0.17366, acc=0.93750
# [51/100] training 29.2% loss=0.12527, acc=0.95312
# [51/100] training 29.4% loss=0.21560, acc=0.92188
# [51/100] training 29.5% loss=0.09648, acc=0.96875
# [51/100] training 29.7% loss=0.22889, acc=0.92188
# [51/100] training 29.8% loss=0.18341, acc=0.95312
# [51/100] training 30.0% loss=0.23305, acc=0.93750
# [51/100] training 30.2% loss=0.12823, acc=0.92188
# [51/100] training 30.4% loss=0.09695, acc=0.96875
# [51/100] training 30.6% loss=0.24484, acc=0.89062
# [51/100] training 30.7% loss=0.17729, acc=0.93750
# [51/100] training 30.9% loss=0.26379, acc=0.92188
# [51/100] training 31.0% loss=0.09172, acc=0.95312
# [51/100] training 31.3% loss=0.19385, acc=0.93750
# [51/100] training 31.4% loss=0.25894, acc=0.89062
# [51/100] training 31.6% loss=0.21039, acc=0.92188
# [51/100] training 31.8% loss=0.23017, acc=0.90625
# [51/100] training 31.9% loss=0.19951, acc=0.92188
# [51/100] training 32.1% loss=0.14479, acc=0.93750
# [51/100] training 32.2% loss=0.20289, acc=0.92188
# [51/100] training 32.5% loss=0.11687, acc=0.93750
# [51/100] training 32.6% loss=0.19305, acc=0.90625
# [51/100] training 32.8% loss=0.19532, acc=0.89062
# [51/100] training 32.9% loss=0.19078, acc=0.92188
# [51/100] training 33.1% loss=0.20097, acc=0.92188
# [51/100] training 33.3% loss=0.15546, acc=0.93750
# [51/100] training 33.4% loss=0.09571, acc=0.96875
# [51/100] training 33.7% loss=0.13609, acc=0.95312
# [51/100] training 33.8% loss=0.17276, acc=0.95312
# [51/100] training 34.0% loss=0.29320, acc=0.89062
# [51/100] training 34.1% loss=0.30590, acc=0.90625
# [51/100] training 34.3% loss=0.17532, acc=0.93750
# [51/100] training 34.5% loss=0.23876, acc=0.93750
# [51/100] training 34.7% loss=0.11675, acc=0.95312
# [51/100] training 34.9% loss=0.11720, acc=0.95312
# [51/100] training 35.0% loss=0.15131, acc=0.96875
# [51/100] training 35.2% loss=0.26478, acc=0.87500
# [51/100] training 35.3% loss=0.21916, acc=0.87500
# [51/100] training 35.5% loss=0.20294, acc=0.93750
# [51/100] training 35.6% loss=0.48860, acc=0.81250
# [51/100] training 35.9% loss=0.18361, acc=0.92188
# [51/100] training 36.1% loss=0.30004, acc=0.87500
# [51/100] training 36.2% loss=0.23085, acc=0.95312
# [51/100] training 36.4% loss=0.19567, acc=0.96875
# [51/100] training 36.5% loss=0.28596, acc=0.92188
# [51/100] training 36.7% loss=0.19323, acc=0.93750
# [51/100] training 36.8% loss=0.12763, acc=0.96875
# [51/100] training 37.1% loss=0.23292, acc=0.92188
# [51/100] training 37.3% loss=0.24709, acc=0.89062
# [51/100] training 37.4% loss=0.14346, acc=0.95312
# [51/100] training 37.6% loss=0.12514, acc=0.95312
# [51/100] training 37.7% loss=0.34618, acc=0.85938
# [51/100] training 37.9% loss=0.16617, acc=0.96875
# [51/100] training 38.1% loss=0.27255, acc=0.85938
# [51/100] training 38.3% loss=0.13492, acc=0.93750
# [51/100] training 38.4% loss=0.10039, acc=0.96875
# [51/100] training 38.6% loss=0.11886, acc=0.93750
# [51/100] training 38.8% loss=0.17274, acc=0.93750
# [51/100] training 38.9% loss=0.13923, acc=0.93750
# [51/100] training 39.1% loss=0.21926, acc=0.89062
# [51/100] training 39.3% loss=0.22305, acc=0.90625
# [51/100] training 39.5% loss=0.17617, acc=0.92188
# [51/100] training 39.6% loss=0.19188, acc=0.92188
# [51/100] training 39.8% loss=0.09619, acc=0.95312
# [51/100] training 40.0% loss=0.22546, acc=0.90625
# [51/100] training 40.1% loss=0.25777, acc=0.87500
# [51/100] training 40.4% loss=0.12797, acc=0.92188
# [51/100] training 40.5% loss=0.30443, acc=0.89062
# [51/100] training 40.7% loss=0.19516, acc=0.92188
# [51/100] training 40.8% loss=0.07536, acc=0.96875
# [51/100] training 41.0% loss=0.16291, acc=0.93750
# [51/100] training 41.1% loss=0.25541, acc=0.90625
# [51/100] training 41.3% loss=0.20827, acc=0.89062
# [51/100] training 41.6% loss=0.31489, acc=0.87500
# [51/100] training 41.7% loss=0.19049, acc=0.92188
# [51/100] training 41.9% loss=0.15544, acc=0.95312
# [51/100] training 42.0% loss=0.25328, acc=0.90625
# [51/100] training 42.2% loss=0.26382, acc=0.87500
# [51/100] training 42.3% loss=0.13982, acc=0.93750
# [51/100] training 42.5% loss=0.17445, acc=0.95312
# [51/100] training 42.8% loss=0.15408, acc=0.95312
# [51/100] training 42.9% loss=0.16867, acc=0.93750
# [51/100] training 43.1% loss=0.21908, acc=0.96875
# [51/100] training 43.2% loss=0.17584, acc=0.93750
# [51/100] training 43.4% loss=0.15432, acc=0.93750
# [51/100] training 43.5% loss=0.23292, acc=0.89062
# [51/100] training 43.8% loss=0.17743, acc=0.93750
# [51/100] training 43.9% loss=0.24420, acc=0.90625
# [51/100] training 44.1% loss=0.13565, acc=0.95312
# [51/100] training 44.3% loss=0.12265, acc=0.96875
# [51/100] training 44.4% loss=0.26826, acc=0.90625
# [51/100] training 44.6% loss=0.14420, acc=0.92188
# [51/100] training 44.7% loss=0.24649, acc=0.87500
# [51/100] training 45.0% loss=0.15419, acc=0.95312
# [51/100] training 45.1% loss=0.19372, acc=0.92188
# [51/100] training 45.3% loss=0.27564, acc=0.90625
# [51/100] training 45.5% loss=0.09807, acc=0.96875
# [51/100] training 45.6% loss=0.16600, acc=0.92188
# [51/100] training 45.8% loss=0.12970, acc=0.93750
# [51/100] training 45.9% loss=0.09748, acc=0.95312
# [51/100] training 46.2% loss=0.08258, acc=0.98438
# [51/100] training 46.3% loss=0.10172, acc=0.96875
# [51/100] training 46.5% loss=0.24478, acc=0.89062
# [51/100] training 46.6% loss=0.26178, acc=0.90625
# [51/100] training 46.8% loss=0.12272, acc=0.95312
# [51/100] training 47.0% loss=0.11574, acc=0.93750
# [51/100] training 47.2% loss=0.19694, acc=0.92188
# [51/100] training 47.4% loss=0.24400, acc=0.90625
# [51/100] training 47.5% loss=0.27810, acc=0.90625
# [51/100] training 47.7% loss=0.14309, acc=0.90625
# [51/100] training 47.8% loss=0.25990, acc=0.93750
# [51/100] training 48.0% loss=0.14092, acc=0.96875
# [51/100] training 48.3% loss=0.08215, acc=0.93750
# [51/100] training 48.4% loss=0.09646, acc=0.96875
# [51/100] training 48.6% loss=0.12602, acc=0.93750
# [51/100] training 48.7% loss=0.19872, acc=0.90625
# [51/100] training 48.9% loss=0.34155, acc=0.87500
# [51/100] training 49.0% loss=0.07484, acc=0.98438
# [51/100] training 49.2% loss=0.14728, acc=0.95312
# [51/100] training 49.3% loss=0.14214, acc=0.92188
# [51/100] training 49.6% loss=0.25538, acc=0.92188
# [51/100] training 49.8% loss=0.11562, acc=0.96875
# [51/100] training 49.9% loss=0.22589, acc=0.92188
# [51/100] training 50.1% loss=0.11254, acc=0.93750
# [51/100] training 50.2% loss=0.17114, acc=0.93750
# [51/100] training 50.4% loss=0.33459, acc=0.85938
# [51/100] training 50.6% loss=0.21368, acc=0.90625
# [51/100] training 50.8% loss=0.17826, acc=0.92188
# [51/100] training 51.0% loss=0.28413, acc=0.90625
# [51/100] training 51.1% loss=0.23672, acc=0.87500
# [51/100] training 51.3% loss=0.25808, acc=0.92188
# [51/100] training 51.4% loss=0.16949, acc=0.95312
# [51/100] training 51.7% loss=0.10465, acc=0.98438
# [51/100] training 51.8% loss=0.28228, acc=0.89062
# [51/100] training 52.0% loss=0.17578, acc=0.93750
# [51/100] training 52.1% loss=0.13187, acc=0.93750
# [51/100] training 52.3% loss=0.18066, acc=0.95312
# [51/100] training 52.5% loss=0.09626, acc=0.98438
# [51/100] training 52.6% loss=0.26270, acc=0.90625
# [51/100] training 52.9% loss=0.31401, acc=0.89062
# [51/100] training 53.0% loss=0.07620, acc=0.98438
# [51/100] training 53.2% loss=0.29478, acc=0.89062
# [51/100] training 53.3% loss=0.13743, acc=0.93750
# [51/100] training 53.5% loss=0.21928, acc=0.90625
# [51/100] training 53.7% loss=0.06908, acc=0.96875
# [51/100] training 53.8% loss=0.26685, acc=0.87500
# [51/100] training 54.1% loss=0.27700, acc=0.84375
# [51/100] training 54.2% loss=0.11146, acc=0.95312
# [51/100] training 54.4% loss=0.17335, acc=0.93750
# [51/100] training 54.5% loss=0.28306, acc=0.92188
# [51/100] training 54.7% loss=0.25512, acc=0.90625
# [51/100] training 54.8% loss=0.11878, acc=0.93750
# [51/100] training 55.1% loss=0.22581, acc=0.87500
# [51/100] training 55.3% loss=0.22916, acc=0.90625
# [51/100] training 55.4% loss=0.17771, acc=0.92188
# [51/100] training 55.6% loss=0.13113, acc=0.96875
# [51/100] training 55.7% loss=0.11177, acc=0.93750
# [51/100] training 55.9% loss=0.16173, acc=0.95312
# [51/100] training 56.0% loss=0.14020, acc=0.93750
# [51/100] training 56.3% loss=0.33128, acc=0.85938
# [51/100] training 56.5% loss=0.25664, acc=0.90625
# [51/100] training 56.6% loss=0.16162, acc=0.92188
# [51/100] training 56.8% loss=0.23050, acc=0.89062
# [51/100] training 56.9% loss=0.14514, acc=0.96875
# [51/100] training 57.1% loss=0.21666, acc=0.85938
# [51/100] training 57.2% loss=0.13782, acc=0.90625
# [51/100] training 57.5% loss=0.14257, acc=0.93750
# [51/100] training 57.6% loss=0.15192, acc=0.93750
# [51/100] training 57.8% loss=0.11858, acc=0.95312
# [51/100] training 58.0% loss=0.10622, acc=0.95312
# [51/100] training 58.1% loss=0.18643, acc=0.92188
# [51/100] training 58.3% loss=0.03810, acc=1.00000
# [51/100] training 58.4% loss=0.21915, acc=0.93750
# [51/100] training 58.7% loss=0.31883, acc=0.92188
# [51/100] training 58.8% loss=0.32501, acc=0.87500
# [51/100] training 59.0% loss=0.14288, acc=0.93750
# [51/100] training 59.2% loss=0.17877, acc=0.92188
# [51/100] training 59.3% loss=0.12866, acc=0.93750
# [51/100] training 59.5% loss=0.14692, acc=0.96875
# [51/100] training 59.7% loss=0.22543, acc=0.90625
# [51/100] training 59.9% loss=0.18147, acc=0.90625
# [51/100] training 60.0% loss=0.15948, acc=0.93750
# [51/100] training 60.2% loss=0.30848, acc=0.87500
# [51/100] training 60.3% loss=0.10920, acc=0.95312
# [51/100] training 60.5% loss=0.18027, acc=0.92188
# [51/100] training 60.8% loss=0.21067, acc=0.89062
# [51/100] training 60.9% loss=0.25385, acc=0.92188
# [51/100] training 61.1% loss=0.18114, acc=0.92188
# [51/100] training 61.2% loss=0.22457, acc=0.90625
# [51/100] training 61.4% loss=0.25201, acc=0.93750
# [51/100] training 61.5% loss=0.35687, acc=0.85938
# [51/100] training 61.7% loss=0.15680, acc=0.95312
# [51/100] training 62.0% loss=0.14875, acc=0.93750
# [51/100] training 62.1% loss=0.24927, acc=0.90625
# [51/100] training 62.3% loss=0.13074, acc=0.96875
# [51/100] training 62.4% loss=0.14467, acc=0.93750
# [51/100] training 62.6% loss=0.16151, acc=0.92188
# [51/100] training 62.7% loss=0.11136, acc=0.98438
# [51/100] training 62.9% loss=0.21977, acc=0.95312
# [51/100] training 63.1% loss=0.21559, acc=0.92188
# [51/100] training 63.3% loss=0.12339, acc=0.95312
# [51/100] training 63.5% loss=0.10541, acc=0.96875
# [51/100] training 63.6% loss=0.19501, acc=0.95312
# [51/100] training 63.8% loss=0.27732, acc=0.92188
# [51/100] training 63.9% loss=0.14250, acc=0.96875
# [51/100] training 64.2% loss=0.13788, acc=0.95312
# [51/100] training 64.3% loss=0.31104, acc=0.87500
# [51/100] training 64.5% loss=0.09589, acc=0.98438
# [51/100] training 64.7% loss=0.15312, acc=0.92188
# [51/100] training 64.8% loss=0.25649, acc=0.92188
# [51/100] training 65.0% loss=0.19341, acc=0.90625
# [51/100] training 65.1% loss=0.15105, acc=0.90625
# [51/100] training 65.4% loss=0.11345, acc=0.96875
# [51/100] training 65.5% loss=0.07552, acc=0.95312
# [51/100] training 65.7% loss=0.15851, acc=0.95312
# [51/100] training 65.8% loss=0.14197, acc=0.92188
# [51/100] training 66.0% loss=0.21816, acc=0.93750
# [51/100] training 66.2% loss=0.05851, acc=0.98438
# [51/100] training 66.3% loss=0.24144, acc=0.90625
# [51/100] training 66.6% loss=0.11839, acc=0.96875
# [51/100] training 66.7% loss=0.10746, acc=0.96875
# [51/100] training 66.9% loss=0.21506, acc=0.93750
# [51/100] training 67.0% loss=0.14975, acc=0.93750
# [51/100] training 67.2% loss=0.11251, acc=0.96875
# [51/100] training 67.4% loss=0.19424, acc=0.93750
# [51/100] training 67.6% loss=0.11052, acc=0.95312
# [51/100] training 67.8% loss=0.12452, acc=0.96875
# [51/100] training 67.9% loss=0.06463, acc=1.00000
# [51/100] training 68.1% loss=0.12749, acc=0.96875
# [51/100] training 68.2% loss=0.16412, acc=0.93750
# [51/100] training 68.4% loss=0.19626, acc=0.96875
# [51/100] training 68.5% loss=0.26190, acc=0.92188
# [51/100] training 68.8% loss=0.05305, acc=1.00000
# [51/100] training 69.0% loss=0.29071, acc=0.90625
# [51/100] training 69.1% loss=0.18238, acc=0.95312
# [51/100] training 69.3% loss=0.13508, acc=0.93750
# [51/100] training 69.4% loss=0.11278, acc=0.95312
# [51/100] training 69.6% loss=0.07243, acc=0.96875
# [51/100] training 69.7% loss=0.21341, acc=0.90625
# [51/100] training 70.0% loss=0.19515, acc=0.92188
# [51/100] training 70.2% loss=0.26795, acc=0.90625
# [51/100] training 70.3% loss=0.18729, acc=0.89062
# [51/100] training 70.5% loss=0.20711, acc=0.93750
# [51/100] training 70.6% loss=0.13760, acc=0.96875
# [51/100] training 70.8% loss=0.26112, acc=0.89062
# [51/100] training 71.0% loss=0.26778, acc=0.92188
# [51/100] training 71.2% loss=0.11408, acc=0.93750
# [51/100] training 71.3% loss=0.07462, acc=0.98438
# [51/100] training 71.5% loss=0.23463, acc=0.90625
# [51/100] training 71.7% loss=0.20625, acc=0.90625
# [51/100] training 71.8% loss=0.17849, acc=0.93750
# [51/100] training 72.0% loss=0.14920, acc=0.95312
# [51/100] training 72.2% loss=0.13592, acc=0.92188
# [51/100] training 72.4% loss=0.21625, acc=0.90625
# [51/100] training 72.5% loss=0.22539, acc=0.89062
# [51/100] training 72.7% loss=0.21692, acc=0.95312
# [51/100] training 72.9% loss=0.13757, acc=0.93750
# [51/100] training 73.0% loss=0.13490, acc=0.95312
# [51/100] training 73.3% loss=0.27827, acc=0.85938
# [51/100] training 73.4% loss=0.14013, acc=0.93750
# [51/100] training 73.6% loss=0.20479, acc=0.92188
# [51/100] training 73.7% loss=0.11536, acc=0.93750
# [51/100] training 73.9% loss=0.08690, acc=0.98438
# [51/100] training 74.0% loss=0.25490, acc=0.90625
# [51/100] training 74.2% loss=0.07140, acc=0.98438
# [51/100] training 74.5% loss=0.09108, acc=0.98438
# [51/100] training 74.6% loss=0.31596, acc=0.89062
# [51/100] training 74.8% loss=0.32386, acc=0.90625
# [51/100] training 74.9% loss=0.18715, acc=0.93750
# [51/100] training 75.1% loss=0.13809, acc=0.93750
# [51/100] training 75.2% loss=0.15826, acc=0.92188
# [51/100] training 75.4% loss=0.13609, acc=0.95312
# [51/100] training 75.7% loss=0.24850, acc=0.84375
# [51/100] training 75.8% loss=0.26084, acc=0.89062
# [51/100] training 76.0% loss=0.18490, acc=0.92188
# [51/100] training 76.1% loss=0.13766, acc=0.95312
# [51/100] training 76.3% loss=0.11430, acc=0.96875
# [51/100] training 76.4% loss=0.26448, acc=0.90625
# [51/100] training 76.7% loss=0.16882, acc=0.93750
# [51/100] training 76.8% loss=0.09863, acc=0.96875
# [51/100] training 77.0% loss=0.08777, acc=0.95312
# [51/100] training 77.2% loss=0.13595, acc=0.95312
# [51/100] training 77.3% loss=0.11892, acc=0.96875
# [51/100] training 77.5% loss=0.25168, acc=0.90625
# [51/100] training 77.6% loss=0.25889, acc=0.92188
# [51/100] training 77.9% loss=0.21204, acc=0.89062
# [51/100] training 78.0% loss=0.17615, acc=0.90625
# [51/100] training 78.2% loss=0.23652, acc=0.92188
# [51/100] training 78.4% loss=0.09273, acc=0.98438
# [51/100] training 78.5% loss=0.27645, acc=0.92188
# [51/100] training 78.7% loss=0.33469, acc=0.90625
# [51/100] training 78.8% loss=0.06375, acc=1.00000
# [51/100] training 79.1% loss=0.11089, acc=0.96875
# [51/100] training 79.2% loss=0.16217, acc=0.93750
# [51/100] training 79.4% loss=0.15999, acc=0.96875
# [51/100] training 79.5% loss=0.14191, acc=0.92188
# [51/100] training 79.7% loss=0.08191, acc=0.96875
# [51/100] training 79.9% loss=0.11638, acc=0.93750
# [51/100] training 80.1% loss=0.04250, acc=0.98438
# [51/100] training 80.3% loss=0.16228, acc=0.92188
# [51/100] training 80.4% loss=0.23757, acc=0.92188
# [51/100] training 80.6% loss=0.26179, acc=0.85938
# [51/100] training 80.7% loss=0.11038, acc=0.95312
# [51/100] training 80.9% loss=0.16016, acc=0.92188
# [51/100] training 81.2% loss=0.18893, acc=0.93750
# [51/100] training 81.3% loss=0.17252, acc=0.93750
# [51/100] training 81.5% loss=0.21280, acc=0.87500
# [51/100] training 81.6% loss=0.33467, acc=0.87500
# [51/100] training 81.8% loss=0.26899, acc=0.89062
# [51/100] training 81.9% loss=0.28559, acc=0.93750
# [51/100] training 82.1% loss=0.13561, acc=0.95312
# [51/100] training 82.2% loss=0.19109, acc=0.92188
# [51/100] training 82.5% loss=0.15992, acc=0.93750
# [51/100] training 82.7% loss=0.24706, acc=0.92188
# [51/100] training 82.8% loss=0.16480, acc=0.95312
# [51/100] training 83.0% loss=0.18191, acc=0.93750
# [51/100] training 83.1% loss=0.14985, acc=0.95312
# [51/100] training 83.3% loss=0.11655, acc=0.96875
# [51/100] training 83.5% loss=0.09980, acc=0.98438
# [51/100] training 83.7% loss=0.28104, acc=0.89062
# [51/100] training 83.9% loss=0.19843, acc=0.95312
# [51/100] training 84.0% loss=0.10381, acc=0.95312
# [51/100] training 84.2% loss=0.06721, acc=0.96875
# [51/100] training 84.3% loss=0.17141, acc=0.92188
# [51/100] training 84.5% loss=0.11225, acc=0.93750
# [51/100] training 84.7% loss=0.06961, acc=0.96875
# [51/100] training 84.9% loss=0.16605, acc=0.92188
# [51/100] training 85.0% loss=0.22997, acc=0.90625
# [51/100] training 85.2% loss=0.17797, acc=0.95312
# [51/100] training 85.4% loss=0.22250, acc=0.95312
# [51/100] training 85.5% loss=0.28851, acc=0.90625
# [51/100] training 85.8% loss=0.21973, acc=0.90625
# [51/100] training 85.9% loss=0.21783, acc=0.87500
# [51/100] training 86.1% loss=0.15480, acc=0.93750
# [51/100] training 86.2% loss=0.11752, acc=0.95312
# [51/100] training 86.4% loss=0.21383, acc=0.95312
# [51/100] training 86.6% loss=0.14156, acc=0.95312
# [51/100] training 86.7% loss=0.13845, acc=0.95312
# [51/100] training 87.0% loss=0.22339, acc=0.95312
# [51/100] training 87.1% loss=0.10645, acc=0.96875
# [51/100] training 87.3% loss=0.14757, acc=0.92188
# [51/100] training 87.4% loss=0.11884, acc=0.95312
# [51/100] training 87.6% loss=0.14393, acc=0.96875
# [51/100] training 87.7% loss=0.29277, acc=0.93750
# [51/100] training 87.9% loss=0.20396, acc=0.87500
# [51/100] training 88.2% loss=0.10663, acc=0.95312
# [51/100] training 88.3% loss=0.21422, acc=0.93750
# [51/100] training 88.5% loss=0.21609, acc=0.90625
# [51/100] training 88.6% loss=0.11805, acc=0.96875
# [51/100] training 88.8% loss=0.10366, acc=0.96875
# [51/100] training 88.9% loss=0.20582, acc=0.90625
# [51/100] training 89.2% loss=0.16597, acc=0.95312
# [51/100] training 89.4% loss=0.14195, acc=0.93750
# [51/100] training 89.5% loss=0.11229, acc=0.93750
# [51/100] training 89.7% loss=0.24782, acc=0.89062
# [51/100] training 89.8% loss=0.19854, acc=0.95312
# [51/100] training 90.0% loss=0.10742, acc=0.96875
# [51/100] training 90.1% loss=0.19542, acc=0.90625
# [51/100] training 90.4% loss=0.28388, acc=0.84375
# [51/100] training 90.5% loss=0.17817, acc=0.92188
# [51/100] training 90.7% loss=0.18026, acc=0.92188
# [51/100] training 90.9% loss=0.14148, acc=0.95312
# [51/100] training 91.0% loss=0.14766, acc=0.93750
# [51/100] training 91.2% loss=0.24316, acc=0.89062
# [51/100] training 91.3% loss=0.38885, acc=0.87500
# [51/100] training 91.6% loss=0.21453, acc=0.87500
# [51/100] training 91.7% loss=0.11319, acc=0.93750
# [51/100] training 91.9% loss=0.24886, acc=0.89062
# [51/100] training 92.1% loss=0.17704, acc=0.92188
# [51/100] training 92.2% loss=0.18877, acc=0.92188
# [51/100] training 92.4% loss=0.11348, acc=0.95312
# [51/100] training 92.6% loss=0.19940, acc=0.92188
# [51/100] training 92.8% loss=0.19384, acc=0.92188
# [51/100] training 92.9% loss=0.18371, acc=0.92188
# [51/100] training 93.1% loss=0.51516, acc=0.89062
# [51/100] training 93.2% loss=0.24982, acc=0.92188
# [51/100] training 93.4% loss=0.28181, acc=0.87500
# [51/100] training 93.7% loss=0.10787, acc=0.93750
# [51/100] training 93.8% loss=0.14960, acc=0.95312
# [51/100] training 94.0% loss=0.12508, acc=0.96875
# [51/100] training 94.1% loss=0.19306, acc=0.90625
# [51/100] training 94.3% loss=0.15671, acc=0.92188
# [51/100] training 94.4% loss=0.15795, acc=0.93750
# [51/100] training 94.6% loss=0.10217, acc=0.96875
# [51/100] training 94.9% loss=0.11132, acc=0.95312
# [51/100] training 95.0% loss=0.21228, acc=0.92188
# [51/100] training 95.2% loss=0.36148, acc=0.87500
# [51/100] training 95.3% loss=0.20775, acc=0.93750
# [51/100] training 95.5% loss=0.22221, acc=0.93750
# [51/100] training 95.6% loss=0.13932, acc=0.95312
# [51/100] training 95.8% loss=0.10815, acc=0.92188
# [51/100] training 96.0% loss=0.18268, acc=0.93750
# [51/100] training 96.2% loss=0.11498, acc=0.93750
# [51/100] training 96.4% loss=0.11964, acc=0.96875
# [51/100] training 96.5% loss=0.18704, acc=0.92188
# [51/100] training 96.7% loss=0.14057, acc=0.96875
# [51/100] training 96.8% loss=0.21220, acc=0.93750
# [51/100] training 97.1% loss=0.14147, acc=0.90625
# [51/100] training 97.2% loss=0.25495, acc=0.92188
# [51/100] training 97.4% loss=0.25733, acc=0.89062
# [51/100] training 97.6% loss=0.20518, acc=0.89062
# [51/100] training 97.7% loss=0.15093, acc=0.90625
# [51/100] training 97.9% loss=0.13738, acc=0.95312
# [51/100] training 98.0% loss=0.21507, acc=0.90625
# [51/100] training 98.3% loss=0.26371, acc=0.89062
# [51/100] training 98.4% loss=0.20557, acc=0.89062
# [51/100] training 98.6% loss=0.34276, acc=0.87500
# [51/100] training 98.7% loss=0.26216, acc=0.93750
# [51/100] training 98.9% loss=0.19145, acc=0.89062
# [51/100] training 99.1% loss=0.22093, acc=0.92188
# [51/100] training 99.2% loss=0.13174, acc=0.95312
# [51/100] training 99.5% loss=0.24675, acc=0.90625
# [51/100] training 99.6% loss=0.14352, acc=0.93750
# [51/100] training 99.8% loss=0.07636, acc=0.96875
# [51/100] training 99.9% loss=0.02776, acc=1.00000
# [51/100] testing 0.9% loss=0.18539, acc=0.90625
# [51/100] testing 1.8% loss=0.34070, acc=0.85938
# [51/100] testing 2.2% loss=0.37262, acc=0.85938
# [51/100] testing 3.1% loss=0.40918, acc=0.85938
# [51/100] testing 3.5% loss=0.12675, acc=0.95312
# [51/100] testing 4.4% loss=0.20093, acc=0.90625
# [51/100] testing 4.8% loss=0.41978, acc=0.82812
# [51/100] testing 5.7% loss=0.13148, acc=0.95312
# [51/100] testing 6.6% loss=0.07398, acc=0.96875
# [51/100] testing 7.0% loss=0.09378, acc=0.96875
# [51/100] testing 7.9% loss=0.24088, acc=0.90625
# [51/100] testing 8.3% loss=0.35232, acc=0.89062
# [51/100] testing 9.2% loss=0.31170, acc=0.90625
# [51/100] testing 9.7% loss=0.10680, acc=0.93750
# [51/100] testing 10.5% loss=0.15628, acc=0.92188
# [51/100] testing 11.0% loss=0.37757, acc=0.85938
# [51/100] testing 11.8% loss=0.22500, acc=0.90625
# [51/100] testing 12.7% loss=0.37531, acc=0.92188
# [51/100] testing 13.2% loss=0.15582, acc=0.92188
# [51/100] testing 14.0% loss=0.52811, acc=0.90625
# [51/100] testing 14.5% loss=0.15315, acc=0.92188
# [51/100] testing 15.4% loss=0.31521, acc=0.92188
# [51/100] testing 15.8% loss=0.16072, acc=0.92188
# [51/100] testing 16.7% loss=0.33479, acc=0.87500
# [51/100] testing 17.5% loss=0.18058, acc=0.92188
# [51/100] testing 18.0% loss=0.24451, acc=0.92188
# [51/100] testing 18.9% loss=0.06984, acc=0.98438
# [51/100] testing 19.3% loss=0.30303, acc=0.92188
# [51/100] testing 20.2% loss=0.32557, acc=0.89062
# [51/100] testing 20.6% loss=0.28876, acc=0.90625
# [51/100] testing 21.5% loss=0.17304, acc=0.95312
# [51/100] testing 21.9% loss=0.48471, acc=0.89062
# [51/100] testing 22.8% loss=0.30383, acc=0.89062
# [51/100] testing 23.7% loss=0.40075, acc=0.87500
# [51/100] testing 24.1% loss=0.22634, acc=0.90625
# [51/100] testing 25.0% loss=0.24861, acc=0.92188
# [51/100] testing 25.4% loss=0.10641, acc=0.95312
# [51/100] testing 26.3% loss=0.36851, acc=0.87500
# [51/100] testing 26.8% loss=0.30324, acc=0.92188
# [51/100] testing 27.6% loss=0.18833, acc=0.93750
# [51/100] testing 28.5% loss=0.26403, acc=0.92188
# [51/100] testing 29.0% loss=0.23230, acc=0.92188
# [51/100] testing 29.8% loss=0.37901, acc=0.93750
# [51/100] testing 30.3% loss=0.25471, acc=0.92188
# [51/100] testing 31.1% loss=0.27118, acc=0.90625
# [51/100] testing 31.6% loss=0.21236, acc=0.96875
# [51/100] testing 32.5% loss=0.20976, acc=0.93750
# [51/100] testing 32.9% loss=0.43496, acc=0.89062
# [51/100] testing 33.8% loss=0.27816, acc=0.93750
# [51/100] testing 34.7% loss=0.33279, acc=0.89062
# [51/100] testing 35.1% loss=0.14931, acc=0.95312
# [51/100] testing 36.0% loss=0.31199, acc=0.90625
# [51/100] testing 36.4% loss=0.28358, acc=0.90625
# [51/100] testing 37.3% loss=0.25785, acc=0.93750
# [51/100] testing 37.7% loss=0.39059, acc=0.85938
# [51/100] testing 38.6% loss=0.19865, acc=0.95312
# [51/100] testing 39.5% loss=0.35118, acc=0.89062
# [51/100] testing 39.9% loss=0.20505, acc=0.92188
# [51/100] testing 40.8% loss=0.35535, acc=0.89062
# [51/100] testing 41.2% loss=0.27040, acc=0.95312
# [51/100] testing 42.1% loss=0.27489, acc=0.89062
# [51/100] testing 42.5% loss=0.18268, acc=0.92188
# [51/100] testing 43.4% loss=0.15394, acc=0.96875
# [51/100] testing 43.9% loss=0.07561, acc=0.96875
# [51/100] testing 44.7% loss=0.19814, acc=0.92188
# [51/100] testing 45.6% loss=0.24674, acc=0.90625
# [51/100] testing 46.1% loss=0.19471, acc=0.90625
# [51/100] testing 46.9% loss=0.22871, acc=0.87500
# [51/100] testing 47.4% loss=0.09215, acc=0.95312
# [51/100] testing 48.3% loss=0.30439, acc=0.85938
# [51/100] testing 48.7% loss=0.24887, acc=0.90625
# [51/100] testing 49.6% loss=0.43338, acc=0.84375
# [51/100] testing 50.4% loss=0.16866, acc=0.92188
# [51/100] testing 50.9% loss=0.30709, acc=0.90625
# [51/100] testing 51.8% loss=0.16559, acc=0.96875
# [51/100] testing 52.2% loss=0.29034, acc=0.90625
# [51/100] testing 53.1% loss=0.19970, acc=0.92188
# [51/100] testing 53.5% loss=0.14231, acc=0.96875
# [51/100] testing 54.4% loss=0.36055, acc=0.89062
# [51/100] testing 54.8% loss=0.26166, acc=0.90625
# [51/100] testing 55.7% loss=0.15993, acc=0.93750
# [51/100] testing 56.6% loss=0.31598, acc=0.85938
# [51/100] testing 57.0% loss=0.36202, acc=0.92188
# [51/100] testing 57.9% loss=0.21894, acc=0.92188
# [51/100] testing 58.3% loss=0.45686, acc=0.84375
# [51/100] testing 59.2% loss=0.23730, acc=0.90625
# [51/100] testing 59.7% loss=0.20374, acc=0.92188
# [51/100] testing 60.5% loss=0.31834, acc=0.90625
# [51/100] testing 61.4% loss=0.11582, acc=0.95312
# [51/100] testing 61.9% loss=0.15931, acc=0.90625
# [51/100] testing 62.7% loss=0.07773, acc=0.96875
# [51/100] testing 63.2% loss=0.46820, acc=0.87500
# [51/100] testing 64.0% loss=0.54966, acc=0.89062
# [51/100] testing 64.5% loss=0.20769, acc=0.85938
# [51/100] testing 65.4% loss=0.20362, acc=0.93750
# [51/100] testing 65.8% loss=0.36549, acc=0.87500
# [51/100] testing 66.7% loss=0.25019, acc=0.90625
# [51/100] testing 67.6% loss=0.39764, acc=0.90625
# [51/100] testing 68.0% loss=0.05183, acc=0.98438
# [51/100] testing 68.9% loss=0.30672, acc=0.93750
# [51/100] testing 69.3% loss=0.21155, acc=0.90625
# [51/100] testing 70.2% loss=0.39779, acc=0.84375
# [51/100] testing 70.6% loss=0.24012, acc=0.92188
# [51/100] testing 71.5% loss=0.34447, acc=0.89062
# [51/100] testing 72.4% loss=0.13429, acc=0.93750
# [51/100] testing 72.8% loss=0.12995, acc=0.93750
# [51/100] testing 73.7% loss=0.17248, acc=0.93750
# [51/100] testing 74.1% loss=0.46377, acc=0.87500
# [51/100] testing 75.0% loss=0.24179, acc=0.89062
# [51/100] testing 75.4% loss=0.42174, acc=0.84375
# [51/100] testing 76.3% loss=0.08888, acc=0.96875
# [51/100] testing 76.8% loss=0.38215, acc=0.84375
# [51/100] testing 77.6% loss=0.16251, acc=0.90625
# [51/100] testing 78.5% loss=0.30186, acc=0.87500
# [51/100] testing 79.0% loss=0.25339, acc=0.92188
# [51/100] testing 79.8% loss=0.32869, acc=0.85938
# [51/100] testing 80.3% loss=0.23252, acc=0.89062
# [51/100] testing 81.2% loss=0.54139, acc=0.89062
# [51/100] testing 81.6% loss=0.23932, acc=0.95312
# [51/100] testing 82.5% loss=0.13002, acc=0.92188
# [51/100] testing 83.3% loss=0.26215, acc=0.95312
# [51/100] testing 83.8% loss=0.18008, acc=0.95312
# [51/100] testing 84.7% loss=0.33341, acc=0.87500
# [51/100] testing 85.1% loss=0.21659, acc=0.89062
# [51/100] testing 86.0% loss=0.35018, acc=0.93750
# [51/100] testing 86.4% loss=0.34472, acc=0.87500
# [51/100] testing 87.3% loss=0.36671, acc=0.85938
# [51/100] testing 87.7% loss=0.17985, acc=0.89062
# [51/100] testing 88.6% loss=0.29928, acc=0.85938
# [51/100] testing 89.5% loss=0.60282, acc=0.82812
# [51/100] testing 89.9% loss=0.13203, acc=0.93750
# [51/100] testing 90.8% loss=0.38372, acc=0.93750
# [51/100] testing 91.2% loss=0.23213, acc=0.92188
# [51/100] testing 92.1% loss=0.27950, acc=0.93750
# [51/100] testing 92.6% loss=0.39104, acc=0.85938
# [51/100] testing 93.4% loss=0.40283, acc=0.82812
# [51/100] testing 94.3% loss=0.11707, acc=0.96875
# [51/100] testing 94.7% loss=0.17632, acc=0.95312
# [51/100] testing 95.6% loss=0.36936, acc=0.85938
# [51/100] testing 96.1% loss=0.16262, acc=0.95312
# [51/100] testing 96.9% loss=0.19616, acc=0.95312
# [51/100] testing 97.4% loss=0.14128, acc=0.95312
# [51/100] testing 98.3% loss=0.18586, acc=0.90625
# [51/100] testing 98.7% loss=0.22724, acc=0.93750
# [51/100] testing 99.6% loss=0.27682, acc=0.89062
# [52/100] training 0.2% loss=0.19551, acc=0.87500
# [52/100] training 0.4% loss=0.39307, acc=0.81250
# [52/100] training 0.5% loss=0.08747, acc=0.98438
# [52/100] training 0.8% loss=0.17504, acc=0.93750
# [52/100] training 0.9% loss=0.10891, acc=0.96875
# [52/100] training 1.1% loss=0.25306, acc=0.93750
# [52/100] training 1.2% loss=0.16407, acc=0.90625
# [52/100] training 1.4% loss=0.11250, acc=0.92188
# [52/100] training 1.6% loss=0.08383, acc=0.95312
# [52/100] training 1.8% loss=0.10658, acc=0.96875
# [52/100] training 2.0% loss=0.41387, acc=0.87500
# [52/100] training 2.1% loss=0.22948, acc=0.87500
# [52/100] training 2.3% loss=0.13741, acc=0.92188
# [52/100] training 2.4% loss=0.17753, acc=0.93750
# [52/100] training 2.6% loss=0.07410, acc=0.98438
# [52/100] training 2.7% loss=0.17179, acc=0.93750
# [52/100] training 3.0% loss=0.12182, acc=0.92188
# [52/100] training 3.2% loss=0.15996, acc=0.95312
# [52/100] training 3.3% loss=0.33098, acc=0.93750
# [52/100] training 3.5% loss=0.14371, acc=0.93750
# [52/100] training 3.6% loss=0.19606, acc=0.90625
# [52/100] training 3.8% loss=0.15300, acc=0.92188
# [52/100] training 3.9% loss=0.17111, acc=0.90625
# [52/100] training 4.2% loss=0.11702, acc=0.96875
# [52/100] training 4.4% loss=0.23447, acc=0.92188
# [52/100] training 4.5% loss=0.13637, acc=0.92188
# [52/100] training 4.7% loss=0.26460, acc=0.89062
# [52/100] training 4.8% loss=0.21100, acc=0.90625
# [52/100] training 5.0% loss=0.09928, acc=0.95312
# [52/100] training 5.2% loss=0.24930, acc=0.93750
# [52/100] training 5.4% loss=0.11948, acc=0.95312
# [52/100] training 5.5% loss=0.16854, acc=0.92188
# [52/100] training 5.7% loss=0.16524, acc=0.95312
# [52/100] training 5.9% loss=0.15681, acc=0.93750
# [52/100] training 6.0% loss=0.22003, acc=0.92188
# [52/100] training 6.3% loss=0.15171, acc=0.92188
# [52/100] training 6.4% loss=0.21086, acc=0.93750
# [52/100] training 6.6% loss=0.18759, acc=0.93750
# [52/100] training 6.7% loss=0.22231, acc=0.90625
# [52/100] training 6.9% loss=0.21053, acc=0.95312
# [52/100] training 7.1% loss=0.23370, acc=0.90625
# [52/100] training 7.2% loss=0.28228, acc=0.89062
# [52/100] training 7.5% loss=0.18708, acc=0.93750
# [52/100] training 7.6% loss=0.25116, acc=0.90625
# [52/100] training 7.8% loss=0.17569, acc=0.93750
# [52/100] training 7.9% loss=0.22150, acc=0.90625
# [52/100] training 8.1% loss=0.11880, acc=0.96875
# [52/100] training 8.2% loss=0.27566, acc=0.92188
# [52/100] training 8.4% loss=0.19596, acc=0.93750
# [52/100] training 8.7% loss=0.23771, acc=0.89062
# [52/100] training 8.8% loss=0.13629, acc=0.93750
# [52/100] training 9.0% loss=0.32459, acc=0.85938
# [52/100] training 9.1% loss=0.18542, acc=0.93750
# [52/100] training 9.3% loss=0.25112, acc=0.92188
# [52/100] training 9.4% loss=0.08053, acc=1.00000
# [52/100] training 9.7% loss=0.13620, acc=0.93750
# [52/100] training 9.9% loss=0.29514, acc=0.89062
# [52/100] training 10.0% loss=0.40720, acc=0.87500
# [52/100] training 10.2% loss=0.12979, acc=0.93750
# [52/100] training 10.3% loss=0.11251, acc=0.93750
# [52/100] training 10.5% loss=0.16900, acc=0.92188
# [52/100] training 10.6% loss=0.20537, acc=0.92188
# [52/100] training 10.9% loss=0.14138, acc=0.96875
# [52/100] training 11.0% loss=0.23582, acc=0.89062
# [52/100] training 11.2% loss=0.12586, acc=0.93750
# [52/100] training 11.4% loss=0.18905, acc=0.92188
# [52/100] training 11.5% loss=0.31937, acc=0.85938
# [52/100] training 11.7% loss=0.09344, acc=0.98438
# [52/100] training 11.8% loss=0.14379, acc=0.95312
# [52/100] training 12.1% loss=0.13328, acc=0.93750
# [52/100] training 12.2% loss=0.10829, acc=0.96875
# [52/100] training 12.4% loss=0.16641, acc=0.96875
# [52/100] training 12.6% loss=0.22653, acc=0.95312
# [52/100] training 12.7% loss=0.20334, acc=0.89062
# [52/100] training 12.9% loss=0.26523, acc=0.89062
# [52/100] training 13.0% loss=0.09316, acc=0.95312
# [52/100] training 13.3% loss=0.20856, acc=0.93750
# [52/100] training 13.4% loss=0.25917, acc=0.85938
# [52/100] training 13.6% loss=0.15036, acc=0.93750
# [52/100] training 13.7% loss=0.23392, acc=0.92188
# [52/100] training 13.9% loss=0.25242, acc=0.89062
# [52/100] training 14.1% loss=0.32272, acc=0.85938
# [52/100] training 14.3% loss=0.22110, acc=0.87500
# [52/100] training 14.5% loss=0.29558, acc=0.92188
# [52/100] training 14.6% loss=0.18205, acc=0.96875
# [52/100] training 14.8% loss=0.17345, acc=0.92188
# [52/100] training 14.9% loss=0.15572, acc=0.92188
# [52/100] training 15.1% loss=0.29276, acc=0.89062
# [52/100] training 15.4% loss=0.22524, acc=0.93750
# [52/100] training 15.5% loss=0.11928, acc=0.98438
# [52/100] training 15.7% loss=0.29918, acc=0.84375
# [52/100] training 15.8% loss=0.10568, acc=0.98438
# [52/100] training 16.0% loss=0.18682, acc=0.92188
# [52/100] training 16.1% loss=0.30558, acc=0.89062
# [52/100] training 16.3% loss=0.30169, acc=0.87500
# [52/100] training 16.4% loss=0.13171, acc=0.96875
# [52/100] training 16.7% loss=0.19607, acc=0.92188
# [52/100] training 16.9% loss=0.26099, acc=0.95312
# [52/100] training 17.0% loss=0.14703, acc=0.93750
# [52/100] training 17.2% loss=0.17820, acc=0.92188
# [52/100] training 17.3% loss=0.24478, acc=0.89062
# [52/100] training 17.5% loss=0.18454, acc=0.93750
# [52/100] training 17.7% loss=0.12213, acc=0.96875
# [52/100] training 17.9% loss=0.18423, acc=0.92188
# [52/100] training 18.1% loss=0.25561, acc=0.92188
# [52/100] training 18.2% loss=0.23496, acc=0.90625
# [52/100] training 18.4% loss=0.27531, acc=0.89062
# [52/100] training 18.5% loss=0.19654, acc=0.93750
# [52/100] training 18.8% loss=0.11306, acc=0.93750
# [52/100] training 18.9% loss=0.18997, acc=0.92188
# [52/100] training 19.1% loss=0.26153, acc=0.90625
# [52/100] training 19.2% loss=0.08196, acc=0.98438
# [52/100] training 19.4% loss=0.16995, acc=0.93750
# [52/100] training 19.6% loss=0.22460, acc=0.89062
# [52/100] training 19.7% loss=0.24758, acc=0.90625
# [52/100] training 20.0% loss=0.08011, acc=0.96875
# [52/100] training 20.1% loss=0.21280, acc=0.92188
# [52/100] training 20.3% loss=0.19473, acc=0.93750
# [52/100] training 20.4% loss=0.14289, acc=0.90625
# [52/100] training 20.6% loss=0.27188, acc=0.90625
# [52/100] training 20.8% loss=0.12136, acc=0.93750
# [52/100] training 20.9% loss=0.16059, acc=0.90625
# [52/100] training 21.2% loss=0.19863, acc=0.90625
# [52/100] training 21.3% loss=0.22244, acc=0.92188
# [52/100] training 21.5% loss=0.27136, acc=0.90625
# [52/100] training 21.6% loss=0.05776, acc=0.98438
# [52/100] training 21.8% loss=0.07915, acc=1.00000
# [52/100] training 21.9% loss=0.17469, acc=0.93750
# [52/100] training 22.2% loss=0.11744, acc=0.93750
# [52/100] training 22.4% loss=0.22214, acc=0.87500
# [52/100] training 22.5% loss=0.07932, acc=0.96875
# [52/100] training 22.7% loss=0.18257, acc=0.92188
# [52/100] training 22.8% loss=0.20792, acc=0.89062
# [52/100] training 23.0% loss=0.09569, acc=0.93750
# [52/100] training 23.1% loss=0.31363, acc=0.85938
# [52/100] training 23.4% loss=0.24406, acc=0.89062
# [52/100] training 23.6% loss=0.29568, acc=0.90625
# [52/100] training 23.7% loss=0.18527, acc=0.90625
# [52/100] training 23.9% loss=0.10059, acc=0.96875
# [52/100] training 24.0% loss=0.09632, acc=0.95312
# [52/100] training 24.2% loss=0.11644, acc=0.93750
# [52/100] training 24.3% loss=0.18205, acc=0.95312
# [52/100] training 24.6% loss=0.14928, acc=0.93750
# [52/100] training 24.7% loss=0.22716, acc=0.92188
# [52/100] training 24.9% loss=0.18645, acc=0.90625
# [52/100] training 25.1% loss=0.25068, acc=0.87500
# [52/100] training 25.2% loss=0.12765, acc=0.95312
# [52/100] training 25.4% loss=0.26319, acc=0.87500
# [52/100] training 25.6% loss=0.13177, acc=0.95312
# [52/100] training 25.8% loss=0.14009, acc=0.95312
# [52/100] training 25.9% loss=0.13728, acc=0.95312
# [52/100] training 26.1% loss=0.16854, acc=0.95312
# [52/100] training 26.3% loss=0.15371, acc=0.93750
# [52/100] training 26.4% loss=0.13521, acc=0.93750
# [52/100] training 26.6% loss=0.04378, acc=0.98438
# [52/100] training 26.8% loss=0.13693, acc=0.93750
# [52/100] training 27.0% loss=0.15398, acc=0.95312
# [52/100] training 27.1% loss=0.18163, acc=0.90625
# [52/100] training 27.3% loss=0.15669, acc=0.95312
# [52/100] training 27.4% loss=0.16225, acc=0.93750
# [52/100] training 27.6% loss=0.34136, acc=0.93750
# [52/100] training 27.9% loss=0.14828, acc=0.93750
# [52/100] training 28.0% loss=0.23043, acc=0.90625
# [52/100] training 28.2% loss=0.12525, acc=0.95312
# [52/100] training 28.3% loss=0.13019, acc=0.92188
# [52/100] training 28.5% loss=0.17472, acc=0.93750
# [52/100] training 28.6% loss=0.18654, acc=0.93750
# [52/100] training 28.8% loss=0.13194, acc=0.95312
# [52/100] training 29.1% loss=0.10604, acc=0.98438
# [52/100] training 29.2% loss=0.12914, acc=0.93750
# [52/100] training 29.4% loss=0.20452, acc=0.90625
# [52/100] training 29.5% loss=0.10989, acc=0.93750
# [52/100] training 29.7% loss=0.24078, acc=0.92188
# [52/100] training 29.8% loss=0.07051, acc=0.95312
# [52/100] training 30.0% loss=0.19588, acc=0.93750
# [52/100] training 30.2% loss=0.07872, acc=0.98438
# [52/100] training 30.4% loss=0.07419, acc=0.96875
# [52/100] training 30.6% loss=0.32043, acc=0.89062
# [52/100] training 30.7% loss=0.11797, acc=0.96875
# [52/100] training 30.9% loss=0.31884, acc=0.93750
# [52/100] training 31.0% loss=0.08159, acc=0.96875
# [52/100] training 31.3% loss=0.11642, acc=0.96875
# [52/100] training 31.4% loss=0.37455, acc=0.84375
# [52/100] training 31.6% loss=0.24114, acc=0.92188
# [52/100] training 31.8% loss=0.25102, acc=0.90625
# [52/100] training 31.9% loss=0.13121, acc=0.95312
# [52/100] training 32.1% loss=0.16721, acc=0.93750
# [52/100] training 32.2% loss=0.29157, acc=0.89062
# [52/100] training 32.5% loss=0.12699, acc=0.93750
# [52/100] training 32.6% loss=0.19161, acc=0.85938
# [52/100] training 32.8% loss=0.16105, acc=0.93750
# [52/100] training 32.9% loss=0.22352, acc=0.90625
# [52/100] training 33.1% loss=0.15915, acc=0.93750
# [52/100] training 33.3% loss=0.16886, acc=0.92188
# [52/100] training 33.4% loss=0.11047, acc=0.95312
# [52/100] training 33.7% loss=0.17293, acc=0.92188
# [52/100] training 33.8% loss=0.15387, acc=0.95312
# [52/100] training 34.0% loss=0.25203, acc=0.90625
# [52/100] training 34.1% loss=0.17854, acc=0.92188
# [52/100] training 34.3% loss=0.25313, acc=0.93750
# [52/100] training 34.5% loss=0.22567, acc=0.92188
# [52/100] training 34.7% loss=0.16999, acc=0.90625
# [52/100] training 34.9% loss=0.18023, acc=0.90625
# [52/100] training 35.0% loss=0.14674, acc=0.93750
# [52/100] training 35.2% loss=0.21105, acc=0.92188
# [52/100] training 35.3% loss=0.19281, acc=0.90625
# [52/100] training 35.5% loss=0.17287, acc=0.92188
# [52/100] training 35.6% loss=0.40713, acc=0.82812
# [52/100] training 35.9% loss=0.10991, acc=0.95312
# [52/100] training 36.1% loss=0.14912, acc=0.92188
# [52/100] training 36.2% loss=0.12336, acc=0.93750
# [52/100] training 36.4% loss=0.27526, acc=0.93750
# [52/100] training 36.5% loss=0.25325, acc=0.90625
# [52/100] training 36.7% loss=0.16216, acc=0.93750
# [52/100] training 36.8% loss=0.05807, acc=0.98438
# [52/100] training 37.1% loss=0.18514, acc=0.92188
# [52/100] training 37.3% loss=0.27768, acc=0.85938
# [52/100] training 37.4% loss=0.12326, acc=0.95312
# [52/100] training 37.6% loss=0.11753, acc=0.93750
# [52/100] training 37.7% loss=0.27159, acc=0.89062
# [52/100] training 37.9% loss=0.10711, acc=0.95312
# [52/100] training 38.1% loss=0.12348, acc=0.95312
# [52/100] training 38.3% loss=0.08814, acc=0.96875
# [52/100] training 38.4% loss=0.06270, acc=0.96875
# [52/100] training 38.6% loss=0.09643, acc=0.96875
# [52/100] training 38.8% loss=0.35209, acc=0.87500
# [52/100] training 38.9% loss=0.30728, acc=0.89062
# [52/100] training 39.1% loss=0.21155, acc=0.90625
# [52/100] training 39.3% loss=0.15364, acc=0.93750
# [52/100] training 39.5% loss=0.11686, acc=0.95312
# [52/100] training 39.6% loss=0.16542, acc=0.95312
# [52/100] training 39.8% loss=0.08395, acc=0.96875
# [52/100] training 40.0% loss=0.15053, acc=0.95312
# [52/100] training 40.1% loss=0.28200, acc=0.85938
# [52/100] training 40.4% loss=0.11524, acc=0.95312
# [52/100] training 40.5% loss=0.18276, acc=0.92188
# [52/100] training 40.7% loss=0.21358, acc=0.92188
# [52/100] training 40.8% loss=0.06514, acc=0.98438
# [52/100] training 41.0% loss=0.09231, acc=0.95312
# [52/100] training 41.1% loss=0.17872, acc=0.92188
# [52/100] training 41.3% loss=0.15040, acc=0.96875
# [52/100] training 41.6% loss=0.17352, acc=0.92188
# [52/100] training 41.7% loss=0.17438, acc=0.96875
# [52/100] training 41.9% loss=0.15326, acc=0.95312
# [52/100] training 42.0% loss=0.32440, acc=0.85938
# [52/100] training 42.2% loss=0.14781, acc=0.93750
# [52/100] training 42.3% loss=0.12411, acc=0.90625
# [52/100] training 42.5% loss=0.13520, acc=0.93750
# [52/100] training 42.8% loss=0.18351, acc=0.90625
# [52/100] training 42.9% loss=0.16958, acc=0.92188
# [52/100] training 43.1% loss=0.19758, acc=0.95312
# [52/100] training 43.2% loss=0.14408, acc=0.95312
# [52/100] training 43.4% loss=0.19846, acc=0.89062
# [52/100] training 43.5% loss=0.23036, acc=0.90625
# [52/100] training 43.8% loss=0.21012, acc=0.92188
# [52/100] training 43.9% loss=0.17713, acc=0.95312
# [52/100] training 44.1% loss=0.15154, acc=0.95312
# [52/100] training 44.3% loss=0.11571, acc=0.95312
# [52/100] training 44.4% loss=0.22761, acc=0.90625
# [52/100] training 44.6% loss=0.17654, acc=0.92188
# [52/100] training 44.7% loss=0.26570, acc=0.89062
# [52/100] training 45.0% loss=0.10357, acc=0.96875
# [52/100] training 45.1% loss=0.27424, acc=0.92188
# [52/100] training 45.3% loss=0.19445, acc=0.90625
# [52/100] training 45.5% loss=0.08212, acc=0.98438
# [52/100] training 45.6% loss=0.34769, acc=0.90625
# [52/100] training 45.8% loss=0.08859, acc=0.98438
# [52/100] training 45.9% loss=0.13680, acc=0.92188
# [52/100] training 46.2% loss=0.15019, acc=0.95312
# [52/100] training 46.3% loss=0.15857, acc=0.93750
# [52/100] training 46.5% loss=0.27906, acc=0.90625
# [52/100] training 46.6% loss=0.18367, acc=0.93750
# [52/100] training 46.8% loss=0.11321, acc=0.95312
# [52/100] training 47.0% loss=0.16287, acc=0.92188
# [52/100] training 47.2% loss=0.18320, acc=0.92188
# [52/100] training 47.4% loss=0.17874, acc=0.93750
# [52/100] training 47.5% loss=0.23266, acc=0.89062
# [52/100] training 47.7% loss=0.12344, acc=0.93750
# [52/100] training 47.8% loss=0.17577, acc=0.93750
# [52/100] training 48.0% loss=0.11073, acc=0.95312
# [52/100] training 48.3% loss=0.07343, acc=0.95312
# [52/100] training 48.4% loss=0.04027, acc=1.00000
# [52/100] training 48.6% loss=0.11264, acc=0.95312
# [52/100] training 48.7% loss=0.27183, acc=0.84375
# [52/100] training 48.9% loss=0.11316, acc=0.95312
# [52/100] training 49.0% loss=0.12495, acc=0.96875
# [52/100] training 49.2% loss=0.16392, acc=0.92188
# [52/100] training 49.3% loss=0.16737, acc=0.95312
# [52/100] training 49.6% loss=0.27737, acc=0.92188
# [52/100] training 49.8% loss=0.24234, acc=0.90625
# [52/100] training 49.9% loss=0.22001, acc=0.90625
# [52/100] training 50.1% loss=0.10747, acc=0.95312
# [52/100] training 50.2% loss=0.19764, acc=0.89062
# [52/100] training 50.4% loss=0.36529, acc=0.87500
# [52/100] training 50.6% loss=0.14773, acc=0.89062
# [52/100] training 50.8% loss=0.36349, acc=0.84375
# [52/100] training 51.0% loss=0.28137, acc=0.85938
# [52/100] training 51.1% loss=0.28160, acc=0.87500
# [52/100] training 51.3% loss=0.20678, acc=0.92188
# [52/100] training 51.4% loss=0.16819, acc=0.93750
# [52/100] training 51.7% loss=0.22224, acc=0.93750
# [52/100] training 51.8% loss=0.35150, acc=0.82812
# [52/100] training 52.0% loss=0.19011, acc=0.92188
# [52/100] training 52.1% loss=0.21241, acc=0.90625
# [52/100] training 52.3% loss=0.21202, acc=0.90625
# [52/100] training 52.5% loss=0.08193, acc=0.98438
# [52/100] training 52.6% loss=0.16703, acc=0.93750
# [52/100] training 52.9% loss=0.27985, acc=0.92188
# [52/100] training 53.0% loss=0.12867, acc=0.93750
# [52/100] training 53.2% loss=0.20749, acc=0.93750
# [52/100] training 53.3% loss=0.08640, acc=0.96875
# [52/100] training 53.5% loss=0.28786, acc=0.89062
# [52/100] training 53.7% loss=0.10302, acc=0.95312
# [52/100] training 53.8% loss=0.22253, acc=0.90625
# [52/100] training 54.1% loss=0.30164, acc=0.89062
# [52/100] training 54.2% loss=0.10650, acc=0.96875
# [52/100] training 54.4% loss=0.14277, acc=0.93750
# [52/100] training 54.5% loss=0.26565, acc=0.92188
# [52/100] training 54.7% loss=0.26398, acc=0.87500
# [52/100] training 54.8% loss=0.09556, acc=0.98438
# [52/100] training 55.1% loss=0.17297, acc=0.93750
# [52/100] training 55.3% loss=0.11310, acc=0.95312
# [52/100] training 55.4% loss=0.12756, acc=0.93750
# [52/100] training 55.6% loss=0.17315, acc=0.95312
# [52/100] training 55.7% loss=0.12382, acc=0.95312
# [52/100] training 55.9% loss=0.15402, acc=0.92188
# [52/100] training 56.0% loss=0.09583, acc=0.95312
# [52/100] training 56.3% loss=0.31660, acc=0.81250
# [52/100] training 56.5% loss=0.14438, acc=0.95312
# [52/100] training 56.6% loss=0.20990, acc=0.89062
# [52/100] training 56.8% loss=0.19841, acc=0.95312
# [52/100] training 56.9% loss=0.31357, acc=0.90625
# [52/100] training 57.1% loss=0.23352, acc=0.93750
# [52/100] training 57.2% loss=0.15780, acc=0.90625
# [52/100] training 57.5% loss=0.15818, acc=0.95312
# [52/100] training 57.6% loss=0.12878, acc=0.96875
# [52/100] training 57.8% loss=0.20016, acc=0.90625
# [52/100] training 58.0% loss=0.11989, acc=0.96875
# [52/100] training 58.1% loss=0.17628, acc=0.93750
# [52/100] training 58.3% loss=0.12841, acc=0.93750
# [52/100] training 58.4% loss=0.17710, acc=0.95312
# [52/100] training 58.7% loss=0.27774, acc=0.90625
# [52/100] training 58.8% loss=0.24064, acc=0.92188
# [52/100] training 59.0% loss=0.08828, acc=0.98438
# [52/100] training 59.2% loss=0.19779, acc=0.90625
# [52/100] training 59.3% loss=0.24030, acc=0.85938
# [52/100] training 59.5% loss=0.18394, acc=0.92188
# [52/100] training 59.7% loss=0.13374, acc=0.96875
# [52/100] training 59.9% loss=0.16383, acc=0.96875
# [52/100] training 60.0% loss=0.18507, acc=0.90625
# [52/100] training 60.2% loss=0.29862, acc=0.87500
# [52/100] training 60.3% loss=0.13055, acc=0.95312
# [52/100] training 60.5% loss=0.28363, acc=0.89062
# [52/100] training 60.8% loss=0.17367, acc=0.92188
# [52/100] training 60.9% loss=0.27690, acc=0.90625
# [52/100] training 61.1% loss=0.22233, acc=0.92188
# [52/100] training 61.2% loss=0.22632, acc=0.89062
# [52/100] training 61.4% loss=0.22950, acc=0.90625
# [52/100] training 61.5% loss=0.25694, acc=0.87500
# [52/100] training 61.7% loss=0.20472, acc=0.92188
# [52/100] training 62.0% loss=0.21209, acc=0.90625
# [52/100] training 62.1% loss=0.29029, acc=0.87500
# [52/100] training 62.3% loss=0.11158, acc=0.96875
# [52/100] training 62.4% loss=0.14383, acc=0.93750
# [52/100] training 62.6% loss=0.30188, acc=0.85938
# [52/100] training 62.7% loss=0.14437, acc=0.95312
# [52/100] training 62.9% loss=0.18354, acc=0.93750
# [52/100] training 63.1% loss=0.28269, acc=0.87500
# [52/100] training 63.3% loss=0.16992, acc=0.93750
# [52/100] training 63.5% loss=0.18506, acc=0.89062
# [52/100] training 63.6% loss=0.30328, acc=0.89062
# [52/100] training 63.8% loss=0.32326, acc=0.89062
# [52/100] training 63.9% loss=0.13013, acc=0.95312
# [52/100] training 64.2% loss=0.16796, acc=0.93750
# [52/100] training 64.3% loss=0.21001, acc=0.92188
# [52/100] training 64.5% loss=0.21681, acc=0.89062
# [52/100] training 64.7% loss=0.14272, acc=0.95312
# [52/100] training 64.8% loss=0.35837, acc=0.82812
# [52/100] training 65.0% loss=0.24957, acc=0.87500
# [52/100] training 65.1% loss=0.14921, acc=0.92188
# [52/100] training 65.4% loss=0.10272, acc=0.95312
# [52/100] training 65.5% loss=0.10868, acc=0.95312
# [52/100] training 65.7% loss=0.17614, acc=0.92188
# [52/100] training 65.8% loss=0.18118, acc=0.93750
# [52/100] training 66.0% loss=0.20465, acc=0.93750
# [52/100] training 66.2% loss=0.07958, acc=0.98438
# [52/100] training 66.3% loss=0.20871, acc=0.93750
# [52/100] training 66.6% loss=0.18289, acc=0.95312
# [52/100] training 66.7% loss=0.09570, acc=0.96875
# [52/100] training 66.9% loss=0.25220, acc=0.92188
# [52/100] training 67.0% loss=0.24712, acc=0.89062
# [52/100] training 67.2% loss=0.09251, acc=0.98438
# [52/100] training 67.4% loss=0.19534, acc=0.95312
# [52/100] training 67.6% loss=0.11921, acc=0.96875
# [52/100] training 67.8% loss=0.08687, acc=0.96875
# [52/100] training 67.9% loss=0.08851, acc=0.95312
# [52/100] training 68.1% loss=0.13668, acc=0.96875
# [52/100] training 68.2% loss=0.10352, acc=0.93750
# [52/100] training 68.4% loss=0.13383, acc=0.95312
# [52/100] training 68.5% loss=0.14951, acc=0.92188
# [52/100] training 68.8% loss=0.19248, acc=0.89062
# [52/100] training 69.0% loss=0.22961, acc=0.93750
# [52/100] training 69.1% loss=0.16576, acc=0.95312
# [52/100] training 69.3% loss=0.14395, acc=0.90625
# [52/100] training 69.4% loss=0.13675, acc=0.95312
# [52/100] training 69.6% loss=0.15386, acc=0.95312
# [52/100] training 69.7% loss=0.15379, acc=0.95312
# [52/100] training 70.0% loss=0.20422, acc=0.90625
# [52/100] training 70.2% loss=0.20384, acc=0.95312
# [52/100] training 70.3% loss=0.15922, acc=0.93750
# [52/100] training 70.5% loss=0.18052, acc=0.92188
# [52/100] training 70.6% loss=0.11294, acc=0.95312
# [52/100] training 70.8% loss=0.16789, acc=0.93750
# [52/100] training 71.0% loss=0.24119, acc=0.87500
# [52/100] training 71.2% loss=0.15340, acc=0.92188
# [52/100] training 71.3% loss=0.11890, acc=0.95312
# [52/100] training 71.5% loss=0.26582, acc=0.93750
# [52/100] training 71.7% loss=0.18627, acc=0.90625
# [52/100] training 71.8% loss=0.21942, acc=0.93750
# [52/100] training 72.0% loss=0.14192, acc=0.96875
# [52/100] training 72.2% loss=0.11372, acc=0.95312
# [52/100] training 72.4% loss=0.15537, acc=0.93750
# [52/100] training 72.5% loss=0.18556, acc=0.89062
# [52/100] training 72.7% loss=0.27633, acc=0.90625
# [52/100] training 72.9% loss=0.09812, acc=0.96875
# [52/100] training 73.0% loss=0.08954, acc=0.96875
# [52/100] training 73.3% loss=0.26888, acc=0.89062
# [52/100] training 73.4% loss=0.11779, acc=0.96875
# [52/100] training 73.6% loss=0.14411, acc=0.95312
# [52/100] training 73.7% loss=0.16143, acc=0.93750
# [52/100] training 73.9% loss=0.11978, acc=0.96875
# [52/100] training 74.0% loss=0.13249, acc=0.93750
# [52/100] training 74.2% loss=0.14575, acc=0.95312
# [52/100] training 74.5% loss=0.11229, acc=0.92188
# [52/100] training 74.6% loss=0.39521, acc=0.84375
# [52/100] training 74.8% loss=0.22255, acc=0.87500
# [52/100] training 74.9% loss=0.23506, acc=0.87500
# [52/100] training 75.1% loss=0.21073, acc=0.95312
# [52/100] training 75.2% loss=0.09347, acc=0.95312
# [52/100] training 75.4% loss=0.13877, acc=0.93750
# [52/100] training 75.7% loss=0.17532, acc=0.92188
# [52/100] training 75.8% loss=0.29968, acc=0.90625
# [52/100] training 76.0% loss=0.20988, acc=0.90625
# [52/100] training 76.1% loss=0.25978, acc=0.85938
# [52/100] training 76.3% loss=0.15996, acc=0.93750
# [52/100] training 76.4% loss=0.24170, acc=0.92188
# [52/100] training 76.7% loss=0.18375, acc=0.92188
# [52/100] training 76.8% loss=0.19030, acc=0.90625
# [52/100] training 77.0% loss=0.10343, acc=0.96875
# [52/100] training 77.2% loss=0.20853, acc=0.90625
# [52/100] training 77.3% loss=0.11877, acc=0.96875
# [52/100] training 77.5% loss=0.18681, acc=0.93750
# [52/100] training 77.6% loss=0.21654, acc=0.92188
# [52/100] training 77.9% loss=0.12201, acc=0.92188
# [52/100] training 78.0% loss=0.16309, acc=0.92188
# [52/100] training 78.2% loss=0.23722, acc=0.92188
# [52/100] training 78.4% loss=0.06679, acc=0.98438
# [52/100] training 78.5% loss=0.17671, acc=0.92188
# [52/100] training 78.7% loss=0.17192, acc=0.90625
# [52/100] training 78.8% loss=0.08145, acc=0.96875
# [52/100] training 79.1% loss=0.14327, acc=0.95312
# [52/100] training 79.2% loss=0.16232, acc=0.95312
# [52/100] training 79.4% loss=0.20918, acc=0.93750
# [52/100] training 79.5% loss=0.18830, acc=0.92188
# [52/100] training 79.7% loss=0.09139, acc=0.96875
# [52/100] training 79.9% loss=0.12776, acc=0.95312
# [52/100] training 80.1% loss=0.09531, acc=0.96875
# [52/100] training 80.3% loss=0.29462, acc=0.87500
# [52/100] training 80.4% loss=0.19030, acc=0.95312
# [52/100] training 80.6% loss=0.23934, acc=0.89062
# [52/100] training 80.7% loss=0.19350, acc=0.92188
# [52/100] training 80.9% loss=0.22944, acc=0.92188
# [52/100] training 81.2% loss=0.25652, acc=0.90625
# [52/100] training 81.3% loss=0.20546, acc=0.92188
# [52/100] training 81.5% loss=0.17694, acc=0.90625
# [52/100] training 81.6% loss=0.34093, acc=0.84375
# [52/100] training 81.8% loss=0.16826, acc=0.95312
# [52/100] training 81.9% loss=0.27830, acc=0.89062
# [52/100] training 82.1% loss=0.12611, acc=0.93750
# [52/100] training 82.2% loss=0.15448, acc=0.93750
# [52/100] training 82.5% loss=0.17030, acc=0.93750
# [52/100] training 82.7% loss=0.18175, acc=0.96875
# [52/100] training 82.8% loss=0.18858, acc=0.92188
# [52/100] training 83.0% loss=0.15251, acc=0.93750
# [52/100] training 83.1% loss=0.15126, acc=0.92188
# [52/100] training 83.3% loss=0.09560, acc=0.96875
# [52/100] training 83.5% loss=0.08104, acc=0.98438
# [52/100] training 83.7% loss=0.32881, acc=0.90625
# [52/100] training 83.9% loss=0.15933, acc=0.95312
# [52/100] training 84.0% loss=0.07989, acc=0.95312
# [52/100] training 84.2% loss=0.09161, acc=0.96875
# [52/100] training 84.3% loss=0.12112, acc=0.93750
# [52/100] training 84.5% loss=0.14561, acc=0.90625
# [52/100] training 84.7% loss=0.13650, acc=0.95312
# [52/100] training 84.9% loss=0.21768, acc=0.90625
# [52/100] training 85.0% loss=0.19564, acc=0.95312
# [52/100] training 85.2% loss=0.16217, acc=0.93750
# [52/100] training 85.4% loss=0.09222, acc=0.95312
# [52/100] training 85.5% loss=0.17899, acc=0.90625
# [52/100] training 85.8% loss=0.21298, acc=0.92188
# [52/100] training 85.9% loss=0.15112, acc=0.92188
# [52/100] training 86.1% loss=0.23739, acc=0.93750
# [52/100] training 86.2% loss=0.08152, acc=0.95312
# [52/100] training 86.4% loss=0.37494, acc=0.87500
# [52/100] training 86.6% loss=0.12037, acc=0.96875
# [52/100] training 86.7% loss=0.13348, acc=0.95312
# [52/100] training 87.0% loss=0.17898, acc=0.95312
# [52/100] training 87.1% loss=0.19421, acc=0.90625
# [52/100] training 87.3% loss=0.12985, acc=0.93750
# [52/100] training 87.4% loss=0.13602, acc=0.93750
# [52/100] training 87.6% loss=0.17201, acc=0.90625
# [52/100] training 87.7% loss=0.29745, acc=0.90625
# [52/100] training 87.9% loss=0.15648, acc=0.92188
# [52/100] training 88.2% loss=0.21728, acc=0.93750
# [52/100] training 88.3% loss=0.26547, acc=0.90625
# [52/100] training 88.5% loss=0.14106, acc=0.95312
# [52/100] training 88.6% loss=0.12716, acc=0.93750
# [52/100] training 88.8% loss=0.16729, acc=0.92188
# [52/100] training 88.9% loss=0.28985, acc=0.85938
# [52/100] training 89.2% loss=0.16707, acc=0.93750
# [52/100] training 89.4% loss=0.22648, acc=0.89062
# [52/100] training 89.5% loss=0.25912, acc=0.87500
# [52/100] training 89.7% loss=0.25255, acc=0.90625
# [52/100] training 89.8% loss=0.14101, acc=0.93750
# [52/100] training 90.0% loss=0.14856, acc=0.93750
# [52/100] training 90.1% loss=0.19111, acc=0.90625
# [52/100] training 90.4% loss=0.15203, acc=0.95312
# [52/100] training 90.5% loss=0.15702, acc=0.95312
# [52/100] training 90.7% loss=0.19010, acc=0.89062
# [52/100] training 90.9% loss=0.18575, acc=0.93750
# [52/100] training 91.0% loss=0.20345, acc=0.93750
# [52/100] training 91.2% loss=0.15392, acc=0.93750
# [52/100] training 91.3% loss=0.23899, acc=0.90625
# [52/100] training 91.6% loss=0.23294, acc=0.93750
# [52/100] training 91.7% loss=0.16067, acc=0.93750
# [52/100] training 91.9% loss=0.33078, acc=0.84375
# [52/100] training 92.1% loss=0.12053, acc=0.95312
# [52/100] training 92.2% loss=0.12390, acc=0.93750
# [52/100] training 92.4% loss=0.17213, acc=0.92188
# [52/100] training 92.6% loss=0.33292, acc=0.84375
# [52/100] training 92.8% loss=0.22462, acc=0.95312
# [52/100] training 92.9% loss=0.17054, acc=0.93750
# [52/100] training 93.1% loss=0.38390, acc=0.89062
# [52/100] training 93.2% loss=0.24452, acc=0.95312
# [52/100] training 93.4% loss=0.22848, acc=0.89062
# [52/100] training 93.7% loss=0.14009, acc=0.93750
# [52/100] training 93.8% loss=0.18483, acc=0.93750
# [52/100] training 94.0% loss=0.15495, acc=0.95312
# [52/100] training 94.1% loss=0.18539, acc=0.95312
# [52/100] training 94.3% loss=0.09565, acc=0.98438
# [52/100] training 94.4% loss=0.15255, acc=0.95312
# [52/100] training 94.6% loss=0.06475, acc=0.98438
# [52/100] training 94.9% loss=0.18449, acc=0.92188
# [52/100] training 95.0% loss=0.24996, acc=0.90625
# [52/100] training 95.2% loss=0.47268, acc=0.82812
# [52/100] training 95.3% loss=0.19665, acc=0.90625
# [52/100] training 95.5% loss=0.09514, acc=0.98438
# [52/100] training 95.6% loss=0.12865, acc=0.96875
# [52/100] training 95.8% loss=0.21319, acc=0.87500
# [52/100] training 96.0% loss=0.17533, acc=0.93750
# [52/100] training 96.2% loss=0.10971, acc=0.96875
# [52/100] training 96.4% loss=0.14546, acc=0.95312
# [52/100] training 96.5% loss=0.16864, acc=0.93750
# [52/100] training 96.7% loss=0.07484, acc=0.96875
# [52/100] training 96.8% loss=0.25346, acc=0.93750
# [52/100] training 97.1% loss=0.23051, acc=0.93750
# [52/100] training 97.2% loss=0.12618, acc=0.93750
# [52/100] training 97.4% loss=0.23251, acc=0.92188
# [52/100] training 97.6% loss=0.22355, acc=0.90625
# [52/100] training 97.7% loss=0.14432, acc=0.93750
# [52/100] training 97.9% loss=0.15389, acc=0.93750
# [52/100] training 98.0% loss=0.12764, acc=0.93750
# [52/100] training 98.3% loss=0.25920, acc=0.85938
# [52/100] training 98.4% loss=0.28500, acc=0.90625
# [52/100] training 98.6% loss=0.26916, acc=0.82812
# [52/100] training 98.7% loss=0.24532, acc=0.90625
# [52/100] training 98.9% loss=0.16498, acc=0.93750
# [52/100] training 99.1% loss=0.14346, acc=0.93750
# [52/100] training 99.2% loss=0.12120, acc=0.95312
# [52/100] training 99.5% loss=0.21215, acc=0.90625
# [52/100] training 99.6% loss=0.18631, acc=0.92188
# [52/100] training 99.8% loss=0.14042, acc=0.92188
# [52/100] training 99.9% loss=0.04937, acc=0.98438
# [52/100] testing 0.9% loss=0.17969, acc=0.92188
# [52/100] testing 1.8% loss=0.38283, acc=0.85938
# [52/100] testing 2.2% loss=0.32567, acc=0.85938
# [52/100] testing 3.1% loss=0.46609, acc=0.84375
# [52/100] testing 3.5% loss=0.14279, acc=0.90625
# [52/100] testing 4.4% loss=0.15550, acc=0.92188
# [52/100] testing 4.8% loss=0.38447, acc=0.84375
# [52/100] testing 5.7% loss=0.25452, acc=0.85938
# [52/100] testing 6.6% loss=0.11134, acc=0.95312
# [52/100] testing 7.0% loss=0.14213, acc=0.92188
# [52/100] testing 7.9% loss=0.26748, acc=0.85938
# [52/100] testing 8.3% loss=0.39989, acc=0.87500
# [52/100] testing 9.2% loss=0.24438, acc=0.92188
# [52/100] testing 9.7% loss=0.09216, acc=0.95312
# [52/100] testing 10.5% loss=0.21901, acc=0.90625
# [52/100] testing 11.0% loss=0.21492, acc=0.87500
# [52/100] testing 11.8% loss=0.14023, acc=0.95312
# [52/100] testing 12.7% loss=0.36787, acc=0.85938
# [52/100] testing 13.2% loss=0.24398, acc=0.90625
# [52/100] testing 14.0% loss=0.41827, acc=0.85938
# [52/100] testing 14.5% loss=0.24663, acc=0.92188
# [52/100] testing 15.4% loss=0.29234, acc=0.89062
# [52/100] testing 15.8% loss=0.23870, acc=0.93750
# [52/100] testing 16.7% loss=0.26807, acc=0.89062
# [52/100] testing 17.5% loss=0.22208, acc=0.89062
# [52/100] testing 18.0% loss=0.21710, acc=0.90625
# [52/100] testing 18.9% loss=0.13742, acc=0.93750
# [52/100] testing 19.3% loss=0.46168, acc=0.82812
# [52/100] testing 20.2% loss=0.35666, acc=0.89062
# [52/100] testing 20.6% loss=0.24835, acc=0.93750
# [52/100] testing 21.5% loss=0.18602, acc=0.90625
# [52/100] testing 21.9% loss=0.42301, acc=0.89062
# [52/100] testing 22.8% loss=0.22107, acc=0.95312
# [52/100] testing 23.7% loss=0.56589, acc=0.85938
# [52/100] testing 24.1% loss=0.14975, acc=0.92188
# [52/100] testing 25.0% loss=0.26358, acc=0.92188
# [52/100] testing 25.4% loss=0.14600, acc=0.92188
# [52/100] testing 26.3% loss=0.26243, acc=0.90625
# [52/100] testing 26.8% loss=0.30626, acc=0.92188
# [52/100] testing 27.6% loss=0.19667, acc=0.92188
# [52/100] testing 28.5% loss=0.21761, acc=0.96875
# [52/100] testing 29.0% loss=0.18882, acc=0.92188
# [52/100] testing 29.8% loss=0.35103, acc=0.95312
# [52/100] testing 30.3% loss=0.32093, acc=0.93750
# [52/100] testing 31.1% loss=0.30886, acc=0.89062
# [52/100] testing 31.6% loss=0.27617, acc=0.90625
# [52/100] testing 32.5% loss=0.25242, acc=0.92188
# [52/100] testing 32.9% loss=0.39661, acc=0.87500
# [52/100] testing 33.8% loss=0.35206, acc=0.85938
# [52/100] testing 34.7% loss=0.39046, acc=0.87500
# [52/100] testing 35.1% loss=0.14301, acc=0.92188
# [52/100] testing 36.0% loss=0.30446, acc=0.92188
# [52/100] testing 36.4% loss=0.30968, acc=0.89062
# [52/100] testing 37.3% loss=0.26567, acc=0.95312
# [52/100] testing 37.7% loss=0.44370, acc=0.82812
# [52/100] testing 38.6% loss=0.24666, acc=0.93750
# [52/100] testing 39.5% loss=0.23748, acc=0.95312
# [52/100] testing 39.9% loss=0.26300, acc=0.92188
# [52/100] testing 40.8% loss=0.28875, acc=0.93750
# [52/100] testing 41.2% loss=0.22987, acc=0.95312
# [52/100] testing 42.1% loss=0.35344, acc=0.90625
# [52/100] testing 42.5% loss=0.18826, acc=0.93750
# [52/100] testing 43.4% loss=0.20587, acc=0.90625
# [52/100] testing 43.9% loss=0.08228, acc=0.98438
# [52/100] testing 44.7% loss=0.23605, acc=0.90625
# [52/100] testing 45.6% loss=0.22042, acc=0.90625
# [52/100] testing 46.1% loss=0.21822, acc=0.89062
# [52/100] testing 46.9% loss=0.27591, acc=0.85938
# [52/100] testing 47.4% loss=0.06985, acc=0.98438
# [52/100] testing 48.3% loss=0.36521, acc=0.87500
# [52/100] testing 48.7% loss=0.30319, acc=0.90625
# [52/100] testing 49.6% loss=0.41873, acc=0.85938
# [52/100] testing 50.4% loss=0.20460, acc=0.92188
# [52/100] testing 50.9% loss=0.40991, acc=0.90625
# [52/100] testing 51.8% loss=0.24388, acc=0.92188
# [52/100] testing 52.2% loss=0.21390, acc=0.93750
# [52/100] testing 53.1% loss=0.18912, acc=0.92188
# [52/100] testing 53.5% loss=0.34901, acc=0.90625
# [52/100] testing 54.4% loss=0.32805, acc=0.82812
# [52/100] testing 54.8% loss=0.33127, acc=0.85938
# [52/100] testing 55.7% loss=0.13410, acc=0.95312
# [52/100] testing 56.6% loss=0.30857, acc=0.87500
# [52/100] testing 57.0% loss=0.36228, acc=0.90625
# [52/100] testing 57.9% loss=0.23020, acc=0.90625
# [52/100] testing 58.3% loss=0.32146, acc=0.87500
# [52/100] testing 59.2% loss=0.19648, acc=0.89062
# [52/100] testing 59.7% loss=0.24304, acc=0.90625
# [52/100] testing 60.5% loss=0.38968, acc=0.87500
# [52/100] testing 61.4% loss=0.12180, acc=0.95312
# [52/100] testing 61.9% loss=0.19892, acc=0.92188
# [52/100] testing 62.7% loss=0.15426, acc=0.95312
# [52/100] testing 63.2% loss=0.40779, acc=0.85938
# [52/100] testing 64.0% loss=0.49121, acc=0.89062
# [52/100] testing 64.5% loss=0.18449, acc=0.92188
# [52/100] testing 65.4% loss=0.08292, acc=0.95312
# [52/100] testing 65.8% loss=0.27437, acc=0.92188
# [52/100] testing 66.7% loss=0.20496, acc=0.92188
# [52/100] testing 67.6% loss=0.47890, acc=0.90625
# [52/100] testing 68.0% loss=0.05637, acc=0.98438
# [52/100] testing 68.9% loss=0.26524, acc=0.95312
# [52/100] testing 69.3% loss=0.26785, acc=0.84375
# [52/100] testing 70.2% loss=0.42934, acc=0.84375
# [52/100] testing 70.6% loss=0.27020, acc=0.92188
# [52/100] testing 71.5% loss=0.22190, acc=0.92188
# [52/100] testing 72.4% loss=0.13067, acc=0.95312
# [52/100] testing 72.8% loss=0.16045, acc=0.90625
# [52/100] testing 73.7% loss=0.22069, acc=0.90625
# [52/100] testing 74.1% loss=0.38797, acc=0.87500
# [52/100] testing 75.0% loss=0.33007, acc=0.90625
# [52/100] testing 75.4% loss=0.41114, acc=0.87500
# [52/100] testing 76.3% loss=0.13989, acc=0.96875
# [52/100] testing 76.8% loss=0.36145, acc=0.84375
# [52/100] testing 77.6% loss=0.13272, acc=0.89062
# [52/100] testing 78.5% loss=0.33015, acc=0.85938
# [52/100] testing 79.0% loss=0.28607, acc=0.90625
# [52/100] testing 79.8% loss=0.29244, acc=0.84375
# [52/100] testing 80.3% loss=0.24027, acc=0.93750
# [52/100] testing 81.2% loss=0.49247, acc=0.84375
# [52/100] testing 81.6% loss=0.15148, acc=0.95312
# [52/100] testing 82.5% loss=0.19510, acc=0.92188
# [52/100] testing 83.3% loss=0.15993, acc=0.95312
# [52/100] testing 83.8% loss=0.18627, acc=0.93750
# [52/100] testing 84.7% loss=0.34527, acc=0.87500
# [52/100] testing 85.1% loss=0.16420, acc=0.92188
# [52/100] testing 86.0% loss=0.35145, acc=0.87500
# [52/100] testing 86.4% loss=0.30957, acc=0.89062
# [52/100] testing 87.3% loss=0.20489, acc=0.92188
# [52/100] testing 87.7% loss=0.19472, acc=0.90625
# [52/100] testing 88.6% loss=0.23202, acc=0.85938
# [52/100] testing 89.5% loss=0.60604, acc=0.79688
# [52/100] testing 89.9% loss=0.14133, acc=0.93750
# [52/100] testing 90.8% loss=0.35477, acc=0.93750
# [52/100] testing 91.2% loss=0.16120, acc=0.95312
# [52/100] testing 92.1% loss=0.27115, acc=0.92188
# [52/100] testing 92.6% loss=0.40916, acc=0.89062
# [52/100] testing 93.4% loss=0.28621, acc=0.85938
# [52/100] testing 94.3% loss=0.13596, acc=0.95312
# [52/100] testing 94.7% loss=0.16746, acc=0.95312
# [52/100] testing 95.6% loss=0.35437, acc=0.87500
# [52/100] testing 96.1% loss=0.11037, acc=0.96875
# [52/100] testing 96.9% loss=0.31703, acc=0.89062
# [52/100] testing 97.4% loss=0.12226, acc=0.96875
# [52/100] testing 98.3% loss=0.15720, acc=0.93750
# [52/100] testing 98.7% loss=0.20727, acc=0.92188
# [52/100] testing 99.6% loss=0.39068, acc=0.87500
# [53/100] training 0.2% loss=0.18457, acc=0.92188
# [53/100] training 0.4% loss=0.36890, acc=0.89062
# [53/100] training 0.5% loss=0.12488, acc=0.96875
# [53/100] training 0.8% loss=0.10160, acc=0.93750
# [53/100] training 0.9% loss=0.15506, acc=0.95312
# [53/100] training 1.1% loss=0.28882, acc=0.95312
# [53/100] training 1.2% loss=0.19615, acc=0.92188
# [53/100] training 1.4% loss=0.13275, acc=0.96875
# [53/100] training 1.6% loss=0.09706, acc=0.98438
# [53/100] training 1.8% loss=0.15406, acc=0.92188
# [53/100] training 2.0% loss=0.21595, acc=0.87500
# [53/100] training 2.1% loss=0.20409, acc=0.89062
# [53/100] training 2.3% loss=0.12397, acc=0.95312
# [53/100] training 2.4% loss=0.19494, acc=0.93750
# [53/100] training 2.6% loss=0.07754, acc=0.96875
# [53/100] training 2.7% loss=0.17635, acc=0.92188
# [53/100] training 3.0% loss=0.21449, acc=0.90625
# [53/100] training 3.2% loss=0.08801, acc=0.96875
# [53/100] training 3.3% loss=0.33395, acc=0.89062
# [53/100] training 3.5% loss=0.14376, acc=0.95312
# [53/100] training 3.6% loss=0.25012, acc=0.89062
# [53/100] training 3.8% loss=0.24029, acc=0.92188
# [53/100] training 3.9% loss=0.12191, acc=0.93750
# [53/100] training 4.2% loss=0.12049, acc=0.93750
# [53/100] training 4.4% loss=0.10706, acc=0.96875
# [53/100] training 4.5% loss=0.08131, acc=1.00000
# [53/100] training 4.7% loss=0.39457, acc=0.85938
# [53/100] training 4.8% loss=0.16314, acc=0.90625
# [53/100] training 5.0% loss=0.06859, acc=0.95312
# [53/100] training 5.2% loss=0.22164, acc=0.93750
# [53/100] training 5.4% loss=0.06800, acc=0.96875
# [53/100] training 5.5% loss=0.20396, acc=0.92188
# [53/100] training 5.7% loss=0.15425, acc=0.93750
# [53/100] training 5.9% loss=0.16442, acc=0.95312
# [53/100] training 6.0% loss=0.23702, acc=0.92188
# [53/100] training 6.3% loss=0.24419, acc=0.87500
# [53/100] training 6.4% loss=0.13369, acc=0.95312
# [53/100] training 6.6% loss=0.13200, acc=0.92188
# [53/100] training 6.7% loss=0.21428, acc=0.89062
# [53/100] training 6.9% loss=0.25684, acc=0.87500
# [53/100] training 7.1% loss=0.13598, acc=0.96875
# [53/100] training 7.2% loss=0.23070, acc=0.90625
# [53/100] training 7.5% loss=0.18961, acc=0.92188
# [53/100] training 7.6% loss=0.19898, acc=0.93750
# [53/100] training 7.8% loss=0.15513, acc=0.95312
# [53/100] training 7.9% loss=0.17320, acc=0.92188
# [53/100] training 8.1% loss=0.09396, acc=0.96875
# [53/100] training 8.2% loss=0.21613, acc=0.95312
# [53/100] training 8.4% loss=0.22448, acc=0.90625
# [53/100] training 8.7% loss=0.17013, acc=0.95312
# [53/100] training 8.8% loss=0.24452, acc=0.90625
# [53/100] training 9.0% loss=0.12625, acc=0.93750
# [53/100] training 9.1% loss=0.12198, acc=0.96875
# [53/100] training 9.3% loss=0.27326, acc=0.87500
# [53/100] training 9.4% loss=0.10855, acc=0.93750
# [53/100] training 9.7% loss=0.19867, acc=0.92188
# [53/100] training 9.9% loss=0.25351, acc=0.85938
# [53/100] training 10.0% loss=0.23122, acc=0.90625
# [53/100] training 10.2% loss=0.09245, acc=0.95312
# [53/100] training 10.3% loss=0.17262, acc=0.92188
# [53/100] training 10.5% loss=0.19223, acc=0.90625
# [53/100] training 10.6% loss=0.22791, acc=0.90625
# [53/100] training 10.9% loss=0.07699, acc=0.96875
# [53/100] training 11.0% loss=0.20595, acc=0.92188
# [53/100] training 11.2% loss=0.11846, acc=0.95312
# [53/100] training 11.4% loss=0.18680, acc=0.92188
# [53/100] training 11.5% loss=0.22255, acc=0.89062
# [53/100] training 11.7% loss=0.08218, acc=0.98438
# [53/100] training 11.8% loss=0.14380, acc=0.95312
# [53/100] training 12.1% loss=0.10510, acc=0.95312
# [53/100] training 12.2% loss=0.06450, acc=0.98438
# [53/100] training 12.4% loss=0.13404, acc=0.92188
# [53/100] training 12.6% loss=0.31078, acc=0.90625
# [53/100] training 12.7% loss=0.18004, acc=0.93750
# [53/100] training 12.9% loss=0.21207, acc=0.92188
# [53/100] training 13.0% loss=0.11197, acc=0.96875
# [53/100] training 13.3% loss=0.23860, acc=0.89062
# [53/100] training 13.4% loss=0.23958, acc=0.95312
# [53/100] training 13.6% loss=0.19489, acc=0.93750
# [53/100] training 13.7% loss=0.20573, acc=0.92188
# [53/100] training 13.9% loss=0.19965, acc=0.89062
# [53/100] training 14.1% loss=0.16211, acc=0.96875
# [53/100] training 14.3% loss=0.22860, acc=0.89062
# [53/100] training 14.5% loss=0.25183, acc=0.93750
# [53/100] training 14.6% loss=0.29561, acc=0.90625
# [53/100] training 14.8% loss=0.24177, acc=0.89062
# [53/100] training 14.9% loss=0.14904, acc=0.96875
# [53/100] training 15.1% loss=0.29988, acc=0.87500
# [53/100] training 15.4% loss=0.20835, acc=0.90625
# [53/100] training 15.5% loss=0.17694, acc=0.95312
# [53/100] training 15.7% loss=0.25670, acc=0.90625
# [53/100] training 15.8% loss=0.15971, acc=0.92188
# [53/100] training 16.0% loss=0.21646, acc=0.95312
# [53/100] training 16.1% loss=0.20906, acc=0.93750
# [53/100] training 16.3% loss=0.11129, acc=0.93750
# [53/100] training 16.4% loss=0.12098, acc=0.92188
# [53/100] training 16.7% loss=0.32625, acc=0.89062
# [53/100] training 16.9% loss=0.16088, acc=0.92188
# [53/100] training 17.0% loss=0.12467, acc=0.96875
# [53/100] training 17.2% loss=0.15016, acc=0.92188
# [53/100] training 17.3% loss=0.20725, acc=0.90625
# [53/100] training 17.5% loss=0.15748, acc=0.93750
# [53/100] training 17.7% loss=0.09524, acc=0.96875
# [53/100] training 17.9% loss=0.19308, acc=0.95312
# [53/100] training 18.1% loss=0.21262, acc=0.93750
# [53/100] training 18.2% loss=0.17953, acc=0.95312
# [53/100] training 18.4% loss=0.24818, acc=0.89062
# [53/100] training 18.5% loss=0.16145, acc=0.93750
# [53/100] training 18.8% loss=0.14755, acc=0.93750
# [53/100] training 18.9% loss=0.10848, acc=0.93750
# [53/100] training 19.1% loss=0.28417, acc=0.90625
# [53/100] training 19.2% loss=0.11254, acc=0.96875
# [53/100] training 19.4% loss=0.10552, acc=0.95312
# [53/100] training 19.6% loss=0.21288, acc=0.90625
# [53/100] training 19.7% loss=0.24375, acc=0.93750
# [53/100] training 20.0% loss=0.14190, acc=0.93750
# [53/100] training 20.1% loss=0.19237, acc=0.92188
# [53/100] training 20.3% loss=0.14809, acc=0.93750
# [53/100] training 20.4% loss=0.31616, acc=0.92188
# [53/100] training 20.6% loss=0.28408, acc=0.89062
# [53/100] training 20.8% loss=0.16414, acc=0.92188
# [53/100] training 20.9% loss=0.18719, acc=0.92188
# [53/100] training 21.2% loss=0.25431, acc=0.90625
# [53/100] training 21.3% loss=0.31998, acc=0.87500
# [53/100] training 21.5% loss=0.21954, acc=0.93750
# [53/100] training 21.6% loss=0.09997, acc=0.96875
# [53/100] training 21.8% loss=0.10979, acc=0.96875
# [53/100] training 21.9% loss=0.20936, acc=0.95312
# [53/100] training 22.2% loss=0.14977, acc=0.95312
# [53/100] training 22.4% loss=0.24709, acc=0.87500
# [53/100] training 22.5% loss=0.09135, acc=0.98438
# [53/100] training 22.7% loss=0.18552, acc=0.92188
# [53/100] training 22.8% loss=0.14953, acc=0.93750
# [53/100] training 23.0% loss=0.09240, acc=0.95312
# [53/100] training 23.1% loss=0.27259, acc=0.90625
# [53/100] training 23.4% loss=0.20638, acc=0.92188
# [53/100] training 23.6% loss=0.17406, acc=0.93750
# [53/100] training 23.7% loss=0.14445, acc=0.95312
# [53/100] training 23.9% loss=0.08062, acc=0.98438
# [53/100] training 24.0% loss=0.12669, acc=0.95312
# [53/100] training 24.2% loss=0.09556, acc=0.96875
# [53/100] training 24.3% loss=0.17164, acc=0.95312
# [53/100] training 24.6% loss=0.21739, acc=0.93750
# [53/100] training 24.7% loss=0.24871, acc=0.93750
# [53/100] training 24.9% loss=0.14077, acc=0.93750
# [53/100] training 25.1% loss=0.20246, acc=0.90625
# [53/100] training 25.2% loss=0.17461, acc=0.92188
# [53/100] training 25.4% loss=0.22074, acc=0.92188
# [53/100] training 25.6% loss=0.13852, acc=0.95312
# [53/100] training 25.8% loss=0.25211, acc=0.89062
# [53/100] training 25.9% loss=0.11162, acc=0.98438
# [53/100] training 26.1% loss=0.17070, acc=0.93750
# [53/100] training 26.3% loss=0.12763, acc=0.96875
# [53/100] training 26.4% loss=0.11246, acc=0.96875
# [53/100] training 26.6% loss=0.10331, acc=0.96875
# [53/100] training 26.8% loss=0.12048, acc=0.95312
# [53/100] training 27.0% loss=0.16198, acc=0.90625
# [53/100] training 27.1% loss=0.08800, acc=0.95312
# [53/100] training 27.3% loss=0.20149, acc=0.89062
# [53/100] training 27.4% loss=0.06573, acc=0.96875
# [53/100] training 27.6% loss=0.32923, acc=0.92188
# [53/100] training 27.9% loss=0.13961, acc=0.95312
# [53/100] training 28.0% loss=0.21371, acc=0.93750
# [53/100] training 28.2% loss=0.16619, acc=0.95312
# [53/100] training 28.3% loss=0.05040, acc=0.98438
# [53/100] training 28.5% loss=0.21145, acc=0.89062
# [53/100] training 28.6% loss=0.19959, acc=0.93750
# [53/100] training 28.8% loss=0.09127, acc=0.96875
# [53/100] training 29.1% loss=0.11237, acc=0.96875
# [53/100] training 29.2% loss=0.12068, acc=0.96875
# [53/100] training 29.4% loss=0.22188, acc=0.90625
# [53/100] training 29.5% loss=0.09737, acc=0.96875
# [53/100] training 29.7% loss=0.24861, acc=0.90625
# [53/100] training 29.8% loss=0.17692, acc=0.96875
# [53/100] training 30.0% loss=0.17298, acc=0.93750
# [53/100] training 30.2% loss=0.12422, acc=0.92188
# [53/100] training 30.4% loss=0.10953, acc=0.98438
# [53/100] training 30.6% loss=0.23040, acc=0.90625
# [53/100] training 30.7% loss=0.26184, acc=0.93750
# [53/100] training 30.9% loss=0.34536, acc=0.90625
# [53/100] training 31.0% loss=0.13667, acc=0.93750
# [53/100] training 31.3% loss=0.17383, acc=0.92188
# [53/100] training 31.4% loss=0.20358, acc=0.93750
# [53/100] training 31.6% loss=0.25858, acc=0.90625
# [53/100] training 31.8% loss=0.20446, acc=0.89062
# [53/100] training 31.9% loss=0.19799, acc=0.95312
# [53/100] training 32.1% loss=0.17059, acc=0.93750
# [53/100] training 32.2% loss=0.24374, acc=0.90625
# [53/100] training 32.5% loss=0.11663, acc=0.96875
# [53/100] training 32.6% loss=0.14833, acc=0.96875
# [53/100] training 32.8% loss=0.14962, acc=0.93750
# [53/100] training 32.9% loss=0.18422, acc=0.92188
# [53/100] training 33.1% loss=0.18744, acc=0.92188
# [53/100] training 33.3% loss=0.22522, acc=0.92188
# [53/100] training 33.4% loss=0.16971, acc=0.95312
# [53/100] training 33.7% loss=0.20527, acc=0.90625
# [53/100] training 33.8% loss=0.22483, acc=0.89062
# [53/100] training 34.0% loss=0.20351, acc=0.90625
# [53/100] training 34.1% loss=0.20946, acc=0.92188
# [53/100] training 34.3% loss=0.19820, acc=0.93750
# [53/100] training 34.5% loss=0.20033, acc=0.90625
# [53/100] training 34.7% loss=0.11542, acc=0.96875
# [53/100] training 34.9% loss=0.13511, acc=0.92188
# [53/100] training 35.0% loss=0.12801, acc=0.95312
# [53/100] training 35.2% loss=0.18914, acc=0.95312
# [53/100] training 35.3% loss=0.22306, acc=0.87500
# [53/100] training 35.5% loss=0.18507, acc=0.92188
# [53/100] training 35.6% loss=0.37637, acc=0.85938
# [53/100] training 35.9% loss=0.17436, acc=0.92188
# [53/100] training 36.1% loss=0.27863, acc=0.89062
# [53/100] training 36.2% loss=0.18734, acc=0.92188
# [53/100] training 36.4% loss=0.28693, acc=0.90625
# [53/100] training 36.5% loss=0.19128, acc=0.93750
# [53/100] training 36.7% loss=0.22284, acc=0.92188
# [53/100] training 36.8% loss=0.14689, acc=0.96875
# [53/100] training 37.1% loss=0.23391, acc=0.89062
# [53/100] training 37.3% loss=0.16878, acc=0.93750
# [53/100] training 37.4% loss=0.07457, acc=1.00000
# [53/100] training 37.6% loss=0.10729, acc=0.95312
# [53/100] training 37.7% loss=0.17052, acc=0.93750
# [53/100] training 37.9% loss=0.20565, acc=0.92188
# [53/100] training 38.1% loss=0.13359, acc=0.93750
# [53/100] training 38.3% loss=0.15576, acc=0.92188
# [53/100] training 38.4% loss=0.09148, acc=0.96875
# [53/100] training 38.6% loss=0.13967, acc=0.93750
# [53/100] training 38.8% loss=0.35519, acc=0.85938
# [53/100] training 38.9% loss=0.15110, acc=0.93750
# [53/100] training 39.1% loss=0.18442, acc=0.95312
# [53/100] training 39.3% loss=0.23919, acc=0.87500
# [53/100] training 39.5% loss=0.20530, acc=0.87500
# [53/100] training 39.6% loss=0.22051, acc=0.96875
# [53/100] training 39.8% loss=0.08663, acc=0.98438
# [53/100] training 40.0% loss=0.18132, acc=0.90625
# [53/100] training 40.1% loss=0.21957, acc=0.89062
# [53/100] training 40.4% loss=0.14790, acc=0.93750
# [53/100] training 40.5% loss=0.21984, acc=0.93750
# [53/100] training 40.7% loss=0.23088, acc=0.90625
# [53/100] training 40.8% loss=0.10617, acc=0.95312
# [53/100] training 41.0% loss=0.20068, acc=0.89062
# [53/100] training 41.1% loss=0.18502, acc=0.89062
# [53/100] training 41.3% loss=0.19836, acc=0.89062
# [53/100] training 41.6% loss=0.18762, acc=0.92188
# [53/100] training 41.7% loss=0.20533, acc=0.90625
# [53/100] training 41.9% loss=0.18648, acc=0.92188
# [53/100] training 42.0% loss=0.14484, acc=0.95312
# [53/100] training 42.2% loss=0.21099, acc=0.92188
# [53/100] training 42.3% loss=0.09234, acc=0.96875
# [53/100] training 42.5% loss=0.12900, acc=0.92188
# [53/100] training 42.8% loss=0.12866, acc=0.93750
# [53/100] training 42.9% loss=0.21427, acc=0.89062
# [53/100] training 43.1% loss=0.21312, acc=0.92188
# [53/100] training 43.2% loss=0.17605, acc=0.93750
# [53/100] training 43.4% loss=0.24060, acc=0.90625
# [53/100] training 43.5% loss=0.24142, acc=0.92188
# [53/100] training 43.8% loss=0.20914, acc=0.90625
# [53/100] training 43.9% loss=0.18273, acc=0.93750
# [53/100] training 44.1% loss=0.14019, acc=0.93750
# [53/100] training 44.3% loss=0.19991, acc=0.92188
# [53/100] training 44.4% loss=0.27259, acc=0.92188
# [53/100] training 44.6% loss=0.21349, acc=0.89062
# [53/100] training 44.7% loss=0.28192, acc=0.87500
# [53/100] training 45.0% loss=0.05756, acc=0.96875
# [53/100] training 45.1% loss=0.30552, acc=0.85938
# [53/100] training 45.3% loss=0.17106, acc=0.93750
# [53/100] training 45.5% loss=0.03823, acc=1.00000
# [53/100] training 45.6% loss=0.13097, acc=0.95312
# [53/100] training 45.8% loss=0.08404, acc=0.95312
# [53/100] training 45.9% loss=0.12612, acc=0.93750
# [53/100] training 46.2% loss=0.22035, acc=0.93750
# [53/100] training 46.3% loss=0.14611, acc=0.95312
# [53/100] training 46.5% loss=0.19463, acc=0.90625
# [53/100] training 46.6% loss=0.21979, acc=0.92188
# [53/100] training 46.8% loss=0.11905, acc=0.98438
# [53/100] training 47.0% loss=0.11831, acc=0.95312
# [53/100] training 47.2% loss=0.13354, acc=0.96875
# [53/100] training 47.4% loss=0.18398, acc=0.90625
# [53/100] training 47.5% loss=0.21127, acc=0.93750
# [53/100] training 47.7% loss=0.14660, acc=0.93750
# [53/100] training 47.8% loss=0.26198, acc=0.90625
# [53/100] training 48.0% loss=0.14423, acc=0.95312
# [53/100] training 48.3% loss=0.09973, acc=0.96875
# [53/100] training 48.4% loss=0.07001, acc=0.98438
# [53/100] training 48.6% loss=0.17437, acc=0.93750
# [53/100] training 48.7% loss=0.11270, acc=0.96875
# [53/100] training 48.9% loss=0.11568, acc=0.95312
# [53/100] training 49.0% loss=0.09730, acc=0.95312
# [53/100] training 49.2% loss=0.15526, acc=0.93750
# [53/100] training 49.3% loss=0.18340, acc=0.92188
# [53/100] training 49.6% loss=0.25377, acc=0.89062
# [53/100] training 49.8% loss=0.19159, acc=0.93750
# [53/100] training 49.9% loss=0.20798, acc=0.90625
# [53/100] training 50.1% loss=0.13180, acc=0.95312
# [53/100] training 50.2% loss=0.21430, acc=0.90625
# [53/100] training 50.4% loss=0.31477, acc=0.87500
# [53/100] training 50.6% loss=0.26208, acc=0.92188
# [53/100] training 50.8% loss=0.20386, acc=0.93750
# [53/100] training 51.0% loss=0.24178, acc=0.89062
# [53/100] training 51.1% loss=0.27538, acc=0.89062
# [53/100] training 51.3% loss=0.27223, acc=0.84375
# [53/100] training 51.4% loss=0.21170, acc=0.90625
# [53/100] training 51.7% loss=0.15624, acc=0.92188
# [53/100] training 51.8% loss=0.23520, acc=0.92188
# [53/100] training 52.0% loss=0.14645, acc=0.92188
# [53/100] training 52.1% loss=0.18917, acc=0.90625
# [53/100] training 52.3% loss=0.30414, acc=0.90625
# [53/100] training 52.5% loss=0.08843, acc=0.95312
# [53/100] training 52.6% loss=0.17583, acc=0.93750
# [53/100] training 52.9% loss=0.26769, acc=0.90625
# [53/100] training 53.0% loss=0.11501, acc=0.92188
# [53/100] training 53.2% loss=0.23151, acc=0.92188
# [53/100] training 53.3% loss=0.11252, acc=0.96875
# [53/100] training 53.5% loss=0.20943, acc=0.92188
# [53/100] training 53.7% loss=0.09362, acc=0.96875
# [53/100] training 53.8% loss=0.26480, acc=0.85938
# [53/100] training 54.1% loss=0.18078, acc=0.89062
# [53/100] training 54.2% loss=0.10732, acc=0.95312
# [53/100] training 54.4% loss=0.15252, acc=0.93750
# [53/100] training 54.5% loss=0.21603, acc=0.95312
# [53/100] training 54.7% loss=0.29068, acc=0.90625
# [53/100] training 54.8% loss=0.11879, acc=0.95312
# [53/100] training 55.1% loss=0.19455, acc=0.90625
# [53/100] training 55.3% loss=0.20006, acc=0.90625
# [53/100] training 55.4% loss=0.13068, acc=0.95312
# [53/100] training 55.6% loss=0.19630, acc=0.92188
# [53/100] training 55.7% loss=0.06310, acc=0.98438
# [53/100] training 55.9% loss=0.11785, acc=0.93750
# [53/100] training 56.0% loss=0.09212, acc=0.96875
# [53/100] training 56.3% loss=0.38584, acc=0.84375
# [53/100] training 56.5% loss=0.14596, acc=0.95312
# [53/100] training 56.6% loss=0.19268, acc=0.90625
# [53/100] training 56.8% loss=0.19045, acc=0.90625
# [53/100] training 56.9% loss=0.18988, acc=0.92188
# [53/100] training 57.1% loss=0.22230, acc=0.90625
# [53/100] training 57.2% loss=0.04711, acc=0.96875
# [53/100] training 57.5% loss=0.16118, acc=0.92188
# [53/100] training 57.6% loss=0.07951, acc=0.96875
# [53/100] training 57.8% loss=0.12619, acc=0.93750
# [53/100] training 58.0% loss=0.13991, acc=0.93750
# [53/100] training 58.1% loss=0.07941, acc=0.95312
# [53/100] training 58.3% loss=0.07321, acc=0.96875
# [53/100] training 58.4% loss=0.23811, acc=0.92188
# [53/100] training 58.7% loss=0.23463, acc=0.92188
# [53/100] training 58.8% loss=0.26080, acc=0.87500
# [53/100] training 59.0% loss=0.12449, acc=0.96875
# [53/100] training 59.2% loss=0.13107, acc=0.93750
# [53/100] training 59.3% loss=0.13817, acc=0.93750
# [53/100] training 59.5% loss=0.11071, acc=0.95312
# [53/100] training 59.7% loss=0.19374, acc=0.93750
# [53/100] training 59.9% loss=0.09936, acc=0.96875
# [53/100] training 60.0% loss=0.11487, acc=0.92188
# [53/100] training 60.2% loss=0.13657, acc=0.92188
# [53/100] training 60.3% loss=0.07623, acc=0.95312
# [53/100] training 60.5% loss=0.25875, acc=0.87500
# [53/100] training 60.8% loss=0.32640, acc=0.85938
# [53/100] training 60.9% loss=0.24362, acc=0.90625
# [53/100] training 61.1% loss=0.27114, acc=0.90625
# [53/100] training 61.2% loss=0.14359, acc=0.92188
# [53/100] training 61.4% loss=0.33604, acc=0.87500
# [53/100] training 61.5% loss=0.33845, acc=0.87500
# [53/100] training 61.7% loss=0.13301, acc=0.95312
# [53/100] training 62.0% loss=0.26815, acc=0.87500
# [53/100] training 62.1% loss=0.32828, acc=0.89062
# [53/100] training 62.3% loss=0.13310, acc=0.95312
# [53/100] training 62.4% loss=0.12953, acc=0.93750
# [53/100] training 62.6% loss=0.26841, acc=0.87500
# [53/100] training 62.7% loss=0.13829, acc=0.93750
# [53/100] training 62.9% loss=0.21883, acc=0.90625
# [53/100] training 63.1% loss=0.18735, acc=0.90625
# [53/100] training 63.3% loss=0.10712, acc=1.00000
# [53/100] training 63.5% loss=0.15740, acc=0.92188
# [53/100] training 63.6% loss=0.19993, acc=0.90625
# [53/100] training 63.8% loss=0.41030, acc=0.87500
# [53/100] training 63.9% loss=0.11753, acc=0.96875
# [53/100] training 64.2% loss=0.13950, acc=0.95312
# [53/100] training 64.3% loss=0.24029, acc=0.85938
# [53/100] training 64.5% loss=0.16053, acc=0.95312
# [53/100] training 64.7% loss=0.20798, acc=0.90625
# [53/100] training 64.8% loss=0.37588, acc=0.82812
# [53/100] training 65.0% loss=0.17555, acc=0.95312
# [53/100] training 65.1% loss=0.21814, acc=0.92188
# [53/100] training 65.4% loss=0.11167, acc=0.98438
# [53/100] training 65.5% loss=0.11664, acc=0.96875
# [53/100] training 65.7% loss=0.13445, acc=0.93750
# [53/100] training 65.8% loss=0.18747, acc=0.92188
# [53/100] training 66.0% loss=0.19275, acc=0.95312
# [53/100] training 66.2% loss=0.10319, acc=0.95312
# [53/100] training 66.3% loss=0.24018, acc=0.92188
# [53/100] training 66.6% loss=0.08898, acc=0.95312
# [53/100] training 66.7% loss=0.16263, acc=0.93750
# [53/100] training 66.9% loss=0.09934, acc=0.96875
# [53/100] training 67.0% loss=0.10714, acc=0.93750
# [53/100] training 67.2% loss=0.11403, acc=0.95312
# [53/100] training 67.4% loss=0.16483, acc=0.95312
# [53/100] training 67.6% loss=0.06675, acc=0.98438
# [53/100] training 67.8% loss=0.17108, acc=0.95312
# [53/100] training 67.9% loss=0.08961, acc=0.95312
# [53/100] training 68.1% loss=0.10330, acc=0.95312
# [53/100] training 68.2% loss=0.10144, acc=0.96875
# [53/100] training 68.4% loss=0.11575, acc=0.98438
# [53/100] training 68.5% loss=0.26595, acc=0.90625
# [53/100] training 68.8% loss=0.07205, acc=0.96875
# [53/100] training 69.0% loss=0.21048, acc=0.93750
# [53/100] training 69.1% loss=0.17436, acc=0.90625
# [53/100] training 69.3% loss=0.16452, acc=0.93750
# [53/100] training 69.4% loss=0.16623, acc=0.93750
# [53/100] training 69.6% loss=0.13973, acc=0.96875
# [53/100] training 69.7% loss=0.15656, acc=0.93750
# [53/100] training 70.0% loss=0.20656, acc=0.93750
# [53/100] training 70.2% loss=0.26277, acc=0.92188
# [53/100] training 70.3% loss=0.22163, acc=0.92188
# [53/100] training 70.5% loss=0.13469, acc=0.96875
# [53/100] training 70.6% loss=0.14748, acc=0.95312
# [53/100] training 70.8% loss=0.24715, acc=0.85938
# [53/100] training 71.0% loss=0.26438, acc=0.89062
# [53/100] training 71.2% loss=0.09962, acc=0.95312
# [53/100] training 71.3% loss=0.08760, acc=0.98438
# [53/100] training 71.5% loss=0.31717, acc=0.92188
# [53/100] training 71.7% loss=0.18652, acc=0.90625
# [53/100] training 71.8% loss=0.20822, acc=0.92188
# [53/100] training 72.0% loss=0.09472, acc=0.98438
# [53/100] training 72.2% loss=0.19774, acc=0.90625
# [53/100] training 72.4% loss=0.15795, acc=0.93750
# [53/100] training 72.5% loss=0.23543, acc=0.89062
# [53/100] training 72.7% loss=0.36201, acc=0.84375
# [53/100] training 72.9% loss=0.14759, acc=0.93750
# [53/100] training 73.0% loss=0.18039, acc=0.93750
# [53/100] training 73.3% loss=0.22752, acc=0.92188
# [53/100] training 73.4% loss=0.12610, acc=0.95312
# [53/100] training 73.6% loss=0.10248, acc=0.98438
# [53/100] training 73.7% loss=0.12308, acc=0.95312
# [53/100] training 73.9% loss=0.14155, acc=0.93750
# [53/100] training 74.0% loss=0.19420, acc=0.92188
# [53/100] training 74.2% loss=0.12227, acc=0.98438
# [53/100] training 74.5% loss=0.14123, acc=0.93750
# [53/100] training 74.6% loss=0.32631, acc=0.87500
# [53/100] training 74.8% loss=0.22333, acc=0.90625
# [53/100] training 74.9% loss=0.31186, acc=0.87500
# [53/100] training 75.1% loss=0.15007, acc=0.95312
# [53/100] training 75.2% loss=0.11887, acc=0.95312
# [53/100] training 75.4% loss=0.17852, acc=0.92188
# [53/100] training 75.7% loss=0.19868, acc=0.92188
# [53/100] training 75.8% loss=0.25874, acc=0.89062
# [53/100] training 76.0% loss=0.11407, acc=0.93750
# [53/100] training 76.1% loss=0.23104, acc=0.89062
# [53/100] training 76.3% loss=0.09566, acc=0.95312
# [53/100] training 76.4% loss=0.21032, acc=0.89062
# [53/100] training 76.7% loss=0.16089, acc=0.95312
# [53/100] training 76.8% loss=0.12189, acc=0.95312
# [53/100] training 77.0% loss=0.06669, acc=0.98438
# [53/100] training 77.2% loss=0.16878, acc=0.95312
# [53/100] training 77.3% loss=0.13501, acc=0.95312
# [53/100] training 77.5% loss=0.20072, acc=0.90625
# [53/100] training 77.6% loss=0.12901, acc=0.92188
# [53/100] training 77.9% loss=0.19346, acc=0.95312
# [53/100] training 78.0% loss=0.09892, acc=0.96875
# [53/100] training 78.2% loss=0.26220, acc=0.90625
# [53/100] training 78.4% loss=0.06860, acc=0.98438
# [53/100] training 78.5% loss=0.16747, acc=0.92188
# [53/100] training 78.7% loss=0.22570, acc=0.90625
# [53/100] training 78.8% loss=0.06862, acc=0.98438
# [53/100] training 79.1% loss=0.11081, acc=0.92188
# [53/100] training 79.2% loss=0.08714, acc=0.96875
# [53/100] training 79.4% loss=0.15992, acc=0.93750
# [53/100] training 79.5% loss=0.11429, acc=0.96875
# [53/100] training 79.7% loss=0.07877, acc=0.98438
# [53/100] training 79.9% loss=0.13953, acc=0.92188
# [53/100] training 80.1% loss=0.06275, acc=0.98438
# [53/100] training 80.3% loss=0.19820, acc=0.90625
# [53/100] training 80.4% loss=0.22758, acc=0.96875
# [53/100] training 80.6% loss=0.33500, acc=0.87500
# [53/100] training 80.7% loss=0.20688, acc=0.89062
# [53/100] training 80.9% loss=0.20322, acc=0.93750
# [53/100] training 81.2% loss=0.19327, acc=0.92188
# [53/100] training 81.3% loss=0.15762, acc=0.92188
# [53/100] training 81.5% loss=0.11037, acc=0.96875
# [53/100] training 81.6% loss=0.19942, acc=0.92188
# [53/100] training 81.8% loss=0.23713, acc=0.90625
# [53/100] training 81.9% loss=0.35287, acc=0.90625
# [53/100] training 82.1% loss=0.16187, acc=0.95312
# [53/100] training 82.2% loss=0.12888, acc=0.95312
# [53/100] training 82.5% loss=0.22845, acc=0.92188
# [53/100] training 82.7% loss=0.19199, acc=0.93750
# [53/100] training 82.8% loss=0.17966, acc=0.92188
# [53/100] training 83.0% loss=0.16186, acc=0.92188
# [53/100] training 83.1% loss=0.14524, acc=0.95312
# [53/100] training 83.3% loss=0.12086, acc=0.95312
# [53/100] training 83.5% loss=0.11903, acc=0.96875
# [53/100] training 83.7% loss=0.22020, acc=0.92188
# [53/100] training 83.9% loss=0.16653, acc=0.95312
# [53/100] training 84.0% loss=0.07838, acc=0.98438
# [53/100] training 84.2% loss=0.12443, acc=0.95312
# [53/100] training 84.3% loss=0.10921, acc=0.96875
# [53/100] training 84.5% loss=0.09227, acc=0.95312
# [53/100] training 84.7% loss=0.13480, acc=0.93750
# [53/100] training 84.9% loss=0.14800, acc=0.93750
# [53/100] training 85.0% loss=0.14356, acc=0.93750
# [53/100] training 85.2% loss=0.24080, acc=0.92188
# [53/100] training 85.4% loss=0.05199, acc=0.98438
# [53/100] training 85.5% loss=0.20544, acc=0.87500
# [53/100] training 85.8% loss=0.15728, acc=0.95312
# [53/100] training 85.9% loss=0.12806, acc=0.95312
# [53/100] training 86.1% loss=0.12071, acc=0.96875
# [53/100] training 86.2% loss=0.10334, acc=0.96875
# [53/100] training 86.4% loss=0.39956, acc=0.81250
# [53/100] training 86.6% loss=0.16844, acc=0.93750
# [53/100] training 86.7% loss=0.08839, acc=0.96875
# [53/100] training 87.0% loss=0.21310, acc=0.92188
# [53/100] training 87.1% loss=0.14973, acc=0.93750
# [53/100] training 87.3% loss=0.16414, acc=0.92188
# [53/100] training 87.4% loss=0.15021, acc=0.93750
# [53/100] training 87.6% loss=0.10090, acc=0.93750
# [53/100] training 87.7% loss=0.19519, acc=0.93750
# [53/100] training 87.9% loss=0.15360, acc=0.93750
# [53/100] training 88.2% loss=0.19925, acc=0.93750
# [53/100] training 88.3% loss=0.22340, acc=0.92188
# [53/100] training 88.5% loss=0.28418, acc=0.93750
# [53/100] training 88.6% loss=0.10142, acc=0.95312
# [53/100] training 88.8% loss=0.14600, acc=0.92188
# [53/100] training 88.9% loss=0.38924, acc=0.79688
# [53/100] training 89.2% loss=0.14456, acc=0.92188
# [53/100] training 89.4% loss=0.20820, acc=0.93750
# [53/100] training 89.5% loss=0.19215, acc=0.95312
# [53/100] training 89.7% loss=0.25563, acc=0.90625
# [53/100] training 89.8% loss=0.14756, acc=0.95312
# [53/100] training 90.0% loss=0.18621, acc=0.93750
# [53/100] training 90.1% loss=0.18014, acc=0.92188
# [53/100] training 90.4% loss=0.19818, acc=0.92188
# [53/100] training 90.5% loss=0.09440, acc=0.98438
# [53/100] training 90.7% loss=0.13664, acc=0.93750
# [53/100] training 90.9% loss=0.12295, acc=0.92188
# [53/100] training 91.0% loss=0.15639, acc=0.93750
# [53/100] training 91.2% loss=0.28174, acc=0.89062
# [53/100] training 91.3% loss=0.38172, acc=0.85938
# [53/100] training 91.6% loss=0.22055, acc=0.92188
# [53/100] training 91.7% loss=0.28956, acc=0.90625
# [53/100] training 91.9% loss=0.30265, acc=0.87500
# [53/100] training 92.1% loss=0.17556, acc=0.92188
# [53/100] training 92.2% loss=0.13571, acc=0.93750
# [53/100] training 92.4% loss=0.21438, acc=0.89062
# [53/100] training 92.6% loss=0.29075, acc=0.87500
# [53/100] training 92.8% loss=0.18712, acc=0.92188
# [53/100] training 92.9% loss=0.33358, acc=0.89062
# [53/100] training 93.1% loss=0.36614, acc=0.85938
# [53/100] training 93.2% loss=0.30054, acc=0.96875
# [53/100] training 93.4% loss=0.21726, acc=0.92188
# [53/100] training 93.7% loss=0.15534, acc=0.90625
# [53/100] training 93.8% loss=0.12435, acc=0.96875
# [53/100] training 94.0% loss=0.12685, acc=0.96875
# [53/100] training 94.1% loss=0.21973, acc=0.90625
# [53/100] training 94.3% loss=0.16893, acc=0.89062
# [53/100] training 94.4% loss=0.16147, acc=0.96875
# [53/100] training 94.6% loss=0.14356, acc=0.93750
# [53/100] training 94.9% loss=0.10696, acc=0.93750
# [53/100] training 95.0% loss=0.23362, acc=0.92188
# [53/100] training 95.2% loss=0.35524, acc=0.87500
# [53/100] training 95.3% loss=0.22884, acc=0.92188
# [53/100] training 95.5% loss=0.12976, acc=0.95312
# [53/100] training 95.6% loss=0.17951, acc=0.92188
# [53/100] training 95.8% loss=0.23659, acc=0.89062
# [53/100] training 96.0% loss=0.25739, acc=0.90625
# [53/100] training 96.2% loss=0.12699, acc=0.93750
# [53/100] training 96.4% loss=0.19609, acc=0.98438
# [53/100] training 96.5% loss=0.18536, acc=0.93750
# [53/100] training 96.7% loss=0.12233, acc=0.96875
# [53/100] training 96.8% loss=0.23885, acc=0.92188
# [53/100] training 97.1% loss=0.14193, acc=0.93750
# [53/100] training 97.2% loss=0.20732, acc=0.93750
# [53/100] training 97.4% loss=0.19684, acc=0.90625
# [53/100] training 97.6% loss=0.13790, acc=0.95312
# [53/100] training 97.7% loss=0.20889, acc=0.89062
# [53/100] training 97.9% loss=0.15248, acc=0.93750
# [53/100] training 98.0% loss=0.18416, acc=0.92188
# [53/100] training 98.3% loss=0.21309, acc=0.89062
# [53/100] training 98.4% loss=0.16122, acc=0.90625
# [53/100] training 98.6% loss=0.35823, acc=0.82812
# [53/100] training 98.7% loss=0.28176, acc=0.90625
# [53/100] training 98.9% loss=0.17258, acc=0.92188
# [53/100] training 99.1% loss=0.23898, acc=0.92188
# [53/100] training 99.2% loss=0.15790, acc=0.93750
# [53/100] training 99.5% loss=0.22095, acc=0.89062
# [53/100] training 99.6% loss=0.13884, acc=0.93750
# [53/100] training 99.8% loss=0.15439, acc=0.95312
# [53/100] training 99.9% loss=0.10065, acc=0.96875
# [53/100] testing 0.9% loss=0.16534, acc=0.93750
# [53/100] testing 1.8% loss=0.42181, acc=0.84375
# [53/100] testing 2.2% loss=0.27524, acc=0.89062
# [53/100] testing 3.1% loss=0.33353, acc=0.89062
# [53/100] testing 3.5% loss=0.14370, acc=0.93750
# [53/100] testing 4.4% loss=0.22647, acc=0.89062
# [53/100] testing 4.8% loss=0.42246, acc=0.84375
# [53/100] testing 5.7% loss=0.22704, acc=0.90625
# [53/100] testing 6.6% loss=0.12675, acc=0.93750
# [53/100] testing 7.0% loss=0.17509, acc=0.93750
# [53/100] testing 7.9% loss=0.25994, acc=0.90625
# [53/100] testing 8.3% loss=0.35002, acc=0.89062
# [53/100] testing 9.2% loss=0.21884, acc=0.96875
# [53/100] testing 9.7% loss=0.11363, acc=0.96875
# [53/100] testing 10.5% loss=0.26852, acc=0.87500
# [53/100] testing 11.0% loss=0.32251, acc=0.87500
# [53/100] testing 11.8% loss=0.17470, acc=0.92188
# [53/100] testing 12.7% loss=0.37775, acc=0.85938
# [53/100] testing 13.2% loss=0.26180, acc=0.93750
# [53/100] testing 14.0% loss=0.36980, acc=0.89062
# [53/100] testing 14.5% loss=0.22801, acc=0.85938
# [53/100] testing 15.4% loss=0.20218, acc=0.93750
# [53/100] testing 15.8% loss=0.17308, acc=0.92188
# [53/100] testing 16.7% loss=0.20440, acc=0.93750
# [53/100] testing 17.5% loss=0.12968, acc=0.92188
# [53/100] testing 18.0% loss=0.19197, acc=0.93750
# [53/100] testing 18.9% loss=0.09980, acc=0.95312
# [53/100] testing 19.3% loss=0.35302, acc=0.90625
# [53/100] testing 20.2% loss=0.35110, acc=0.87500
# [53/100] testing 20.6% loss=0.31009, acc=0.89062
# [53/100] testing 21.5% loss=0.20327, acc=0.95312
# [53/100] testing 21.9% loss=0.47282, acc=0.87500
# [53/100] testing 22.8% loss=0.24760, acc=0.89062
# [53/100] testing 23.7% loss=0.45510, acc=0.85938
# [53/100] testing 24.1% loss=0.21621, acc=0.92188
# [53/100] testing 25.0% loss=0.31574, acc=0.89062
# [53/100] testing 25.4% loss=0.07127, acc=0.96875
# [53/100] testing 26.3% loss=0.37692, acc=0.89062
# [53/100] testing 26.8% loss=0.30110, acc=0.90625
# [53/100] testing 27.6% loss=0.24521, acc=0.89062
# [53/100] testing 28.5% loss=0.22454, acc=0.93750
# [53/100] testing 29.0% loss=0.27061, acc=0.92188
# [53/100] testing 29.8% loss=0.43642, acc=0.90625
# [53/100] testing 30.3% loss=0.24001, acc=0.92188
# [53/100] testing 31.1% loss=0.39141, acc=0.89062
# [53/100] testing 31.6% loss=0.18958, acc=0.90625
# [53/100] testing 32.5% loss=0.23806, acc=0.90625
# [53/100] testing 32.9% loss=0.38517, acc=0.89062
# [53/100] testing 33.8% loss=0.37916, acc=0.85938
# [53/100] testing 34.7% loss=0.39861, acc=0.89062
# [53/100] testing 35.1% loss=0.14627, acc=0.93750
# [53/100] testing 36.0% loss=0.38136, acc=0.85938
# [53/100] testing 36.4% loss=0.26003, acc=0.90625
# [53/100] testing 37.3% loss=0.27871, acc=0.89062
# [53/100] testing 37.7% loss=0.34189, acc=0.87500
# [53/100] testing 38.6% loss=0.17618, acc=0.96875
# [53/100] testing 39.5% loss=0.27177, acc=0.93750
# [53/100] testing 39.9% loss=0.18260, acc=0.93750
# [53/100] testing 40.8% loss=0.34516, acc=0.89062
# [53/100] testing 41.2% loss=0.18980, acc=0.93750
# [53/100] testing 42.1% loss=0.27849, acc=0.92188
# [53/100] testing 42.5% loss=0.12560, acc=0.96875
# [53/100] testing 43.4% loss=0.23548, acc=0.92188
# [53/100] testing 43.9% loss=0.11056, acc=0.95312
# [53/100] testing 44.7% loss=0.18374, acc=0.90625
# [53/100] testing 45.6% loss=0.18485, acc=0.92188
# [53/100] testing 46.1% loss=0.25796, acc=0.89062
# [53/100] testing 46.9% loss=0.14010, acc=0.96875
# [53/100] testing 47.4% loss=0.17930, acc=0.93750
# [53/100] testing 48.3% loss=0.35887, acc=0.89062
# [53/100] testing 48.7% loss=0.41525, acc=0.87500
# [53/100] testing 49.6% loss=0.48704, acc=0.85938
# [53/100] testing 50.4% loss=0.16881, acc=0.95312
# [53/100] testing 50.9% loss=0.23424, acc=0.92188
# [53/100] testing 51.8% loss=0.21780, acc=0.92188
# [53/100] testing 52.2% loss=0.23838, acc=0.89062
# [53/100] testing 53.1% loss=0.23247, acc=0.90625
# [53/100] testing 53.5% loss=0.19388, acc=0.93750
# [53/100] testing 54.4% loss=0.29394, acc=0.87500
# [53/100] testing 54.8% loss=0.27299, acc=0.90625
# [53/100] testing 55.7% loss=0.17680, acc=0.93750
# [53/100] testing 56.6% loss=0.23356, acc=0.90625
# [53/100] testing 57.0% loss=0.33353, acc=0.89062
# [53/100] testing 57.9% loss=0.28590, acc=0.89062
# [53/100] testing 58.3% loss=0.36466, acc=0.87500
# [53/100] testing 59.2% loss=0.24621, acc=0.92188
# [53/100] testing 59.7% loss=0.21215, acc=0.92188
# [53/100] testing 60.5% loss=0.24517, acc=0.85938
# [53/100] testing 61.4% loss=0.10893, acc=0.92188
# [53/100] testing 61.9% loss=0.17743, acc=0.92188
# [53/100] testing 62.7% loss=0.24735, acc=0.92188
# [53/100] testing 63.2% loss=0.42116, acc=0.90625
# [53/100] testing 64.0% loss=0.41188, acc=0.92188
# [53/100] testing 64.5% loss=0.27371, acc=0.87500
# [53/100] testing 65.4% loss=0.15956, acc=0.93750
# [53/100] testing 65.8% loss=0.27869, acc=0.90625
# [53/100] testing 66.7% loss=0.17490, acc=0.93750
# [53/100] testing 67.6% loss=0.43284, acc=0.89062
# [53/100] testing 68.0% loss=0.09399, acc=0.95312
# [53/100] testing 68.9% loss=0.24257, acc=0.93750
# [53/100] testing 69.3% loss=0.22185, acc=0.89062
# [53/100] testing 70.2% loss=0.34908, acc=0.84375
# [53/100] testing 70.6% loss=0.23239, acc=0.90625
# [53/100] testing 71.5% loss=0.28202, acc=0.92188
# [53/100] testing 72.4% loss=0.12868, acc=0.95312
# [53/100] testing 72.8% loss=0.18775, acc=0.92188
# [53/100] testing 73.7% loss=0.13175, acc=0.96875
# [53/100] testing 74.1% loss=0.36198, acc=0.89062
# [53/100] testing 75.0% loss=0.15776, acc=0.93750
# [53/100] testing 75.4% loss=0.37114, acc=0.89062
# [53/100] testing 76.3% loss=0.05232, acc=1.00000
# [53/100] testing 76.8% loss=0.22583, acc=0.89062
# [53/100] testing 77.6% loss=0.16252, acc=0.93750
# [53/100] testing 78.5% loss=0.27133, acc=0.85938
# [53/100] testing 79.0% loss=0.17781, acc=0.92188
# [53/100] testing 79.8% loss=0.20721, acc=0.89062
# [53/100] testing 80.3% loss=0.25836, acc=0.90625
# [53/100] testing 81.2% loss=0.38087, acc=0.90625
# [53/100] testing 81.6% loss=0.23511, acc=0.90625
# [53/100] testing 82.5% loss=0.16421, acc=0.93750
# [53/100] testing 83.3% loss=0.14692, acc=0.93750
# [53/100] testing 83.8% loss=0.19915, acc=0.95312
# [53/100] testing 84.7% loss=0.25592, acc=0.87500
# [53/100] testing 85.1% loss=0.23880, acc=0.92188
# [53/100] testing 86.0% loss=0.30097, acc=0.90625
# [53/100] testing 86.4% loss=0.40464, acc=0.87500
# [53/100] testing 87.3% loss=0.30702, acc=0.84375
# [53/100] testing 87.7% loss=0.26685, acc=0.90625
# [53/100] testing 88.6% loss=0.26192, acc=0.87500
# [53/100] testing 89.5% loss=0.52735, acc=0.81250
# [53/100] testing 89.9% loss=0.15548, acc=0.92188
# [53/100] testing 90.8% loss=0.31805, acc=0.92188
# [53/100] testing 91.2% loss=0.19775, acc=0.92188
# [53/100] testing 92.1% loss=0.26149, acc=0.90625
# [53/100] testing 92.6% loss=0.35879, acc=0.89062
# [53/100] testing 93.4% loss=0.26453, acc=0.87500
# [53/100] testing 94.3% loss=0.10331, acc=0.96875
# [53/100] testing 94.7% loss=0.18959, acc=0.93750
# [53/100] testing 95.6% loss=0.42502, acc=0.85938
# [53/100] testing 96.1% loss=0.21080, acc=0.90625
# [53/100] testing 96.9% loss=0.24129, acc=0.92188
# [53/100] testing 97.4% loss=0.13147, acc=0.96875
# [53/100] testing 98.3% loss=0.17720, acc=0.92188
# [53/100] testing 98.7% loss=0.21245, acc=0.87500
# [53/100] testing 99.6% loss=0.36629, acc=0.89062
# [54/100] training 0.2% loss=0.25538, acc=0.89062
# [54/100] training 0.4% loss=0.36848, acc=0.85938
# [54/100] training 0.5% loss=0.07490, acc=0.98438
# [54/100] training 0.8% loss=0.09683, acc=0.93750
# [54/100] training 0.9% loss=0.14200, acc=0.96875
# [54/100] training 1.1% loss=0.23072, acc=0.92188
# [54/100] training 1.2% loss=0.16972, acc=0.92188
# [54/100] training 1.4% loss=0.18831, acc=0.95312
# [54/100] training 1.6% loss=0.13904, acc=0.93750
# [54/100] training 1.8% loss=0.13496, acc=0.95312
# [54/100] training 2.0% loss=0.30781, acc=0.87500
# [54/100] training 2.1% loss=0.11173, acc=0.96875
# [54/100] training 2.3% loss=0.14302, acc=0.96875
# [54/100] training 2.4% loss=0.23186, acc=0.93750
# [54/100] training 2.6% loss=0.07172, acc=0.98438
# [54/100] training 2.7% loss=0.17372, acc=0.90625
# [54/100] training 3.0% loss=0.15788, acc=0.95312
# [54/100] training 3.2% loss=0.06302, acc=0.98438
# [54/100] training 3.3% loss=0.26545, acc=0.92188
# [54/100] training 3.5% loss=0.11696, acc=0.95312
# [54/100] training 3.6% loss=0.18081, acc=0.89062
# [54/100] training 3.8% loss=0.23511, acc=0.95312
# [54/100] training 3.9% loss=0.14373, acc=0.93750
# [54/100] training 4.2% loss=0.09031, acc=0.95312
# [54/100] training 4.4% loss=0.09847, acc=0.95312
# [54/100] training 4.5% loss=0.15772, acc=0.96875
# [54/100] training 4.7% loss=0.35361, acc=0.90625
# [54/100] training 4.8% loss=0.08493, acc=0.96875
# [54/100] training 5.0% loss=0.09853, acc=0.95312
# [54/100] training 5.2% loss=0.24550, acc=0.87500
# [54/100] training 5.4% loss=0.07076, acc=0.98438
# [54/100] training 5.5% loss=0.19127, acc=0.93750
# [54/100] training 5.7% loss=0.25260, acc=0.92188
# [54/100] training 5.9% loss=0.12734, acc=0.95312
# [54/100] training 6.0% loss=0.15503, acc=0.93750
# [54/100] training 6.3% loss=0.15989, acc=0.93750
# [54/100] training 6.4% loss=0.17760, acc=0.92188
# [54/100] training 6.6% loss=0.16208, acc=0.92188
# [54/100] training 6.7% loss=0.25216, acc=0.92188
# [54/100] training 6.9% loss=0.12497, acc=0.92188
# [54/100] training 7.1% loss=0.17893, acc=0.93750
# [54/100] training 7.2% loss=0.15762, acc=0.95312
# [54/100] training 7.5% loss=0.15496, acc=0.92188
# [54/100] training 7.6% loss=0.18531, acc=0.92188
# [54/100] training 7.8% loss=0.22241, acc=0.93750
# [54/100] training 7.9% loss=0.18082, acc=0.90625
# [54/100] training 8.1% loss=0.07515, acc=0.96875
# [54/100] training 8.2% loss=0.17099, acc=0.95312
# [54/100] training 8.4% loss=0.30658, acc=0.87500
# [54/100] training 8.7% loss=0.20755, acc=0.92188
# [54/100] training 8.8% loss=0.15517, acc=0.95312
# [54/100] training 9.0% loss=0.29190, acc=0.89062
# [54/100] training 9.1% loss=0.24357, acc=0.92188
# [54/100] training 9.3% loss=0.34499, acc=0.89062
# [54/100] training 9.4% loss=0.12714, acc=0.93750
# [54/100] training 9.7% loss=0.21289, acc=0.93750
# [54/100] training 9.9% loss=0.28125, acc=0.92188
# [54/100] training 10.0% loss=0.21569, acc=0.89062
# [54/100] training 10.2% loss=0.11506, acc=0.96875
# [54/100] training 10.3% loss=0.16256, acc=0.93750
# [54/100] training 10.5% loss=0.23702, acc=0.87500
# [54/100] training 10.6% loss=0.14936, acc=0.93750
# [54/100] training 10.9% loss=0.12500, acc=0.96875
# [54/100] training 11.0% loss=0.23863, acc=0.90625
# [54/100] training 11.2% loss=0.07202, acc=0.98438
# [54/100] training 11.4% loss=0.26196, acc=0.87500
# [54/100] training 11.5% loss=0.40579, acc=0.89062
# [54/100] training 11.7% loss=0.09882, acc=0.96875
# [54/100] training 11.8% loss=0.12268, acc=0.95312
# [54/100] training 12.1% loss=0.19218, acc=0.89062
# [54/100] training 12.2% loss=0.09563, acc=0.93750
# [54/100] training 12.4% loss=0.24155, acc=0.90625
# [54/100] training 12.6% loss=0.22235, acc=0.96875
# [54/100] training 12.7% loss=0.14615, acc=0.93750
# [54/100] training 12.9% loss=0.25433, acc=0.89062
# [54/100] training 13.0% loss=0.13767, acc=0.96875
# [54/100] training 13.3% loss=0.10754, acc=0.93750
# [54/100] training 13.4% loss=0.19017, acc=0.90625
# [54/100] training 13.6% loss=0.10284, acc=0.93750
# [54/100] training 13.7% loss=0.34182, acc=0.89062
# [54/100] training 13.9% loss=0.25773, acc=0.92188
# [54/100] training 14.1% loss=0.25522, acc=0.93750
# [54/100] training 14.3% loss=0.16952, acc=0.92188
# [54/100] training 14.5% loss=0.24231, acc=0.92188
# [54/100] training 14.6% loss=0.20112, acc=0.92188
# [54/100] training 14.8% loss=0.30382, acc=0.89062
# [54/100] training 14.9% loss=0.15347, acc=0.92188
# [54/100] training 15.1% loss=0.26357, acc=0.87500
# [54/100] training 15.4% loss=0.25198, acc=0.90625
# [54/100] training 15.5% loss=0.10015, acc=0.98438
# [54/100] training 15.7% loss=0.25360, acc=0.90625
# [54/100] training 15.8% loss=0.13577, acc=0.93750
# [54/100] training 16.0% loss=0.18652, acc=0.98438
# [54/100] training 16.1% loss=0.20873, acc=0.93750
# [54/100] training 16.3% loss=0.23903, acc=0.92188
# [54/100] training 16.4% loss=0.16117, acc=0.92188
# [54/100] training 16.7% loss=0.19195, acc=0.92188
# [54/100] training 16.9% loss=0.18007, acc=0.92188
# [54/100] training 17.0% loss=0.18573, acc=0.93750
# [54/100] training 17.2% loss=0.22867, acc=0.93750
# [54/100] training 17.3% loss=0.11896, acc=0.96875
# [54/100] training 17.5% loss=0.19118, acc=0.90625
# [54/100] training 17.7% loss=0.15345, acc=0.95312
# [54/100] training 17.9% loss=0.22211, acc=0.93750
# [54/100] training 18.1% loss=0.25521, acc=0.89062
# [54/100] training 18.2% loss=0.13623, acc=0.95312
# [54/100] training 18.4% loss=0.31915, acc=0.85938
# [54/100] training 18.5% loss=0.24549, acc=0.90625
# [54/100] training 18.8% loss=0.17792, acc=0.95312
# [54/100] training 18.9% loss=0.10616, acc=0.93750
# [54/100] training 19.1% loss=0.29946, acc=0.89062
# [54/100] training 19.2% loss=0.12899, acc=0.95312
# [54/100] training 19.4% loss=0.14031, acc=0.93750
# [54/100] training 19.6% loss=0.24106, acc=0.90625
# [54/100] training 19.7% loss=0.23850, acc=0.90625
# [54/100] training 20.0% loss=0.11624, acc=0.95312
# [54/100] training 20.1% loss=0.18716, acc=0.95312
# [54/100] training 20.3% loss=0.17189, acc=0.93750
# [54/100] training 20.4% loss=0.12976, acc=0.93750
# [54/100] training 20.6% loss=0.24539, acc=0.90625
# [54/100] training 20.8% loss=0.09023, acc=0.96875
# [54/100] training 20.9% loss=0.15164, acc=0.92188
# [54/100] training 21.2% loss=0.20159, acc=0.92188
# [54/100] training 21.3% loss=0.20933, acc=0.93750
# [54/100] training 21.5% loss=0.27295, acc=0.93750
# [54/100] training 21.6% loss=0.06480, acc=0.96875
# [54/100] training 21.8% loss=0.14662, acc=0.95312
# [54/100] training 21.9% loss=0.33458, acc=0.87500
# [54/100] training 22.2% loss=0.16627, acc=0.93750
# [54/100] training 22.4% loss=0.24214, acc=0.89062
# [54/100] training 22.5% loss=0.16765, acc=0.92188
# [54/100] training 22.7% loss=0.20741, acc=0.87500
# [54/100] training 22.8% loss=0.15805, acc=0.95312
# [54/100] training 23.0% loss=0.06612, acc=1.00000
# [54/100] training 23.1% loss=0.32097, acc=0.89062
# [54/100] training 23.4% loss=0.19561, acc=0.93750
# [54/100] training 23.6% loss=0.27314, acc=0.92188
# [54/100] training 23.7% loss=0.21574, acc=0.92188
# [54/100] training 23.9% loss=0.12350, acc=0.93750
# [54/100] training 24.0% loss=0.21035, acc=0.92188
# [54/100] training 24.2% loss=0.15820, acc=0.92188
# [54/100] training 24.3% loss=0.14272, acc=0.95312
# [54/100] training 24.6% loss=0.24537, acc=0.92188
# [54/100] training 24.7% loss=0.24511, acc=0.90625
# [54/100] training 24.9% loss=0.23287, acc=0.92188
# [54/100] training 25.1% loss=0.28631, acc=0.85938
# [54/100] training 25.2% loss=0.13546, acc=0.95312
# [54/100] training 25.4% loss=0.23218, acc=0.89062
# [54/100] training 25.6% loss=0.13409, acc=0.96875
# [54/100] training 25.8% loss=0.21620, acc=0.92188
# [54/100] training 25.9% loss=0.13581, acc=0.96875
# [54/100] training 26.1% loss=0.13210, acc=0.95312
# [54/100] training 26.3% loss=0.09622, acc=0.95312
# [54/100] training 26.4% loss=0.07577, acc=0.95312
# [54/100] training 26.6% loss=0.10062, acc=0.96875
# [54/100] training 26.8% loss=0.16346, acc=0.92188
# [54/100] training 27.0% loss=0.18720, acc=0.92188
# [54/100] training 27.1% loss=0.08544, acc=0.96875
# [54/100] training 27.3% loss=0.12539, acc=0.93750
# [54/100] training 27.4% loss=0.10530, acc=0.96875
# [54/100] training 27.6% loss=0.23188, acc=0.93750
# [54/100] training 27.9% loss=0.13236, acc=0.95312
# [54/100] training 28.0% loss=0.36261, acc=0.84375
# [54/100] training 28.2% loss=0.11091, acc=0.95312
# [54/100] training 28.3% loss=0.09776, acc=0.95312
# [54/100] training 28.5% loss=0.17076, acc=0.93750
# [54/100] training 28.6% loss=0.16147, acc=0.96875
# [54/100] training 28.8% loss=0.06494, acc=1.00000
# [54/100] training 29.1% loss=0.11078, acc=0.95312
# [54/100] training 29.2% loss=0.06297, acc=1.00000
# [54/100] training 29.4% loss=0.20956, acc=0.90625
# [54/100] training 29.5% loss=0.08857, acc=0.98438
# [54/100] training 29.7% loss=0.17067, acc=0.92188
# [54/100] training 29.8% loss=0.25619, acc=0.92188
# [54/100] training 30.0% loss=0.16964, acc=0.89062
# [54/100] training 30.2% loss=0.14424, acc=0.95312
# [54/100] training 30.4% loss=0.08033, acc=0.95312
# [54/100] training 30.6% loss=0.31622, acc=0.90625
# [54/100] training 30.7% loss=0.22961, acc=0.93750
# [54/100] training 30.9% loss=0.51085, acc=0.87500
# [54/100] training 31.0% loss=0.10246, acc=0.95312
# [54/100] training 31.3% loss=0.20725, acc=0.95312
# [54/100] training 31.4% loss=0.23326, acc=0.89062
# [54/100] training 31.6% loss=0.23143, acc=0.90625
# [54/100] training 31.8% loss=0.17166, acc=0.93750
# [54/100] training 31.9% loss=0.18805, acc=0.89062
# [54/100] training 32.1% loss=0.14650, acc=0.93750
# [54/100] training 32.2% loss=0.27072, acc=0.93750
# [54/100] training 32.5% loss=0.12782, acc=0.95312
# [54/100] training 32.6% loss=0.16857, acc=0.90625
# [54/100] training 32.8% loss=0.14977, acc=0.95312
# [54/100] training 32.9% loss=0.18118, acc=0.92188
# [54/100] training 33.1% loss=0.09863, acc=0.96875
# [54/100] training 33.3% loss=0.16878, acc=0.95312
# [54/100] training 33.4% loss=0.18267, acc=0.93750
# [54/100] training 33.7% loss=0.15319, acc=0.95312
# [54/100] training 33.8% loss=0.17616, acc=0.92188
# [54/100] training 34.0% loss=0.24853, acc=0.93750
# [54/100] training 34.1% loss=0.17487, acc=0.95312
# [54/100] training 34.3% loss=0.21068, acc=0.93750
# [54/100] training 34.5% loss=0.27259, acc=0.90625
# [54/100] training 34.7% loss=0.17506, acc=0.96875
# [54/100] training 34.9% loss=0.18649, acc=0.93750
# [54/100] training 35.0% loss=0.11777, acc=0.95312
# [54/100] training 35.2% loss=0.24890, acc=0.87500
# [54/100] training 35.3% loss=0.27337, acc=0.90625
# [54/100] training 35.5% loss=0.22287, acc=0.89062
# [54/100] training 35.6% loss=0.26296, acc=0.89062
# [54/100] training 35.9% loss=0.18936, acc=0.95312
# [54/100] training 36.1% loss=0.32048, acc=0.87500
# [54/100] training 36.2% loss=0.18615, acc=0.90625
# [54/100] training 36.4% loss=0.19771, acc=0.96875
# [54/100] training 36.5% loss=0.20251, acc=0.90625
# [54/100] training 36.7% loss=0.25993, acc=0.90625
# [54/100] training 36.8% loss=0.09311, acc=1.00000
# [54/100] training 37.1% loss=0.23547, acc=0.93750
# [54/100] training 37.3% loss=0.16084, acc=0.92188
# [54/100] training 37.4% loss=0.12787, acc=0.96875
# [54/100] training 37.6% loss=0.13253, acc=0.96875
# [54/100] training 37.7% loss=0.19769, acc=0.92188
# [54/100] training 37.9% loss=0.13184, acc=0.95312
# [54/100] training 38.1% loss=0.26020, acc=0.92188
# [54/100] training 38.3% loss=0.10010, acc=0.95312
# [54/100] training 38.4% loss=0.08403, acc=0.96875
# [54/100] training 38.6% loss=0.20903, acc=0.92188
# [54/100] training 38.8% loss=0.30834, acc=0.84375
# [54/100] training 38.9% loss=0.10807, acc=0.93750
# [54/100] training 39.1% loss=0.16429, acc=0.95312
# [54/100] training 39.3% loss=0.24270, acc=0.90625
# [54/100] training 39.5% loss=0.17081, acc=0.90625
# [54/100] training 39.6% loss=0.19294, acc=0.92188
# [54/100] training 39.8% loss=0.11927, acc=0.95312
# [54/100] training 40.0% loss=0.19869, acc=0.95312
# [54/100] training 40.1% loss=0.25246, acc=0.89062
# [54/100] training 40.4% loss=0.11216, acc=0.96875
# [54/100] training 40.5% loss=0.15374, acc=0.93750
# [54/100] training 40.7% loss=0.23569, acc=0.90625
# [54/100] training 40.8% loss=0.13239, acc=0.92188
# [54/100] training 41.0% loss=0.07936, acc=0.98438
# [54/100] training 41.1% loss=0.13794, acc=0.95312
# [54/100] training 41.3% loss=0.22046, acc=0.90625
# [54/100] training 41.6% loss=0.18294, acc=0.90625
# [54/100] training 41.7% loss=0.16765, acc=0.92188
# [54/100] training 41.9% loss=0.08127, acc=0.98438
# [54/100] training 42.0% loss=0.18818, acc=0.93750
# [54/100] training 42.2% loss=0.15535, acc=0.95312
# [54/100] training 42.3% loss=0.06678, acc=0.98438
# [54/100] training 42.5% loss=0.09325, acc=0.96875
# [54/100] training 42.8% loss=0.11191, acc=0.93750
# [54/100] training 42.9% loss=0.07625, acc=0.96875
# [54/100] training 43.1% loss=0.10510, acc=0.98438
# [54/100] training 43.2% loss=0.16193, acc=0.95312
# [54/100] training 43.4% loss=0.22418, acc=0.93750
# [54/100] training 43.5% loss=0.26923, acc=0.89062
# [54/100] training 43.8% loss=0.15816, acc=0.92188
# [54/100] training 43.9% loss=0.10693, acc=0.96875
# [54/100] training 44.1% loss=0.19743, acc=0.95312
# [54/100] training 44.3% loss=0.12652, acc=0.95312
# [54/100] training 44.4% loss=0.30185, acc=0.89062
# [54/100] training 44.6% loss=0.16494, acc=0.89062
# [54/100] training 44.7% loss=0.28780, acc=0.90625
# [54/100] training 45.0% loss=0.10386, acc=0.95312
# [54/100] training 45.1% loss=0.17762, acc=0.93750
# [54/100] training 45.3% loss=0.20129, acc=0.90625
# [54/100] training 45.5% loss=0.06057, acc=0.96875
# [54/100] training 45.6% loss=0.18739, acc=0.93750
# [54/100] training 45.8% loss=0.11896, acc=0.96875
# [54/100] training 45.9% loss=0.13590, acc=0.92188
# [54/100] training 46.2% loss=0.06669, acc=0.95312
# [54/100] training 46.3% loss=0.07736, acc=0.93750
# [54/100] training 46.5% loss=0.30142, acc=0.90625
# [54/100] training 46.6% loss=0.22891, acc=0.90625
# [54/100] training 46.8% loss=0.13938, acc=0.92188
# [54/100] training 47.0% loss=0.13213, acc=0.93750
# [54/100] training 47.2% loss=0.12741, acc=0.93750
# [54/100] training 47.4% loss=0.20522, acc=0.92188
# [54/100] training 47.5% loss=0.25419, acc=0.93750
# [54/100] training 47.7% loss=0.11449, acc=0.95312
# [54/100] training 47.8% loss=0.22400, acc=0.89062
# [54/100] training 48.0% loss=0.16299, acc=0.92188
# [54/100] training 48.3% loss=0.11333, acc=0.95312
# [54/100] training 48.4% loss=0.10684, acc=0.93750
# [54/100] training 48.6% loss=0.10459, acc=0.98438
# [54/100] training 48.7% loss=0.18948, acc=0.95312
# [54/100] training 48.9% loss=0.11324, acc=0.95312
# [54/100] training 49.0% loss=0.10480, acc=0.93750
# [54/100] training 49.2% loss=0.19413, acc=0.90625
# [54/100] training 49.3% loss=0.22529, acc=0.89062
# [54/100] training 49.6% loss=0.21430, acc=0.93750
# [54/100] training 49.8% loss=0.27654, acc=0.92188
# [54/100] training 49.9% loss=0.21261, acc=0.90625
# [54/100] training 50.1% loss=0.12203, acc=0.95312
# [54/100] training 50.2% loss=0.25403, acc=0.87500
# [54/100] training 50.4% loss=0.26901, acc=0.89062
# [54/100] training 50.6% loss=0.13852, acc=0.95312
# [54/100] training 50.8% loss=0.22549, acc=0.90625
# [54/100] training 51.0% loss=0.30311, acc=0.84375
# [54/100] training 51.1% loss=0.23627, acc=0.90625
# [54/100] training 51.3% loss=0.24146, acc=0.87500
# [54/100] training 51.4% loss=0.17990, acc=0.95312
# [54/100] training 51.7% loss=0.08519, acc=1.00000
# [54/100] training 51.8% loss=0.22219, acc=0.90625
# [54/100] training 52.0% loss=0.18065, acc=0.93750
# [54/100] training 52.1% loss=0.17259, acc=0.93750
# [54/100] training 52.3% loss=0.29824, acc=0.90625
# [54/100] training 52.5% loss=0.06952, acc=0.98438
# [54/100] training 52.6% loss=0.14109, acc=0.96875
# [54/100] training 52.9% loss=0.19653, acc=0.89062
# [54/100] training 53.0% loss=0.16596, acc=0.92188
# [54/100] training 53.2% loss=0.28648, acc=0.95312
# [54/100] training 53.3% loss=0.09761, acc=0.96875
# [54/100] training 53.5% loss=0.24767, acc=0.90625
# [54/100] training 53.7% loss=0.04525, acc=0.98438
# [54/100] training 53.8% loss=0.26811, acc=0.90625
# [54/100] training 54.1% loss=0.20948, acc=0.89062
# [54/100] training 54.2% loss=0.11270, acc=0.96875
# [54/100] training 54.4% loss=0.11433, acc=0.93750
# [54/100] training 54.5% loss=0.25471, acc=0.89062
# [54/100] training 54.7% loss=0.30280, acc=0.87500
# [54/100] training 54.8% loss=0.11606, acc=0.93750
# [54/100] training 55.1% loss=0.20217, acc=0.90625
# [54/100] training 55.3% loss=0.14766, acc=0.93750
# [54/100] training 55.4% loss=0.11583, acc=0.93750
# [54/100] training 55.6% loss=0.19564, acc=0.92188
# [54/100] training 55.7% loss=0.11719, acc=0.95312
# [54/100] training 55.9% loss=0.08193, acc=0.98438
# [54/100] training 56.0% loss=0.18574, acc=0.93750
# [54/100] training 56.3% loss=0.18553, acc=0.92188
# [54/100] training 56.5% loss=0.20815, acc=0.93750
# [54/100] training 56.6% loss=0.17678, acc=0.90625
# [54/100] training 56.8% loss=0.22302, acc=0.90625
# [54/100] training 56.9% loss=0.18351, acc=0.90625
# [54/100] training 57.1% loss=0.16488, acc=0.93750
# [54/100] training 57.2% loss=0.11161, acc=0.96875
# [54/100] training 57.5% loss=0.12849, acc=0.92188
# [54/100] training 57.6% loss=0.08408, acc=0.96875
# [54/100] training 57.8% loss=0.17715, acc=0.93750
# [54/100] training 58.0% loss=0.16381, acc=0.95312
# [54/100] training 58.1% loss=0.11584, acc=0.93750
# [54/100] training 58.3% loss=0.20525, acc=0.92188
# [54/100] training 58.4% loss=0.24261, acc=0.89062
# [54/100] training 58.7% loss=0.18748, acc=0.92188
# [54/100] training 58.8% loss=0.24735, acc=0.90625
# [54/100] training 59.0% loss=0.17384, acc=0.92188
# [54/100] training 59.2% loss=0.23505, acc=0.89062
# [54/100] training 59.3% loss=0.11506, acc=0.95312
# [54/100] training 59.5% loss=0.20491, acc=0.93750
# [54/100] training 59.7% loss=0.24467, acc=0.84375
# [54/100] training 59.9% loss=0.20089, acc=0.92188
# [54/100] training 60.0% loss=0.12960, acc=0.95312
# [54/100] training 60.2% loss=0.25358, acc=0.90625
# [54/100] training 60.3% loss=0.14960, acc=0.92188
# [54/100] training 60.5% loss=0.17747, acc=0.89062
# [54/100] training 60.8% loss=0.25608, acc=0.89062
# [54/100] training 60.9% loss=0.22996, acc=0.93750
# [54/100] training 61.1% loss=0.14782, acc=0.93750
# [54/100] training 61.2% loss=0.19880, acc=0.92188
# [54/100] training 61.4% loss=0.32202, acc=0.85938
# [54/100] training 61.5% loss=0.42973, acc=0.79688
# [54/100] training 61.7% loss=0.19858, acc=0.92188
# [54/100] training 62.0% loss=0.27419, acc=0.87500
# [54/100] training 62.1% loss=0.36559, acc=0.84375
# [54/100] training 62.3% loss=0.14244, acc=0.95312
# [54/100] training 62.4% loss=0.16565, acc=0.95312
# [54/100] training 62.6% loss=0.17989, acc=0.92188
# [54/100] training 62.7% loss=0.07302, acc=1.00000
# [54/100] training 62.9% loss=0.22831, acc=0.92188
# [54/100] training 63.1% loss=0.15457, acc=0.96875
# [54/100] training 63.3% loss=0.13359, acc=0.95312
# [54/100] training 63.5% loss=0.14200, acc=0.93750
# [54/100] training 63.6% loss=0.25637, acc=0.90625
# [54/100] training 63.8% loss=0.33599, acc=0.87500
# [54/100] training 63.9% loss=0.12121, acc=0.96875
# [54/100] training 64.2% loss=0.16547, acc=0.93750
# [54/100] training 64.3% loss=0.22125, acc=0.87500
# [54/100] training 64.5% loss=0.19209, acc=0.92188
# [54/100] training 64.7% loss=0.12946, acc=0.96875
# [54/100] training 64.8% loss=0.35331, acc=0.89062
# [54/100] training 65.0% loss=0.11820, acc=0.96875
# [54/100] training 65.1% loss=0.13016, acc=0.93750
# [54/100] training 65.4% loss=0.21977, acc=0.93750
# [54/100] training 65.5% loss=0.10461, acc=0.96875
# [54/100] training 65.7% loss=0.19774, acc=0.93750
# [54/100] training 65.8% loss=0.35073, acc=0.84375
# [54/100] training 66.0% loss=0.14797, acc=0.95312
# [54/100] training 66.2% loss=0.08824, acc=0.96875
# [54/100] training 66.3% loss=0.22953, acc=0.89062
# [54/100] training 66.6% loss=0.09751, acc=0.93750
# [54/100] training 66.7% loss=0.07842, acc=0.98438
# [54/100] training 66.9% loss=0.22399, acc=0.90625
# [54/100] training 67.0% loss=0.14498, acc=0.95312
# [54/100] training 67.2% loss=0.13781, acc=0.93750
# [54/100] training 67.4% loss=0.09572, acc=0.95312
# [54/100] training 67.6% loss=0.08548, acc=0.96875
# [54/100] training 67.8% loss=0.07377, acc=0.98438
# [54/100] training 67.9% loss=0.05680, acc=1.00000
# [54/100] training 68.1% loss=0.18993, acc=0.92188
# [54/100] training 68.2% loss=0.08396, acc=0.96875
# [54/100] training 68.4% loss=0.23208, acc=0.90625
# [54/100] training 68.5% loss=0.16362, acc=0.90625
# [54/100] training 68.8% loss=0.09378, acc=0.95312
# [54/100] training 69.0% loss=0.13008, acc=0.95312
# [54/100] training 69.1% loss=0.22771, acc=0.92188
# [54/100] training 69.3% loss=0.14225, acc=0.92188
# [54/100] training 69.4% loss=0.15878, acc=0.93750
# [54/100] training 69.6% loss=0.12461, acc=0.95312
# [54/100] training 69.7% loss=0.17810, acc=0.93750
# [54/100] training 70.0% loss=0.27358, acc=0.92188
# [54/100] training 70.2% loss=0.21137, acc=0.92188
# [54/100] training 70.3% loss=0.14500, acc=0.93750
# [54/100] training 70.5% loss=0.17681, acc=0.93750
# [54/100] training 70.6% loss=0.09224, acc=0.98438
# [54/100] training 70.8% loss=0.23211, acc=0.89062
# [54/100] training 71.0% loss=0.18515, acc=0.92188
# [54/100] training 71.2% loss=0.15601, acc=0.92188
# [54/100] training 71.3% loss=0.08844, acc=0.98438
# [54/100] training 71.5% loss=0.18628, acc=0.92188
# [54/100] training 71.7% loss=0.24286, acc=0.89062
# [54/100] training 71.8% loss=0.24411, acc=0.90625
# [54/100] training 72.0% loss=0.14586, acc=0.93750
# [54/100] training 72.2% loss=0.27735, acc=0.92188
# [54/100] training 72.4% loss=0.14521, acc=0.95312
# [54/100] training 72.5% loss=0.15993, acc=0.92188
# [54/100] training 72.7% loss=0.28954, acc=0.87500
# [54/100] training 72.9% loss=0.12752, acc=0.95312
# [54/100] training 73.0% loss=0.07999, acc=0.98438
# [54/100] training 73.3% loss=0.22846, acc=0.93750
# [54/100] training 73.4% loss=0.13462, acc=0.92188
# [54/100] training 73.6% loss=0.16455, acc=0.93750
# [54/100] training 73.7% loss=0.21505, acc=0.90625
# [54/100] training 73.9% loss=0.09850, acc=0.95312
# [54/100] training 74.0% loss=0.13201, acc=0.93750
# [54/100] training 74.2% loss=0.14181, acc=0.93750
# [54/100] training 74.5% loss=0.10828, acc=0.95312
# [54/100] training 74.6% loss=0.34821, acc=0.89062
# [54/100] training 74.8% loss=0.27283, acc=0.90625
# [54/100] training 74.9% loss=0.29812, acc=0.85938
# [54/100] training 75.1% loss=0.25117, acc=0.90625
# [54/100] training 75.2% loss=0.11608, acc=0.96875
# [54/100] training 75.4% loss=0.12810, acc=0.95312
# [54/100] training 75.7% loss=0.20035, acc=0.93750
# [54/100] training 75.8% loss=0.19959, acc=0.92188
# [54/100] training 76.0% loss=0.15221, acc=0.95312
# [54/100] training 76.1% loss=0.18700, acc=0.90625
# [54/100] training 76.3% loss=0.14295, acc=0.92188
# [54/100] training 76.4% loss=0.24710, acc=0.90625
# [54/100] training 76.7% loss=0.15536, acc=0.93750
# [54/100] training 76.8% loss=0.08542, acc=0.96875
# [54/100] training 77.0% loss=0.07109, acc=1.00000
# [54/100] training 77.2% loss=0.10071, acc=0.95312
# [54/100] training 77.3% loss=0.08638, acc=0.96875
# [54/100] training 77.5% loss=0.16591, acc=0.90625
# [54/100] training 77.6% loss=0.23856, acc=0.95312
# [54/100] training 77.9% loss=0.18150, acc=0.93750
# [54/100] training 78.0% loss=0.15326, acc=0.95312
# [54/100] training 78.2% loss=0.20345, acc=0.93750
# [54/100] training 78.4% loss=0.06125, acc=1.00000
# [54/100] training 78.5% loss=0.22817, acc=0.87500
# [54/100] training 78.7% loss=0.18482, acc=0.93750
# [54/100] training 78.8% loss=0.18816, acc=0.93750
# [54/100] training 79.1% loss=0.10253, acc=0.96875
# [54/100] training 79.2% loss=0.12037, acc=0.96875
# [54/100] training 79.4% loss=0.22891, acc=0.92188
# [54/100] training 79.5% loss=0.11536, acc=0.96875
# [54/100] training 79.7% loss=0.06828, acc=0.98438
# [54/100] training 79.9% loss=0.12928, acc=0.95312
# [54/100] training 80.1% loss=0.07896, acc=0.96875
# [54/100] training 80.3% loss=0.23197, acc=0.90625
# [54/100] training 80.4% loss=0.21531, acc=0.92188
# [54/100] training 80.6% loss=0.18818, acc=0.93750
# [54/100] training 80.7% loss=0.16798, acc=0.95312
# [54/100] training 80.9% loss=0.21410, acc=0.93750
# [54/100] training 81.2% loss=0.18588, acc=0.92188
# [54/100] training 81.3% loss=0.19740, acc=0.89062
# [54/100] training 81.5% loss=0.17461, acc=0.92188
# [54/100] training 81.6% loss=0.37124, acc=0.89062
# [54/100] training 81.8% loss=0.18544, acc=0.90625
# [54/100] training 81.9% loss=0.39103, acc=0.92188
# [54/100] training 82.1% loss=0.35543, acc=0.79688
# [54/100] training 82.2% loss=0.15436, acc=0.93750
# [54/100] training 82.5% loss=0.21066, acc=0.89062
# [54/100] training 82.7% loss=0.16022, acc=0.95312
# [54/100] training 82.8% loss=0.21214, acc=0.93750
# [54/100] training 83.0% loss=0.24976, acc=0.93750
# [54/100] training 83.1% loss=0.33097, acc=0.85938
# [54/100] training 83.3% loss=0.20279, acc=0.93750
# [54/100] training 83.5% loss=0.18146, acc=0.90625
# [54/100] training 83.7% loss=0.74115, acc=0.79688
# [54/100] training 83.9% loss=0.73906, acc=0.76562
# [54/100] training 84.0% loss=0.24482, acc=0.89062
# [54/100] training 84.2% loss=0.24313, acc=0.87500
# [54/100] training 84.3% loss=0.27284, acc=0.89062
# [54/100] training 84.5% loss=0.40717, acc=0.87500
# [54/100] training 84.7% loss=0.45632, acc=0.81250
# [54/100] training 84.9% loss=0.30120, acc=0.89062
# [54/100] training 85.0% loss=0.33481, acc=0.85938
# [54/100] training 85.2% loss=0.29428, acc=0.87500
# [54/100] training 85.4% loss=0.18531, acc=0.93750
# [54/100] training 85.5% loss=0.34037, acc=0.82812
# [54/100] training 85.8% loss=0.21202, acc=0.89062
# [54/100] training 85.9% loss=0.24368, acc=0.87500
# [54/100] training 86.1% loss=0.21158, acc=0.90625
# [54/100] training 86.2% loss=0.13509, acc=0.95312
# [54/100] training 86.4% loss=0.40868, acc=0.82812
# [54/100] training 86.6% loss=0.35197, acc=0.81250
# [54/100] training 86.7% loss=0.24199, acc=0.90625
# [54/100] training 87.0% loss=0.29186, acc=0.85938
# [54/100] training 87.1% loss=0.22301, acc=0.93750
# [54/100] training 87.3% loss=0.25633, acc=0.92188
# [54/100] training 87.4% loss=0.27214, acc=0.89062
# [54/100] training 87.6% loss=0.27805, acc=0.87500
# [54/100] training 87.7% loss=0.45942, acc=0.85938
# [54/100] training 87.9% loss=0.27516, acc=0.89062
# [54/100] training 88.2% loss=0.20711, acc=0.93750
# [54/100] training 88.3% loss=0.26647, acc=0.90625
# [54/100] training 88.5% loss=0.21569, acc=0.89062
# [54/100] training 88.6% loss=0.24795, acc=0.92188
# [54/100] training 88.8% loss=0.25253, acc=0.87500
# [54/100] training 88.9% loss=0.24992, acc=0.87500
# [54/100] training 89.2% loss=0.19883, acc=0.93750
# [54/100] training 89.4% loss=0.22652, acc=0.89062
# [54/100] training 89.5% loss=0.33161, acc=0.85938
# [54/100] training 89.7% loss=0.23128, acc=0.90625
# [54/100] training 89.8% loss=0.31563, acc=0.87500
# [54/100] training 90.0% loss=0.18064, acc=0.93750
# [54/100] training 90.1% loss=0.29399, acc=0.89062
# [54/100] training 90.4% loss=0.23006, acc=0.90625
# [54/100] training 90.5% loss=0.26761, acc=0.87500
# [54/100] training 90.7% loss=0.24133, acc=0.89062
# [54/100] training 90.9% loss=0.22304, acc=0.89062
# [54/100] training 91.0% loss=0.32977, acc=0.89062
# [54/100] training 91.2% loss=0.22675, acc=0.92188
# [54/100] training 91.3% loss=0.29624, acc=0.82812
# [54/100] training 91.6% loss=0.43893, acc=0.89062
# [54/100] training 91.7% loss=0.27511, acc=0.92188
# [54/100] training 91.9% loss=0.42921, acc=0.87500
# [54/100] training 92.1% loss=0.29424, acc=0.87500
# [54/100] training 92.2% loss=0.19824, acc=0.93750
# [54/100] training 92.4% loss=0.23510, acc=0.90625
# [54/100] training 92.6% loss=0.32400, acc=0.82812
# [54/100] training 92.8% loss=0.30126, acc=0.85938
# [54/100] training 92.9% loss=0.38720, acc=0.89062
# [54/100] training 93.1% loss=0.35417, acc=0.85938
# [54/100] training 93.2% loss=0.36167, acc=0.85938
# [54/100] training 93.4% loss=0.22507, acc=0.90625
# [54/100] training 93.7% loss=0.12021, acc=0.95312
# [54/100] training 93.8% loss=0.34240, acc=0.76562
# [54/100] training 94.0% loss=0.11125, acc=0.95312
# [54/100] training 94.1% loss=0.24560, acc=0.92188
# [54/100] training 94.3% loss=0.17843, acc=0.95312
# [54/100] training 94.4% loss=0.26260, acc=0.89062
# [54/100] training 94.6% loss=0.17426, acc=0.90625
# [54/100] training 94.9% loss=0.11441, acc=0.93750
# [54/100] training 95.0% loss=0.57855, acc=0.81250
# [54/100] training 95.2% loss=0.31110, acc=0.87500
# [54/100] training 95.3% loss=0.32277, acc=0.85938
# [54/100] training 95.5% loss=0.20331, acc=0.93750
# [54/100] training 95.6% loss=0.30755, acc=0.87500
# [54/100] training 95.8% loss=0.32209, acc=0.81250
# [54/100] training 96.0% loss=0.27469, acc=0.93750
# [54/100] training 96.2% loss=0.18875, acc=0.90625
# [54/100] training 96.4% loss=0.23231, acc=0.93750
# [54/100] training 96.5% loss=0.28759, acc=0.84375
# [54/100] training 96.7% loss=0.19049, acc=0.89062
# [54/100] training 96.8% loss=0.33999, acc=0.92188
# [54/100] training 97.1% loss=0.23897, acc=0.90625
# [54/100] training 97.2% loss=0.28522, acc=0.85938
# [54/100] training 97.4% loss=0.18760, acc=0.89062
# [54/100] training 97.6% loss=0.23869, acc=0.87500
# [54/100] training 97.7% loss=0.19539, acc=0.93750
# [54/100] training 97.9% loss=0.16006, acc=0.90625
# [54/100] training 98.0% loss=0.13696, acc=0.95312
# [54/100] training 98.3% loss=0.23928, acc=0.90625
# [54/100] training 98.4% loss=0.22979, acc=0.87500
# [54/100] training 98.6% loss=0.34305, acc=0.79688
# [54/100] training 98.7% loss=0.30210, acc=0.89062
# [54/100] training 98.9% loss=0.22389, acc=0.90625
# [54/100] training 99.1% loss=0.17109, acc=0.93750
# [54/100] training 99.2% loss=0.16343, acc=0.95312
# [54/100] training 99.5% loss=0.37283, acc=0.84375
# [54/100] training 99.6% loss=0.27740, acc=0.89062
# [54/100] training 99.8% loss=0.20741, acc=0.92188
# [54/100] training 99.9% loss=0.09078, acc=0.96875
# [54/100] testing 0.9% loss=0.15026, acc=0.92188
# [54/100] testing 1.8% loss=0.27254, acc=0.85938
# [54/100] testing 2.2% loss=0.14708, acc=0.96875
# [54/100] testing 3.1% loss=0.33323, acc=0.87500
# [54/100] testing 3.5% loss=0.15131, acc=0.93750
# [54/100] testing 4.4% loss=0.17473, acc=0.89062
# [54/100] testing 4.8% loss=0.43822, acc=0.81250
# [54/100] testing 5.7% loss=0.31345, acc=0.87500
# [54/100] testing 6.6% loss=0.12887, acc=0.93750
# [54/100] testing 7.0% loss=0.15003, acc=0.92188
# [54/100] testing 7.9% loss=0.33949, acc=0.87500
# [54/100] testing 8.3% loss=0.21922, acc=0.92188
# [54/100] testing 9.2% loss=0.29277, acc=0.87500
# [54/100] testing 9.7% loss=0.18561, acc=0.92188
# [54/100] testing 10.5% loss=0.19581, acc=0.90625
# [54/100] testing 11.0% loss=0.31285, acc=0.87500
# [54/100] testing 11.8% loss=0.27871, acc=0.89062
# [54/100] testing 12.7% loss=0.27185, acc=0.87500
# [54/100] testing 13.2% loss=0.18318, acc=0.93750
# [54/100] testing 14.0% loss=0.33756, acc=0.92188
# [54/100] testing 14.5% loss=0.24912, acc=0.89062
# [54/100] testing 15.4% loss=0.29582, acc=0.89062
# [54/100] testing 15.8% loss=0.17881, acc=0.92188
# [54/100] testing 16.7% loss=0.13874, acc=0.96875
# [54/100] testing 17.5% loss=0.16287, acc=0.90625
# [54/100] testing 18.0% loss=0.16084, acc=0.90625
# [54/100] testing 18.9% loss=0.11233, acc=0.96875
# [54/100] testing 19.3% loss=0.34848, acc=0.87500
# [54/100] testing 20.2% loss=0.25151, acc=0.90625
# [54/100] testing 20.6% loss=0.36948, acc=0.87500
# [54/100] testing 21.5% loss=0.21634, acc=0.93750
# [54/100] testing 21.9% loss=0.38512, acc=0.87500
# [54/100] testing 22.8% loss=0.25629, acc=0.92188
# [54/100] testing 23.7% loss=0.37682, acc=0.84375
# [54/100] testing 24.1% loss=0.22774, acc=0.92188
# [54/100] testing 25.0% loss=0.34853, acc=0.89062
# [54/100] testing 25.4% loss=0.11519, acc=0.95312
# [54/100] testing 26.3% loss=0.21474, acc=0.90625
# [54/100] testing 26.8% loss=0.25951, acc=0.93750
# [54/100] testing 27.6% loss=0.27049, acc=0.89062
# [54/100] testing 28.5% loss=0.25886, acc=0.87500
# [54/100] testing 29.0% loss=0.21100, acc=0.95312
# [54/100] testing 29.8% loss=0.38834, acc=0.89062
# [54/100] testing 30.3% loss=0.21295, acc=0.93750
# [54/100] testing 31.1% loss=0.27513, acc=0.89062
# [54/100] testing 31.6% loss=0.15579, acc=0.95312
# [54/100] testing 32.5% loss=0.30500, acc=0.85938
# [54/100] testing 32.9% loss=0.35091, acc=0.87500
# [54/100] testing 33.8% loss=0.28295, acc=0.85938
# [54/100] testing 34.7% loss=0.33849, acc=0.89062
# [54/100] testing 35.1% loss=0.21603, acc=0.93750
# [54/100] testing 36.0% loss=0.27275, acc=0.92188
# [54/100] testing 36.4% loss=0.24074, acc=0.92188
# [54/100] testing 37.3% loss=0.29695, acc=0.90625
# [54/100] testing 37.7% loss=0.47348, acc=0.85938
# [54/100] testing 38.6% loss=0.19471, acc=0.93750
# [54/100] testing 39.5% loss=0.27511, acc=0.92188
# [54/100] testing 39.9% loss=0.19625, acc=0.92188
# [54/100] testing 40.8% loss=0.28255, acc=0.89062
# [54/100] testing 41.2% loss=0.33790, acc=0.93750
# [54/100] testing 42.1% loss=0.27457, acc=0.90625
# [54/100] testing 42.5% loss=0.15890, acc=0.92188
# [54/100] testing 43.4% loss=0.32994, acc=0.89062
# [54/100] testing 43.9% loss=0.08754, acc=0.98438
# [54/100] testing 44.7% loss=0.28760, acc=0.85938
# [54/100] testing 45.6% loss=0.34517, acc=0.85938
# [54/100] testing 46.1% loss=0.30709, acc=0.87500
# [54/100] testing 46.9% loss=0.24123, acc=0.90625
# [54/100] testing 47.4% loss=0.15000, acc=0.93750
# [54/100] testing 48.3% loss=0.36651, acc=0.84375
# [54/100] testing 48.7% loss=0.34536, acc=0.82812
# [54/100] testing 49.6% loss=0.39251, acc=0.82812
# [54/100] testing 50.4% loss=0.24138, acc=0.90625
# [54/100] testing 50.9% loss=0.25686, acc=0.92188
# [54/100] testing 51.8% loss=0.27521, acc=0.90625
# [54/100] testing 52.2% loss=0.26020, acc=0.89062
# [54/100] testing 53.1% loss=0.19225, acc=0.92188
# [54/100] testing 53.5% loss=0.21797, acc=0.90625
# [54/100] testing 54.4% loss=0.29663, acc=0.84375
# [54/100] testing 54.8% loss=0.36757, acc=0.87500
# [54/100] testing 55.7% loss=0.12929, acc=0.95312
# [54/100] testing 56.6% loss=0.25084, acc=0.90625
# [54/100] testing 57.0% loss=0.32155, acc=0.90625
# [54/100] testing 57.9% loss=0.28434, acc=0.89062
# [54/100] testing 58.3% loss=0.35422, acc=0.87500
# [54/100] testing 59.2% loss=0.32033, acc=0.85938
# [54/100] testing 59.7% loss=0.14285, acc=0.96875
# [54/100] testing 60.5% loss=0.44701, acc=0.84375
# [54/100] testing 61.4% loss=0.11585, acc=0.93750
# [54/100] testing 61.9% loss=0.18766, acc=0.93750
# [54/100] testing 62.7% loss=0.21926, acc=0.90625
# [54/100] testing 63.2% loss=0.29735, acc=0.90625
# [54/100] testing 64.0% loss=0.43492, acc=0.84375
# [54/100] testing 64.5% loss=0.17813, acc=0.92188
# [54/100] testing 65.4% loss=0.12784, acc=0.95312
# [54/100] testing 65.8% loss=0.27491, acc=0.87500
# [54/100] testing 66.7% loss=0.19137, acc=0.89062
# [54/100] testing 67.6% loss=0.38062, acc=0.87500
# [54/100] testing 68.0% loss=0.12472, acc=0.95312
# [54/100] testing 68.9% loss=0.28427, acc=0.87500
# [54/100] testing 69.3% loss=0.33149, acc=0.82812
# [54/100] testing 70.2% loss=0.34457, acc=0.85938
# [54/100] testing 70.6% loss=0.24579, acc=0.90625
# [54/100] testing 71.5% loss=0.18239, acc=0.92188
# [54/100] testing 72.4% loss=0.14850, acc=0.92188
# [54/100] testing 72.8% loss=0.23913, acc=0.87500
# [54/100] testing 73.7% loss=0.19363, acc=0.93750
# [54/100] testing 74.1% loss=0.37018, acc=0.82812
# [54/100] testing 75.0% loss=0.23011, acc=0.87500
# [54/100] testing 75.4% loss=0.42360, acc=0.85938
# [54/100] testing 76.3% loss=0.08738, acc=0.96875
# [54/100] testing 76.8% loss=0.21952, acc=0.89062
# [54/100] testing 77.6% loss=0.18601, acc=0.89062
# [54/100] testing 78.5% loss=0.27774, acc=0.87500
# [54/100] testing 79.0% loss=0.30521, acc=0.85938
# [54/100] testing 79.8% loss=0.23631, acc=0.85938
# [54/100] testing 80.3% loss=0.23140, acc=0.89062
# [54/100] testing 81.2% loss=0.35218, acc=0.84375
# [54/100] testing 81.6% loss=0.22372, acc=0.89062
# [54/100] testing 82.5% loss=0.18192, acc=0.90625
# [54/100] testing 83.3% loss=0.25505, acc=0.95312
# [54/100] testing 83.8% loss=0.09629, acc=0.95312
# [54/100] testing 84.7% loss=0.38196, acc=0.87500
# [54/100] testing 85.1% loss=0.27297, acc=0.90625
# [54/100] testing 86.0% loss=0.29351, acc=0.90625
# [54/100] testing 86.4% loss=0.35874, acc=0.85938
# [54/100] testing 87.3% loss=0.36703, acc=0.82812
# [54/100] testing 87.7% loss=0.24162, acc=0.90625
# [54/100] testing 88.6% loss=0.26320, acc=0.85938
# [54/100] testing 89.5% loss=0.49393, acc=0.81250
# [54/100] testing 89.9% loss=0.20808, acc=0.90625
# [54/100] testing 90.8% loss=0.23322, acc=0.96875
# [54/100] testing 91.2% loss=0.10705, acc=0.96875
# [54/100] testing 92.1% loss=0.32737, acc=0.85938
# [54/100] testing 92.6% loss=0.34001, acc=0.89062
# [54/100] testing 93.4% loss=0.40429, acc=0.85938
# [54/100] testing 94.3% loss=0.11563, acc=0.93750
# [54/100] testing 94.7% loss=0.17221, acc=0.95312
# [54/100] testing 95.6% loss=0.34979, acc=0.85938
# [54/100] testing 96.1% loss=0.16618, acc=0.92188
# [54/100] testing 96.9% loss=0.28493, acc=0.90625
# [54/100] testing 97.4% loss=0.14303, acc=0.96875
# [54/100] testing 98.3% loss=0.21447, acc=0.90625
# [54/100] testing 98.7% loss=0.23945, acc=0.90625
# [54/100] testing 99.6% loss=0.23322, acc=0.89062
# [55/100] training 0.2% loss=0.28191, acc=0.85938
# [55/100] training 0.4% loss=0.25921, acc=0.89062
# [55/100] training 0.5% loss=0.10575, acc=0.95312
# [55/100] training 0.8% loss=0.17925, acc=0.90625
# [55/100] training 0.9% loss=0.20333, acc=0.87500
# [55/100] training 1.1% loss=0.25913, acc=0.93750
# [55/100] training 1.2% loss=0.23024, acc=0.92188
# [55/100] training 1.4% loss=0.23621, acc=0.90625
# [55/100] training 1.6% loss=0.14059, acc=0.93750
# [55/100] training 1.8% loss=0.18164, acc=0.90625
# [55/100] training 2.0% loss=0.32102, acc=0.84375
# [55/100] training 2.1% loss=0.22053, acc=0.89062
# [55/100] training 2.3% loss=0.22208, acc=0.89062
# [55/100] training 2.4% loss=0.19051, acc=0.89062
# [55/100] training 2.6% loss=0.21526, acc=0.89062
# [55/100] training 2.7% loss=0.31938, acc=0.82812
# [55/100] training 3.0% loss=0.25465, acc=0.89062
# [55/100] training 3.2% loss=0.20590, acc=0.90625
# [55/100] training 3.3% loss=0.29233, acc=0.89062
# [55/100] training 3.5% loss=0.30648, acc=0.84375
# [55/100] training 3.6% loss=0.26628, acc=0.85938
# [55/100] training 3.8% loss=0.25444, acc=0.87500
# [55/100] training 3.9% loss=0.21863, acc=0.92188
# [55/100] training 4.2% loss=0.22670, acc=0.89062
# [55/100] training 4.4% loss=0.16127, acc=0.93750
# [55/100] training 4.5% loss=0.18929, acc=0.92188
# [55/100] training 4.7% loss=0.29508, acc=0.90625
# [55/100] training 4.8% loss=0.23760, acc=0.87500
# [55/100] training 5.0% loss=0.16074, acc=0.95312
# [55/100] training 5.2% loss=0.32413, acc=0.87500
# [55/100] training 5.4% loss=0.20030, acc=0.87500
# [55/100] training 5.5% loss=0.24072, acc=0.87500
# [55/100] training 5.7% loss=0.18617, acc=0.93750
# [55/100] training 5.9% loss=0.22894, acc=0.89062
# [55/100] training 6.0% loss=0.33820, acc=0.90625
# [55/100] training 6.3% loss=0.23333, acc=0.89062
# [55/100] training 6.4% loss=0.24696, acc=0.84375
# [55/100] training 6.6% loss=0.25810, acc=0.89062
# [55/100] training 6.7% loss=0.37063, acc=0.82812
# [55/100] training 6.9% loss=0.15587, acc=0.95312
# [55/100] training 7.1% loss=0.16446, acc=0.93750
# [55/100] training 7.2% loss=0.23979, acc=0.89062
# [55/100] training 7.5% loss=0.16866, acc=0.92188
# [55/100] training 7.6% loss=0.25418, acc=0.90625
# [55/100] training 7.8% loss=0.20089, acc=0.93750
# [55/100] training 7.9% loss=0.40878, acc=0.85938
# [55/100] training 8.1% loss=0.11523, acc=0.96875
# [55/100] training 8.2% loss=0.25156, acc=0.93750
# [55/100] training 8.4% loss=0.20369, acc=0.92188
# [55/100] training 8.7% loss=0.21972, acc=0.92188
# [55/100] training 8.8% loss=0.31145, acc=0.85938
# [55/100] training 9.0% loss=0.24334, acc=0.93750
# [55/100] training 9.1% loss=0.28681, acc=0.89062
# [55/100] training 9.3% loss=0.35752, acc=0.87500
# [55/100] training 9.4% loss=0.16683, acc=0.93750
# [55/100] training 9.7% loss=0.24369, acc=0.89062
# [55/100] training 9.9% loss=0.26668, acc=0.87500
# [55/100] training 10.0% loss=0.28432, acc=0.87500
# [55/100] training 10.2% loss=0.19286, acc=0.92188
# [55/100] training 10.3% loss=0.26419, acc=0.87500
# [55/100] training 10.5% loss=0.24679, acc=0.93750
# [55/100] training 10.6% loss=0.17004, acc=0.93750
# [55/100] training 10.9% loss=0.17223, acc=0.95312
# [55/100] training 11.0% loss=0.29202, acc=0.90625
# [55/100] training 11.2% loss=0.14293, acc=0.95312
# [55/100] training 11.4% loss=0.29881, acc=0.90625
# [55/100] training 11.5% loss=0.45056, acc=0.87500
# [55/100] training 11.7% loss=0.09989, acc=0.98438
# [55/100] training 11.8% loss=0.12886, acc=0.95312
# [55/100] training 12.1% loss=0.15117, acc=0.93750
# [55/100] training 12.2% loss=0.12704, acc=0.93750
# [55/100] training 12.4% loss=0.15148, acc=0.93750
# [55/100] training 12.6% loss=0.28132, acc=0.90625
# [55/100] training 12.7% loss=0.14957, acc=0.92188
# [55/100] training 12.9% loss=0.23631, acc=0.90625
# [55/100] training 13.0% loss=0.24081, acc=0.92188
# [55/100] training 13.3% loss=0.11672, acc=0.96875
# [55/100] training 13.4% loss=0.26390, acc=0.84375
# [55/100] training 13.6% loss=0.23233, acc=0.92188
# [55/100] training 13.7% loss=0.21640, acc=0.90625
# [55/100] training 13.9% loss=0.20184, acc=0.90625
# [55/100] training 14.1% loss=0.22144, acc=0.90625
# [55/100] training 14.3% loss=0.13670, acc=0.92188
# [55/100] training 14.5% loss=0.31905, acc=0.85938
# [55/100] training 14.6% loss=0.17221, acc=0.90625
# [55/100] training 14.8% loss=0.23441, acc=0.89062
# [55/100] training 14.9% loss=0.13147, acc=0.95312
# [55/100] training 15.1% loss=0.29047, acc=0.87500
# [55/100] training 15.4% loss=0.35362, acc=0.82812
# [55/100] training 15.5% loss=0.16356, acc=0.93750
# [55/100] training 15.7% loss=0.40164, acc=0.84375
# [55/100] training 15.8% loss=0.13056, acc=0.93750
# [55/100] training 16.0% loss=0.35216, acc=0.85938
# [55/100] training 16.1% loss=0.39662, acc=0.85938
# [55/100] training 16.3% loss=0.23414, acc=0.92188
# [55/100] training 16.4% loss=0.22283, acc=0.92188
# [55/100] training 16.7% loss=0.29680, acc=0.79688
# [55/100] training 16.9% loss=0.19659, acc=0.92188
# [55/100] training 17.0% loss=0.21256, acc=0.92188
# [55/100] training 17.2% loss=0.14109, acc=0.93750
# [55/100] training 17.3% loss=0.15839, acc=0.93750
# [55/100] training 17.5% loss=0.19617, acc=0.90625
# [55/100] training 17.7% loss=0.16304, acc=0.90625
# [55/100] training 17.9% loss=0.24557, acc=0.90625
# [55/100] training 18.1% loss=0.36770, acc=0.84375
# [55/100] training 18.2% loss=0.18583, acc=0.92188
# [55/100] training 18.4% loss=0.28076, acc=0.84375
# [55/100] training 18.5% loss=0.21148, acc=0.95312
# [55/100] training 18.8% loss=0.17077, acc=0.92188
# [55/100] training 18.9% loss=0.09044, acc=0.98438
# [55/100] training 19.1% loss=0.22773, acc=0.89062
# [55/100] training 19.2% loss=0.11058, acc=0.95312
# [55/100] training 19.4% loss=0.15478, acc=0.92188
# [55/100] training 19.6% loss=0.33300, acc=0.85938
# [55/100] training 19.7% loss=0.15629, acc=0.95312
# [55/100] training 20.0% loss=0.09146, acc=0.98438
# [55/100] training 20.1% loss=0.23771, acc=0.92188
# [55/100] training 20.3% loss=0.33131, acc=0.90625
# [55/100] training 20.4% loss=0.20278, acc=0.89062
# [55/100] training 20.6% loss=0.29255, acc=0.90625
# [55/100] training 20.8% loss=0.11549, acc=0.96875
# [55/100] training 20.9% loss=0.16084, acc=0.89062
# [55/100] training 21.2% loss=0.27953, acc=0.89062
# [55/100] training 21.3% loss=0.22433, acc=0.90625
# [55/100] training 21.5% loss=0.32921, acc=0.89062
# [55/100] training 21.6% loss=0.13253, acc=0.95312
# [55/100] training 21.8% loss=0.14270, acc=0.93750
# [55/100] training 21.9% loss=0.24073, acc=0.93750
# [55/100] training 22.2% loss=0.13110, acc=0.93750
# [55/100] training 22.4% loss=0.23283, acc=0.92188
# [55/100] training 22.5% loss=0.08633, acc=0.96875
# [55/100] training 22.7% loss=0.19715, acc=0.90625
# [55/100] training 22.8% loss=0.25153, acc=0.90625
# [55/100] training 23.0% loss=0.06823, acc=0.98438
# [55/100] training 23.1% loss=0.40490, acc=0.87500
# [55/100] training 23.4% loss=0.19782, acc=0.92188
# [55/100] training 23.6% loss=0.19748, acc=0.93750
# [55/100] training 23.7% loss=0.17653, acc=0.93750
# [55/100] training 23.9% loss=0.20022, acc=0.93750
# [55/100] training 24.0% loss=0.15956, acc=0.93750
# [55/100] training 24.2% loss=0.19926, acc=0.90625
# [55/100] training 24.3% loss=0.23614, acc=0.92188
# [55/100] training 24.6% loss=0.23802, acc=0.87500
# [55/100] training 24.7% loss=0.34280, acc=0.85938
# [55/100] training 24.9% loss=0.22495, acc=0.93750
# [55/100] training 25.1% loss=0.22363, acc=0.90625
# [55/100] training 25.2% loss=0.18723, acc=0.90625
# [55/100] training 25.4% loss=0.19750, acc=0.93750
# [55/100] training 25.6% loss=0.15131, acc=0.93750
# [55/100] training 25.8% loss=0.22922, acc=0.90625
# [55/100] training 25.9% loss=0.16149, acc=0.92188
# [55/100] training 26.1% loss=0.12223, acc=0.96875
# [55/100] training 26.3% loss=0.17509, acc=0.92188
# [55/100] training 26.4% loss=0.15363, acc=0.93750
# [55/100] training 26.6% loss=0.02983, acc=1.00000
# [55/100] training 26.8% loss=0.15174, acc=0.93750
# [55/100] training 27.0% loss=0.12847, acc=0.93750
# [55/100] training 27.1% loss=0.17083, acc=0.95312
# [55/100] training 27.3% loss=0.12419, acc=0.92188
# [55/100] training 27.4% loss=0.10495, acc=0.95312
# [55/100] training 27.6% loss=0.28618, acc=0.92188
# [55/100] training 27.9% loss=0.07858, acc=0.98438
# [55/100] training 28.0% loss=0.24690, acc=0.90625
# [55/100] training 28.2% loss=0.15741, acc=0.95312
# [55/100] training 28.3% loss=0.07834, acc=0.95312
# [55/100] training 28.5% loss=0.25748, acc=0.84375
# [55/100] training 28.6% loss=0.24739, acc=0.90625
# [55/100] training 28.8% loss=0.15779, acc=0.98438
# [55/100] training 29.1% loss=0.16837, acc=0.93750
# [55/100] training 29.2% loss=0.12186, acc=0.96875
# [55/100] training 29.4% loss=0.34347, acc=0.81250
# [55/100] training 29.5% loss=0.06206, acc=0.98438
# [55/100] training 29.7% loss=0.21886, acc=0.89062
# [55/100] training 29.8% loss=0.21122, acc=0.92188
# [55/100] training 30.0% loss=0.22013, acc=0.85938
# [55/100] training 30.2% loss=0.09946, acc=0.92188
# [55/100] training 30.4% loss=0.06708, acc=0.96875
# [55/100] training 30.6% loss=0.31544, acc=0.89062
# [55/100] training 30.7% loss=0.17282, acc=0.93750
# [55/100] training 30.9% loss=0.32261, acc=0.92188
# [55/100] training 31.0% loss=0.12303, acc=0.95312
# [55/100] training 31.3% loss=0.13753, acc=0.93750
# [55/100] training 31.4% loss=0.23166, acc=0.92188
# [55/100] training 31.6% loss=0.18637, acc=0.93750
# [55/100] training 31.8% loss=0.17917, acc=0.92188
# [55/100] training 31.9% loss=0.24343, acc=0.90625
# [55/100] training 32.1% loss=0.14121, acc=0.95312
# [55/100] training 32.2% loss=0.26685, acc=0.92188
# [55/100] training 32.5% loss=0.14101, acc=0.92188
# [55/100] training 32.6% loss=0.18946, acc=0.90625
# [55/100] training 32.8% loss=0.18336, acc=0.92188
# [55/100] training 32.9% loss=0.20122, acc=0.93750
# [55/100] training 33.1% loss=0.10459, acc=0.98438
# [55/100] training 33.3% loss=0.21429, acc=0.87500
# [55/100] training 33.4% loss=0.12848, acc=0.95312
# [55/100] training 33.7% loss=0.22232, acc=0.92188
# [55/100] training 33.8% loss=0.18845, acc=0.90625
# [55/100] training 34.0% loss=0.28979, acc=0.84375
# [55/100] training 34.1% loss=0.19372, acc=0.93750
# [55/100] training 34.3% loss=0.12947, acc=0.95312
# [55/100] training 34.5% loss=0.28117, acc=0.89062
# [55/100] training 34.7% loss=0.16879, acc=0.92188
# [55/100] training 34.9% loss=0.18629, acc=0.90625
# [55/100] training 35.0% loss=0.15707, acc=0.93750
# [55/100] training 35.2% loss=0.24519, acc=0.90625
# [55/100] training 35.3% loss=0.18969, acc=0.90625
# [55/100] training 35.5% loss=0.21571, acc=0.95312
# [55/100] training 35.6% loss=0.37981, acc=0.82812
# [55/100] training 35.9% loss=0.14597, acc=0.95312
# [55/100] training 36.1% loss=0.31849, acc=0.89062
# [55/100] training 36.2% loss=0.26406, acc=0.90625
# [55/100] training 36.4% loss=0.25565, acc=0.93750
# [55/100] training 36.5% loss=0.26734, acc=0.90625
# [55/100] training 36.7% loss=0.14640, acc=0.92188
# [55/100] training 36.8% loss=0.10756, acc=0.96875
# [55/100] training 37.1% loss=0.26368, acc=0.90625
# [55/100] training 37.3% loss=0.14464, acc=0.95312
# [55/100] training 37.4% loss=0.15399, acc=0.93750
# [55/100] training 37.6% loss=0.09438, acc=0.96875
# [55/100] training 37.7% loss=0.15851, acc=0.93750
# [55/100] training 37.9% loss=0.19540, acc=0.92188
# [55/100] training 38.1% loss=0.19741, acc=0.92188
# [55/100] training 38.3% loss=0.09672, acc=0.95312
# [55/100] training 38.4% loss=0.10829, acc=0.95312
# [55/100] training 38.6% loss=0.13932, acc=0.93750
# [55/100] training 38.8% loss=0.20482, acc=0.90625
# [55/100] training 38.9% loss=0.27725, acc=0.90625
# [55/100] training 39.1% loss=0.19928, acc=0.95312
# [55/100] training 39.3% loss=0.22275, acc=0.93750
# [55/100] training 39.5% loss=0.19959, acc=0.89062
# [55/100] training 39.6% loss=0.15141, acc=0.93750
# [55/100] training 39.8% loss=0.14151, acc=0.95312
# [55/100] training 40.0% loss=0.24123, acc=0.90625
# [55/100] training 40.1% loss=0.24835, acc=0.85938
# [55/100] training 40.4% loss=0.13740, acc=0.93750
# [55/100] training 40.5% loss=0.23158, acc=0.92188
# [55/100] training 40.7% loss=0.27056, acc=0.89062
# [55/100] training 40.8% loss=0.09798, acc=0.98438
# [55/100] training 41.0% loss=0.11196, acc=0.95312
# [55/100] training 41.1% loss=0.15983, acc=0.92188
# [55/100] training 41.3% loss=0.22291, acc=0.89062
# [55/100] training 41.6% loss=0.25484, acc=0.90625
# [55/100] training 41.7% loss=0.15541, acc=0.95312
# [55/100] training 41.9% loss=0.07205, acc=0.98438
# [55/100] training 42.0% loss=0.16204, acc=0.93750
# [55/100] training 42.2% loss=0.26820, acc=0.87500
# [55/100] training 42.3% loss=0.20434, acc=0.92188
# [55/100] training 42.5% loss=0.10838, acc=0.98438
# [55/100] training 42.8% loss=0.12143, acc=0.96875
# [55/100] training 42.9% loss=0.13696, acc=0.96875
# [55/100] training 43.1% loss=0.13722, acc=0.96875
# [55/100] training 43.2% loss=0.17803, acc=0.90625
# [55/100] training 43.4% loss=0.20573, acc=0.95312
# [55/100] training 43.5% loss=0.15697, acc=0.92188
# [55/100] training 43.8% loss=0.14583, acc=0.93750
# [55/100] training 43.9% loss=0.13571, acc=0.93750
# [55/100] training 44.1% loss=0.22950, acc=0.95312
# [55/100] training 44.3% loss=0.12157, acc=0.93750
# [55/100] training 44.4% loss=0.25359, acc=0.89062
# [55/100] training 44.6% loss=0.23053, acc=0.93750
# [55/100] training 44.7% loss=0.41741, acc=0.85938
# [55/100] training 45.0% loss=0.13890, acc=0.93750
# [55/100] training 45.1% loss=0.18846, acc=0.92188
# [55/100] training 45.3% loss=0.23033, acc=0.89062
# [55/100] training 45.5% loss=0.08933, acc=0.98438
# [55/100] training 45.6% loss=0.21361, acc=0.89062
# [55/100] training 45.8% loss=0.10959, acc=0.98438
# [55/100] training 45.9% loss=0.11305, acc=0.95312
# [55/100] training 46.2% loss=0.09239, acc=0.96875
# [55/100] training 46.3% loss=0.07643, acc=0.96875
# [55/100] training 46.5% loss=0.33324, acc=0.90625
# [55/100] training 46.6% loss=0.25933, acc=0.92188
# [55/100] training 46.8% loss=0.09629, acc=0.95312
# [55/100] training 47.0% loss=0.20669, acc=0.90625
# [55/100] training 47.2% loss=0.22458, acc=0.90625
# [55/100] training 47.4% loss=0.21108, acc=0.90625
# [55/100] training 47.5% loss=0.22193, acc=0.89062
# [55/100] training 47.7% loss=0.15868, acc=0.93750
# [55/100] training 47.8% loss=0.24227, acc=0.89062
# [55/100] training 48.0% loss=0.11263, acc=0.96875
# [55/100] training 48.3% loss=0.11527, acc=0.95312
# [55/100] training 48.4% loss=0.07789, acc=0.98438
# [55/100] training 48.6% loss=0.12625, acc=0.96875
# [55/100] training 48.7% loss=0.20494, acc=0.92188
# [55/100] training 48.9% loss=0.08576, acc=0.98438
# [55/100] training 49.0% loss=0.11717, acc=0.95312
# [55/100] training 49.2% loss=0.09709, acc=0.96875
# [55/100] training 49.3% loss=0.12819, acc=0.93750
# [55/100] training 49.6% loss=0.32430, acc=0.90625
# [55/100] training 49.8% loss=0.21708, acc=0.89062
# [55/100] training 49.9% loss=0.22484, acc=0.92188
# [55/100] training 50.1% loss=0.10534, acc=0.92188
# [55/100] training 50.2% loss=0.17674, acc=0.93750
# [55/100] training 50.4% loss=0.35435, acc=0.87500
# [55/100] training 50.6% loss=0.21053, acc=0.93750
# [55/100] training 50.8% loss=0.20582, acc=0.93750
# [55/100] training 51.0% loss=0.19043, acc=0.92188
# [55/100] training 51.1% loss=0.23671, acc=0.89062
# [55/100] training 51.3% loss=0.22342, acc=0.89062
# [55/100] training 51.4% loss=0.26079, acc=0.90625
# [55/100] training 51.7% loss=0.11334, acc=0.95312
# [55/100] training 51.8% loss=0.22035, acc=0.89062
# [55/100] training 52.0% loss=0.12629, acc=0.95312
# [55/100] training 52.1% loss=0.18386, acc=0.93750
# [55/100] training 52.3% loss=0.22573, acc=0.93750
# [55/100] training 52.5% loss=0.12256, acc=0.93750
# [55/100] training 52.6% loss=0.11689, acc=0.96875
# [55/100] training 52.9% loss=0.18519, acc=0.93750
# [55/100] training 53.0% loss=0.08755, acc=0.96875
# [55/100] training 53.2% loss=0.18121, acc=0.96875
# [55/100] training 53.3% loss=0.11336, acc=0.95312
# [55/100] training 53.5% loss=0.21506, acc=0.90625
# [55/100] training 53.7% loss=0.09871, acc=0.95312
# [55/100] training 53.8% loss=0.17283, acc=0.90625
# [55/100] training 54.1% loss=0.25933, acc=0.82812
# [55/100] training 54.2% loss=0.06160, acc=1.00000
# [55/100] training 54.4% loss=0.10574, acc=0.95312
# [55/100] training 54.5% loss=0.24391, acc=0.89062
# [55/100] training 54.7% loss=0.32161, acc=0.89062
# [55/100] training 54.8% loss=0.13682, acc=0.95312
# [55/100] training 55.1% loss=0.13402, acc=0.90625
# [55/100] training 55.3% loss=0.13091, acc=0.93750
# [55/100] training 55.4% loss=0.07814, acc=0.98438
# [55/100] training 55.6% loss=0.13509, acc=0.93750
# [55/100] training 55.7% loss=0.07782, acc=0.98438
# [55/100] training 55.9% loss=0.07139, acc=0.96875
# [55/100] training 56.0% loss=0.10831, acc=0.96875
# [55/100] training 56.3% loss=0.26750, acc=0.89062
# [55/100] training 56.5% loss=0.09931, acc=0.93750
# [55/100] training 56.6% loss=0.15027, acc=0.93750
# [55/100] training 56.8% loss=0.12560, acc=0.93750
# [55/100] training 56.9% loss=0.17050, acc=0.90625
# [55/100] training 57.1% loss=0.20933, acc=0.93750
# [55/100] training 57.2% loss=0.14013, acc=0.93750
# [55/100] training 57.5% loss=0.16768, acc=0.92188
# [55/100] training 57.6% loss=0.10020, acc=0.96875
# [55/100] training 57.8% loss=0.23128, acc=0.90625
# [55/100] training 58.0% loss=0.16810, acc=0.93750
# [55/100] training 58.1% loss=0.07914, acc=0.96875
# [55/100] training 58.3% loss=0.15511, acc=0.95312
# [55/100] training 58.4% loss=0.21365, acc=0.92188
# [55/100] training 58.7% loss=0.17839, acc=0.93750
# [55/100] training 58.8% loss=0.16280, acc=0.95312
# [55/100] training 59.0% loss=0.10437, acc=0.95312
# [55/100] training 59.2% loss=0.29275, acc=0.89062
# [55/100] training 59.3% loss=0.16160, acc=0.93750
# [55/100] training 59.5% loss=0.21704, acc=0.93750
# [55/100] training 59.7% loss=0.14482, acc=0.96875
# [55/100] training 59.9% loss=0.11838, acc=0.95312
# [55/100] training 60.0% loss=0.17354, acc=0.92188
# [55/100] training 60.2% loss=0.14060, acc=0.93750
# [55/100] training 60.3% loss=0.15145, acc=0.93750
# [55/100] training 60.5% loss=0.30041, acc=0.90625
# [55/100] training 60.8% loss=0.24484, acc=0.85938
# [55/100] training 60.9% loss=0.19849, acc=0.96875
# [55/100] training 61.1% loss=0.21819, acc=0.89062
# [55/100] training 61.2% loss=0.13612, acc=0.95312
# [55/100] training 61.4% loss=0.23918, acc=0.92188
# [55/100] training 61.5% loss=0.28922, acc=0.87500
# [55/100] training 61.7% loss=0.20373, acc=0.92188
# [55/100] training 62.0% loss=0.30696, acc=0.89062
# [55/100] training 62.1% loss=0.27219, acc=0.87500
# [55/100] training 62.3% loss=0.11962, acc=0.92188
# [55/100] training 62.4% loss=0.07135, acc=0.98438
# [55/100] training 62.6% loss=0.24280, acc=0.89062
# [55/100] training 62.7% loss=0.12951, acc=0.93750
# [55/100] training 62.9% loss=0.20699, acc=0.92188
# [55/100] training 63.1% loss=0.22165, acc=0.93750
# [55/100] training 63.3% loss=0.09324, acc=0.98438
# [55/100] training 63.5% loss=0.14368, acc=0.95312
# [55/100] training 63.6% loss=0.25241, acc=0.90625
# [55/100] training 63.8% loss=0.18346, acc=0.90625
# [55/100] training 63.9% loss=0.10557, acc=0.96875
# [55/100] training 64.2% loss=0.12286, acc=0.95312
# [55/100] training 64.3% loss=0.19716, acc=0.92188
# [55/100] training 64.5% loss=0.24882, acc=0.90625
# [55/100] training 64.7% loss=0.15275, acc=0.92188
# [55/100] training 64.8% loss=0.30859, acc=0.82812
# [55/100] training 65.0% loss=0.21595, acc=0.90625
# [55/100] training 65.1% loss=0.23731, acc=0.92188
# [55/100] training 65.4% loss=0.16211, acc=0.96875
# [55/100] training 65.5% loss=0.09592, acc=0.96875
# [55/100] training 65.7% loss=0.21041, acc=0.93750
# [55/100] training 65.8% loss=0.22807, acc=0.90625
# [55/100] training 66.0% loss=0.13477, acc=0.96875
# [55/100] training 66.2% loss=0.04152, acc=1.00000
# [55/100] training 66.3% loss=0.22949, acc=0.92188
# [55/100] training 66.6% loss=0.12563, acc=0.93750
# [55/100] training 66.7% loss=0.08833, acc=0.98438
# [55/100] training 66.9% loss=0.29390, acc=0.89062
# [55/100] training 67.0% loss=0.15688, acc=0.90625
# [55/100] training 67.2% loss=0.10237, acc=0.96875
# [55/100] training 67.4% loss=0.10731, acc=0.95312
# [55/100] training 67.6% loss=0.10417, acc=0.96875
# [55/100] training 67.8% loss=0.12763, acc=0.96875
# [55/100] training 67.9% loss=0.14934, acc=0.93750
# [55/100] training 68.1% loss=0.08737, acc=0.96875
# [55/100] training 68.2% loss=0.08648, acc=0.96875
# [55/100] training 68.4% loss=0.09872, acc=0.96875
# [55/100] training 68.5% loss=0.15083, acc=0.90625
# [55/100] training 68.8% loss=0.15282, acc=0.93750
# [55/100] training 69.0% loss=0.20811, acc=0.93750
# [55/100] training 69.1% loss=0.07474, acc=0.96875
# [55/100] training 69.3% loss=0.08175, acc=0.96875
# [55/100] training 69.4% loss=0.10045, acc=0.95312
# [55/100] training 69.6% loss=0.11400, acc=0.93750
# [55/100] training 69.7% loss=0.23001, acc=0.93750
# [55/100] training 70.0% loss=0.15868, acc=0.89062
# [55/100] training 70.2% loss=0.28565, acc=0.90625
# [55/100] training 70.3% loss=0.18048, acc=0.95312
# [55/100] training 70.5% loss=0.15556, acc=0.95312
# [55/100] training 70.6% loss=0.08455, acc=0.98438
# [55/100] training 70.8% loss=0.15576, acc=0.95312
# [55/100] training 71.0% loss=0.19176, acc=0.92188
# [55/100] training 71.2% loss=0.19815, acc=0.90625
# [55/100] training 71.3% loss=0.11152, acc=0.98438
# [55/100] training 71.5% loss=0.14459, acc=0.95312
# [55/100] training 71.7% loss=0.18594, acc=0.92188
# [55/100] training 71.8% loss=0.27334, acc=0.90625
# [55/100] training 72.0% loss=0.11239, acc=0.95312
# [55/100] training 72.2% loss=0.13171, acc=0.93750
# [55/100] training 72.4% loss=0.26183, acc=0.92188
# [55/100] training 72.5% loss=0.18359, acc=0.92188
# [55/100] training 72.7% loss=0.22139, acc=0.92188
# [55/100] training 72.9% loss=0.14980, acc=0.96875
# [55/100] training 73.0% loss=0.14556, acc=0.95312
# [55/100] training 73.3% loss=0.28289, acc=0.89062
# [55/100] training 73.4% loss=0.10072, acc=0.95312
# [55/100] training 73.6% loss=0.15630, acc=0.93750
# [55/100] training 73.7% loss=0.13402, acc=0.95312
# [55/100] training 73.9% loss=0.12563, acc=0.95312
# [55/100] training 74.0% loss=0.11834, acc=0.92188
# [55/100] training 74.2% loss=0.17196, acc=0.92188
# [55/100] training 74.5% loss=0.07522, acc=0.96875
# [55/100] training 74.6% loss=0.41725, acc=0.85938
# [55/100] training 74.8% loss=0.19792, acc=0.95312
# [55/100] training 74.9% loss=0.20033, acc=0.90625
# [55/100] training 75.1% loss=0.12250, acc=0.96875
# [55/100] training 75.2% loss=0.15042, acc=0.93750
# [55/100] training 75.4% loss=0.15316, acc=0.92188
# [55/100] training 75.7% loss=0.09109, acc=0.96875
# [55/100] training 75.8% loss=0.17580, acc=0.95312
# [55/100] training 76.0% loss=0.22001, acc=0.89062
# [55/100] training 76.1% loss=0.28139, acc=0.87500
# [55/100] training 76.3% loss=0.08608, acc=0.95312
# [55/100] training 76.4% loss=0.25034, acc=0.90625
# [55/100] training 76.7% loss=0.13808, acc=0.96875
# [55/100] training 76.8% loss=0.19394, acc=0.92188
# [55/100] training 77.0% loss=0.13077, acc=0.95312
# [55/100] training 77.2% loss=0.16259, acc=0.95312
# [55/100] training 77.3% loss=0.10399, acc=0.93750
# [55/100] training 77.5% loss=0.27811, acc=0.89062
# [55/100] training 77.6% loss=0.19612, acc=0.89062
# [55/100] training 77.9% loss=0.14446, acc=0.93750
# [55/100] training 78.0% loss=0.22220, acc=0.89062
# [55/100] training 78.2% loss=0.21335, acc=0.89062
# [55/100] training 78.4% loss=0.14067, acc=0.95312
# [55/100] training 78.5% loss=0.26503, acc=0.90625
# [55/100] training 78.7% loss=0.25993, acc=0.87500
# [55/100] training 78.8% loss=0.10266, acc=0.95312
# [55/100] training 79.1% loss=0.10177, acc=0.98438
# [55/100] training 79.2% loss=0.14911, acc=0.96875
# [55/100] training 79.4% loss=0.17183, acc=0.93750
# [55/100] training 79.5% loss=0.13237, acc=0.95312
# [55/100] training 79.7% loss=0.06181, acc=1.00000
# [55/100] training 79.9% loss=0.12600, acc=0.95312
# [55/100] training 80.1% loss=0.13650, acc=0.95312
# [55/100] training 80.3% loss=0.26810, acc=0.89062
# [55/100] training 80.4% loss=0.15691, acc=0.95312
# [55/100] training 80.6% loss=0.24004, acc=0.90625
# [55/100] training 80.7% loss=0.10120, acc=0.96875
# [55/100] training 80.9% loss=0.14963, acc=0.95312
# [55/100] training 81.2% loss=0.30094, acc=0.89062
# [55/100] training 81.3% loss=0.17827, acc=0.95312
# [55/100] training 81.5% loss=0.23466, acc=0.92188
# [55/100] training 81.6% loss=0.24217, acc=0.87500
# [55/100] training 81.8% loss=0.18152, acc=0.90625
# [55/100] training 81.9% loss=0.35026, acc=0.90625
# [55/100] training 82.1% loss=0.16871, acc=0.93750
# [55/100] training 82.2% loss=0.14996, acc=0.95312
# [55/100] training 82.5% loss=0.16432, acc=0.95312
# [55/100] training 82.7% loss=0.21060, acc=0.93750
# [55/100] training 82.8% loss=0.16863, acc=0.96875
# [55/100] training 83.0% loss=0.18192, acc=0.95312
# [55/100] training 83.1% loss=0.12644, acc=0.96875
# [55/100] training 83.3% loss=0.11173, acc=0.98438
# [55/100] training 83.5% loss=0.12931, acc=0.96875
# [55/100] training 83.7% loss=0.32615, acc=0.92188
# [55/100] training 83.9% loss=0.17779, acc=0.89062
# [55/100] training 84.0% loss=0.13221, acc=0.95312
# [55/100] training 84.2% loss=0.06748, acc=0.95312
# [55/100] training 84.3% loss=0.28211, acc=0.87500
# [55/100] training 84.5% loss=0.11571, acc=0.93750
# [55/100] training 84.7% loss=0.13874, acc=0.93750
# [55/100] training 84.9% loss=0.11849, acc=0.93750
# [55/100] training 85.0% loss=0.25630, acc=0.89062
# [55/100] training 85.2% loss=0.14785, acc=0.93750
# [55/100] training 85.4% loss=0.10919, acc=0.93750
# [55/100] training 85.5% loss=0.24805, acc=0.89062
# [55/100] training 85.8% loss=0.16292, acc=0.93750
# [55/100] training 85.9% loss=0.12097, acc=0.93750
# [55/100] training 86.1% loss=0.16258, acc=0.93750
# [55/100] training 86.2% loss=0.12025, acc=0.93750
# [55/100] training 86.4% loss=0.31153, acc=0.89062
# [55/100] training 86.6% loss=0.19338, acc=0.92188
# [55/100] training 86.7% loss=0.14229, acc=0.92188
# [55/100] training 87.0% loss=0.20787, acc=0.87500
# [55/100] training 87.1% loss=0.18263, acc=0.92188
# [55/100] training 87.3% loss=0.15571, acc=0.93750
# [55/100] training 87.4% loss=0.24177, acc=0.90625
# [55/100] training 87.6% loss=0.18736, acc=0.93750
# [55/100] training 87.7% loss=0.32130, acc=0.92188
# [55/100] training 87.9% loss=0.21459, acc=0.93750
# [55/100] training 88.2% loss=0.12079, acc=0.95312
# [55/100] training 88.3% loss=0.17947, acc=0.96875
# [55/100] training 88.5% loss=0.22820, acc=0.90625
# [55/100] training 88.6% loss=0.09012, acc=0.98438
# [55/100] training 88.8% loss=0.15469, acc=0.93750
# [55/100] training 88.9% loss=0.31686, acc=0.87500
# [55/100] training 89.2% loss=0.12185, acc=0.95312
# [55/100] training 89.4% loss=0.19669, acc=0.95312
# [55/100] training 89.5% loss=0.18950, acc=0.93750
# [55/100] training 89.7% loss=0.20780, acc=0.90625
# [55/100] training 89.8% loss=0.10545, acc=0.96875
# [55/100] training 90.0% loss=0.10559, acc=0.98438
# [55/100] training 90.1% loss=0.13149, acc=0.96875
# [55/100] training 90.4% loss=0.16183, acc=0.92188
# [55/100] training 90.5% loss=0.08825, acc=0.96875
# [55/100] training 90.7% loss=0.09417, acc=0.98438
# [55/100] training 90.9% loss=0.15230, acc=0.92188
# [55/100] training 91.0% loss=0.19796, acc=0.90625
# [55/100] training 91.2% loss=0.16688, acc=0.92188
# [55/100] training 91.3% loss=0.32834, acc=0.85938
# [55/100] training 91.6% loss=0.30037, acc=0.85938
# [55/100] training 91.7% loss=0.15980, acc=0.93750
# [55/100] training 91.9% loss=0.33377, acc=0.87500
# [55/100] training 92.1% loss=0.18722, acc=0.90625
# [55/100] training 92.2% loss=0.14930, acc=0.95312
# [55/100] training 92.4% loss=0.23024, acc=0.92188
# [55/100] training 92.6% loss=0.31211, acc=0.85938
# [55/100] training 92.8% loss=0.15593, acc=0.92188
# [55/100] training 92.9% loss=0.20704, acc=0.92188
# [55/100] training 93.1% loss=0.44663, acc=0.89062
# [55/100] training 93.2% loss=0.24033, acc=0.90625
# [55/100] training 93.4% loss=0.24868, acc=0.89062
# [55/100] training 93.7% loss=0.11899, acc=0.95312
# [55/100] training 93.8% loss=0.09557, acc=0.98438
# [55/100] training 94.0% loss=0.17152, acc=0.95312
# [55/100] training 94.1% loss=0.15429, acc=0.93750
# [55/100] training 94.3% loss=0.09986, acc=0.96875
# [55/100] training 94.4% loss=0.20775, acc=0.92188
# [55/100] training 94.6% loss=0.11971, acc=0.93750
# [55/100] training 94.9% loss=0.20620, acc=0.92188
# [55/100] training 95.0% loss=0.23422, acc=0.92188
# [55/100] training 95.2% loss=0.32674, acc=0.85938
# [55/100] training 95.3% loss=0.20528, acc=0.92188
# [55/100] training 95.5% loss=0.08579, acc=0.95312
# [55/100] training 95.6% loss=0.23991, acc=0.93750
# [55/100] training 95.8% loss=0.13279, acc=0.93750
# [55/100] training 96.0% loss=0.15918, acc=0.92188
# [55/100] training 96.2% loss=0.09110, acc=0.98438
# [55/100] training 96.4% loss=0.17065, acc=0.98438
# [55/100] training 96.5% loss=0.19817, acc=0.92188
# [55/100] training 96.7% loss=0.16507, acc=0.93750
# [55/100] training 96.8% loss=0.23320, acc=0.90625
# [55/100] training 97.1% loss=0.16655, acc=0.92188
# [55/100] training 97.2% loss=0.25417, acc=0.89062
# [55/100] training 97.4% loss=0.13889, acc=0.95312
# [55/100] training 97.6% loss=0.16721, acc=0.87500
# [55/100] training 97.7% loss=0.15471, acc=0.92188
# [55/100] training 97.9% loss=0.13292, acc=0.95312
# [55/100] training 98.0% loss=0.12811, acc=0.92188
# [55/100] training 98.3% loss=0.28699, acc=0.89062
# [55/100] training 98.4% loss=0.16296, acc=0.92188
# [55/100] training 98.6% loss=0.24352, acc=0.87500
# [55/100] training 98.7% loss=0.34812, acc=0.90625
# [55/100] training 98.9% loss=0.13156, acc=0.95312
# [55/100] training 99.1% loss=0.17641, acc=0.92188
# [55/100] training 99.2% loss=0.15614, acc=0.90625
# [55/100] training 99.5% loss=0.27458, acc=0.87500
# [55/100] training 99.6% loss=0.14623, acc=0.95312
# [55/100] training 99.8% loss=0.11176, acc=0.96875
# [55/100] training 99.9% loss=0.05921, acc=0.98438
# [55/100] testing 0.9% loss=0.17747, acc=0.93750
# [55/100] testing 1.8% loss=0.32198, acc=0.85938
# [55/100] testing 2.2% loss=0.23803, acc=0.92188
# [55/100] testing 3.1% loss=0.30461, acc=0.85938
# [55/100] testing 3.5% loss=0.13364, acc=0.93750
# [55/100] testing 4.4% loss=0.25234, acc=0.89062
# [55/100] testing 4.8% loss=0.43225, acc=0.84375
# [55/100] testing 5.7% loss=0.26549, acc=0.87500
# [55/100] testing 6.6% loss=0.12515, acc=0.95312
# [55/100] testing 7.0% loss=0.11162, acc=0.95312
# [55/100] testing 7.9% loss=0.28242, acc=0.95312
# [55/100] testing 8.3% loss=0.27632, acc=0.92188
# [55/100] testing 9.2% loss=0.29026, acc=0.89062
# [55/100] testing 9.7% loss=0.07644, acc=0.96875
# [55/100] testing 10.5% loss=0.24066, acc=0.90625
# [55/100] testing 11.0% loss=0.22278, acc=0.90625
# [55/100] testing 11.8% loss=0.16574, acc=0.98438
# [55/100] testing 12.7% loss=0.36152, acc=0.89062
# [55/100] testing 13.2% loss=0.23804, acc=0.89062
# [55/100] testing 14.0% loss=0.36806, acc=0.89062
# [55/100] testing 14.5% loss=0.24127, acc=0.90625
# [55/100] testing 15.4% loss=0.24042, acc=0.93750
# [55/100] testing 15.8% loss=0.15939, acc=0.90625
# [55/100] testing 16.7% loss=0.26597, acc=0.89062
# [55/100] testing 17.5% loss=0.22649, acc=0.92188
# [55/100] testing 18.0% loss=0.19538, acc=0.92188
# [55/100] testing 18.9% loss=0.09592, acc=0.95312
# [55/100] testing 19.3% loss=0.27170, acc=0.90625
# [55/100] testing 20.2% loss=0.28774, acc=0.92188
# [55/100] testing 20.6% loss=0.24279, acc=0.93750
# [55/100] testing 21.5% loss=0.16313, acc=0.93750
# [55/100] testing 21.9% loss=0.39294, acc=0.85938
# [55/100] testing 22.8% loss=0.33906, acc=0.93750
# [55/100] testing 23.7% loss=0.42094, acc=0.85938
# [55/100] testing 24.1% loss=0.20840, acc=0.95312
# [55/100] testing 25.0% loss=0.29977, acc=0.90625
# [55/100] testing 25.4% loss=0.10941, acc=0.96875
# [55/100] testing 26.3% loss=0.26962, acc=0.87500
# [55/100] testing 26.8% loss=0.32923, acc=0.89062
# [55/100] testing 27.6% loss=0.13307, acc=0.92188
# [55/100] testing 28.5% loss=0.22916, acc=0.92188
# [55/100] testing 29.0% loss=0.18733, acc=0.95312
# [55/100] testing 29.8% loss=0.31699, acc=0.92188
# [55/100] testing 30.3% loss=0.28783, acc=0.93750
# [55/100] testing 31.1% loss=0.33013, acc=0.85938
# [55/100] testing 31.6% loss=0.21887, acc=0.92188
# [55/100] testing 32.5% loss=0.31570, acc=0.89062
# [55/100] testing 32.9% loss=0.39268, acc=0.89062
# [55/100] testing 33.8% loss=0.30939, acc=0.87500
# [55/100] testing 34.7% loss=0.39514, acc=0.85938
# [55/100] testing 35.1% loss=0.11851, acc=0.96875
# [55/100] testing 36.0% loss=0.30739, acc=0.87500
# [55/100] testing 36.4% loss=0.25468, acc=0.90625
# [55/100] testing 37.3% loss=0.20201, acc=0.93750
# [55/100] testing 37.7% loss=0.45220, acc=0.82812
# [55/100] testing 38.6% loss=0.15472, acc=0.95312
# [55/100] testing 39.5% loss=0.29902, acc=0.92188
# [55/100] testing 39.9% loss=0.29071, acc=0.89062
# [55/100] testing 40.8% loss=0.32154, acc=0.90625
# [55/100] testing 41.2% loss=0.23982, acc=0.96875
# [55/100] testing 42.1% loss=0.31168, acc=0.90625
# [55/100] testing 42.5% loss=0.16609, acc=0.92188
# [55/100] testing 43.4% loss=0.31266, acc=0.87500
# [55/100] testing 43.9% loss=0.06823, acc=0.98438
# [55/100] testing 44.7% loss=0.25613, acc=0.92188
# [55/100] testing 45.6% loss=0.26401, acc=0.90625
# [55/100] testing 46.1% loss=0.22092, acc=0.89062
# [55/100] testing 46.9% loss=0.17351, acc=0.93750
# [55/100] testing 47.4% loss=0.16236, acc=0.95312
# [55/100] testing 48.3% loss=0.41251, acc=0.87500
# [55/100] testing 48.7% loss=0.32241, acc=0.87500
# [55/100] testing 49.6% loss=0.43554, acc=0.82812
# [55/100] testing 50.4% loss=0.11790, acc=0.95312
# [55/100] testing 50.9% loss=0.34292, acc=0.89062
# [55/100] testing 51.8% loss=0.21071, acc=0.92188
# [55/100] testing 52.2% loss=0.16807, acc=0.93750
# [55/100] testing 53.1% loss=0.18218, acc=0.92188
# [55/100] testing 53.5% loss=0.16767, acc=0.93750
# [55/100] testing 54.4% loss=0.30242, acc=0.89062
# [55/100] testing 54.8% loss=0.30239, acc=0.89062
# [55/100] testing 55.7% loss=0.10815, acc=0.95312
# [55/100] testing 56.6% loss=0.40456, acc=0.87500
# [55/100] testing 57.0% loss=0.33985, acc=0.92188
# [55/100] testing 57.9% loss=0.30883, acc=0.89062
# [55/100] testing 58.3% loss=0.27805, acc=0.92188
# [55/100] testing 59.2% loss=0.22389, acc=0.90625
# [55/100] testing 59.7% loss=0.19281, acc=0.90625
# [55/100] testing 60.5% loss=0.49231, acc=0.82812
# [55/100] testing 61.4% loss=0.14877, acc=0.95312
# [55/100] testing 61.9% loss=0.15794, acc=0.90625
# [55/100] testing 62.7% loss=0.20667, acc=0.93750
# [55/100] testing 63.2% loss=0.40528, acc=0.85938
# [55/100] testing 64.0% loss=0.36904, acc=0.90625
# [55/100] testing 64.5% loss=0.15246, acc=0.93750
# [55/100] testing 65.4% loss=0.11034, acc=0.95312
# [55/100] testing 65.8% loss=0.27851, acc=0.89062
# [55/100] testing 66.7% loss=0.20099, acc=0.90625
# [55/100] testing 67.6% loss=0.43834, acc=0.87500
# [55/100] testing 68.0% loss=0.18054, acc=0.95312
# [55/100] testing 68.9% loss=0.24454, acc=0.90625
# [55/100] testing 69.3% loss=0.22724, acc=0.87500
# [55/100] testing 70.2% loss=0.38560, acc=0.85938
# [55/100] testing 70.6% loss=0.23971, acc=0.89062
# [55/100] testing 71.5% loss=0.21312, acc=0.93750
# [55/100] testing 72.4% loss=0.14651, acc=0.93750
# [55/100] testing 72.8% loss=0.16179, acc=0.92188
# [55/100] testing 73.7% loss=0.14512, acc=0.95312
# [55/100] testing 74.1% loss=0.37804, acc=0.87500
# [55/100] testing 75.0% loss=0.21732, acc=0.90625
# [55/100] testing 75.4% loss=0.43479, acc=0.89062
# [55/100] testing 76.3% loss=0.04464, acc=0.98438
# [55/100] testing 76.8% loss=0.19382, acc=0.90625
# [55/100] testing 77.6% loss=0.13307, acc=0.95312
# [55/100] testing 78.5% loss=0.26934, acc=0.85938
# [55/100] testing 79.0% loss=0.19184, acc=0.90625
# [55/100] testing 79.8% loss=0.18962, acc=0.95312
# [55/100] testing 80.3% loss=0.29990, acc=0.90625
# [55/100] testing 81.2% loss=0.44174, acc=0.85938
# [55/100] testing 81.6% loss=0.22347, acc=0.90625
# [55/100] testing 82.5% loss=0.19846, acc=0.89062
# [55/100] testing 83.3% loss=0.18902, acc=0.92188
# [55/100] testing 83.8% loss=0.14356, acc=0.95312
# [55/100] testing 84.7% loss=0.24714, acc=0.89062
# [55/100] testing 85.1% loss=0.22601, acc=0.90625
# [55/100] testing 86.0% loss=0.29280, acc=0.92188
# [55/100] testing 86.4% loss=0.37875, acc=0.84375
# [55/100] testing 87.3% loss=0.26681, acc=0.87500
# [55/100] testing 87.7% loss=0.23133, acc=0.90625
# [55/100] testing 88.6% loss=0.21668, acc=0.90625
# [55/100] testing 89.5% loss=0.58438, acc=0.78125
# [55/100] testing 89.9% loss=0.11560, acc=0.96875
# [55/100] testing 90.8% loss=0.24172, acc=0.95312
# [55/100] testing 91.2% loss=0.15144, acc=0.93750
# [55/100] testing 92.1% loss=0.27844, acc=0.90625
# [55/100] testing 92.6% loss=0.33002, acc=0.87500
# [55/100] testing 93.4% loss=0.27351, acc=0.87500
# [55/100] testing 94.3% loss=0.10437, acc=0.96875
# [55/100] testing 94.7% loss=0.19328, acc=0.90625
# [55/100] testing 95.6% loss=0.50425, acc=0.81250
# [55/100] testing 96.1% loss=0.20886, acc=0.92188
# [55/100] testing 96.9% loss=0.24269, acc=0.90625
# [55/100] testing 97.4% loss=0.15961, acc=0.95312
# [55/100] testing 98.3% loss=0.13444, acc=0.93750
# [55/100] testing 98.7% loss=0.21256, acc=0.92188
# [55/100] testing 99.6% loss=0.25663, acc=0.90625
# [56/100] training 0.2% loss=0.30124, acc=0.89062
# [56/100] training 0.4% loss=0.31432, acc=0.89062
# [56/100] training 0.5% loss=0.08576, acc=0.96875
# [56/100] training 0.8% loss=0.13373, acc=0.95312
# [56/100] training 0.9% loss=0.15669, acc=0.93750
# [56/100] training 1.1% loss=0.26783, acc=0.96875
# [56/100] training 1.2% loss=0.21299, acc=0.90625
# [56/100] training 1.4% loss=0.13002, acc=0.92188
# [56/100] training 1.6% loss=0.12069, acc=0.95312
# [56/100] training 1.8% loss=0.16485, acc=0.90625
# [56/100] training 2.0% loss=0.28848, acc=0.82812
# [56/100] training 2.1% loss=0.19183, acc=0.92188
# [56/100] training 2.3% loss=0.12219, acc=0.93750
# [56/100] training 2.4% loss=0.20195, acc=0.93750
# [56/100] training 2.6% loss=0.09866, acc=0.96875
# [56/100] training 2.7% loss=0.14696, acc=0.95312
# [56/100] training 3.0% loss=0.14416, acc=0.93750
# [56/100] training 3.2% loss=0.10251, acc=0.96875
# [56/100] training 3.3% loss=0.30093, acc=0.92188
# [56/100] training 3.5% loss=0.22850, acc=0.89062
# [56/100] training 3.6% loss=0.29932, acc=0.85938
# [56/100] training 3.8% loss=0.22434, acc=0.93750
# [56/100] training 3.9% loss=0.19007, acc=0.93750
# [56/100] training 4.2% loss=0.06767, acc=1.00000
# [56/100] training 4.4% loss=0.10738, acc=0.96875
# [56/100] training 4.5% loss=0.10477, acc=0.95312
# [56/100] training 4.7% loss=0.40614, acc=0.84375
# [56/100] training 4.8% loss=0.25421, acc=0.89062
# [56/100] training 5.0% loss=0.03368, acc=1.00000
# [56/100] training 5.2% loss=0.12022, acc=0.95312
# [56/100] training 5.4% loss=0.06228, acc=0.98438
# [56/100] training 5.5% loss=0.25701, acc=0.84375
# [56/100] training 5.7% loss=0.19084, acc=0.93750
# [56/100] training 5.9% loss=0.14580, acc=0.95312
# [56/100] training 6.0% loss=0.24287, acc=0.89062
# [56/100] training 6.3% loss=0.18234, acc=0.90625
# [56/100] training 6.4% loss=0.16881, acc=0.89062
# [56/100] training 6.6% loss=0.17143, acc=0.89062
# [56/100] training 6.7% loss=0.25316, acc=0.89062
# [56/100] training 6.9% loss=0.08489, acc=0.96875
# [56/100] training 7.1% loss=0.13028, acc=0.93750
# [56/100] training 7.2% loss=0.23617, acc=0.92188
# [56/100] training 7.5% loss=0.15452, acc=0.92188
# [56/100] training 7.6% loss=0.22459, acc=0.90625
# [56/100] training 7.8% loss=0.09618, acc=0.95312
# [56/100] training 7.9% loss=0.24534, acc=0.90625
# [56/100] training 8.1% loss=0.08556, acc=0.98438
# [56/100] training 8.2% loss=0.20557, acc=0.95312
# [56/100] training 8.4% loss=0.20093, acc=0.89062
# [56/100] training 8.7% loss=0.17411, acc=0.92188
# [56/100] training 8.8% loss=0.18170, acc=0.90625
# [56/100] training 9.0% loss=0.27185, acc=0.90625
# [56/100] training 9.1% loss=0.16023, acc=0.92188
# [56/100] training 9.3% loss=0.31706, acc=0.90625
# [56/100] training 9.4% loss=0.10407, acc=0.95312
# [56/100] training 9.7% loss=0.16775, acc=0.95312
# [56/100] training 9.9% loss=0.28029, acc=0.87500
# [56/100] training 10.0% loss=0.25343, acc=0.87500
# [56/100] training 10.2% loss=0.09995, acc=0.96875
# [56/100] training 10.3% loss=0.14593, acc=0.93750
# [56/100] training 10.5% loss=0.25996, acc=0.90625
# [56/100] training 10.6% loss=0.17898, acc=0.90625
# [56/100] training 10.9% loss=0.12329, acc=0.95312
# [56/100] training 11.0% loss=0.28855, acc=0.92188
# [56/100] training 11.2% loss=0.11181, acc=0.96875
# [56/100] training 11.4% loss=0.14219, acc=0.92188
# [56/100] training 11.5% loss=0.32403, acc=0.87500
# [56/100] training 11.7% loss=0.09158, acc=0.98438
# [56/100] training 11.8% loss=0.11338, acc=0.96875
# [56/100] training 12.1% loss=0.17543, acc=0.93750
# [56/100] training 12.2% loss=0.08202, acc=0.96875
# [56/100] training 12.4% loss=0.12427, acc=0.92188
# [56/100] training 12.6% loss=0.24339, acc=0.92188
# [56/100] training 12.7% loss=0.19316, acc=0.92188
# [56/100] training 12.9% loss=0.17304, acc=0.92188
# [56/100] training 13.0% loss=0.20985, acc=0.93750
# [56/100] training 13.3% loss=0.12386, acc=0.95312
# [56/100] training 13.4% loss=0.24790, acc=0.87500
# [56/100] training 13.6% loss=0.13341, acc=0.93750
# [56/100] training 13.7% loss=0.30271, acc=0.89062
# [56/100] training 13.9% loss=0.27844, acc=0.90625
# [56/100] training 14.1% loss=0.28865, acc=0.89062
# [56/100] training 14.3% loss=0.09146, acc=0.95312
# [56/100] training 14.5% loss=0.27163, acc=0.89062
# [56/100] training 14.6% loss=0.30218, acc=0.90625
# [56/100] training 14.8% loss=0.20693, acc=0.89062
# [56/100] training 14.9% loss=0.12839, acc=0.95312
# [56/100] training 15.1% loss=0.24088, acc=0.89062
# [56/100] training 15.4% loss=0.22147, acc=0.89062
# [56/100] training 15.5% loss=0.13938, acc=0.93750
# [56/100] training 15.7% loss=0.23610, acc=0.92188
# [56/100] training 15.8% loss=0.09528, acc=0.95312
# [56/100] training 16.0% loss=0.20222, acc=0.90625
# [56/100] training 16.1% loss=0.31635, acc=0.92188
# [56/100] training 16.3% loss=0.17453, acc=0.95312
# [56/100] training 16.4% loss=0.19088, acc=0.93750
# [56/100] training 16.7% loss=0.21687, acc=0.90625
# [56/100] training 16.9% loss=0.18097, acc=0.92188
# [56/100] training 17.0% loss=0.15244, acc=0.92188
# [56/100] training 17.2% loss=0.12463, acc=0.93750
# [56/100] training 17.3% loss=0.11731, acc=0.93750
# [56/100] training 17.5% loss=0.22503, acc=0.95312
# [56/100] training 17.7% loss=0.14398, acc=0.93750
# [56/100] training 17.9% loss=0.26290, acc=0.92188
# [56/100] training 18.1% loss=0.17326, acc=0.92188
# [56/100] training 18.2% loss=0.20499, acc=0.90625
# [56/100] training 18.4% loss=0.25447, acc=0.89062
# [56/100] training 18.5% loss=0.14908, acc=0.96875
# [56/100] training 18.8% loss=0.13496, acc=0.95312
# [56/100] training 18.9% loss=0.07214, acc=0.96875
# [56/100] training 19.1% loss=0.15383, acc=0.93750
# [56/100] training 19.2% loss=0.08733, acc=0.96875
# [56/100] training 19.4% loss=0.06907, acc=0.96875
# [56/100] training 19.6% loss=0.16870, acc=0.92188
# [56/100] training 19.7% loss=0.19024, acc=0.89062
# [56/100] training 20.0% loss=0.11272, acc=0.95312
# [56/100] training 20.1% loss=0.24081, acc=0.92188
# [56/100] training 20.3% loss=0.19117, acc=0.87500
# [56/100] training 20.4% loss=0.20435, acc=0.93750
# [56/100] training 20.6% loss=0.25974, acc=0.92188
# [56/100] training 20.8% loss=0.10159, acc=0.96875
# [56/100] training 20.9% loss=0.16960, acc=0.90625
# [56/100] training 21.2% loss=0.22202, acc=0.95312
# [56/100] training 21.3% loss=0.23528, acc=0.92188
# [56/100] training 21.5% loss=0.32489, acc=0.89062
# [56/100] training 21.6% loss=0.09637, acc=0.96875
# [56/100] training 21.8% loss=0.11275, acc=0.96875
# [56/100] training 21.9% loss=0.23326, acc=0.89062
# [56/100] training 22.2% loss=0.16549, acc=0.90625
# [56/100] training 22.4% loss=0.21390, acc=0.92188
# [56/100] training 22.5% loss=0.12831, acc=0.96875
# [56/100] training 22.7% loss=0.18056, acc=0.92188
# [56/100] training 22.8% loss=0.20183, acc=0.92188
# [56/100] training 23.0% loss=0.08831, acc=0.96875
# [56/100] training 23.1% loss=0.27289, acc=0.92188
# [56/100] training 23.4% loss=0.22586, acc=0.87500
# [56/100] training 23.6% loss=0.21029, acc=0.90625
# [56/100] training 23.7% loss=0.21033, acc=0.89062
# [56/100] training 23.9% loss=0.17421, acc=0.93750
# [56/100] training 24.0% loss=0.11345, acc=0.95312
# [56/100] training 24.2% loss=0.15032, acc=0.92188
# [56/100] training 24.3% loss=0.14448, acc=0.92188
# [56/100] training 24.6% loss=0.23268, acc=0.92188
# [56/100] training 24.7% loss=0.25958, acc=0.90625
# [56/100] training 24.9% loss=0.20695, acc=0.95312
# [56/100] training 25.1% loss=0.19196, acc=0.92188
# [56/100] training 25.2% loss=0.12471, acc=0.96875
# [56/100] training 25.4% loss=0.15617, acc=0.95312
# [56/100] training 25.6% loss=0.10422, acc=0.98438
# [56/100] training 25.8% loss=0.25765, acc=0.89062
# [56/100] training 25.9% loss=0.19382, acc=0.92188
# [56/100] training 26.1% loss=0.20777, acc=0.90625
# [56/100] training 26.3% loss=0.14185, acc=0.95312
# [56/100] training 26.4% loss=0.13866, acc=0.93750
# [56/100] training 26.6% loss=0.04890, acc=1.00000
# [56/100] training 26.8% loss=0.10796, acc=0.95312
# [56/100] training 27.0% loss=0.21330, acc=0.92188
# [56/100] training 27.1% loss=0.13047, acc=0.93750
# [56/100] training 27.3% loss=0.11631, acc=0.92188
# [56/100] training 27.4% loss=0.20761, acc=0.93750
# [56/100] training 27.6% loss=0.25762, acc=0.92188
# [56/100] training 27.9% loss=0.09443, acc=0.98438
# [56/100] training 28.0% loss=0.28439, acc=0.87500
# [56/100] training 28.2% loss=0.15675, acc=0.93750
# [56/100] training 28.3% loss=0.05636, acc=0.98438
# [56/100] training 28.5% loss=0.16859, acc=0.92188
# [56/100] training 28.6% loss=0.18146, acc=0.90625
# [56/100] training 28.8% loss=0.12348, acc=0.98438
# [56/100] training 29.1% loss=0.10046, acc=0.95312
# [56/100] training 29.2% loss=0.12258, acc=0.95312
# [56/100] training 29.4% loss=0.26148, acc=0.85938
# [56/100] training 29.5% loss=0.13040, acc=0.95312
# [56/100] training 29.7% loss=0.19900, acc=0.93750
# [56/100] training 29.8% loss=0.16441, acc=0.93750
# [56/100] training 30.0% loss=0.26643, acc=0.89062
# [56/100] training 30.2% loss=0.07522, acc=0.96875
# [56/100] training 30.4% loss=0.10115, acc=0.96875
# [56/100] training 30.6% loss=0.16173, acc=0.93750
# [56/100] training 30.7% loss=0.22442, acc=0.93750
# [56/100] training 30.9% loss=0.27336, acc=0.92188
# [56/100] training 31.0% loss=0.10830, acc=0.95312
# [56/100] training 31.3% loss=0.12092, acc=0.95312
# [56/100] training 31.4% loss=0.26187, acc=0.89062
# [56/100] training 31.6% loss=0.19146, acc=0.92188
# [56/100] training 31.8% loss=0.16104, acc=0.95312
# [56/100] training 31.9% loss=0.20477, acc=0.93750
# [56/100] training 32.1% loss=0.14724, acc=0.95312
# [56/100] training 32.2% loss=0.18084, acc=0.95312
# [56/100] training 32.5% loss=0.11309, acc=0.93750
# [56/100] training 32.6% loss=0.11453, acc=0.93750
# [56/100] training 32.8% loss=0.19247, acc=0.93750
# [56/100] training 32.9% loss=0.18162, acc=0.93750
# [56/100] training 33.1% loss=0.12815, acc=0.95312
# [56/100] training 33.3% loss=0.16857, acc=0.95312
# [56/100] training 33.4% loss=0.18290, acc=0.93750
# [56/100] training 33.7% loss=0.17938, acc=0.90625
# [56/100] training 33.8% loss=0.21446, acc=0.93750
# [56/100] training 34.0% loss=0.19146, acc=0.92188
# [56/100] training 34.1% loss=0.12182, acc=0.93750
# [56/100] training 34.3% loss=0.17545, acc=0.93750
# [56/100] training 34.5% loss=0.16030, acc=0.95312
# [56/100] training 34.7% loss=0.18576, acc=0.92188
# [56/100] training 34.9% loss=0.17465, acc=0.93750
# [56/100] training 35.0% loss=0.19506, acc=0.92188
# [56/100] training 35.2% loss=0.19112, acc=0.90625
# [56/100] training 35.3% loss=0.15285, acc=0.92188
# [56/100] training 35.5% loss=0.19552, acc=0.93750
# [56/100] training 35.6% loss=0.26322, acc=0.92188
# [56/100] training 35.9% loss=0.21968, acc=0.90625
# [56/100] training 36.1% loss=0.31754, acc=0.89062
# [56/100] training 36.2% loss=0.23164, acc=0.90625
# [56/100] training 36.4% loss=0.27526, acc=0.87500
# [56/100] training 36.5% loss=0.24082, acc=0.90625
# [56/100] training 36.7% loss=0.18060, acc=0.93750
# [56/100] training 36.8% loss=0.11020, acc=0.96875
# [56/100] training 37.1% loss=0.20894, acc=0.90625
# [56/100] training 37.3% loss=0.13739, acc=0.96875
# [56/100] training 37.4% loss=0.12702, acc=0.92188
# [56/100] training 37.6% loss=0.11336, acc=0.95312
# [56/100] training 37.7% loss=0.22955, acc=0.92188
# [56/100] training 37.9% loss=0.19678, acc=0.92188
# [56/100] training 38.1% loss=0.26985, acc=0.89062
# [56/100] training 38.3% loss=0.10362, acc=0.95312
# [56/100] training 38.4% loss=0.07251, acc=0.98438
# [56/100] training 38.6% loss=0.16103, acc=0.90625
# [56/100] training 38.8% loss=0.20334, acc=0.89062
# [56/100] training 38.9% loss=0.11499, acc=0.93750
# [56/100] training 39.1% loss=0.19029, acc=0.92188
# [56/100] training 39.3% loss=0.34583, acc=0.89062
# [56/100] training 39.5% loss=0.28083, acc=0.89062
# [56/100] training 39.6% loss=0.07289, acc=0.98438
# [56/100] training 39.8% loss=0.05634, acc=1.00000
# [56/100] training 40.0% loss=0.16639, acc=0.95312
# [56/100] training 40.1% loss=0.42571, acc=0.89062
# [56/100] training 40.4% loss=0.14880, acc=0.95312
# [56/100] training 40.5% loss=0.26113, acc=0.92188
# [56/100] training 40.7% loss=0.14437, acc=0.96875
# [56/100] training 40.8% loss=0.08483, acc=0.96875
# [56/100] training 41.0% loss=0.12840, acc=0.95312
# [56/100] training 41.1% loss=0.10653, acc=0.96875
# [56/100] training 41.3% loss=0.11146, acc=0.95312
# [56/100] training 41.6% loss=0.24174, acc=0.92188
# [56/100] training 41.7% loss=0.26287, acc=0.90625
# [56/100] training 41.9% loss=0.12411, acc=0.95312
# [56/100] training 42.0% loss=0.13816, acc=0.95312
# [56/100] training 42.2% loss=0.21175, acc=0.89062
# [56/100] training 42.3% loss=0.15408, acc=0.92188
# [56/100] training 42.5% loss=0.09297, acc=0.95312
# [56/100] training 42.8% loss=0.10071, acc=0.98438
# [56/100] training 42.9% loss=0.11079, acc=0.98438
# [56/100] training 43.1% loss=0.18231, acc=0.96875
# [56/100] training 43.2% loss=0.07823, acc=0.96875
# [56/100] training 43.4% loss=0.19221, acc=0.93750
# [56/100] training 43.5% loss=0.17867, acc=0.92188
# [56/100] training 43.8% loss=0.18700, acc=0.90625
# [56/100] training 43.9% loss=0.14676, acc=0.95312
# [56/100] training 44.1% loss=0.26544, acc=0.93750
# [56/100] training 44.3% loss=0.21730, acc=0.92188
# [56/100] training 44.4% loss=0.28479, acc=0.87500
# [56/100] training 44.6% loss=0.20617, acc=0.92188
# [56/100] training 44.7% loss=0.19642, acc=0.93750
# [56/100] training 45.0% loss=0.11970, acc=0.93750
# [56/100] training 45.1% loss=0.24768, acc=0.85938
# [56/100] training 45.3% loss=0.20323, acc=0.90625
# [56/100] training 45.5% loss=0.03560, acc=1.00000
# [56/100] training 45.6% loss=0.20005, acc=0.90625
# [56/100] training 45.8% loss=0.07272, acc=0.96875
# [56/100] training 45.9% loss=0.15030, acc=0.93750
# [56/100] training 46.2% loss=0.04559, acc=0.98438
# [56/100] training 46.3% loss=0.10756, acc=0.96875
# [56/100] training 46.5% loss=0.29357, acc=0.92188
# [56/100] training 46.6% loss=0.25911, acc=0.93750
# [56/100] training 46.8% loss=0.10797, acc=0.95312
# [56/100] training 47.0% loss=0.16367, acc=0.92188
# [56/100] training 47.2% loss=0.15153, acc=0.93750
# [56/100] training 47.4% loss=0.18459, acc=0.92188
# [56/100] training 47.5% loss=0.23116, acc=0.89062
# [56/100] training 47.7% loss=0.13011, acc=0.95312
# [56/100] training 47.8% loss=0.21272, acc=0.93750
# [56/100] training 48.0% loss=0.15494, acc=0.92188
# [56/100] training 48.3% loss=0.07552, acc=0.98438
# [56/100] training 48.4% loss=0.04009, acc=1.00000
# [56/100] training 48.6% loss=0.21348, acc=0.92188
# [56/100] training 48.7% loss=0.22141, acc=0.89062
# [56/100] training 48.9% loss=0.15924, acc=0.93750
# [56/100] training 49.0% loss=0.05690, acc=0.98438
# [56/100] training 49.2% loss=0.19410, acc=0.90625
# [56/100] training 49.3% loss=0.14611, acc=0.93750
# [56/100] training 49.6% loss=0.28549, acc=0.89062
# [56/100] training 49.8% loss=0.19890, acc=0.93750
# [56/100] training 49.9% loss=0.21430, acc=0.89062
# [56/100] training 50.1% loss=0.11749, acc=0.93750
# [56/100] training 50.2% loss=0.20323, acc=0.92188
# [56/100] training 50.4% loss=0.40402, acc=0.89062
# [56/100] training 50.6% loss=0.14373, acc=0.95312
# [56/100] training 50.8% loss=0.22251, acc=0.89062
# [56/100] training 51.0% loss=0.24253, acc=0.90625
# [56/100] training 51.1% loss=0.16793, acc=0.93750
# [56/100] training 51.3% loss=0.22537, acc=0.90625
# [56/100] training 51.4% loss=0.17058, acc=0.95312
# [56/100] training 51.7% loss=0.12083, acc=0.95312
# [56/100] training 51.8% loss=0.17718, acc=0.92188
# [56/100] training 52.0% loss=0.11124, acc=0.96875
# [56/100] training 52.1% loss=0.15376, acc=0.93750
# [56/100] training 52.3% loss=0.22728, acc=0.92188
# [56/100] training 52.5% loss=0.05314, acc=0.98438
# [56/100] training 52.6% loss=0.15401, acc=0.92188
# [56/100] training 52.9% loss=0.16459, acc=0.95312
# [56/100] training 53.0% loss=0.15670, acc=0.92188
# [56/100] training 53.2% loss=0.24645, acc=0.95312
# [56/100] training 53.3% loss=0.09488, acc=0.96875
# [56/100] training 53.5% loss=0.20122, acc=0.90625
# [56/100] training 53.7% loss=0.05005, acc=1.00000
# [56/100] training 53.8% loss=0.34922, acc=0.85938
# [56/100] training 54.1% loss=0.24444, acc=0.89062
# [56/100] training 54.2% loss=0.08335, acc=0.98438
# [56/100] training 54.4% loss=0.20451, acc=0.93750
# [56/100] training 54.5% loss=0.31751, acc=0.87500
# [56/100] training 54.7% loss=0.26765, acc=0.92188
# [56/100] training 54.8% loss=0.16635, acc=0.92188
# [56/100] training 55.1% loss=0.20696, acc=0.89062
# [56/100] training 55.3% loss=0.12928, acc=0.95312
# [56/100] training 55.4% loss=0.12658, acc=0.93750
# [56/100] training 55.6% loss=0.18659, acc=0.93750
# [56/100] training 55.7% loss=0.07614, acc=0.96875
# [56/100] training 55.9% loss=0.11582, acc=0.92188
# [56/100] training 56.0% loss=0.10726, acc=0.95312
# [56/100] training 56.3% loss=0.39918, acc=0.89062
# [56/100] training 56.5% loss=0.18966, acc=0.90625
# [56/100] training 56.6% loss=0.48738, acc=0.79688
# [56/100] training 56.8% loss=0.09195, acc=0.96875
# [56/100] training 56.9% loss=0.10239, acc=0.95312
# [56/100] training 57.1% loss=0.18743, acc=0.90625
# [56/100] training 57.2% loss=0.20515, acc=0.90625
# [56/100] training 57.5% loss=0.16202, acc=0.90625
# [56/100] training 57.6% loss=0.17454, acc=0.92188
# [56/100] training 57.8% loss=0.18812, acc=0.89062
# [56/100] training 58.0% loss=0.14065, acc=0.93750
# [56/100] training 58.1% loss=0.16300, acc=0.93750
# [56/100] training 58.3% loss=0.11384, acc=0.92188
# [56/100] training 58.4% loss=0.20513, acc=0.92188
# [56/100] training 58.7% loss=0.35676, acc=0.89062
# [56/100] training 58.8% loss=0.16199, acc=0.92188
# [56/100] training 59.0% loss=0.17069, acc=0.92188
# [56/100] training 59.2% loss=0.27518, acc=0.84375
# [56/100] training 59.3% loss=0.23007, acc=0.90625
# [56/100] training 59.5% loss=0.24350, acc=0.92188
# [56/100] training 59.7% loss=0.18506, acc=0.93750
# [56/100] training 59.9% loss=0.18174, acc=0.98438
# [56/100] training 60.0% loss=0.19823, acc=0.93750
# [56/100] training 60.2% loss=0.16037, acc=0.95312
# [56/100] training 60.3% loss=0.11267, acc=0.96875
# [56/100] training 60.5% loss=0.24135, acc=0.90625
# [56/100] training 60.8% loss=0.24981, acc=0.89062
# [56/100] training 60.9% loss=0.33099, acc=0.87500
# [56/100] training 61.1% loss=0.25877, acc=0.90625
# [56/100] training 61.2% loss=0.13953, acc=0.93750
# [56/100] training 61.4% loss=0.28296, acc=0.90625
# [56/100] training 61.5% loss=0.29092, acc=0.90625
# [56/100] training 61.7% loss=0.18318, acc=0.90625
# [56/100] training 62.0% loss=0.19638, acc=0.90625
# [56/100] training 62.1% loss=0.37433, acc=0.85938
# [56/100] training 62.3% loss=0.11497, acc=0.95312
# [56/100] training 62.4% loss=0.17298, acc=0.93750
# [56/100] training 62.6% loss=0.19124, acc=0.90625
# [56/100] training 62.7% loss=0.13523, acc=0.93750
# [56/100] training 62.9% loss=0.21610, acc=0.90625
# [56/100] training 63.1% loss=0.19741, acc=0.92188
# [56/100] training 63.3% loss=0.14176, acc=0.95312
# [56/100] training 63.5% loss=0.20645, acc=0.90625
# [56/100] training 63.6% loss=0.24123, acc=0.89062
# [56/100] training 63.8% loss=0.30847, acc=0.85938
# [56/100] training 63.9% loss=0.08796, acc=0.95312
# [56/100] training 64.2% loss=0.10787, acc=0.95312
# [56/100] training 64.3% loss=0.23218, acc=0.92188
# [56/100] training 64.5% loss=0.11350, acc=0.96875
# [56/100] training 64.7% loss=0.09767, acc=1.00000
# [56/100] training 64.8% loss=0.30571, acc=0.85938
# [56/100] training 65.0% loss=0.16769, acc=0.93750
# [56/100] training 65.1% loss=0.16512, acc=0.90625
# [56/100] training 65.4% loss=0.21506, acc=0.92188
# [56/100] training 65.5% loss=0.07964, acc=0.98438
# [56/100] training 65.7% loss=0.20623, acc=0.95312
# [56/100] training 65.8% loss=0.14971, acc=0.93750
# [56/100] training 66.0% loss=0.17729, acc=0.93750
# [56/100] training 66.2% loss=0.07844, acc=0.95312
# [56/100] training 66.3% loss=0.16312, acc=0.95312
# [56/100] training 66.6% loss=0.17703, acc=0.92188
# [56/100] training 66.7% loss=0.07306, acc=0.96875
# [56/100] training 66.9% loss=0.23820, acc=0.93750
# [56/100] training 67.0% loss=0.14085, acc=0.93750
# [56/100] training 67.2% loss=0.09621, acc=0.95312
# [56/100] training 67.4% loss=0.10932, acc=0.96875
# [56/100] training 67.6% loss=0.13741, acc=0.93750
# [56/100] training 67.8% loss=0.12856, acc=0.93750
# [56/100] training 67.9% loss=0.11599, acc=0.92188
# [56/100] training 68.1% loss=0.29990, acc=0.90625
# [56/100] training 68.2% loss=0.08891, acc=0.95312
# [56/100] training 68.4% loss=0.04614, acc=0.98438
# [56/100] training 68.5% loss=0.17386, acc=0.93750
# [56/100] training 68.8% loss=0.09648, acc=0.96875
# [56/100] training 69.0% loss=0.19007, acc=0.90625
# [56/100] training 69.1% loss=0.14801, acc=0.96875
# [56/100] training 69.3% loss=0.14279, acc=0.92188
# [56/100] training 69.4% loss=0.10563, acc=0.96875
# [56/100] training 69.6% loss=0.13806, acc=0.93750
# [56/100] training 69.7% loss=0.09978, acc=0.96875
# [56/100] training 70.0% loss=0.08483, acc=0.98438
# [56/100] training 70.2% loss=0.25251, acc=0.93750
# [56/100] training 70.3% loss=0.15845, acc=0.95312
# [56/100] training 70.5% loss=0.14635, acc=0.93750
# [56/100] training 70.6% loss=0.10636, acc=0.95312
# [56/100] training 70.8% loss=0.19424, acc=0.89062
# [56/100] training 71.0% loss=0.24062, acc=0.90625
# [56/100] training 71.2% loss=0.11282, acc=0.92188
# [56/100] training 71.3% loss=0.10389, acc=0.95312
# [56/100] training 71.5% loss=0.20697, acc=0.95312
# [56/100] training 71.7% loss=0.29401, acc=0.90625
# [56/100] training 71.8% loss=0.25404, acc=0.92188
# [56/100] training 72.0% loss=0.18095, acc=0.87500
# [56/100] training 72.2% loss=0.20602, acc=0.92188
# [56/100] training 72.4% loss=0.24008, acc=0.90625
# [56/100] training 72.5% loss=0.15562, acc=0.92188
# [56/100] training 72.7% loss=0.24538, acc=0.89062
# [56/100] training 72.9% loss=0.12843, acc=0.93750
# [56/100] training 73.0% loss=0.07735, acc=0.98438
# [56/100] training 73.3% loss=0.22121, acc=0.92188
# [56/100] training 73.4% loss=0.13438, acc=0.95312
# [56/100] training 73.6% loss=0.11597, acc=0.98438
# [56/100] training 73.7% loss=0.12129, acc=0.95312
# [56/100] training 73.9% loss=0.11970, acc=0.98438
# [56/100] training 74.0% loss=0.20209, acc=0.90625
# [56/100] training 74.2% loss=0.08668, acc=0.95312
# [56/100] training 74.5% loss=0.11334, acc=0.98438
# [56/100] training 74.6% loss=0.39842, acc=0.87500
# [56/100] training 74.8% loss=0.20408, acc=0.92188
# [56/100] training 74.9% loss=0.19791, acc=0.92188
# [56/100] training 75.1% loss=0.12050, acc=0.95312
# [56/100] training 75.2% loss=0.09816, acc=1.00000
# [56/100] training 75.4% loss=0.14747, acc=0.95312
# [56/100] training 75.7% loss=0.17100, acc=0.95312
# [56/100] training 75.8% loss=0.28256, acc=0.87500
# [56/100] training 76.0% loss=0.13878, acc=0.95312
# [56/100] training 76.1% loss=0.29230, acc=0.84375
# [56/100] training 76.3% loss=0.11372, acc=0.96875
# [56/100] training 76.4% loss=0.27532, acc=0.87500
# [56/100] training 76.7% loss=0.17585, acc=0.92188
# [56/100] training 76.8% loss=0.13020, acc=0.95312
# [56/100] training 77.0% loss=0.12367, acc=0.93750
# [56/100] training 77.2% loss=0.17633, acc=0.95312
# [56/100] training 77.3% loss=0.11794, acc=0.93750
# [56/100] training 77.5% loss=0.30024, acc=0.90625
# [56/100] training 77.6% loss=0.29991, acc=0.87500
# [56/100] training 77.9% loss=0.18430, acc=0.93750
# [56/100] training 78.0% loss=0.11470, acc=0.95312
# [56/100] training 78.2% loss=0.18757, acc=0.93750
# [56/100] training 78.4% loss=0.04789, acc=1.00000
# [56/100] training 78.5% loss=0.20515, acc=0.92188
# [56/100] training 78.7% loss=0.19660, acc=0.92188
# [56/100] training 78.8% loss=0.09763, acc=0.96875
# [56/100] training 79.1% loss=0.08886, acc=0.96875
# [56/100] training 79.2% loss=0.12319, acc=0.98438
# [56/100] training 79.4% loss=0.18567, acc=0.93750
# [56/100] training 79.5% loss=0.22559, acc=0.92188
# [56/100] training 79.7% loss=0.06078, acc=0.98438
# [56/100] training 79.9% loss=0.11241, acc=0.93750
# [56/100] training 80.1% loss=0.17771, acc=0.93750
# [56/100] training 80.3% loss=0.21119, acc=0.92188
# [56/100] training 80.4% loss=0.17722, acc=0.93750
# [56/100] training 80.6% loss=0.24036, acc=0.89062
# [56/100] training 80.7% loss=0.12264, acc=0.98438
# [56/100] training 80.9% loss=0.14684, acc=0.93750
# [56/100] training 81.2% loss=0.19875, acc=0.90625
# [56/100] training 81.3% loss=0.18812, acc=0.93750
# [56/100] training 81.5% loss=0.14845, acc=0.92188
# [56/100] training 81.6% loss=0.25195, acc=0.87500
# [56/100] training 81.8% loss=0.16592, acc=0.92188
# [56/100] training 81.9% loss=0.41516, acc=0.87500
# [56/100] training 82.1% loss=0.15799, acc=0.93750
# [56/100] training 82.2% loss=0.15139, acc=0.98438
# [56/100] training 82.5% loss=0.17358, acc=0.93750
# [56/100] training 82.7% loss=0.26994, acc=0.93750
# [56/100] training 82.8% loss=0.13272, acc=0.95312
# [56/100] training 83.0% loss=0.10714, acc=0.96875
# [56/100] training 83.1% loss=0.16611, acc=0.95312
# [56/100] training 83.3% loss=0.11531, acc=0.95312
# [56/100] training 83.5% loss=0.12735, acc=0.96875
# [56/100] training 83.7% loss=0.36944, acc=0.85938
# [56/100] training 83.9% loss=0.21513, acc=0.90625
# [56/100] training 84.0% loss=0.12103, acc=0.95312
# [56/100] training 84.2% loss=0.10778, acc=0.96875
# [56/100] training 84.3% loss=0.14886, acc=0.95312
# [56/100] training 84.5% loss=0.16785, acc=0.93750
# [56/100] training 84.7% loss=0.09292, acc=0.96875
# [56/100] training 84.9% loss=0.14232, acc=0.93750
# [56/100] training 85.0% loss=0.19997, acc=0.93750
# [56/100] training 85.2% loss=0.09606, acc=0.96875
# [56/100] training 85.4% loss=0.11101, acc=0.96875
# [56/100] training 85.5% loss=0.25705, acc=0.90625
# [56/100] training 85.8% loss=0.22320, acc=0.93750
# [56/100] training 85.9% loss=0.19188, acc=0.92188
# [56/100] training 86.1% loss=0.20970, acc=0.90625
# [56/100] training 86.2% loss=0.09771, acc=0.96875
# [56/100] training 86.4% loss=0.35718, acc=0.87500
# [56/100] training 86.6% loss=0.16879, acc=0.92188
# [56/100] training 86.7% loss=0.10972, acc=0.93750
# [56/100] training 87.0% loss=0.26079, acc=0.89062
# [56/100] training 87.1% loss=0.16632, acc=0.92188
# [56/100] training 87.3% loss=0.10338, acc=0.98438
# [56/100] training 87.4% loss=0.20919, acc=0.95312
# [56/100] training 87.6% loss=0.14135, acc=0.95312
# [56/100] training 87.7% loss=0.16359, acc=0.93750
# [56/100] training 87.9% loss=0.14081, acc=0.92188
# [56/100] training 88.2% loss=0.12907, acc=0.93750
# [56/100] training 88.3% loss=0.18155, acc=0.92188
# [56/100] training 88.5% loss=0.25737, acc=0.89062
# [56/100] training 88.6% loss=0.14786, acc=0.92188
# [56/100] training 88.8% loss=0.16670, acc=0.93750
# [56/100] training 88.9% loss=0.23632, acc=0.90625
# [56/100] training 89.2% loss=0.20354, acc=0.89062
# [56/100] training 89.4% loss=0.15201, acc=0.95312
# [56/100] training 89.5% loss=0.19117, acc=0.92188
# [56/100] training 89.7% loss=0.18660, acc=0.95312
# [56/100] training 89.8% loss=0.12407, acc=0.96875
# [56/100] training 90.0% loss=0.15069, acc=0.93750
# [56/100] training 90.1% loss=0.17286, acc=0.92188
# [56/100] training 90.4% loss=0.16524, acc=0.93750
# [56/100] training 90.5% loss=0.12973, acc=0.93750
# [56/100] training 90.7% loss=0.18575, acc=0.93750
# [56/100] training 90.9% loss=0.08091, acc=0.96875
# [56/100] training 91.0% loss=0.24507, acc=0.92188
# [56/100] training 91.2% loss=0.19838, acc=0.93750
# [56/100] training 91.3% loss=0.29987, acc=0.87500
# [56/100] training 91.6% loss=0.17391, acc=0.93750
# [56/100] training 91.7% loss=0.10996, acc=0.96875
# [56/100] training 91.9% loss=0.24477, acc=0.85938
# [56/100] training 92.1% loss=0.20584, acc=0.90625
# [56/100] training 92.2% loss=0.13844, acc=0.93750
# [56/100] training 92.4% loss=0.17436, acc=0.95312
# [56/100] training 92.6% loss=0.35416, acc=0.85938
# [56/100] training 92.8% loss=0.13091, acc=0.93750
# [56/100] training 92.9% loss=0.25312, acc=0.87500
# [56/100] training 93.1% loss=0.39108, acc=0.85938
# [56/100] training 93.2% loss=0.19102, acc=0.93750
# [56/100] training 93.4% loss=0.24787, acc=0.89062
# [56/100] training 93.7% loss=0.12913, acc=0.98438
# [56/100] training 93.8% loss=0.14760, acc=0.90625
# [56/100] training 94.0% loss=0.14751, acc=0.95312
# [56/100] training 94.1% loss=0.20654, acc=0.92188
# [56/100] training 94.3% loss=0.11541, acc=0.96875
# [56/100] training 94.4% loss=0.27348, acc=0.93750
# [56/100] training 94.6% loss=0.11113, acc=0.93750
# [56/100] training 94.9% loss=0.09957, acc=0.95312
# [56/100] training 95.0% loss=0.12140, acc=0.93750
# [56/100] training 95.2% loss=0.30164, acc=0.89062
# [56/100] training 95.3% loss=0.25234, acc=0.92188
# [56/100] training 95.5% loss=0.11505, acc=0.95312
# [56/100] training 95.6% loss=0.20526, acc=0.90625
# [56/100] training 95.8% loss=0.07433, acc=0.98438
# [56/100] training 96.0% loss=0.21390, acc=0.90625
# [56/100] training 96.2% loss=0.09005, acc=0.98438
# [56/100] training 96.4% loss=0.16671, acc=0.96875
# [56/100] training 96.5% loss=0.16477, acc=0.90625
# [56/100] training 96.7% loss=0.19965, acc=0.92188
# [56/100] training 96.8% loss=0.27427, acc=0.92188
# [56/100] training 97.1% loss=0.09627, acc=0.96875
# [56/100] training 97.2% loss=0.19484, acc=0.93750
# [56/100] training 97.4% loss=0.20248, acc=0.90625
# [56/100] training 97.6% loss=0.24852, acc=0.89062
# [56/100] training 97.7% loss=0.20454, acc=0.89062
# [56/100] training 97.9% loss=0.14937, acc=0.93750
# [56/100] training 98.0% loss=0.18199, acc=0.93750
# [56/100] training 98.3% loss=0.25292, acc=0.89062
# [56/100] training 98.4% loss=0.15829, acc=0.95312
# [56/100] training 98.6% loss=0.32396, acc=0.89062
# [56/100] training 98.7% loss=0.26631, acc=0.92188
# [56/100] training 98.9% loss=0.17621, acc=0.92188
# [56/100] training 99.1% loss=0.18990, acc=0.95312
# [56/100] training 99.2% loss=0.12186, acc=0.92188
# [56/100] training 99.5% loss=0.27899, acc=0.89062
# [56/100] training 99.6% loss=0.18134, acc=0.95312
# [56/100] training 99.8% loss=0.13216, acc=0.93750
# [56/100] training 99.9% loss=0.03702, acc=0.98438
# [56/100] testing 0.9% loss=0.23300, acc=0.89062
# [56/100] testing 1.8% loss=0.32184, acc=0.87500
# [56/100] testing 2.2% loss=0.36327, acc=0.89062
# [56/100] testing 3.1% loss=0.35321, acc=0.89062
# [56/100] testing 3.5% loss=0.09889, acc=0.95312
# [56/100] testing 4.4% loss=0.13570, acc=0.92188
# [56/100] testing 4.8% loss=0.27148, acc=0.89062
# [56/100] testing 5.7% loss=0.30776, acc=0.89062
# [56/100] testing 6.6% loss=0.09192, acc=0.96875
# [56/100] testing 7.0% loss=0.05848, acc=1.00000
# [56/100] testing 7.9% loss=0.32286, acc=0.92188
# [56/100] testing 8.3% loss=0.26210, acc=0.93750
# [56/100] testing 9.2% loss=0.24219, acc=0.95312
# [56/100] testing 9.7% loss=0.07790, acc=0.98438
# [56/100] testing 10.5% loss=0.22507, acc=0.89062
# [56/100] testing 11.0% loss=0.31659, acc=0.92188
# [56/100] testing 11.8% loss=0.17538, acc=0.93750
# [56/100] testing 12.7% loss=0.43765, acc=0.90625
# [56/100] testing 13.2% loss=0.24356, acc=0.87500
# [56/100] testing 14.0% loss=0.50825, acc=0.89062
# [56/100] testing 14.5% loss=0.23390, acc=0.87500
# [56/100] testing 15.4% loss=0.29681, acc=0.93750
# [56/100] testing 15.8% loss=0.11618, acc=0.96875
# [56/100] testing 16.7% loss=0.17161, acc=0.92188
# [56/100] testing 17.5% loss=0.18327, acc=0.90625
# [56/100] testing 18.0% loss=0.16733, acc=0.93750
# [56/100] testing 18.9% loss=0.05101, acc=0.98438
# [56/100] testing 19.3% loss=0.31524, acc=0.87500
# [56/100] testing 20.2% loss=0.23368, acc=0.90625
# [56/100] testing 20.6% loss=0.24105, acc=0.92188
# [56/100] testing 21.5% loss=0.18240, acc=0.92188
# [56/100] testing 21.9% loss=0.45694, acc=0.84375
# [56/100] testing 22.8% loss=0.40975, acc=0.90625
# [56/100] testing 23.7% loss=0.33590, acc=0.87500
# [56/100] testing 24.1% loss=0.12446, acc=0.92188
# [56/100] testing 25.0% loss=0.28728, acc=0.89062
# [56/100] testing 25.4% loss=0.05372, acc=0.98438
# [56/100] testing 26.3% loss=0.29476, acc=0.89062
# [56/100] testing 26.8% loss=0.31956, acc=0.90625
# [56/100] testing 27.6% loss=0.11355, acc=0.96875
# [56/100] testing 28.5% loss=0.24629, acc=0.90625
# [56/100] testing 29.0% loss=0.24443, acc=0.92188
# [56/100] testing 29.8% loss=0.38092, acc=0.90625
# [56/100] testing 30.3% loss=0.27384, acc=0.95312
# [56/100] testing 31.1% loss=0.26030, acc=0.89062
# [56/100] testing 31.6% loss=0.29217, acc=0.92188
# [56/100] testing 32.5% loss=0.30811, acc=0.92188
# [56/100] testing 32.9% loss=0.47614, acc=0.92188
# [56/100] testing 33.8% loss=0.36583, acc=0.89062
# [56/100] testing 34.7% loss=0.45163, acc=0.89062
# [56/100] testing 35.1% loss=0.10790, acc=0.95312
# [56/100] testing 36.0% loss=0.33522, acc=0.90625
# [56/100] testing 36.4% loss=0.21669, acc=0.93750
# [56/100] testing 37.3% loss=0.19568, acc=0.96875
# [56/100] testing 37.7% loss=0.37592, acc=0.87500
# [56/100] testing 38.6% loss=0.21002, acc=0.93750
# [56/100] testing 39.5% loss=0.27010, acc=0.95312
# [56/100] testing 39.9% loss=0.29957, acc=0.89062
# [56/100] testing 40.8% loss=0.34131, acc=0.90625
# [56/100] testing 41.2% loss=0.25593, acc=0.95312
# [56/100] testing 42.1% loss=0.26183, acc=0.92188
# [56/100] testing 42.5% loss=0.14059, acc=0.95312
# [56/100] testing 43.4% loss=0.23587, acc=0.92188
# [56/100] testing 43.9% loss=0.11366, acc=0.93750
# [56/100] testing 44.7% loss=0.25661, acc=0.90625
# [56/100] testing 45.6% loss=0.33999, acc=0.89062
# [56/100] testing 46.1% loss=0.23040, acc=0.92188
# [56/100] testing 46.9% loss=0.15886, acc=0.93750
# [56/100] testing 47.4% loss=0.11768, acc=0.95312
# [56/100] testing 48.3% loss=0.37857, acc=0.87500
# [56/100] testing 48.7% loss=0.22750, acc=0.90625
# [56/100] testing 49.6% loss=0.45621, acc=0.87500
# [56/100] testing 50.4% loss=0.16675, acc=0.95312
# [56/100] testing 50.9% loss=0.25777, acc=0.92188
# [56/100] testing 51.8% loss=0.38795, acc=0.92188
# [56/100] testing 52.2% loss=0.17065, acc=0.93750
# [56/100] testing 53.1% loss=0.25079, acc=0.89062
# [56/100] testing 53.5% loss=0.25345, acc=0.92188
# [56/100] testing 54.4% loss=0.45089, acc=0.85938
# [56/100] testing 54.8% loss=0.35377, acc=0.89062
# [56/100] testing 55.7% loss=0.12797, acc=0.93750
# [56/100] testing 56.6% loss=0.35875, acc=0.85938
# [56/100] testing 57.0% loss=0.35727, acc=0.90625
# [56/100] testing 57.9% loss=0.24244, acc=0.92188
# [56/100] testing 58.3% loss=0.29167, acc=0.90625
# [56/100] testing 59.2% loss=0.18853, acc=0.92188
# [56/100] testing 59.7% loss=0.33249, acc=0.87500
# [56/100] testing 60.5% loss=0.44163, acc=0.87500
# [56/100] testing 61.4% loss=0.11409, acc=0.95312
# [56/100] testing 61.9% loss=0.18614, acc=0.90625
# [56/100] testing 62.7% loss=0.16198, acc=0.95312
# [56/100] testing 63.2% loss=0.48365, acc=0.84375
# [56/100] testing 64.0% loss=0.54267, acc=0.89062
# [56/100] testing 64.5% loss=0.16270, acc=0.95312
# [56/100] testing 65.4% loss=0.13698, acc=0.93750
# [56/100] testing 65.8% loss=0.26655, acc=0.85938
# [56/100] testing 66.7% loss=0.10114, acc=0.95312
# [56/100] testing 67.6% loss=0.47864, acc=0.90625
# [56/100] testing 68.0% loss=0.11078, acc=0.95312
# [56/100] testing 68.9% loss=0.26274, acc=0.92188
# [56/100] testing 69.3% loss=0.27619, acc=0.90625
# [56/100] testing 70.2% loss=0.42861, acc=0.85938
# [56/100] testing 70.6% loss=0.32916, acc=0.90625
# [56/100] testing 71.5% loss=0.28082, acc=0.90625
# [56/100] testing 72.4% loss=0.16952, acc=0.92188
# [56/100] testing 72.8% loss=0.13845, acc=0.93750
# [56/100] testing 73.7% loss=0.10701, acc=0.96875
# [56/100] testing 74.1% loss=0.43433, acc=0.85938
# [56/100] testing 75.0% loss=0.20188, acc=0.92188
# [56/100] testing 75.4% loss=0.39926, acc=0.85938
# [56/100] testing 76.3% loss=0.05492, acc=0.98438
# [56/100] testing 76.8% loss=0.26836, acc=0.89062
# [56/100] testing 77.6% loss=0.13943, acc=0.92188
# [56/100] testing 78.5% loss=0.35689, acc=0.85938
# [56/100] testing 79.0% loss=0.30354, acc=0.87500
# [56/100] testing 79.8% loss=0.29603, acc=0.89062
# [56/100] testing 80.3% loss=0.18777, acc=0.92188
# [56/100] testing 81.2% loss=0.65089, acc=0.89062
# [56/100] testing 81.6% loss=0.23391, acc=0.92188
# [56/100] testing 82.5% loss=0.17571, acc=0.90625
# [56/100] testing 83.3% loss=0.17747, acc=0.95312
# [56/100] testing 83.8% loss=0.21063, acc=0.95312
# [56/100] testing 84.7% loss=0.28625, acc=0.87500
# [56/100] testing 85.1% loss=0.18422, acc=0.92188
# [56/100] testing 86.0% loss=0.26351, acc=0.90625
# [56/100] testing 86.4% loss=0.44872, acc=0.85938
# [56/100] testing 87.3% loss=0.32851, acc=0.87500
# [56/100] testing 87.7% loss=0.23803, acc=0.90625
# [56/100] testing 88.6% loss=0.29883, acc=0.89062
# [56/100] testing 89.5% loss=0.48508, acc=0.82812
# [56/100] testing 89.9% loss=0.19609, acc=0.89062
# [56/100] testing 90.8% loss=0.30474, acc=0.93750
# [56/100] testing 91.2% loss=0.14274, acc=0.95312
# [56/100] testing 92.1% loss=0.16589, acc=0.95312
# [56/100] testing 92.6% loss=0.54161, acc=0.89062
# [56/100] testing 93.4% loss=0.33779, acc=0.90625
# [56/100] testing 94.3% loss=0.16245, acc=0.95312
# [56/100] testing 94.7% loss=0.15053, acc=0.93750
# [56/100] testing 95.6% loss=0.64063, acc=0.81250
# [56/100] testing 96.1% loss=0.17750, acc=0.95312
# [56/100] testing 96.9% loss=0.23689, acc=0.90625
# [56/100] testing 97.4% loss=0.12594, acc=0.96875
# [56/100] testing 98.3% loss=0.24950, acc=0.93750
# [56/100] testing 98.7% loss=0.17512, acc=0.90625
# [56/100] testing 99.6% loss=0.29775, acc=0.87500
# [57/100] training 0.2% loss=0.14822, acc=0.92188
# [57/100] training 0.4% loss=0.29125, acc=0.92188
# [57/100] training 0.5% loss=0.12376, acc=0.96875
# [57/100] training 0.8% loss=0.14223, acc=0.96875
# [57/100] training 0.9% loss=0.22797, acc=0.96875
# [57/100] training 1.1% loss=0.27625, acc=0.95312
# [57/100] training 1.2% loss=0.13323, acc=0.95312
# [57/100] training 1.4% loss=0.12801, acc=0.95312
# [57/100] training 1.6% loss=0.12747, acc=0.92188
# [57/100] training 1.8% loss=0.13881, acc=0.96875
# [57/100] training 2.0% loss=0.18060, acc=0.92188
# [57/100] training 2.1% loss=0.17828, acc=0.90625
# [57/100] training 2.3% loss=0.14703, acc=0.95312
# [57/100] training 2.4% loss=0.21264, acc=0.89062
# [57/100] training 2.6% loss=0.05456, acc=0.98438
# [57/100] training 2.7% loss=0.08731, acc=0.96875
# [57/100] training 3.0% loss=0.10790, acc=0.95312
# [57/100] training 3.2% loss=0.07247, acc=0.95312
# [57/100] training 3.3% loss=0.36528, acc=0.89062
# [57/100] training 3.5% loss=0.19581, acc=0.93750
# [57/100] training 3.6% loss=0.25744, acc=0.89062
# [57/100] training 3.8% loss=0.21585, acc=0.90625
# [57/100] training 3.9% loss=0.09815, acc=0.96875
# [57/100] training 4.2% loss=0.07220, acc=0.96875
# [57/100] training 4.4% loss=0.15601, acc=0.95312
# [57/100] training 4.5% loss=0.11832, acc=0.96875
# [57/100] training 4.7% loss=0.36406, acc=0.92188
# [57/100] training 4.8% loss=0.17196, acc=0.90625
# [57/100] training 5.0% loss=0.09341, acc=0.95312
# [57/100] training 5.2% loss=0.25815, acc=0.92188
# [57/100] training 5.4% loss=0.10686, acc=0.95312
# [57/100] training 5.5% loss=0.22321, acc=0.87500
# [57/100] training 5.7% loss=0.18235, acc=0.95312
# [57/100] training 5.9% loss=0.12473, acc=0.96875
# [57/100] training 6.0% loss=0.23986, acc=0.89062
# [57/100] training 6.3% loss=0.21041, acc=0.89062
# [57/100] training 6.4% loss=0.17298, acc=0.93750
# [57/100] training 6.6% loss=0.18986, acc=0.90625
# [57/100] training 6.7% loss=0.25843, acc=0.82812
# [57/100] training 6.9% loss=0.13436, acc=0.93750
# [57/100] training 7.1% loss=0.09342, acc=0.93750
# [57/100] training 7.2% loss=0.23159, acc=0.89062
# [57/100] training 7.5% loss=0.21203, acc=0.89062
# [57/100] training 7.6% loss=0.23030, acc=0.92188
# [57/100] training 7.8% loss=0.16618, acc=0.95312
# [57/100] training 7.9% loss=0.13704, acc=0.93750
# [57/100] training 8.1% loss=0.09318, acc=0.95312
# [57/100] training 8.2% loss=0.21011, acc=0.92188
# [57/100] training 8.4% loss=0.28646, acc=0.89062
# [57/100] training 8.7% loss=0.19971, acc=0.92188
# [57/100] training 8.8% loss=0.19650, acc=0.92188
# [57/100] training 9.0% loss=0.16510, acc=0.92188
# [57/100] training 9.1% loss=0.17216, acc=0.93750
# [57/100] training 9.3% loss=0.34037, acc=0.90625
# [57/100] training 9.4% loss=0.13357, acc=0.93750
# [57/100] training 9.7% loss=0.11977, acc=0.95312
# [57/100] training 9.9% loss=0.19919, acc=0.90625
# [57/100] training 10.0% loss=0.17077, acc=0.92188
# [57/100] training 10.2% loss=0.11407, acc=0.95312
# [57/100] training 10.3% loss=0.17989, acc=0.96875
# [57/100] training 10.5% loss=0.23505, acc=0.93750
# [57/100] training 10.6% loss=0.11635, acc=0.95312
# [57/100] training 10.9% loss=0.16708, acc=0.93750
# [57/100] training 11.0% loss=0.24091, acc=0.90625
# [57/100] training 11.2% loss=0.07505, acc=0.98438
# [57/100] training 11.4% loss=0.20062, acc=0.89062
# [57/100] training 11.5% loss=0.27975, acc=0.87500
# [57/100] training 11.7% loss=0.11074, acc=0.96875
# [57/100] training 11.8% loss=0.15741, acc=0.90625
# [57/100] training 12.1% loss=0.13940, acc=0.92188
# [57/100] training 12.2% loss=0.17222, acc=0.95312
# [57/100] training 12.4% loss=0.20350, acc=0.89062
# [57/100] training 12.6% loss=0.40730, acc=0.93750
# [57/100] training 12.7% loss=0.14919, acc=0.95312
# [57/100] training 12.9% loss=0.21240, acc=0.90625
# [57/100] training 13.0% loss=0.16920, acc=0.93750
# [57/100] training 13.3% loss=0.20688, acc=0.92188
# [57/100] training 13.4% loss=0.20706, acc=0.95312
# [57/100] training 13.6% loss=0.11899, acc=0.96875
# [57/100] training 13.7% loss=0.27547, acc=0.92188
# [57/100] training 13.9% loss=0.26492, acc=0.93750
# [57/100] training 14.1% loss=0.30230, acc=0.92188
# [57/100] training 14.3% loss=0.10191, acc=0.96875
# [57/100] training 14.5% loss=0.20358, acc=0.92188
# [57/100] training 14.6% loss=0.27401, acc=0.89062
# [57/100] training 14.8% loss=0.17722, acc=0.90625
# [57/100] training 14.9% loss=0.11311, acc=0.95312
# [57/100] training 15.1% loss=0.26928, acc=0.87500
# [57/100] training 15.4% loss=0.20306, acc=0.89062
# [57/100] training 15.5% loss=0.15598, acc=0.93750
# [57/100] training 15.7% loss=0.21483, acc=0.93750
# [57/100] training 15.8% loss=0.11823, acc=0.93750
# [57/100] training 16.0% loss=0.19418, acc=0.93750
# [57/100] training 16.1% loss=0.25123, acc=0.92188
# [57/100] training 16.3% loss=0.18960, acc=0.90625
# [57/100] training 16.4% loss=0.10444, acc=0.95312
# [57/100] training 16.7% loss=0.19588, acc=0.92188
# [57/100] training 16.9% loss=0.17126, acc=0.92188
# [57/100] training 17.0% loss=0.22044, acc=0.90625
# [57/100] training 17.2% loss=0.16908, acc=0.92188
# [57/100] training 17.3% loss=0.17089, acc=0.93750
# [57/100] training 17.5% loss=0.20891, acc=0.90625
# [57/100] training 17.7% loss=0.14084, acc=0.92188
# [57/100] training 17.9% loss=0.15920, acc=0.95312
# [57/100] training 18.1% loss=0.24419, acc=0.90625
# [57/100] training 18.2% loss=0.18369, acc=0.95312
# [57/100] training 18.4% loss=0.26907, acc=0.89062
# [57/100] training 18.5% loss=0.17022, acc=0.96875
# [57/100] training 18.8% loss=0.16856, acc=0.93750
# [57/100] training 18.9% loss=0.10780, acc=0.95312
# [57/100] training 19.1% loss=0.23016, acc=0.89062
# [57/100] training 19.2% loss=0.13125, acc=0.96875
# [57/100] training 19.4% loss=0.10901, acc=0.95312
# [57/100] training 19.6% loss=0.21529, acc=0.89062
# [57/100] training 19.7% loss=0.21150, acc=0.93750
# [57/100] training 20.0% loss=0.17900, acc=0.92188
# [57/100] training 20.1% loss=0.15967, acc=0.90625
# [57/100] training 20.3% loss=0.14270, acc=0.93750
# [57/100] training 20.4% loss=0.22023, acc=0.93750
# [57/100] training 20.6% loss=0.27216, acc=0.90625
# [57/100] training 20.8% loss=0.14116, acc=0.93750
# [57/100] training 20.9% loss=0.15892, acc=0.93750
# [57/100] training 21.2% loss=0.45752, acc=0.90625
# [57/100] training 21.3% loss=0.24077, acc=0.90625
# [57/100] training 21.5% loss=0.23277, acc=0.95312
# [57/100] training 21.6% loss=0.05184, acc=1.00000
# [57/100] training 21.8% loss=0.10204, acc=0.93750
# [57/100] training 21.9% loss=0.17090, acc=0.92188
# [57/100] training 22.2% loss=0.13890, acc=0.92188
# [57/100] training 22.4% loss=0.20082, acc=0.90625
# [57/100] training 22.5% loss=0.12014, acc=0.93750
# [57/100] training 22.7% loss=0.19619, acc=0.92188
# [57/100] training 22.8% loss=0.17051, acc=0.96875
# [57/100] training 23.0% loss=0.11625, acc=0.95312
# [57/100] training 23.1% loss=0.24642, acc=0.92188
# [57/100] training 23.4% loss=0.16334, acc=0.90625
# [57/100] training 23.6% loss=0.23070, acc=0.89062
# [57/100] training 23.7% loss=0.16806, acc=0.92188
# [57/100] training 23.9% loss=0.13579, acc=0.92188
# [57/100] training 24.0% loss=0.08630, acc=0.98438
# [57/100] training 24.2% loss=0.19540, acc=0.90625
# [57/100] training 24.3% loss=0.17650, acc=0.96875
# [57/100] training 24.6% loss=0.22804, acc=0.92188
# [57/100] training 24.7% loss=0.27337, acc=0.89062
# [57/100] training 24.9% loss=0.16061, acc=0.93750
# [57/100] training 25.1% loss=0.27111, acc=0.87500
# [57/100] training 25.2% loss=0.15317, acc=0.95312
# [57/100] training 25.4% loss=0.21713, acc=0.89062
# [57/100] training 25.6% loss=0.18477, acc=0.92188
# [57/100] training 25.8% loss=0.19085, acc=0.92188
# [57/100] training 25.9% loss=0.17100, acc=0.89062
# [57/100] training 26.1% loss=0.12833, acc=0.93750
# [57/100] training 26.3% loss=0.19791, acc=0.90625
# [57/100] training 26.4% loss=0.12118, acc=0.92188
# [57/100] training 26.6% loss=0.08246, acc=0.98438
# [57/100] training 26.8% loss=0.09603, acc=0.95312
# [57/100] training 27.0% loss=0.12186, acc=0.96875
# [57/100] training 27.1% loss=0.12812, acc=0.93750
# [57/100] training 27.3% loss=0.32109, acc=0.92188
# [57/100] training 27.4% loss=0.08940, acc=0.98438
# [57/100] training 27.6% loss=0.29772, acc=0.85938
# [57/100] training 27.9% loss=0.16653, acc=0.92188
# [57/100] training 28.0% loss=0.56776, acc=0.82812
# [57/100] training 28.2% loss=0.14243, acc=0.95312
# [57/100] training 28.3% loss=0.08603, acc=0.98438
# [57/100] training 28.5% loss=0.15422, acc=0.92188
# [57/100] training 28.6% loss=0.23270, acc=0.92188
# [57/100] training 28.8% loss=0.13175, acc=0.98438
# [57/100] training 29.1% loss=0.13213, acc=0.96875
# [57/100] training 29.2% loss=0.11761, acc=0.95312
# [57/100] training 29.4% loss=0.25948, acc=0.89062
# [57/100] training 29.5% loss=0.08356, acc=0.98438
# [57/100] training 29.7% loss=0.22586, acc=0.90625
# [57/100] training 29.8% loss=0.18304, acc=0.95312
# [57/100] training 30.0% loss=0.13180, acc=0.93750
# [57/100] training 30.2% loss=0.07116, acc=0.96875
# [57/100] training 30.4% loss=0.08774, acc=0.95312
# [57/100] training 30.6% loss=0.19714, acc=0.89062
# [57/100] training 30.7% loss=0.19413, acc=0.95312
# [57/100] training 30.9% loss=0.30155, acc=0.92188
# [57/100] training 31.0% loss=0.07729, acc=0.96875
# [57/100] training 31.3% loss=0.29581, acc=0.90625
# [57/100] training 31.4% loss=0.25636, acc=0.89062
# [57/100] training 31.6% loss=0.24890, acc=0.93750
# [57/100] training 31.8% loss=0.13522, acc=0.93750
# [57/100] training 31.9% loss=0.12659, acc=0.96875
# [57/100] training 32.1% loss=0.10824, acc=0.96875
# [57/100] training 32.2% loss=0.25717, acc=0.92188
# [57/100] training 32.5% loss=0.09365, acc=0.95312
# [57/100] training 32.6% loss=0.10361, acc=0.95312
# [57/100] training 32.8% loss=0.18620, acc=0.93750
# [57/100] training 32.9% loss=0.28459, acc=0.90625
# [57/100] training 33.1% loss=0.12972, acc=0.93750
# [57/100] training 33.3% loss=0.18700, acc=0.93750
# [57/100] training 33.4% loss=0.20187, acc=0.92188
# [57/100] training 33.7% loss=0.23262, acc=0.89062
# [57/100] training 33.8% loss=0.18544, acc=0.93750
# [57/100] training 34.0% loss=0.21727, acc=0.92188
# [57/100] training 34.1% loss=0.18494, acc=0.95312
# [57/100] training 34.3% loss=0.13515, acc=0.96875
# [57/100] training 34.5% loss=0.17877, acc=0.90625
# [57/100] training 34.7% loss=0.12572, acc=0.95312
# [57/100] training 34.9% loss=0.11509, acc=0.96875
# [57/100] training 35.0% loss=0.14317, acc=0.96875
# [57/100] training 35.2% loss=0.22824, acc=0.90625
# [57/100] training 35.3% loss=0.16869, acc=0.95312
# [57/100] training 35.5% loss=0.13123, acc=0.95312
# [57/100] training 35.6% loss=0.31411, acc=0.92188
# [57/100] training 35.9% loss=0.22820, acc=0.92188
# [57/100] training 36.1% loss=0.24824, acc=0.92188
# [57/100] training 36.2% loss=0.20186, acc=0.92188
# [57/100] training 36.4% loss=0.27042, acc=0.85938
# [57/100] training 36.5% loss=0.25046, acc=0.89062
# [57/100] training 36.7% loss=0.18079, acc=0.93750
# [57/100] training 36.8% loss=0.11508, acc=0.96875
# [57/100] training 37.1% loss=0.23190, acc=0.89062
# [57/100] training 37.3% loss=0.18787, acc=0.89062
# [57/100] training 37.4% loss=0.09940, acc=0.96875
# [57/100] training 37.6% loss=0.09797, acc=0.95312
# [57/100] training 37.7% loss=0.12855, acc=0.95312
# [57/100] training 37.9% loss=0.19608, acc=0.92188
# [57/100] training 38.1% loss=0.17804, acc=0.90625
# [57/100] training 38.3% loss=0.11213, acc=0.95312
# [57/100] training 38.4% loss=0.04193, acc=1.00000
# [57/100] training 38.6% loss=0.16442, acc=0.93750
# [57/100] training 38.8% loss=0.25131, acc=0.89062
# [57/100] training 38.9% loss=0.10815, acc=0.95312
# [57/100] training 39.1% loss=0.23346, acc=0.90625
# [57/100] training 39.3% loss=0.17762, acc=0.90625
# [57/100] training 39.5% loss=0.15357, acc=0.93750
# [57/100] training 39.6% loss=0.15619, acc=0.92188
# [57/100] training 39.8% loss=0.07585, acc=0.96875
# [57/100] training 40.0% loss=0.21748, acc=0.90625
# [57/100] training 40.1% loss=0.30566, acc=0.87500
# [57/100] training 40.4% loss=0.13735, acc=0.93750
# [57/100] training 40.5% loss=0.27221, acc=0.92188
# [57/100] training 40.7% loss=0.15703, acc=0.95312
# [57/100] training 40.8% loss=0.16125, acc=0.92188
# [57/100] training 41.0% loss=0.13590, acc=0.95312
# [57/100] training 41.1% loss=0.26288, acc=0.87500
# [57/100] training 41.3% loss=0.20110, acc=0.90625
# [57/100] training 41.6% loss=0.28801, acc=0.85938
# [57/100] training 41.7% loss=0.20490, acc=0.89062
# [57/100] training 41.9% loss=0.14238, acc=0.96875
# [57/100] training 42.0% loss=0.19112, acc=0.95312
# [57/100] training 42.2% loss=0.27757, acc=0.85938
# [57/100] training 42.3% loss=0.13483, acc=0.95312
# [57/100] training 42.5% loss=0.10269, acc=0.95312
# [57/100] training 42.8% loss=0.07075, acc=0.98438
# [57/100] training 42.9% loss=0.17925, acc=0.93750
# [57/100] training 43.1% loss=0.17431, acc=0.95312
# [57/100] training 43.2% loss=0.14341, acc=0.95312
# [57/100] training 43.4% loss=0.19319, acc=0.92188
# [57/100] training 43.5% loss=0.28451, acc=0.90625
# [57/100] training 43.8% loss=0.24794, acc=0.92188
# [57/100] training 43.9% loss=0.14817, acc=0.95312
# [57/100] training 44.1% loss=0.18830, acc=0.95312
# [57/100] training 44.3% loss=0.13883, acc=0.95312
# [57/100] training 44.4% loss=0.30291, acc=0.87500
# [57/100] training 44.6% loss=0.17962, acc=0.92188
# [57/100] training 44.7% loss=0.28660, acc=0.87500
# [57/100] training 45.0% loss=0.07892, acc=0.95312
# [57/100] training 45.1% loss=0.16914, acc=0.93750
# [57/100] training 45.3% loss=0.19659, acc=0.89062
# [57/100] training 45.5% loss=0.05363, acc=1.00000
# [57/100] training 45.6% loss=0.33649, acc=0.85938
# [57/100] training 45.8% loss=0.11113, acc=0.95312
# [57/100] training 45.9% loss=0.16610, acc=0.90625
# [57/100] training 46.2% loss=0.07302, acc=0.98438
# [57/100] training 46.3% loss=0.07734, acc=0.96875
# [57/100] training 46.5% loss=0.27091, acc=0.92188
# [57/100] training 46.6% loss=0.28709, acc=0.92188
# [57/100] training 46.8% loss=0.08594, acc=0.98438
# [57/100] training 47.0% loss=0.13886, acc=0.93750
# [57/100] training 47.2% loss=0.22280, acc=0.89062
# [57/100] training 47.4% loss=0.16627, acc=0.92188
# [57/100] training 47.5% loss=0.22131, acc=0.90625
# [57/100] training 47.7% loss=0.13975, acc=0.93750
# [57/100] training 47.8% loss=0.25830, acc=0.90625
# [57/100] training 48.0% loss=0.13241, acc=0.95312
# [57/100] training 48.3% loss=0.09185, acc=0.96875
# [57/100] training 48.4% loss=0.03586, acc=1.00000
# [57/100] training 48.6% loss=0.06678, acc=0.98438
# [57/100] training 48.7% loss=0.18534, acc=0.89062
# [57/100] training 48.9% loss=0.11962, acc=0.96875
# [57/100] training 49.0% loss=0.09996, acc=0.96875
# [57/100] training 49.2% loss=0.18239, acc=0.90625
# [57/100] training 49.3% loss=0.08998, acc=0.98438
# [57/100] training 49.6% loss=0.29014, acc=0.95312
# [57/100] training 49.8% loss=0.17818, acc=0.95312
# [57/100] training 49.9% loss=0.27722, acc=0.89062
# [57/100] training 50.1% loss=0.05576, acc=0.98438
# [57/100] training 50.2% loss=0.26048, acc=0.92188
# [57/100] training 50.4% loss=0.25985, acc=0.90625
# [57/100] training 50.6% loss=0.19897, acc=0.95312
# [57/100] training 50.8% loss=0.23234, acc=0.87500
# [57/100] training 51.0% loss=0.23570, acc=0.92188
# [57/100] training 51.1% loss=0.22395, acc=0.90625
# [57/100] training 51.3% loss=0.25709, acc=0.90625
# [57/100] training 51.4% loss=0.18709, acc=0.96875
# [57/100] training 51.7% loss=0.21758, acc=0.92188
# [57/100] training 51.8% loss=0.14790, acc=0.92188
# [57/100] training 52.0% loss=0.21232, acc=0.90625
# [57/100] training 52.1% loss=0.20804, acc=0.90625
# [57/100] training 52.3% loss=0.27183, acc=0.92188
# [57/100] training 52.5% loss=0.06666, acc=0.98438
# [57/100] training 52.6% loss=0.16495, acc=0.93750
# [57/100] training 52.9% loss=0.28359, acc=0.92188
# [57/100] training 53.0% loss=0.13067, acc=0.92188
# [57/100] training 53.2% loss=0.29058, acc=0.92188
# [57/100] training 53.3% loss=0.14886, acc=0.96875
# [57/100] training 53.5% loss=0.28141, acc=0.84375
# [57/100] training 53.7% loss=0.06742, acc=0.98438
# [57/100] training 53.8% loss=0.24467, acc=0.87500
# [57/100] training 54.1% loss=0.19706, acc=0.89062
# [57/100] training 54.2% loss=0.09201, acc=0.95312
# [57/100] training 54.4% loss=0.13976, acc=0.95312
# [57/100] training 54.5% loss=0.16903, acc=0.95312
# [57/100] training 54.7% loss=0.21548, acc=0.93750
# [57/100] training 54.8% loss=0.09385, acc=0.98438
# [57/100] training 55.1% loss=0.18512, acc=0.93750
# [57/100] training 55.3% loss=0.16108, acc=0.90625
# [57/100] training 55.4% loss=0.11312, acc=0.93750
# [57/100] training 55.6% loss=0.18221, acc=0.95312
# [57/100] training 55.7% loss=0.04267, acc=0.98438
# [57/100] training 55.9% loss=0.14277, acc=0.95312
# [57/100] training 56.0% loss=0.17336, acc=0.92188
# [57/100] training 56.3% loss=0.31620, acc=0.87500
# [57/100] training 56.5% loss=0.14681, acc=0.95312
# [57/100] training 56.6% loss=0.16856, acc=0.90625
# [57/100] training 56.8% loss=0.15189, acc=0.92188
# [57/100] training 56.9% loss=0.12789, acc=0.95312
# [57/100] training 57.1% loss=0.15580, acc=0.92188
# [57/100] training 57.2% loss=0.08330, acc=0.98438
# [57/100] training 57.5% loss=0.13705, acc=0.90625
# [57/100] training 57.6% loss=0.12343, acc=0.90625
# [57/100] training 57.8% loss=0.12652, acc=0.93750
# [57/100] training 58.0% loss=0.12307, acc=0.95312
# [57/100] training 58.1% loss=0.10709, acc=0.93750
# [57/100] training 58.3% loss=0.10148, acc=0.95312
# [57/100] training 58.4% loss=0.28527, acc=0.92188
# [57/100] training 58.7% loss=0.22067, acc=0.90625
# [57/100] training 58.8% loss=0.17872, acc=0.89062
# [57/100] training 59.0% loss=0.12690, acc=0.93750
# [57/100] training 59.2% loss=0.19851, acc=0.90625
# [57/100] training 59.3% loss=0.14842, acc=0.93750
# [57/100] training 59.5% loss=0.16960, acc=0.90625
# [57/100] training 59.7% loss=0.19444, acc=0.95312
# [57/100] training 59.9% loss=0.13894, acc=0.95312
# [57/100] training 60.0% loss=0.21275, acc=0.92188
# [57/100] training 60.2% loss=0.19009, acc=0.90625
# [57/100] training 60.3% loss=0.16772, acc=0.90625
# [57/100] training 60.5% loss=0.28308, acc=0.87500
# [57/100] training 60.8% loss=0.27306, acc=0.85938
# [57/100] training 60.9% loss=0.29326, acc=0.84375
# [57/100] training 61.1% loss=0.20292, acc=0.89062
# [57/100] training 61.2% loss=0.17491, acc=0.89062
# [57/100] training 61.4% loss=0.19415, acc=0.93750
# [57/100] training 61.5% loss=0.34053, acc=0.87500
# [57/100] training 61.7% loss=0.12888, acc=0.95312
# [57/100] training 62.0% loss=0.18069, acc=0.95312
# [57/100] training 62.1% loss=0.30674, acc=0.85938
# [57/100] training 62.3% loss=0.14780, acc=0.95312
# [57/100] training 62.4% loss=0.13516, acc=0.93750
# [57/100] training 62.6% loss=0.23988, acc=0.85938
# [57/100] training 62.7% loss=0.12777, acc=0.92188
# [57/100] training 62.9% loss=0.19173, acc=0.89062
# [57/100] training 63.1% loss=0.22758, acc=0.93750
# [57/100] training 63.3% loss=0.16547, acc=0.93750
# [57/100] training 63.5% loss=0.13071, acc=0.92188
# [57/100] training 63.6% loss=0.17017, acc=0.96875
# [57/100] training 63.8% loss=0.27208, acc=0.87500
# [57/100] training 63.9% loss=0.07881, acc=0.96875
# [57/100] training 64.2% loss=0.18515, acc=0.92188
# [57/100] training 64.3% loss=0.19171, acc=0.90625
# [57/100] training 64.5% loss=0.25110, acc=0.89062
# [57/100] training 64.7% loss=0.12115, acc=0.96875
# [57/100] training 64.8% loss=0.22601, acc=0.92188
# [57/100] training 65.0% loss=0.14906, acc=0.93750
# [57/100] training 65.1% loss=0.20959, acc=0.93750
# [57/100] training 65.4% loss=0.11414, acc=0.95312
# [57/100] training 65.5% loss=0.08210, acc=0.95312
# [57/100] training 65.7% loss=0.08860, acc=0.93750
# [57/100] training 65.8% loss=0.14917, acc=0.93750
# [57/100] training 66.0% loss=0.14415, acc=0.95312
# [57/100] training 66.2% loss=0.05640, acc=0.98438
# [57/100] training 66.3% loss=0.24366, acc=0.90625
# [57/100] training 66.6% loss=0.20514, acc=0.90625
# [57/100] training 66.7% loss=0.18494, acc=0.93750
# [57/100] training 66.9% loss=0.16261, acc=0.90625
# [57/100] training 67.0% loss=0.06673, acc=1.00000
# [57/100] training 67.2% loss=0.12288, acc=0.95312
# [57/100] training 67.4% loss=0.11800, acc=0.95312
# [57/100] training 67.6% loss=0.15590, acc=0.92188
# [57/100] training 67.8% loss=0.10793, acc=0.95312
# [57/100] training 67.9% loss=0.08101, acc=0.93750
# [57/100] training 68.1% loss=0.19295, acc=0.93750
# [57/100] training 68.2% loss=0.06533, acc=0.98438
# [57/100] training 68.4% loss=0.19427, acc=0.95312
# [57/100] training 68.5% loss=0.20118, acc=0.89062
# [57/100] training 68.8% loss=0.09184, acc=0.95312
# [57/100] training 69.0% loss=0.14951, acc=0.93750
# [57/100] training 69.1% loss=0.26392, acc=0.93750
# [57/100] training 69.3% loss=0.10582, acc=0.98438
# [57/100] training 69.4% loss=0.13511, acc=0.93750
# [57/100] training 69.6% loss=0.07233, acc=0.98438
# [57/100] training 69.7% loss=0.28797, acc=0.92188
# [57/100] training 70.0% loss=0.20777, acc=0.92188
# [57/100] training 70.2% loss=0.20084, acc=0.90625
# [57/100] training 70.3% loss=0.19154, acc=0.89062
# [57/100] training 70.5% loss=0.18317, acc=0.92188
# [57/100] training 70.6% loss=0.13429, acc=0.92188
# [57/100] training 70.8% loss=0.20781, acc=0.92188
# [57/100] training 71.0% loss=0.28899, acc=0.92188
# [57/100] training 71.2% loss=0.10378, acc=0.96875
# [57/100] training 71.3% loss=0.20025, acc=0.92188
# [57/100] training 71.5% loss=0.14846, acc=0.96875
# [57/100] training 71.7% loss=0.25305, acc=0.92188
# [57/100] training 71.8% loss=0.20974, acc=0.93750
# [57/100] training 72.0% loss=0.10032, acc=0.93750
# [57/100] training 72.2% loss=0.23620, acc=0.90625
# [57/100] training 72.4% loss=0.24276, acc=0.89062
# [57/100] training 72.5% loss=0.16045, acc=0.93750
# [57/100] training 72.7% loss=0.23628, acc=0.93750
# [57/100] training 72.9% loss=0.25314, acc=0.87500
# [57/100] training 73.0% loss=0.10268, acc=0.96875
# [57/100] training 73.3% loss=0.27861, acc=0.85938
# [57/100] training 73.4% loss=0.16485, acc=0.92188
# [57/100] training 73.6% loss=0.12854, acc=0.96875
# [57/100] training 73.7% loss=0.14806, acc=0.93750
# [57/100] training 73.9% loss=0.15146, acc=0.92188
# [57/100] training 74.0% loss=0.09753, acc=0.96875
# [57/100] training 74.2% loss=0.10080, acc=0.95312
# [57/100] training 74.5% loss=0.06960, acc=0.98438
# [57/100] training 74.6% loss=0.34521, acc=0.90625
# [57/100] training 74.8% loss=0.25328, acc=0.84375
# [57/100] training 74.9% loss=0.32601, acc=0.85938
# [57/100] training 75.1% loss=0.08497, acc=0.96875
# [57/100] training 75.2% loss=0.11565, acc=0.93750
# [57/100] training 75.4% loss=0.11852, acc=0.92188
# [57/100] training 75.7% loss=0.22843, acc=0.93750
# [57/100] training 75.8% loss=0.15426, acc=0.93750
# [57/100] training 76.0% loss=0.16708, acc=0.93750
# [57/100] training 76.1% loss=0.19569, acc=0.93750
# [57/100] training 76.3% loss=0.05019, acc=1.00000
# [57/100] training 76.4% loss=0.28328, acc=0.87500
# [57/100] training 76.7% loss=0.16552, acc=0.93750
# [57/100] training 76.8% loss=0.11151, acc=0.95312
# [57/100] training 77.0% loss=0.11024, acc=0.92188
# [57/100] training 77.2% loss=0.13339, acc=0.93750
# [57/100] training 77.3% loss=0.05333, acc=0.98438
# [57/100] training 77.5% loss=0.17387, acc=0.90625
# [57/100] training 77.6% loss=0.22521, acc=0.90625
# [57/100] training 77.9% loss=0.16624, acc=0.92188
# [57/100] training 78.0% loss=0.13718, acc=0.93750
# [57/100] training 78.2% loss=0.25687, acc=0.90625
# [57/100] training 78.4% loss=0.11204, acc=0.93750
# [57/100] training 78.5% loss=0.25745, acc=0.92188
# [57/100] training 78.7% loss=0.16672, acc=0.89062
# [57/100] training 78.8% loss=0.13293, acc=0.96875
# [57/100] training 79.1% loss=0.07685, acc=0.98438
# [57/100] training 79.2% loss=0.13576, acc=0.95312
# [57/100] training 79.4% loss=0.17418, acc=0.95312
# [57/100] training 79.5% loss=0.17359, acc=0.92188
# [57/100] training 79.7% loss=0.06129, acc=0.98438
# [57/100] training 79.9% loss=0.11061, acc=0.95312
# [57/100] training 80.1% loss=0.10501, acc=0.93750
# [57/100] training 80.3% loss=0.18767, acc=0.90625
# [57/100] training 80.4% loss=0.27652, acc=0.90625
# [57/100] training 80.6% loss=0.33869, acc=0.87500
# [57/100] training 80.7% loss=0.14272, acc=0.93750
# [57/100] training 80.9% loss=0.19620, acc=0.90625
# [57/100] training 81.2% loss=0.17843, acc=0.92188
# [57/100] training 81.3% loss=0.13663, acc=0.95312
# [57/100] training 81.5% loss=0.19174, acc=0.90625
# [57/100] training 81.6% loss=0.26464, acc=0.87500
# [57/100] training 81.8% loss=0.10645, acc=0.93750
# [57/100] training 81.9% loss=0.27698, acc=0.93750
# [57/100] training 82.1% loss=0.11017, acc=0.95312
# [57/100] training 82.2% loss=0.13594, acc=0.93750
# [57/100] training 82.5% loss=0.11774, acc=0.95312
# [57/100] training 82.7% loss=0.25693, acc=0.90625
# [57/100] training 82.8% loss=0.17017, acc=0.90625
# [57/100] training 83.0% loss=0.10852, acc=0.96875
# [57/100] training 83.1% loss=0.19710, acc=0.90625
# [57/100] training 83.3% loss=0.11064, acc=0.95312
# [57/100] training 83.5% loss=0.19083, acc=0.93750
# [57/100] training 83.7% loss=0.37493, acc=0.84375
# [57/100] training 83.9% loss=0.34406, acc=0.84375
# [57/100] training 84.0% loss=0.10585, acc=0.96875
# [57/100] training 84.2% loss=0.09880, acc=0.95312
# [57/100] training 84.3% loss=0.11763, acc=0.96875
# [57/100] training 84.5% loss=0.09529, acc=0.96875
# [57/100] training 84.7% loss=0.08430, acc=0.93750
# [57/100] training 84.9% loss=0.18200, acc=0.95312
# [57/100] training 85.0% loss=0.12675, acc=0.93750
# [57/100] training 85.2% loss=0.11398, acc=0.96875
# [57/100] training 85.4% loss=0.05787, acc=0.96875
# [57/100] training 85.5% loss=0.31790, acc=0.85938
# [57/100] training 85.8% loss=0.20988, acc=0.92188
# [57/100] training 85.9% loss=0.10927, acc=0.95312
# [57/100] training 86.1% loss=0.14671, acc=0.93750
# [57/100] training 86.2% loss=0.12757, acc=0.93750
# [57/100] training 86.4% loss=0.24603, acc=0.93750
# [57/100] training 86.6% loss=0.10927, acc=0.96875
# [57/100] training 86.7% loss=0.07850, acc=0.98438
# [57/100] training 87.0% loss=0.17249, acc=0.92188
# [57/100] training 87.1% loss=0.12538, acc=0.95312
# [57/100] training 87.3% loss=0.11414, acc=0.93750
# [57/100] training 87.4% loss=0.14663, acc=0.92188
# [57/100] training 87.6% loss=0.11964, acc=0.95312
# [57/100] training 87.7% loss=0.35728, acc=0.90625
# [57/100] training 87.9% loss=0.29154, acc=0.90625
# [57/100] training 88.2% loss=0.08011, acc=0.96875
# [57/100] training 88.3% loss=0.24551, acc=0.90625
# [57/100] training 88.5% loss=0.14371, acc=0.95312
# [57/100] training 88.6% loss=0.13924, acc=0.92188
# [57/100] training 88.8% loss=0.17396, acc=0.93750
# [57/100] training 88.9% loss=0.34729, acc=0.84375
# [57/100] training 89.2% loss=0.12952, acc=0.93750
# [57/100] training 89.4% loss=0.15962, acc=0.93750
# [57/100] training 89.5% loss=0.23469, acc=0.92188
# [57/100] training 89.7% loss=0.14010, acc=0.92188
# [57/100] training 89.8% loss=0.14202, acc=0.96875
# [57/100] training 90.0% loss=0.14041, acc=0.93750
# [57/100] training 90.1% loss=0.13281, acc=0.96875
# [57/100] training 90.4% loss=0.22735, acc=0.89062
# [57/100] training 90.5% loss=0.09494, acc=0.96875
# [57/100] training 90.7% loss=0.13280, acc=0.95312
# [57/100] training 90.9% loss=0.11573, acc=0.96875
# [57/100] training 91.0% loss=0.26055, acc=0.89062
# [57/100] training 91.2% loss=0.16326, acc=0.95312
# [57/100] training 91.3% loss=0.24465, acc=0.90625
# [57/100] training 91.6% loss=0.24186, acc=0.89062
# [57/100] training 91.7% loss=0.12154, acc=0.95312
# [57/100] training 91.9% loss=0.30442, acc=0.87500
# [57/100] training 92.1% loss=0.11020, acc=0.95312
# [57/100] training 92.2% loss=0.15164, acc=0.92188
# [57/100] training 92.4% loss=0.21417, acc=0.89062
# [57/100] training 92.6% loss=0.30152, acc=0.82812
# [57/100] training 92.8% loss=0.19471, acc=0.90625
# [57/100] training 92.9% loss=0.29718, acc=0.90625
# [57/100] training 93.1% loss=0.40909, acc=0.89062
# [57/100] training 93.2% loss=0.27592, acc=0.90625
# [57/100] training 93.4% loss=0.25286, acc=0.87500
# [57/100] training 93.7% loss=0.16276, acc=0.95312
# [57/100] training 93.8% loss=0.19444, acc=0.93750
# [57/100] training 94.0% loss=0.15322, acc=0.95312
# [57/100] training 94.1% loss=0.17956, acc=0.92188
# [57/100] training 94.3% loss=0.07013, acc=0.98438
# [57/100] training 94.4% loss=0.21175, acc=0.93750
# [57/100] training 94.6% loss=0.12095, acc=0.93750
# [57/100] training 94.9% loss=0.10858, acc=0.96875
# [57/100] training 95.0% loss=0.19337, acc=0.95312
# [57/100] training 95.2% loss=0.38577, acc=0.87500
# [57/100] training 95.3% loss=0.26166, acc=0.92188
# [57/100] training 95.5% loss=0.08414, acc=0.96875
# [57/100] training 95.6% loss=0.18112, acc=0.95312
# [57/100] training 95.8% loss=0.10546, acc=0.98438
# [57/100] training 96.0% loss=0.20427, acc=0.90625
# [57/100] training 96.2% loss=0.16136, acc=0.92188
# [57/100] training 96.4% loss=0.11889, acc=0.95312
# [57/100] training 96.5% loss=0.14994, acc=0.95312
# [57/100] training 96.7% loss=0.08610, acc=0.98438
# [57/100] training 96.8% loss=0.27626, acc=0.90625
# [57/100] training 97.1% loss=0.23387, acc=0.93750
# [57/100] training 97.2% loss=0.21350, acc=0.92188
# [57/100] training 97.4% loss=0.23108, acc=0.90625
# [57/100] training 97.6% loss=0.19999, acc=0.87500
# [57/100] training 97.7% loss=0.23262, acc=0.89062
# [57/100] training 97.9% loss=0.18333, acc=0.92188
# [57/100] training 98.0% loss=0.19066, acc=0.92188
# [57/100] training 98.3% loss=0.27446, acc=0.82812
# [57/100] training 98.4% loss=0.17293, acc=0.95312
# [57/100] training 98.6% loss=0.26449, acc=0.90625
# [57/100] training 98.7% loss=0.25348, acc=0.90625
# [57/100] training 98.9% loss=0.22659, acc=0.92188
# [57/100] training 99.1% loss=0.16909, acc=0.92188
# [57/100] training 99.2% loss=0.15746, acc=0.92188
# [57/100] training 99.5% loss=0.24384, acc=0.89062
# [57/100] training 99.6% loss=0.21006, acc=0.90625
# [57/100] training 99.8% loss=0.06637, acc=0.96875
# [57/100] training 99.9% loss=0.02631, acc=1.00000
# [57/100] testing 0.9% loss=0.15938, acc=0.96875
# [57/100] testing 1.8% loss=0.46937, acc=0.85938
# [57/100] testing 2.2% loss=0.28025, acc=0.92188
# [57/100] testing 3.1% loss=0.40300, acc=0.87500
# [57/100] testing 3.5% loss=0.11463, acc=0.93750
# [57/100] testing 4.4% loss=0.18962, acc=0.90625
# [57/100] testing 4.8% loss=0.37629, acc=0.84375
# [57/100] testing 5.7% loss=0.28204, acc=0.87500
# [57/100] testing 6.6% loss=0.15372, acc=0.93750
# [57/100] testing 7.0% loss=0.07534, acc=0.98438
# [57/100] testing 7.9% loss=0.24843, acc=0.89062
# [57/100] testing 8.3% loss=0.25369, acc=0.92188
# [57/100] testing 9.2% loss=0.32106, acc=0.93750
# [57/100] testing 9.7% loss=0.09333, acc=0.96875
# [57/100] testing 10.5% loss=0.18840, acc=0.93750
# [57/100] testing 11.0% loss=0.29112, acc=0.87500
# [57/100] testing 11.8% loss=0.15904, acc=0.93750
# [57/100] testing 12.7% loss=0.45442, acc=0.90625
# [57/100] testing 13.2% loss=0.23744, acc=0.89062
# [57/100] testing 14.0% loss=0.39524, acc=0.90625
# [57/100] testing 14.5% loss=0.24987, acc=0.89062
# [57/100] testing 15.4% loss=0.29072, acc=0.92188
# [57/100] testing 15.8% loss=0.15978, acc=0.95312
# [57/100] testing 16.7% loss=0.23006, acc=0.93750
# [57/100] testing 17.5% loss=0.19151, acc=0.93750
# [57/100] testing 18.0% loss=0.20791, acc=0.93750
# [57/100] testing 18.9% loss=0.06739, acc=0.98438
# [57/100] testing 19.3% loss=0.26678, acc=0.92188
# [57/100] testing 20.2% loss=0.34629, acc=0.89062
# [57/100] testing 20.6% loss=0.26828, acc=0.93750
# [57/100] testing 21.5% loss=0.19085, acc=0.90625
# [57/100] testing 21.9% loss=0.48087, acc=0.85938
# [57/100] testing 22.8% loss=0.28033, acc=0.92188
# [57/100] testing 23.7% loss=0.48404, acc=0.85938
# [57/100] testing 24.1% loss=0.24039, acc=0.90625
# [57/100] testing 25.0% loss=0.40574, acc=0.87500
# [57/100] testing 25.4% loss=0.11541, acc=0.96875
# [57/100] testing 26.3% loss=0.26935, acc=0.89062
# [57/100] testing 26.8% loss=0.32755, acc=0.90625
# [57/100] testing 27.6% loss=0.15929, acc=0.92188
# [57/100] testing 28.5% loss=0.26191, acc=0.92188
# [57/100] testing 29.0% loss=0.24877, acc=0.92188
# [57/100] testing 29.8% loss=0.41446, acc=0.90625
# [57/100] testing 30.3% loss=0.34140, acc=0.92188
# [57/100] testing 31.1% loss=0.39073, acc=0.90625
# [57/100] testing 31.6% loss=0.25825, acc=0.92188
# [57/100] testing 32.5% loss=0.31721, acc=0.92188
# [57/100] testing 32.9% loss=0.60846, acc=0.85938
# [57/100] testing 33.8% loss=0.33768, acc=0.89062
# [57/100] testing 34.7% loss=0.51130, acc=0.89062
# [57/100] testing 35.1% loss=0.13346, acc=0.92188
# [57/100] testing 36.0% loss=0.28521, acc=0.92188
# [57/100] testing 36.4% loss=0.28822, acc=0.92188
# [57/100] testing 37.3% loss=0.29510, acc=0.95312
# [57/100] testing 37.7% loss=0.32741, acc=0.89062
# [57/100] testing 38.6% loss=0.24207, acc=0.95312
# [57/100] testing 39.5% loss=0.26114, acc=0.95312
# [57/100] testing 39.9% loss=0.25699, acc=0.89062
# [57/100] testing 40.8% loss=0.40597, acc=0.89062
# [57/100] testing 41.2% loss=0.26029, acc=0.93750
# [57/100] testing 42.1% loss=0.25682, acc=0.92188
# [57/100] testing 42.5% loss=0.13376, acc=0.93750
# [57/100] testing 43.4% loss=0.24686, acc=0.92188
# [57/100] testing 43.9% loss=0.09074, acc=0.96875
# [57/100] testing 44.7% loss=0.28237, acc=0.90625
# [57/100] testing 45.6% loss=0.24817, acc=0.93750
# [57/100] testing 46.1% loss=0.28679, acc=0.89062
# [57/100] testing 46.9% loss=0.14783, acc=0.95312
# [57/100] testing 47.4% loss=0.10369, acc=0.93750
# [57/100] testing 48.3% loss=0.37209, acc=0.90625
# [57/100] testing 48.7% loss=0.35442, acc=0.89062
# [57/100] testing 49.6% loss=0.57044, acc=0.81250
# [57/100] testing 50.4% loss=0.18221, acc=0.93750
# [57/100] testing 50.9% loss=0.28383, acc=0.89062
# [57/100] testing 51.8% loss=0.26361, acc=0.89062
# [57/100] testing 52.2% loss=0.16808, acc=0.92188
# [57/100] testing 53.1% loss=0.18561, acc=0.92188
# [57/100] testing 53.5% loss=0.22198, acc=0.92188
# [57/100] testing 54.4% loss=0.32366, acc=0.89062
# [57/100] testing 54.8% loss=0.49517, acc=0.85938
# [57/100] testing 55.7% loss=0.09886, acc=0.96875
# [57/100] testing 56.6% loss=0.38003, acc=0.87500
# [57/100] testing 57.0% loss=0.39309, acc=0.87500
# [57/100] testing 57.9% loss=0.31668, acc=0.89062
# [57/100] testing 58.3% loss=0.33916, acc=0.89062
# [57/100] testing 59.2% loss=0.20352, acc=0.92188
# [57/100] testing 59.7% loss=0.23168, acc=0.90625
# [57/100] testing 60.5% loss=0.35377, acc=0.85938
# [57/100] testing 61.4% loss=0.16865, acc=0.92188
# [57/100] testing 61.9% loss=0.21818, acc=0.90625
# [57/100] testing 62.7% loss=0.17847, acc=0.92188
# [57/100] testing 63.2% loss=0.39369, acc=0.87500
# [57/100] testing 64.0% loss=0.43280, acc=0.90625
# [57/100] testing 64.5% loss=0.14220, acc=0.90625
# [57/100] testing 65.4% loss=0.14557, acc=0.93750
# [57/100] testing 65.8% loss=0.37651, acc=0.89062
# [57/100] testing 66.7% loss=0.15435, acc=0.92188
# [57/100] testing 67.6% loss=0.43645, acc=0.92188
# [57/100] testing 68.0% loss=0.11946, acc=0.93750
# [57/100] testing 68.9% loss=0.33546, acc=0.92188
# [57/100] testing 69.3% loss=0.18928, acc=0.92188
# [57/100] testing 70.2% loss=0.45082, acc=0.84375
# [57/100] testing 70.6% loss=0.26179, acc=0.90625
# [57/100] testing 71.5% loss=0.34681, acc=0.89062
# [57/100] testing 72.4% loss=0.15570, acc=0.96875
# [57/100] testing 72.8% loss=0.23315, acc=0.89062
# [57/100] testing 73.7% loss=0.15231, acc=0.96875
# [57/100] testing 74.1% loss=0.46866, acc=0.85938
# [57/100] testing 75.0% loss=0.18893, acc=0.92188
# [57/100] testing 75.4% loss=0.35920, acc=0.87500
# [57/100] testing 76.3% loss=0.05325, acc=0.98438
# [57/100] testing 76.8% loss=0.27500, acc=0.87500
# [57/100] testing 77.6% loss=0.13810, acc=0.93750
# [57/100] testing 78.5% loss=0.46675, acc=0.89062
# [57/100] testing 79.0% loss=0.26586, acc=0.87500
# [57/100] testing 79.8% loss=0.36823, acc=0.89062
# [57/100] testing 80.3% loss=0.35370, acc=0.87500
# [57/100] testing 81.2% loss=0.53630, acc=0.85938
# [57/100] testing 81.6% loss=0.33606, acc=0.89062
# [57/100] testing 82.5% loss=0.19252, acc=0.92188
# [57/100] testing 83.3% loss=0.24315, acc=0.92188
# [57/100] testing 83.8% loss=0.19059, acc=0.95312
# [57/100] testing 84.7% loss=0.27304, acc=0.85938
# [57/100] testing 85.1% loss=0.20095, acc=0.92188
# [57/100] testing 86.0% loss=0.31342, acc=0.90625
# [57/100] testing 86.4% loss=0.44129, acc=0.84375
# [57/100] testing 87.3% loss=0.35009, acc=0.81250
# [57/100] testing 87.7% loss=0.23319, acc=0.90625
# [57/100] testing 88.6% loss=0.29146, acc=0.87500
# [57/100] testing 89.5% loss=0.54159, acc=0.79688
# [57/100] testing 89.9% loss=0.06395, acc=0.98438
# [57/100] testing 90.8% loss=0.25108, acc=0.96875
# [57/100] testing 91.2% loss=0.24653, acc=0.93750
# [57/100] testing 92.1% loss=0.26234, acc=0.93750
# [57/100] testing 92.6% loss=0.51167, acc=0.89062
# [57/100] testing 93.4% loss=0.32606, acc=0.85938
# [57/100] testing 94.3% loss=0.18934, acc=0.93750
# [57/100] testing 94.7% loss=0.18972, acc=0.95312
# [57/100] testing 95.6% loss=0.36742, acc=0.85938
# [57/100] testing 96.1% loss=0.17312, acc=0.95312
# [57/100] testing 96.9% loss=0.17992, acc=0.93750
# [57/100] testing 97.4% loss=0.12132, acc=0.98438
# [57/100] testing 98.3% loss=0.13171, acc=0.93750
# [57/100] testing 98.7% loss=0.20780, acc=0.92188
# [57/100] testing 99.6% loss=0.26623, acc=0.89062
# [58/100] training 0.2% loss=0.22006, acc=0.90625
# [58/100] training 0.4% loss=0.28251, acc=0.93750
# [58/100] training 0.5% loss=0.09826, acc=0.96875
# [58/100] training 0.8% loss=0.16963, acc=0.93750
# [58/100] training 0.9% loss=0.14752, acc=0.95312
# [58/100] training 1.1% loss=0.19717, acc=0.95312
# [58/100] training 1.2% loss=0.17594, acc=0.90625
# [58/100] training 1.4% loss=0.11320, acc=0.96875
# [58/100] training 1.6% loss=0.09178, acc=0.96875
# [58/100] training 1.8% loss=0.12117, acc=0.98438
# [58/100] training 2.0% loss=0.20776, acc=0.90625
# [58/100] training 2.1% loss=0.20824, acc=0.90625
# [58/100] training 2.3% loss=0.09986, acc=0.98438
# [58/100] training 2.4% loss=0.11162, acc=0.95312
# [58/100] training 2.6% loss=0.07937, acc=0.96875
# [58/100] training 2.7% loss=0.18161, acc=0.90625
# [58/100] training 3.0% loss=0.20073, acc=0.92188
# [58/100] training 3.2% loss=0.10912, acc=0.95312
# [58/100] training 3.3% loss=0.34218, acc=0.89062
# [58/100] training 3.5% loss=0.14482, acc=0.92188
# [58/100] training 3.6% loss=0.31571, acc=0.89062
# [58/100] training 3.8% loss=0.14914, acc=0.92188
# [58/100] training 3.9% loss=0.15522, acc=0.93750
# [58/100] training 4.2% loss=0.06113, acc=0.98438
# [58/100] training 4.4% loss=0.08612, acc=0.96875
# [58/100] training 4.5% loss=0.16726, acc=0.92188
# [58/100] training 4.7% loss=0.28029, acc=0.87500
# [58/100] training 4.8% loss=0.23246, acc=0.92188
# [58/100] training 5.0% loss=0.14856, acc=0.93750
# [58/100] training 5.2% loss=0.21029, acc=0.92188
# [58/100] training 5.4% loss=0.07793, acc=0.98438
# [58/100] training 5.5% loss=0.18862, acc=0.92188
# [58/100] training 5.7% loss=0.20915, acc=0.93750
# [58/100] training 5.9% loss=0.20627, acc=0.95312
# [58/100] training 6.0% loss=0.21806, acc=0.89062
# [58/100] training 6.3% loss=0.14427, acc=0.98438
# [58/100] training 6.4% loss=0.14886, acc=0.96875
# [58/100] training 6.6% loss=0.12768, acc=0.96875
# [58/100] training 6.7% loss=0.17975, acc=0.92188
# [58/100] training 6.9% loss=0.11439, acc=0.95312
# [58/100] training 7.1% loss=0.12205, acc=0.95312
# [58/100] training 7.2% loss=0.20644, acc=0.92188
# [58/100] training 7.5% loss=0.17998, acc=0.90625
# [58/100] training 7.6% loss=0.22958, acc=0.90625
# [58/100] training 7.8% loss=0.12165, acc=0.96875
# [58/100] training 7.9% loss=0.16646, acc=0.90625
# [58/100] training 8.1% loss=0.10956, acc=0.95312
# [58/100] training 8.2% loss=0.14872, acc=0.95312
# [58/100] training 8.4% loss=0.27369, acc=0.92188
# [58/100] training 8.7% loss=0.18827, acc=0.93750
# [58/100] training 8.8% loss=0.19270, acc=0.95312
# [58/100] training 9.0% loss=0.09392, acc=0.98438
# [58/100] training 9.1% loss=0.17850, acc=0.95312
# [58/100] training 9.3% loss=0.18020, acc=0.93750
# [58/100] training 9.4% loss=0.10011, acc=0.98438
# [58/100] training 9.7% loss=0.17595, acc=0.93750
# [58/100] training 9.9% loss=0.26398, acc=0.92188
# [58/100] training 10.0% loss=0.21622, acc=0.89062
# [58/100] training 10.2% loss=0.13715, acc=0.92188
# [58/100] training 10.3% loss=0.20950, acc=0.90625
# [58/100] training 10.5% loss=0.25198, acc=0.89062
# [58/100] training 10.6% loss=0.18982, acc=0.93750
# [58/100] training 10.9% loss=0.20447, acc=0.92188
# [58/100] training 11.0% loss=0.25025, acc=0.92188
# [58/100] training 11.2% loss=0.10312, acc=1.00000
# [58/100] training 11.4% loss=0.23445, acc=0.90625
# [58/100] training 11.5% loss=0.33995, acc=0.87500
# [58/100] training 11.7% loss=0.05338, acc=0.98438
# [58/100] training 11.8% loss=0.17505, acc=0.93750
# [58/100] training 12.1% loss=0.17664, acc=0.92188
# [58/100] training 12.2% loss=0.08393, acc=0.96875
# [58/100] training 12.4% loss=0.10764, acc=0.93750
# [58/100] training 12.6% loss=0.23398, acc=0.92188
# [58/100] training 12.7% loss=0.07002, acc=0.96875
# [58/100] training 12.9% loss=0.21721, acc=0.90625
# [58/100] training 13.0% loss=0.19054, acc=0.90625
# [58/100] training 13.3% loss=0.14105, acc=0.93750
# [58/100] training 13.4% loss=0.21448, acc=0.90625
# [58/100] training 13.6% loss=0.09582, acc=0.95312
# [58/100] training 13.7% loss=0.33255, acc=0.84375
# [58/100] training 13.9% loss=0.31607, acc=0.89062
# [58/100] training 14.1% loss=0.26364, acc=0.92188
# [58/100] training 14.3% loss=0.20109, acc=0.90625
# [58/100] training 14.5% loss=0.17946, acc=0.90625
# [58/100] training 14.6% loss=0.07495, acc=0.96875
# [58/100] training 14.8% loss=0.33572, acc=0.89062
# [58/100] training 14.9% loss=0.08406, acc=0.98438
# [58/100] training 15.1% loss=0.25291, acc=0.90625
# [58/100] training 15.4% loss=0.21738, acc=0.92188
# [58/100] training 15.5% loss=0.17462, acc=0.93750
# [58/100] training 15.7% loss=0.25960, acc=0.90625
# [58/100] training 15.8% loss=0.15751, acc=0.93750
# [58/100] training 16.0% loss=0.18952, acc=0.93750
# [58/100] training 16.1% loss=0.28234, acc=0.89062
# [58/100] training 16.3% loss=0.19283, acc=0.90625
# [58/100] training 16.4% loss=0.15179, acc=0.93750
# [58/100] training 16.7% loss=0.19385, acc=0.93750
# [58/100] training 16.9% loss=0.28400, acc=0.89062
# [58/100] training 17.0% loss=0.15770, acc=0.92188
# [58/100] training 17.2% loss=0.07170, acc=0.98438
# [58/100] training 17.3% loss=0.18879, acc=0.92188
# [58/100] training 17.5% loss=0.23780, acc=0.85938
# [58/100] training 17.7% loss=0.22016, acc=0.92188
# [58/100] training 17.9% loss=0.15498, acc=0.92188
# [58/100] training 18.1% loss=0.19084, acc=0.89062
# [58/100] training 18.2% loss=0.18462, acc=0.95312
# [58/100] training 18.4% loss=0.27614, acc=0.87500
# [58/100] training 18.5% loss=0.14673, acc=0.95312
# [58/100] training 18.8% loss=0.11873, acc=0.96875
# [58/100] training 18.9% loss=0.09854, acc=0.96875
# [58/100] training 19.1% loss=0.21251, acc=0.92188
# [58/100] training 19.2% loss=0.12168, acc=0.95312
# [58/100] training 19.4% loss=0.17340, acc=0.92188
# [58/100] training 19.6% loss=0.20646, acc=0.90625
# [58/100] training 19.7% loss=0.27312, acc=0.89062
# [58/100] training 20.0% loss=0.10636, acc=0.95312
# [58/100] training 20.1% loss=0.15629, acc=0.93750
# [58/100] training 20.3% loss=0.33077, acc=0.90625
# [58/100] training 20.4% loss=0.16228, acc=0.93750
# [58/100] training 20.6% loss=0.25691, acc=0.89062
# [58/100] training 20.8% loss=0.14842, acc=0.95312
# [58/100] training 20.9% loss=0.22623, acc=0.90625
# [58/100] training 21.2% loss=0.12243, acc=0.95312
# [58/100] training 21.3% loss=0.23160, acc=0.89062
# [58/100] training 21.5% loss=0.26392, acc=0.92188
# [58/100] training 21.6% loss=0.07056, acc=0.98438
# [58/100] training 21.8% loss=0.15452, acc=0.93750
# [58/100] training 21.9% loss=0.20833, acc=0.92188
# [58/100] training 22.2% loss=0.15090, acc=0.98438
# [58/100] training 22.4% loss=0.19900, acc=0.90625
# [58/100] training 22.5% loss=0.10211, acc=0.95312
# [58/100] training 22.7% loss=0.14731, acc=0.95312
# [58/100] training 22.8% loss=0.15702, acc=0.95312
# [58/100] training 23.0% loss=0.09639, acc=0.96875
# [58/100] training 23.1% loss=0.26550, acc=0.90625
# [58/100] training 23.4% loss=0.29719, acc=0.85938
# [58/100] training 23.6% loss=0.17203, acc=0.93750
# [58/100] training 23.7% loss=0.14061, acc=0.95312
# [58/100] training 23.9% loss=0.20026, acc=0.90625
# [58/100] training 24.0% loss=0.09885, acc=0.96875
# [58/100] training 24.2% loss=0.09791, acc=0.98438
# [58/100] training 24.3% loss=0.19690, acc=0.92188
# [58/100] training 24.6% loss=0.19677, acc=0.92188
# [58/100] training 24.7% loss=0.26203, acc=0.87500
# [58/100] training 24.9% loss=0.18635, acc=0.90625
# [58/100] training 25.1% loss=0.20775, acc=0.89062
# [58/100] training 25.2% loss=0.13843, acc=0.93750
# [58/100] training 25.4% loss=0.21325, acc=0.90625
# [58/100] training 25.6% loss=0.15797, acc=0.95312
# [58/100] training 25.8% loss=0.18783, acc=0.93750
# [58/100] training 25.9% loss=0.15008, acc=0.95312
# [58/100] training 26.1% loss=0.16719, acc=0.92188
# [58/100] training 26.3% loss=0.14610, acc=0.93750
# [58/100] training 26.4% loss=0.13469, acc=0.90625
# [58/100] training 26.6% loss=0.04693, acc=0.98438
# [58/100] training 26.8% loss=0.10183, acc=0.96875
# [58/100] training 27.0% loss=0.25411, acc=0.90625
# [58/100] training 27.1% loss=0.20121, acc=0.93750
# [58/100] training 27.3% loss=0.16000, acc=0.92188
# [58/100] training 27.4% loss=0.12238, acc=0.95312
# [58/100] training 27.6% loss=0.21927, acc=0.92188
# [58/100] training 27.9% loss=0.23662, acc=0.90625
# [58/100] training 28.0% loss=0.29799, acc=0.87500
# [58/100] training 28.2% loss=0.10673, acc=0.98438
# [58/100] training 28.3% loss=0.05943, acc=0.96875
# [58/100] training 28.5% loss=0.11618, acc=0.95312
# [58/100] training 28.6% loss=0.15043, acc=0.93750
# [58/100] training 28.8% loss=0.10382, acc=0.98438
# [58/100] training 29.1% loss=0.15361, acc=0.93750
# [58/100] training 29.2% loss=0.19357, acc=0.93750
# [58/100] training 29.4% loss=0.19548, acc=0.92188
# [58/100] training 29.5% loss=0.09727, acc=0.95312
# [58/100] training 29.7% loss=0.23283, acc=0.95312
# [58/100] training 29.8% loss=0.15094, acc=0.95312
# [58/100] training 30.0% loss=0.13721, acc=0.93750
# [58/100] training 30.2% loss=0.10702, acc=0.95312
# [58/100] training 30.4% loss=0.05155, acc=0.98438
# [58/100] training 30.6% loss=0.14809, acc=0.93750
# [58/100] training 30.7% loss=0.16747, acc=0.93750
# [58/100] training 30.9% loss=0.40804, acc=0.93750
# [58/100] training 31.0% loss=0.04220, acc=1.00000
# [58/100] training 31.3% loss=0.12683, acc=0.95312
# [58/100] training 31.4% loss=0.24490, acc=0.90625
# [58/100] training 31.6% loss=0.34233, acc=0.90625
# [58/100] training 31.8% loss=0.18068, acc=0.90625
# [58/100] training 31.9% loss=0.29775, acc=0.89062
# [58/100] training 32.1% loss=0.20746, acc=0.92188
# [58/100] training 32.2% loss=0.27069, acc=0.87500
# [58/100] training 32.5% loss=0.12238, acc=0.95312
# [58/100] training 32.6% loss=0.22920, acc=0.92188
# [58/100] training 32.8% loss=0.18096, acc=0.93750
# [58/100] training 32.9% loss=0.23900, acc=0.90625
# [58/100] training 33.1% loss=0.13762, acc=0.95312
# [58/100] training 33.3% loss=0.32339, acc=0.89062
# [58/100] training 33.4% loss=0.24704, acc=0.90625
# [58/100] training 33.7% loss=0.09333, acc=0.98438
# [58/100] training 33.8% loss=0.23485, acc=0.90625
# [58/100] training 34.0% loss=0.13839, acc=0.92188
# [58/100] training 34.1% loss=0.16358, acc=0.95312
# [58/100] training 34.3% loss=0.19297, acc=0.92188
# [58/100] training 34.5% loss=0.20666, acc=0.93750
# [58/100] training 34.7% loss=0.08359, acc=0.96875
# [58/100] training 34.9% loss=0.21901, acc=0.90625
# [58/100] training 35.0% loss=0.18186, acc=0.92188
# [58/100] training 35.2% loss=0.18424, acc=0.92188
# [58/100] training 35.3% loss=0.24677, acc=0.92188
# [58/100] training 35.5% loss=0.15818, acc=0.92188
# [58/100] training 35.6% loss=0.42815, acc=0.87500
# [58/100] training 35.9% loss=0.16664, acc=0.92188
# [58/100] training 36.1% loss=0.36750, acc=0.84375
# [58/100] training 36.2% loss=0.26051, acc=0.89062
# [58/100] training 36.4% loss=0.22351, acc=0.93750
# [58/100] training 36.5% loss=0.21024, acc=0.92188
# [58/100] training 36.7% loss=0.19773, acc=0.95312
# [58/100] training 36.8% loss=0.10777, acc=0.93750
# [58/100] training 37.1% loss=0.22854, acc=0.89062
# [58/100] training 37.3% loss=0.22324, acc=0.92188
# [58/100] training 37.4% loss=0.10800, acc=0.96875
# [58/100] training 37.6% loss=0.07673, acc=0.96875
# [58/100] training 37.7% loss=0.16695, acc=0.92188
# [58/100] training 37.9% loss=0.18754, acc=0.95312
# [58/100] training 38.1% loss=0.16426, acc=0.92188
# [58/100] training 38.3% loss=0.12860, acc=0.92188
# [58/100] training 38.4% loss=0.04417, acc=0.96875
# [58/100] training 38.6% loss=0.08615, acc=0.96875
# [58/100] training 38.8% loss=0.34383, acc=0.87500
# [58/100] training 38.9% loss=0.12147, acc=0.92188
# [58/100] training 39.1% loss=0.18313, acc=0.95312
# [58/100] training 39.3% loss=0.26025, acc=0.89062
# [58/100] training 39.5% loss=0.28179, acc=0.85938
# [58/100] training 39.6% loss=0.15396, acc=0.93750
# [58/100] training 39.8% loss=0.08174, acc=0.98438
# [58/100] training 40.0% loss=0.20237, acc=0.90625
# [58/100] training 40.1% loss=0.21426, acc=0.89062
# [58/100] training 40.4% loss=0.13845, acc=0.95312
# [58/100] training 40.5% loss=0.18381, acc=0.93750
# [58/100] training 40.7% loss=0.19616, acc=0.93750
# [58/100] training 40.8% loss=0.15247, acc=0.93750
# [58/100] training 41.0% loss=0.18686, acc=0.89062
# [58/100] training 41.1% loss=0.28080, acc=0.87500
# [58/100] training 41.3% loss=0.23718, acc=0.92188
# [58/100] training 41.6% loss=0.28883, acc=0.82812
# [58/100] training 41.7% loss=0.20701, acc=0.90625
# [58/100] training 41.9% loss=0.13936, acc=0.93750
# [58/100] training 42.0% loss=0.21487, acc=0.92188
# [58/100] training 42.2% loss=0.19997, acc=0.92188
# [58/100] training 42.3% loss=0.15482, acc=0.96875
# [58/100] training 42.5% loss=0.17228, acc=0.95312
# [58/100] training 42.8% loss=0.10559, acc=0.93750
# [58/100] training 42.9% loss=0.13142, acc=0.93750
# [58/100] training 43.1% loss=0.18920, acc=0.95312
# [58/100] training 43.2% loss=0.11120, acc=0.98438
# [58/100] training 43.4% loss=0.17472, acc=0.95312
# [58/100] training 43.5% loss=0.23963, acc=0.90625
# [58/100] training 43.8% loss=0.22675, acc=0.89062
# [58/100] training 43.9% loss=0.16564, acc=0.93750
# [58/100] training 44.1% loss=0.24136, acc=0.93750
# [58/100] training 44.3% loss=0.11617, acc=0.95312
# [58/100] training 44.4% loss=0.14220, acc=0.95312
# [58/100] training 44.6% loss=0.22242, acc=0.92188
# [58/100] training 44.7% loss=0.29693, acc=0.85938
# [58/100] training 45.0% loss=0.13524, acc=0.93750
# [58/100] training 45.1% loss=0.13103, acc=0.93750
# [58/100] training 45.3% loss=0.16419, acc=0.92188
# [58/100] training 45.5% loss=0.05991, acc=0.96875
# [58/100] training 45.6% loss=0.17480, acc=0.93750
# [58/100] training 45.8% loss=0.09575, acc=0.96875
# [58/100] training 45.9% loss=0.09581, acc=0.95312
# [58/100] training 46.2% loss=0.09798, acc=0.96875
# [58/100] training 46.3% loss=0.13307, acc=0.95312
# [58/100] training 46.5% loss=0.23173, acc=0.90625
# [58/100] training 46.6% loss=0.22773, acc=0.93750
# [58/100] training 46.8% loss=0.09732, acc=0.96875
# [58/100] training 47.0% loss=0.11650, acc=0.95312
# [58/100] training 47.2% loss=0.16742, acc=0.90625
# [58/100] training 47.4% loss=0.21690, acc=0.92188
# [58/100] training 47.5% loss=0.23919, acc=0.93750
# [58/100] training 47.7% loss=0.16782, acc=0.93750
# [58/100] training 47.8% loss=0.31810, acc=0.90625
# [58/100] training 48.0% loss=0.16943, acc=0.93750
# [58/100] training 48.3% loss=0.12318, acc=0.95312
# [58/100] training 48.4% loss=0.06350, acc=0.98438
# [58/100] training 48.6% loss=0.10156, acc=0.93750
# [58/100] training 48.7% loss=0.17964, acc=0.93750
# [58/100] training 48.9% loss=0.09792, acc=0.98438
# [58/100] training 49.0% loss=0.07776, acc=0.96875
# [58/100] training 49.2% loss=0.20941, acc=0.92188
# [58/100] training 49.3% loss=0.16702, acc=0.95312
# [58/100] training 49.6% loss=0.29270, acc=0.90625
# [58/100] training 49.8% loss=0.21951, acc=0.92188
# [58/100] training 49.9% loss=0.14296, acc=0.95312
# [58/100] training 50.1% loss=0.08566, acc=0.96875
# [58/100] training 50.2% loss=0.18847, acc=0.93750
# [58/100] training 50.4% loss=0.26702, acc=0.89062
# [58/100] training 50.6% loss=0.21658, acc=0.92188
# [58/100] training 50.8% loss=0.16925, acc=0.90625
# [58/100] training 51.0% loss=0.21313, acc=0.90625
# [58/100] training 51.1% loss=0.21328, acc=0.93750
# [58/100] training 51.3% loss=0.21330, acc=0.93750
# [58/100] training 51.4% loss=0.19530, acc=0.92188
# [58/100] training 51.7% loss=0.09666, acc=0.96875
# [58/100] training 51.8% loss=0.22658, acc=0.90625
# [58/100] training 52.0% loss=0.16366, acc=0.92188
# [58/100] training 52.1% loss=0.19823, acc=0.93750
# [58/100] training 52.3% loss=0.17687, acc=0.92188
# [58/100] training 52.5% loss=0.03902, acc=1.00000
# [58/100] training 52.6% loss=0.16664, acc=0.93750
# [58/100] training 52.9% loss=0.15689, acc=0.95312
# [58/100] training 53.0% loss=0.10091, acc=0.95312
# [58/100] training 53.2% loss=0.22240, acc=0.93750
# [58/100] training 53.3% loss=0.15088, acc=0.95312
# [58/100] training 53.5% loss=0.19501, acc=0.92188
# [58/100] training 53.7% loss=0.12905, acc=0.95312
# [58/100] training 53.8% loss=0.28330, acc=0.87500
# [58/100] training 54.1% loss=0.17897, acc=0.93750
# [58/100] training 54.2% loss=0.06798, acc=0.98438
# [58/100] training 54.4% loss=0.11142, acc=0.95312
# [58/100] training 54.5% loss=0.34298, acc=0.89062
# [58/100] training 54.7% loss=0.38505, acc=0.81250
# [58/100] training 54.8% loss=0.17910, acc=0.92188
# [58/100] training 55.1% loss=0.21172, acc=0.92188
# [58/100] training 55.3% loss=0.17228, acc=0.95312
# [58/100] training 55.4% loss=0.13165, acc=0.95312
# [58/100] training 55.6% loss=0.15661, acc=0.95312
# [58/100] training 55.7% loss=0.11530, acc=0.98438
# [58/100] training 55.9% loss=0.08334, acc=0.96875
# [58/100] training 56.0% loss=0.18122, acc=0.92188
# [58/100] training 56.3% loss=0.25675, acc=0.90625
# [58/100] training 56.5% loss=0.19571, acc=0.92188
# [58/100] training 56.6% loss=0.14969, acc=0.92188
# [58/100] training 56.8% loss=0.25238, acc=0.92188
# [58/100] training 56.9% loss=0.21863, acc=0.90625
# [58/100] training 57.1% loss=0.24071, acc=0.90625
# [58/100] training 57.2% loss=0.09821, acc=0.95312
# [58/100] training 57.5% loss=0.09130, acc=0.95312
# [58/100] training 57.6% loss=0.15528, acc=0.92188
# [58/100] training 57.8% loss=0.11336, acc=0.96875
# [58/100] training 58.0% loss=0.17885, acc=0.92188
# [58/100] training 58.1% loss=0.07746, acc=0.98438
# [58/100] training 58.3% loss=0.29146, acc=0.92188
# [58/100] training 58.4% loss=0.20755, acc=0.90625
# [58/100] training 58.7% loss=0.22799, acc=0.90625
# [58/100] training 58.8% loss=0.18645, acc=0.93750
# [58/100] training 59.0% loss=0.19351, acc=0.92188
# [58/100] training 59.2% loss=0.16267, acc=0.93750
# [58/100] training 59.3% loss=0.19991, acc=0.92188
# [58/100] training 59.5% loss=0.20361, acc=0.90625
# [58/100] training 59.7% loss=0.16515, acc=0.95312
# [58/100] training 59.9% loss=0.08785, acc=0.98438
# [58/100] training 60.0% loss=0.19190, acc=0.93750
# [58/100] training 60.2% loss=0.15659, acc=0.93750
# [58/100] training 60.3% loss=0.09923, acc=0.95312
# [58/100] training 60.5% loss=0.38552, acc=0.90625
# [58/100] training 60.8% loss=0.38444, acc=0.84375
# [58/100] training 60.9% loss=0.18163, acc=0.93750
# [58/100] training 61.1% loss=0.27679, acc=0.92188
# [58/100] training 61.2% loss=0.10892, acc=0.95312
# [58/100] training 61.4% loss=0.23646, acc=0.92188
# [58/100] training 61.5% loss=0.25817, acc=0.87500
# [58/100] training 61.7% loss=0.20110, acc=0.89062
# [58/100] training 62.0% loss=0.29986, acc=0.89062
# [58/100] training 62.1% loss=0.20046, acc=0.89062
# [58/100] training 62.3% loss=0.18694, acc=0.93750
# [58/100] training 62.4% loss=0.10827, acc=0.96875
# [58/100] training 62.6% loss=0.25957, acc=0.85938
# [58/100] training 62.7% loss=0.15180, acc=0.93750
# [58/100] training 62.9% loss=0.19908, acc=0.89062
# [58/100] training 63.1% loss=0.20065, acc=0.90625
# [58/100] training 63.3% loss=0.12743, acc=1.00000
# [58/100] training 63.5% loss=0.17048, acc=0.93750
# [58/100] training 63.6% loss=0.18573, acc=0.90625
# [58/100] training 63.8% loss=0.37605, acc=0.85938
# [58/100] training 63.9% loss=0.07568, acc=0.98438
# [58/100] training 64.2% loss=0.08238, acc=0.98438
# [58/100] training 64.3% loss=0.24279, acc=0.89062
# [58/100] training 64.5% loss=0.15624, acc=0.95312
# [58/100] training 64.7% loss=0.19200, acc=0.93750
# [58/100] training 64.8% loss=0.35754, acc=0.81250
# [58/100] training 65.0% loss=0.14850, acc=0.93750
# [58/100] training 65.1% loss=0.14347, acc=0.93750
# [58/100] training 65.4% loss=0.07992, acc=0.96875
# [58/100] training 65.5% loss=0.13703, acc=0.95312
# [58/100] training 65.7% loss=0.13206, acc=0.95312
# [58/100] training 65.8% loss=0.24740, acc=0.85938
# [58/100] training 66.0% loss=0.21486, acc=0.92188
# [58/100] training 66.2% loss=0.10760, acc=0.95312
# [58/100] training 66.3% loss=0.21406, acc=0.93750
# [58/100] training 66.6% loss=0.24092, acc=0.89062
# [58/100] training 66.7% loss=0.09169, acc=0.96875
# [58/100] training 66.9% loss=0.24389, acc=0.87500
# [58/100] training 67.0% loss=0.19240, acc=0.89062
# [58/100] training 67.2% loss=0.14310, acc=0.93750
# [58/100] training 67.4% loss=0.15568, acc=0.93750
# [58/100] training 67.6% loss=0.12983, acc=0.95312
# [58/100] training 67.8% loss=0.13216, acc=0.95312
# [58/100] training 67.9% loss=0.14113, acc=0.93750
# [58/100] training 68.1% loss=0.16945, acc=0.90625
# [58/100] training 68.2% loss=0.07641, acc=0.98438
# [58/100] training 68.4% loss=0.17078, acc=0.95312
# [58/100] training 68.5% loss=0.23111, acc=0.93750
# [58/100] training 68.8% loss=0.10254, acc=0.96875
# [58/100] training 69.0% loss=0.18302, acc=0.95312
# [58/100] training 69.1% loss=0.20621, acc=0.96875
# [58/100] training 69.3% loss=0.12044, acc=0.93750
# [58/100] training 69.4% loss=0.13753, acc=0.90625
# [58/100] training 69.6% loss=0.05484, acc=0.96875
# [58/100] training 69.7% loss=0.14309, acc=0.95312
# [58/100] training 70.0% loss=0.28261, acc=0.89062
# [58/100] training 70.2% loss=0.14318, acc=0.95312
# [58/100] training 70.3% loss=0.15127, acc=0.93750
# [58/100] training 70.5% loss=0.12530, acc=0.95312
# [58/100] training 70.6% loss=0.12261, acc=0.95312
# [58/100] training 70.8% loss=0.26102, acc=0.90625
# [58/100] training 71.0% loss=0.24997, acc=0.90625
# [58/100] training 71.2% loss=0.12593, acc=0.95312
# [58/100] training 71.3% loss=0.10462, acc=0.96875
# [58/100] training 71.5% loss=0.28686, acc=0.92188
# [58/100] training 71.7% loss=0.24583, acc=0.92188
# [58/100] training 71.8% loss=0.17634, acc=0.95312
# [58/100] training 72.0% loss=0.10690, acc=0.96875
# [58/100] training 72.2% loss=0.17421, acc=0.93750
# [58/100] training 72.4% loss=0.15184, acc=0.93750
# [58/100] training 72.5% loss=0.17800, acc=0.93750
# [58/100] training 72.7% loss=0.30917, acc=0.89062
# [58/100] training 72.9% loss=0.08220, acc=0.96875
# [58/100] training 73.0% loss=0.11217, acc=0.93750
# [58/100] training 73.3% loss=0.29844, acc=0.90625
# [58/100] training 73.4% loss=0.14060, acc=0.95312
# [58/100] training 73.6% loss=0.17699, acc=0.90625
# [58/100] training 73.7% loss=0.24457, acc=0.92188
# [58/100] training 73.9% loss=0.11789, acc=0.93750
# [58/100] training 74.0% loss=0.21197, acc=0.90625
# [58/100] training 74.2% loss=0.12545, acc=0.95312
# [58/100] training 74.5% loss=0.16254, acc=0.93750
# [58/100] training 74.6% loss=0.26114, acc=0.87500
# [58/100] training 74.8% loss=0.34761, acc=0.87500
# [58/100] training 74.9% loss=0.19949, acc=0.92188
# [58/100] training 75.1% loss=0.11114, acc=0.96875
# [58/100] training 75.2% loss=0.09784, acc=0.95312
# [58/100] training 75.4% loss=0.15469, acc=0.93750
# [58/100] training 75.7% loss=0.15595, acc=0.93750
# [58/100] training 75.8% loss=0.24860, acc=0.89062
# [58/100] training 76.0% loss=0.10949, acc=0.95312
# [58/100] training 76.1% loss=0.24989, acc=0.89062
# [58/100] training 76.3% loss=0.05820, acc=0.98438
# [58/100] training 76.4% loss=0.29439, acc=0.87500
# [58/100] training 76.7% loss=0.19676, acc=0.93750
# [58/100] training 76.8% loss=0.08647, acc=0.96875
# [58/100] training 77.0% loss=0.12630, acc=0.93750
# [58/100] training 77.2% loss=0.21875, acc=0.93750
# [58/100] training 77.3% loss=0.06363, acc=0.96875
# [58/100] training 77.5% loss=0.17015, acc=0.93750
# [58/100] training 77.6% loss=0.19021, acc=0.89062
# [58/100] training 77.9% loss=0.16983, acc=0.92188
# [58/100] training 78.0% loss=0.12880, acc=0.95312
# [58/100] training 78.2% loss=0.15034, acc=0.93750
# [58/100] training 78.4% loss=0.07562, acc=0.96875
# [58/100] training 78.5% loss=0.20178, acc=0.89062
# [58/100] training 78.7% loss=0.16644, acc=0.89062
# [58/100] training 78.8% loss=0.07965, acc=0.95312
# [58/100] training 79.1% loss=0.12525, acc=0.96875
# [58/100] training 79.2% loss=0.11337, acc=0.96875
# [58/100] training 79.4% loss=0.17657, acc=0.93750
# [58/100] training 79.5% loss=0.14281, acc=0.93750
# [58/100] training 79.7% loss=0.06733, acc=0.96875
# [58/100] training 79.9% loss=0.10690, acc=0.96875
# [58/100] training 80.1% loss=0.05163, acc=1.00000
# [58/100] training 80.3% loss=0.22538, acc=0.90625
# [58/100] training 80.4% loss=0.18782, acc=0.93750
# [58/100] training 80.6% loss=0.13109, acc=0.93750
# [58/100] training 80.7% loss=0.17582, acc=0.93750
# [58/100] training 80.9% loss=0.18895, acc=0.93750
# [58/100] training 81.2% loss=0.27071, acc=0.85938
# [58/100] training 81.3% loss=0.27630, acc=0.85938
# [58/100] training 81.5% loss=0.17126, acc=0.93750
# [58/100] training 81.6% loss=0.35621, acc=0.89062
# [58/100] training 81.8% loss=0.13511, acc=0.93750
# [58/100] training 81.9% loss=0.30350, acc=0.93750
# [58/100] training 82.1% loss=0.13689, acc=0.95312
# [58/100] training 82.2% loss=0.19656, acc=0.90625
# [58/100] training 82.5% loss=0.13565, acc=0.96875
# [58/100] training 82.7% loss=0.17108, acc=0.95312
# [58/100] training 82.8% loss=0.17053, acc=0.92188
# [58/100] training 83.0% loss=0.16228, acc=0.95312
# [58/100] training 83.1% loss=0.16285, acc=0.95312
# [58/100] training 83.3% loss=0.12120, acc=0.96875
# [58/100] training 83.5% loss=0.10715, acc=0.95312
# [58/100] training 83.7% loss=0.35607, acc=0.84375
# [58/100] training 83.9% loss=0.18337, acc=0.90625
# [58/100] training 84.0% loss=0.09576, acc=0.96875
# [58/100] training 84.2% loss=0.10099, acc=0.98438
# [58/100] training 84.3% loss=0.09482, acc=0.96875
# [58/100] training 84.5% loss=0.10749, acc=0.96875
# [58/100] training 84.7% loss=0.19057, acc=0.90625
# [58/100] training 84.9% loss=0.10868, acc=0.96875
# [58/100] training 85.0% loss=0.15125, acc=0.92188
# [58/100] training 85.2% loss=0.16877, acc=0.92188
# [58/100] training 85.4% loss=0.10010, acc=0.96875
# [58/100] training 85.5% loss=0.27907, acc=0.89062
# [58/100] training 85.8% loss=0.24566, acc=0.92188
# [58/100] training 85.9% loss=0.16666, acc=0.92188
# [58/100] training 86.1% loss=0.12159, acc=0.96875
# [58/100] training 86.2% loss=0.12480, acc=0.93750
# [58/100] training 86.4% loss=0.28913, acc=0.85938
# [58/100] training 86.6% loss=0.14845, acc=0.92188
# [58/100] training 86.7% loss=0.08873, acc=0.98438
# [58/100] training 87.0% loss=0.15985, acc=0.95312
# [58/100] training 87.1% loss=0.16122, acc=0.93750
# [58/100] training 87.3% loss=0.12495, acc=0.95312
# [58/100] training 87.4% loss=0.15465, acc=0.92188
# [58/100] training 87.6% loss=0.08567, acc=0.95312
# [58/100] training 87.7% loss=0.27028, acc=0.92188
# [58/100] training 87.9% loss=0.18703, acc=0.92188
# [58/100] training 88.2% loss=0.10882, acc=0.98438
# [58/100] training 88.3% loss=0.24690, acc=0.90625
# [58/100] training 88.5% loss=0.15768, acc=0.96875
# [58/100] training 88.6% loss=0.07511, acc=0.98438
# [58/100] training 88.8% loss=0.09606, acc=0.98438
# [58/100] training 88.9% loss=0.26977, acc=0.89062
# [58/100] training 89.2% loss=0.15804, acc=0.95312
# [58/100] training 89.4% loss=0.07497, acc=0.96875
# [58/100] training 89.5% loss=0.30067, acc=0.92188
# [58/100] training 89.7% loss=0.19948, acc=0.92188
# [58/100] training 89.8% loss=0.16365, acc=0.93750
# [58/100] training 90.0% loss=0.11073, acc=0.96875
# [58/100] training 90.1% loss=0.18627, acc=0.90625
# [58/100] training 90.4% loss=0.16231, acc=0.95312
# [58/100] training 90.5% loss=0.10344, acc=0.92188
# [58/100] training 90.7% loss=0.14328, acc=0.95312
# [58/100] training 90.9% loss=0.12364, acc=0.95312
# [58/100] training 91.0% loss=0.12905, acc=0.95312
# [58/100] training 91.2% loss=0.21077, acc=0.89062
# [58/100] training 91.3% loss=0.27888, acc=0.89062
# [58/100] training 91.6% loss=0.22408, acc=0.92188
# [58/100] training 91.7% loss=0.15549, acc=0.93750
# [58/100] training 91.9% loss=0.23111, acc=0.89062
# [58/100] training 92.1% loss=0.16253, acc=0.89062
# [58/100] training 92.2% loss=0.12571, acc=0.92188
# [58/100] training 92.4% loss=0.13981, acc=0.93750
# [58/100] training 92.6% loss=0.19612, acc=0.89062
# [58/100] training 92.8% loss=0.19463, acc=0.93750
# [58/100] training 92.9% loss=0.19028, acc=0.93750
# [58/100] training 93.1% loss=0.32944, acc=0.89062
# [58/100] training 93.2% loss=0.25934, acc=0.90625
# [58/100] training 93.4% loss=0.26875, acc=0.90625
# [58/100] training 93.7% loss=0.11003, acc=0.96875
# [58/100] training 93.8% loss=0.13443, acc=0.93750
# [58/100] training 94.0% loss=0.20330, acc=0.92188
# [58/100] training 94.1% loss=0.13029, acc=0.95312
# [58/100] training 94.3% loss=0.14238, acc=0.95312
# [58/100] training 94.4% loss=0.20439, acc=0.93750
# [58/100] training 94.6% loss=0.08252, acc=0.98438
# [58/100] training 94.9% loss=0.09366, acc=0.95312
# [58/100] training 95.0% loss=0.15799, acc=0.95312
# [58/100] training 95.2% loss=0.28867, acc=0.87500
# [58/100] training 95.3% loss=0.18162, acc=0.93750
# [58/100] training 95.5% loss=0.08242, acc=0.95312
# [58/100] training 95.6% loss=0.14655, acc=0.93750
# [58/100] training 95.8% loss=0.13005, acc=0.93750
# [58/100] training 96.0% loss=0.21917, acc=0.90625
# [58/100] training 96.2% loss=0.14300, acc=0.96875
# [58/100] training 96.4% loss=0.12702, acc=0.95312
# [58/100] training 96.5% loss=0.18600, acc=0.95312
# [58/100] training 96.7% loss=0.09991, acc=0.96875
# [58/100] training 96.8% loss=0.23137, acc=0.90625
# [58/100] training 97.1% loss=0.17100, acc=0.93750
# [58/100] training 97.2% loss=0.22548, acc=0.90625
# [58/100] training 97.4% loss=0.21136, acc=0.90625
# [58/100] training 97.6% loss=0.22626, acc=0.90625
# [58/100] training 97.7% loss=0.13733, acc=0.93750
# [58/100] training 97.9% loss=0.15905, acc=0.92188
# [58/100] training 98.0% loss=0.14443, acc=0.93750
# [58/100] training 98.3% loss=0.22477, acc=0.90625
# [58/100] training 98.4% loss=0.15436, acc=0.93750
# [58/100] training 98.6% loss=0.34922, acc=0.84375
# [58/100] training 98.7% loss=0.28028, acc=0.92188
# [58/100] training 98.9% loss=0.14678, acc=0.98438
# [58/100] training 99.1% loss=0.16439, acc=0.90625
# [58/100] training 99.2% loss=0.15538, acc=0.93750
# [58/100] training 99.5% loss=0.20131, acc=0.95312
# [58/100] training 99.6% loss=0.22061, acc=0.92188
# [58/100] training 99.8% loss=0.09874, acc=0.96875
# [58/100] training 99.9% loss=0.03207, acc=0.98438
# [58/100] testing 0.9% loss=0.18026, acc=0.90625
# [58/100] testing 1.8% loss=0.28404, acc=0.85938
# [58/100] testing 2.2% loss=0.31064, acc=0.89062
# [58/100] testing 3.1% loss=0.39376, acc=0.90625
# [58/100] testing 3.5% loss=0.11481, acc=0.96875
# [58/100] testing 4.4% loss=0.11284, acc=0.93750
# [58/100] testing 4.8% loss=0.36970, acc=0.84375
# [58/100] testing 5.7% loss=0.18739, acc=0.89062
# [58/100] testing 6.6% loss=0.14953, acc=0.93750
# [58/100] testing 7.0% loss=0.08317, acc=0.98438
# [58/100] testing 7.9% loss=0.20544, acc=0.92188
# [58/100] testing 8.3% loss=0.33175, acc=0.90625
# [58/100] testing 9.2% loss=0.25496, acc=0.95312
# [58/100] testing 9.7% loss=0.08322, acc=0.98438
# [58/100] testing 10.5% loss=0.21057, acc=0.89062
# [58/100] testing 11.0% loss=0.25716, acc=0.92188
# [58/100] testing 11.8% loss=0.23385, acc=0.90625
# [58/100] testing 12.7% loss=0.33999, acc=0.89062
# [58/100] testing 13.2% loss=0.22460, acc=0.89062
# [58/100] testing 14.0% loss=0.39172, acc=0.90625
# [58/100] testing 14.5% loss=0.15696, acc=0.90625
# [58/100] testing 15.4% loss=0.30405, acc=0.92188
# [58/100] testing 15.8% loss=0.20361, acc=0.92188
# [58/100] testing 16.7% loss=0.23265, acc=0.93750
# [58/100] testing 17.5% loss=0.24386, acc=0.87500
# [58/100] testing 18.0% loss=0.16255, acc=0.93750
# [58/100] testing 18.9% loss=0.07475, acc=0.95312
# [58/100] testing 19.3% loss=0.30829, acc=0.87500
# [58/100] testing 20.2% loss=0.22539, acc=0.90625
# [58/100] testing 20.6% loss=0.27260, acc=0.90625
# [58/100] testing 21.5% loss=0.18639, acc=0.92188
# [58/100] testing 21.9% loss=0.48872, acc=0.84375
# [58/100] testing 22.8% loss=0.30092, acc=0.89062
# [58/100] testing 23.7% loss=0.37094, acc=0.89062
# [58/100] testing 24.1% loss=0.17673, acc=0.92188
# [58/100] testing 25.0% loss=0.25329, acc=0.90625
# [58/100] testing 25.4% loss=0.12340, acc=0.96875
# [58/100] testing 26.3% loss=0.31847, acc=0.92188
# [58/100] testing 26.8% loss=0.32128, acc=0.90625
# [58/100] testing 27.6% loss=0.20028, acc=0.95312
# [58/100] testing 28.5% loss=0.18087, acc=0.95312
# [58/100] testing 29.0% loss=0.20136, acc=0.95312
# [58/100] testing 29.8% loss=0.35265, acc=0.90625
# [58/100] testing 30.3% loss=0.21429, acc=0.95312
# [58/100] testing 31.1% loss=0.29803, acc=0.89062
# [58/100] testing 31.6% loss=0.28176, acc=0.95312
# [58/100] testing 32.5% loss=0.35763, acc=0.92188
# [58/100] testing 32.9% loss=0.51777, acc=0.89062
# [58/100] testing 33.8% loss=0.29969, acc=0.92188
# [58/100] testing 34.7% loss=0.47248, acc=0.89062
# [58/100] testing 35.1% loss=0.13347, acc=0.95312
# [58/100] testing 36.0% loss=0.35875, acc=0.87500
# [58/100] testing 36.4% loss=0.22634, acc=0.92188
# [58/100] testing 37.3% loss=0.25271, acc=0.95312
# [58/100] testing 37.7% loss=0.44361, acc=0.87500
# [58/100] testing 38.6% loss=0.17281, acc=0.95312
# [58/100] testing 39.5% loss=0.28839, acc=0.93750
# [58/100] testing 39.9% loss=0.23940, acc=0.90625
# [58/100] testing 40.8% loss=0.30230, acc=0.92188
# [58/100] testing 41.2% loss=0.30017, acc=0.93750
# [58/100] testing 42.1% loss=0.34841, acc=0.85938
# [58/100] testing 42.5% loss=0.18246, acc=0.93750
# [58/100] testing 43.4% loss=0.16569, acc=0.92188
# [58/100] testing 43.9% loss=0.09188, acc=0.98438
# [58/100] testing 44.7% loss=0.19618, acc=0.90625
# [58/100] testing 45.6% loss=0.27412, acc=0.90625
# [58/100] testing 46.1% loss=0.26843, acc=0.87500
# [58/100] testing 46.9% loss=0.18391, acc=0.95312
# [58/100] testing 47.4% loss=0.20020, acc=0.93750
# [58/100] testing 48.3% loss=0.39965, acc=0.89062
# [58/100] testing 48.7% loss=0.45101, acc=0.85938
# [58/100] testing 49.6% loss=0.39311, acc=0.87500
# [58/100] testing 50.4% loss=0.13048, acc=0.95312
# [58/100] testing 50.9% loss=0.31327, acc=0.89062
# [58/100] testing 51.8% loss=0.23977, acc=0.90625
# [58/100] testing 52.2% loss=0.25884, acc=0.90625
# [58/100] testing 53.1% loss=0.17422, acc=0.92188
# [58/100] testing 53.5% loss=0.19311, acc=0.93750
# [58/100] testing 54.4% loss=0.30844, acc=0.87500
# [58/100] testing 54.8% loss=0.33723, acc=0.90625
# [58/100] testing 55.7% loss=0.04347, acc=0.98438
# [58/100] testing 56.6% loss=0.30781, acc=0.87500
# [58/100] testing 57.0% loss=0.34244, acc=0.92188
# [58/100] testing 57.9% loss=0.27454, acc=0.92188
# [58/100] testing 58.3% loss=0.31776, acc=0.89062
# [58/100] testing 59.2% loss=0.30473, acc=0.90625
# [58/100] testing 59.7% loss=0.18131, acc=0.90625
# [58/100] testing 60.5% loss=0.38969, acc=0.87500
# [58/100] testing 61.4% loss=0.11545, acc=0.96875
# [58/100] testing 61.9% loss=0.15021, acc=0.93750
# [58/100] testing 62.7% loss=0.14276, acc=0.92188
# [58/100] testing 63.2% loss=0.43432, acc=0.87500
# [58/100] testing 64.0% loss=0.40917, acc=0.92188
# [58/100] testing 64.5% loss=0.18073, acc=0.90625
# [58/100] testing 65.4% loss=0.13400, acc=0.95312
# [58/100] testing 65.8% loss=0.27160, acc=0.87500
# [58/100] testing 66.7% loss=0.16473, acc=0.95312
# [58/100] testing 67.6% loss=0.47336, acc=0.87500
# [58/100] testing 68.0% loss=0.08995, acc=0.96875
# [58/100] testing 68.9% loss=0.37065, acc=0.92188
# [58/100] testing 69.3% loss=0.24405, acc=0.92188
# [58/100] testing 70.2% loss=0.56196, acc=0.84375
# [58/100] testing 70.6% loss=0.21613, acc=0.95312
# [58/100] testing 71.5% loss=0.27456, acc=0.92188
# [58/100] testing 72.4% loss=0.16226, acc=0.93750
# [58/100] testing 72.8% loss=0.21866, acc=0.90625
# [58/100] testing 73.7% loss=0.08841, acc=0.95312
# [58/100] testing 74.1% loss=0.39239, acc=0.87500
# [58/100] testing 75.0% loss=0.22329, acc=0.90625
# [58/100] testing 75.4% loss=0.40284, acc=0.90625
# [58/100] testing 76.3% loss=0.08801, acc=0.95312
# [58/100] testing 76.8% loss=0.19277, acc=0.85938
# [58/100] testing 77.6% loss=0.17102, acc=0.92188
# [58/100] testing 78.5% loss=0.32755, acc=0.90625
# [58/100] testing 79.0% loss=0.16728, acc=0.92188
# [58/100] testing 79.8% loss=0.22548, acc=0.90625
# [58/100] testing 80.3% loss=0.21887, acc=0.89062
# [58/100] testing 81.2% loss=0.42159, acc=0.92188
# [58/100] testing 81.6% loss=0.16854, acc=0.93750
# [58/100] testing 82.5% loss=0.14196, acc=0.90625
# [58/100] testing 83.3% loss=0.24283, acc=0.93750
# [58/100] testing 83.8% loss=0.12204, acc=0.95312
# [58/100] testing 84.7% loss=0.26712, acc=0.87500
# [58/100] testing 85.1% loss=0.20268, acc=0.92188
# [58/100] testing 86.0% loss=0.28272, acc=0.92188
# [58/100] testing 86.4% loss=0.33059, acc=0.89062
# [58/100] testing 87.3% loss=0.31270, acc=0.87500
# [58/100] testing 87.7% loss=0.31786, acc=0.87500
# [58/100] testing 88.6% loss=0.20262, acc=0.89062
# [58/100] testing 89.5% loss=0.64100, acc=0.79688
# [58/100] testing 89.9% loss=0.13673, acc=0.92188
# [58/100] testing 90.8% loss=0.40786, acc=0.92188
# [58/100] testing 91.2% loss=0.20517, acc=0.93750
# [58/100] testing 92.1% loss=0.25463, acc=0.90625
# [58/100] testing 92.6% loss=0.39910, acc=0.87500
# [58/100] testing 93.4% loss=0.28855, acc=0.90625
# [58/100] testing 94.3% loss=0.16453, acc=0.95312
# [58/100] testing 94.7% loss=0.20799, acc=0.90625
# [58/100] testing 95.6% loss=0.43034, acc=0.84375
# [58/100] testing 96.1% loss=0.19942, acc=0.90625
# [58/100] testing 96.9% loss=0.20710, acc=0.90625
# [58/100] testing 97.4% loss=0.14287, acc=0.96875
# [58/100] testing 98.3% loss=0.15554, acc=0.92188
# [58/100] testing 98.7% loss=0.14350, acc=0.93750
# [58/100] testing 99.6% loss=0.32457, acc=0.89062
# [59/100] training 0.2% loss=0.18407, acc=0.92188
# [59/100] training 0.4% loss=0.42125, acc=0.87500
# [59/100] training 0.5% loss=0.09387, acc=0.96875
# [59/100] training 0.8% loss=0.07452, acc=0.98438
# [59/100] training 0.9% loss=0.11966, acc=0.95312
# [59/100] training 1.1% loss=0.23252, acc=0.95312
# [59/100] training 1.2% loss=0.11544, acc=0.96875
# [59/100] training 1.4% loss=0.15781, acc=0.93750
# [59/100] training 1.6% loss=0.11553, acc=0.96875
# [59/100] training 1.8% loss=0.12027, acc=0.95312
# [59/100] training 2.0% loss=0.20228, acc=0.89062
# [59/100] training 2.1% loss=0.16640, acc=0.95312
# [59/100] training 2.3% loss=0.17458, acc=0.92188
# [59/100] training 2.4% loss=0.15100, acc=0.95312
# [59/100] training 2.6% loss=0.06540, acc=0.98438
# [59/100] training 2.7% loss=0.19203, acc=0.92188
# [59/100] training 3.0% loss=0.12779, acc=0.98438
# [59/100] training 3.2% loss=0.11035, acc=0.93750
# [59/100] training 3.3% loss=0.30234, acc=0.92188
# [59/100] training 3.5% loss=0.18400, acc=0.89062
# [59/100] training 3.6% loss=0.17011, acc=0.93750
# [59/100] training 3.8% loss=0.18060, acc=0.92188
# [59/100] training 3.9% loss=0.13834, acc=0.96875
# [59/100] training 4.2% loss=0.08783, acc=0.96875
# [59/100] training 4.4% loss=0.15167, acc=0.93750
# [59/100] training 4.5% loss=0.12123, acc=0.93750
# [59/100] training 4.7% loss=0.32091, acc=0.87500
# [59/100] training 4.8% loss=0.24154, acc=0.90625
# [59/100] training 5.0% loss=0.09008, acc=0.96875
# [59/100] training 5.2% loss=0.23580, acc=0.90625
# [59/100] training 5.4% loss=0.06729, acc=0.98438
# [59/100] training 5.5% loss=0.15440, acc=0.93750
# [59/100] training 5.7% loss=0.15429, acc=0.93750
# [59/100] training 5.9% loss=0.11345, acc=0.95312
# [59/100] training 6.0% loss=0.22924, acc=0.90625
# [59/100] training 6.3% loss=0.20722, acc=0.95312
# [59/100] training 6.4% loss=0.12684, acc=0.92188
# [59/100] training 6.6% loss=0.16814, acc=0.90625
# [59/100] training 6.7% loss=0.21476, acc=0.92188
# [59/100] training 6.9% loss=0.09934, acc=0.95312
# [59/100] training 7.1% loss=0.14169, acc=0.95312
# [59/100] training 7.2% loss=0.23738, acc=0.90625
# [59/100] training 7.5% loss=0.10873, acc=0.92188
# [59/100] training 7.6% loss=0.29093, acc=0.92188
# [59/100] training 7.8% loss=0.11894, acc=0.95312
# [59/100] training 7.9% loss=0.12946, acc=0.92188
# [59/100] training 8.1% loss=0.13691, acc=0.93750
# [59/100] training 8.2% loss=0.25484, acc=0.93750
# [59/100] training 8.4% loss=0.27048, acc=0.92188
# [59/100] training 8.7% loss=0.19539, acc=0.92188
# [59/100] training 8.8% loss=0.20036, acc=0.87500
# [59/100] training 9.0% loss=0.25734, acc=0.87500
# [59/100] training 9.1% loss=0.15171, acc=0.96875
# [59/100] training 9.3% loss=0.29236, acc=0.89062
# [59/100] training 9.4% loss=0.17108, acc=0.93750
# [59/100] training 9.7% loss=0.19618, acc=0.90625
# [59/100] training 9.9% loss=0.23757, acc=0.87500
# [59/100] training 10.0% loss=0.16217, acc=0.90625
# [59/100] training 10.2% loss=0.10759, acc=0.95312
# [59/100] training 10.3% loss=0.10381, acc=0.98438
# [59/100] training 10.5% loss=0.22922, acc=0.93750
# [59/100] training 10.6% loss=0.18301, acc=0.92188
# [59/100] training 10.9% loss=0.15743, acc=0.95312
# [59/100] training 11.0% loss=0.24693, acc=0.92188
# [59/100] training 11.2% loss=0.11315, acc=0.95312
# [59/100] training 11.4% loss=0.13191, acc=0.93750
# [59/100] training 11.5% loss=0.18893, acc=0.93750
# [59/100] training 11.7% loss=0.07503, acc=0.98438
# [59/100] training 11.8% loss=0.12337, acc=0.95312
# [59/100] training 12.1% loss=0.19025, acc=0.96875
# [59/100] training 12.2% loss=0.04009, acc=1.00000
# [59/100] training 12.4% loss=0.14918, acc=0.92188
# [59/100] training 12.6% loss=0.32828, acc=0.92188
# [59/100] training 12.7% loss=0.22200, acc=0.92188
# [59/100] training 12.9% loss=0.22088, acc=0.90625
# [59/100] training 13.0% loss=0.17826, acc=0.93750
# [59/100] training 13.3% loss=0.24495, acc=0.89062
# [59/100] training 13.4% loss=0.27534, acc=0.89062
# [59/100] training 13.6% loss=0.15242, acc=0.92188
# [59/100] training 13.7% loss=0.23452, acc=0.92188
# [59/100] training 13.9% loss=0.24506, acc=0.90625
# [59/100] training 14.1% loss=0.27425, acc=0.92188
# [59/100] training 14.3% loss=0.11020, acc=0.95312
# [59/100] training 14.5% loss=0.19438, acc=0.93750
# [59/100] training 14.6% loss=0.26639, acc=0.92188
# [59/100] training 14.8% loss=0.10222, acc=0.95312
# [59/100] training 14.9% loss=0.09867, acc=0.96875
# [59/100] training 15.1% loss=0.33386, acc=0.87500
# [59/100] training 15.4% loss=0.27668, acc=0.87500
# [59/100] training 15.5% loss=0.05968, acc=1.00000
# [59/100] training 15.7% loss=0.27555, acc=0.87500
# [59/100] training 15.8% loss=0.10273, acc=0.96875
# [59/100] training 16.0% loss=0.30798, acc=0.85938
# [59/100] training 16.1% loss=0.22716, acc=0.92188
# [59/100] training 16.3% loss=0.19231, acc=0.93750
# [59/100] training 16.4% loss=0.15001, acc=0.96875
# [59/100] training 16.7% loss=0.15812, acc=0.92188
# [59/100] training 16.9% loss=0.25405, acc=0.87500
# [59/100] training 17.0% loss=0.13942, acc=0.95312
# [59/100] training 17.2% loss=0.14273, acc=0.93750
# [59/100] training 17.3% loss=0.18191, acc=0.95312
# [59/100] training 17.5% loss=0.20928, acc=0.92188
# [59/100] training 17.7% loss=0.12306, acc=0.93750
# [59/100] training 17.9% loss=0.13158, acc=0.96875
# [59/100] training 18.1% loss=0.22165, acc=0.92188
# [59/100] training 18.2% loss=0.22084, acc=0.90625
# [59/100] training 18.4% loss=0.20733, acc=0.90625
# [59/100] training 18.5% loss=0.18598, acc=0.93750
# [59/100] training 18.8% loss=0.15211, acc=0.95312
# [59/100] training 18.9% loss=0.09627, acc=0.95312
# [59/100] training 19.1% loss=0.16445, acc=0.92188
# [59/100] training 19.2% loss=0.08937, acc=0.95312
# [59/100] training 19.4% loss=0.09414, acc=0.98438
# [59/100] training 19.6% loss=0.17469, acc=0.90625
# [59/100] training 19.7% loss=0.25825, acc=0.85938
# [59/100] training 20.0% loss=0.16098, acc=0.92188
# [59/100] training 20.1% loss=0.14918, acc=0.93750
# [59/100] training 20.3% loss=0.16407, acc=0.95312
# [59/100] training 20.4% loss=0.19477, acc=0.92188
# [59/100] training 20.6% loss=0.28865, acc=0.92188
# [59/100] training 20.8% loss=0.09097, acc=0.95312
# [59/100] training 20.9% loss=0.16447, acc=0.92188
# [59/100] training 21.2% loss=0.22629, acc=0.93750
# [59/100] training 21.3% loss=0.20843, acc=0.89062
# [59/100] training 21.5% loss=0.28568, acc=0.92188
# [59/100] training 21.6% loss=0.03978, acc=1.00000
# [59/100] training 21.8% loss=0.09598, acc=0.96875
# [59/100] training 21.9% loss=0.25275, acc=0.92188
# [59/100] training 22.2% loss=0.18545, acc=0.92188
# [59/100] training 22.4% loss=0.20637, acc=0.92188
# [59/100] training 22.5% loss=0.12104, acc=0.95312
# [59/100] training 22.7% loss=0.15961, acc=0.93750
# [59/100] training 22.8% loss=0.21356, acc=0.92188
# [59/100] training 23.0% loss=0.05959, acc=1.00000
# [59/100] training 23.1% loss=0.26034, acc=0.89062
# [59/100] training 23.4% loss=0.13975, acc=0.95312
# [59/100] training 23.6% loss=0.25338, acc=0.93750
# [59/100] training 23.7% loss=0.15849, acc=0.93750
# [59/100] training 23.9% loss=0.19897, acc=0.89062
# [59/100] training 24.0% loss=0.08863, acc=0.95312
# [59/100] training 24.2% loss=0.13675, acc=0.93750
# [59/100] training 24.3% loss=0.18136, acc=0.96875
# [59/100] training 24.6% loss=0.16747, acc=0.90625
# [59/100] training 24.7% loss=0.22471, acc=0.90625
# [59/100] training 24.9% loss=0.21282, acc=0.92188
# [59/100] training 25.1% loss=0.16650, acc=0.90625
# [59/100] training 25.2% loss=0.17238, acc=0.92188
# [59/100] training 25.4% loss=0.22216, acc=0.87500
# [59/100] training 25.6% loss=0.15980, acc=0.93750
# [59/100] training 25.8% loss=0.27670, acc=0.84375
# [59/100] training 25.9% loss=0.25806, acc=0.85938
# [59/100] training 26.1% loss=0.16743, acc=0.90625
# [59/100] training 26.3% loss=0.11751, acc=0.95312
# [59/100] training 26.4% loss=0.13668, acc=0.92188
# [59/100] training 26.6% loss=0.11170, acc=0.96875
# [59/100] training 26.8% loss=0.10734, acc=0.95312
# [59/100] training 27.0% loss=0.11881, acc=0.90625
# [59/100] training 27.1% loss=0.12496, acc=0.92188
# [59/100] training 27.3% loss=0.23104, acc=0.85938
# [59/100] training 27.4% loss=0.06400, acc=0.98438
# [59/100] training 27.6% loss=0.32952, acc=0.90625
# [59/100] training 27.9% loss=0.25280, acc=0.90625
# [59/100] training 28.0% loss=0.22826, acc=0.90625
# [59/100] training 28.2% loss=0.14755, acc=0.95312
# [59/100] training 28.3% loss=0.13553, acc=0.95312
# [59/100] training 28.5% loss=0.19284, acc=0.92188
# [59/100] training 28.6% loss=0.12959, acc=0.98438
# [59/100] training 28.8% loss=0.14267, acc=0.93750
# [59/100] training 29.1% loss=0.08493, acc=1.00000
# [59/100] training 29.2% loss=0.18686, acc=0.93750
# [59/100] training 29.4% loss=0.24871, acc=0.87500
# [59/100] training 29.5% loss=0.10390, acc=0.96875
# [59/100] training 29.7% loss=0.15916, acc=0.95312
# [59/100] training 29.8% loss=0.20049, acc=0.93750
# [59/100] training 30.0% loss=0.14199, acc=0.96875
# [59/100] training 30.2% loss=0.12369, acc=0.95312
# [59/100] training 30.4% loss=0.09841, acc=0.92188
# [59/100] training 30.6% loss=0.22458, acc=0.93750
# [59/100] training 30.7% loss=0.14247, acc=0.93750
# [59/100] training 30.9% loss=0.26850, acc=0.93750
# [59/100] training 31.0% loss=0.04972, acc=0.98438
# [59/100] training 31.3% loss=0.16091, acc=0.93750
# [59/100] training 31.4% loss=0.25367, acc=0.89062
# [59/100] training 31.6% loss=0.24190, acc=0.93750
# [59/100] training 31.8% loss=0.14124, acc=0.93750
# [59/100] training 31.9% loss=0.22327, acc=0.90625
# [59/100] training 32.1% loss=0.16624, acc=0.92188
# [59/100] training 32.2% loss=0.27074, acc=0.90625
# [59/100] training 32.5% loss=0.11865, acc=0.93750
# [59/100] training 32.6% loss=0.10298, acc=0.98438
# [59/100] training 32.8% loss=0.11886, acc=0.96875
# [59/100] training 32.9% loss=0.19745, acc=0.90625
# [59/100] training 33.1% loss=0.12708, acc=0.93750
# [59/100] training 33.3% loss=0.17231, acc=0.93750
# [59/100] training 33.4% loss=0.08318, acc=0.98438
# [59/100] training 33.7% loss=0.14594, acc=0.95312
# [59/100] training 33.8% loss=0.18860, acc=0.93750
# [59/100] training 34.0% loss=0.12198, acc=0.95312
# [59/100] training 34.1% loss=0.18290, acc=0.95312
# [59/100] training 34.3% loss=0.17293, acc=0.93750
# [59/100] training 34.5% loss=0.25106, acc=0.87500
# [59/100] training 34.7% loss=0.16177, acc=0.95312
# [59/100] training 34.9% loss=0.14665, acc=0.93750
# [59/100] training 35.0% loss=0.15847, acc=0.93750
# [59/100] training 35.2% loss=0.16951, acc=0.89062
# [59/100] training 35.3% loss=0.21926, acc=0.89062
# [59/100] training 35.5% loss=0.18022, acc=0.93750
# [59/100] training 35.6% loss=0.39476, acc=0.84375
# [59/100] training 35.9% loss=0.20637, acc=0.93750
# [59/100] training 36.1% loss=0.24920, acc=0.89062
# [59/100] training 36.2% loss=0.18879, acc=0.90625
# [59/100] training 36.4% loss=0.27930, acc=0.90625
# [59/100] training 36.5% loss=0.23439, acc=0.93750
# [59/100] training 36.7% loss=0.27378, acc=0.87500
# [59/100] training 36.8% loss=0.08367, acc=0.96875
# [59/100] training 37.1% loss=0.16031, acc=0.95312
# [59/100] training 37.3% loss=0.15529, acc=0.92188
# [59/100] training 37.4% loss=0.14141, acc=0.92188
# [59/100] training 37.6% loss=0.11802, acc=0.95312
# [59/100] training 37.7% loss=0.18483, acc=0.95312
# [59/100] training 37.9% loss=0.12345, acc=0.95312
# [59/100] training 38.1% loss=0.12130, acc=0.93750
# [59/100] training 38.3% loss=0.08620, acc=0.95312
# [59/100] training 38.4% loss=0.15185, acc=0.96875
# [59/100] training 38.6% loss=0.18057, acc=0.93750
# [59/100] training 38.8% loss=0.21252, acc=0.93750
# [59/100] training 38.9% loss=0.17291, acc=0.92188
# [59/100] training 39.1% loss=0.29241, acc=0.89062
# [59/100] training 39.3% loss=0.22548, acc=0.90625
# [59/100] training 39.5% loss=0.28259, acc=0.87500
# [59/100] training 39.6% loss=0.11584, acc=0.93750
# [59/100] training 39.8% loss=0.09007, acc=0.98438
# [59/100] training 40.0% loss=0.27992, acc=0.84375
# [59/100] training 40.1% loss=0.24353, acc=0.90625
# [59/100] training 40.4% loss=0.13372, acc=0.93750
# [59/100] training 40.5% loss=0.25498, acc=0.89062
# [59/100] training 40.7% loss=0.15349, acc=0.93750
# [59/100] training 40.8% loss=0.09920, acc=0.93750
# [59/100] training 41.0% loss=0.10630, acc=0.93750
# [59/100] training 41.1% loss=0.23170, acc=0.87500
# [59/100] training 41.3% loss=0.24802, acc=0.92188
# [59/100] training 41.6% loss=0.22972, acc=0.90625
# [59/100] training 41.7% loss=0.13884, acc=0.95312
# [59/100] training 41.9% loss=0.14247, acc=0.92188
# [59/100] training 42.0% loss=0.15480, acc=0.93750
# [59/100] training 42.2% loss=0.21191, acc=0.92188
# [59/100] training 42.3% loss=0.09846, acc=0.98438
# [59/100] training 42.5% loss=0.15708, acc=0.92188
# [59/100] training 42.8% loss=0.10636, acc=0.96875
# [59/100] training 42.9% loss=0.14592, acc=0.95312
# [59/100] training 43.1% loss=0.17088, acc=0.93750
# [59/100] training 43.2% loss=0.13170, acc=0.95312
# [59/100] training 43.4% loss=0.14943, acc=0.95312
# [59/100] training 43.5% loss=0.12893, acc=0.95312
# [59/100] training 43.8% loss=0.20575, acc=0.93750
# [59/100] training 43.9% loss=0.12679, acc=0.95312
# [59/100] training 44.1% loss=0.18243, acc=0.95312
# [59/100] training 44.3% loss=0.15169, acc=0.95312
# [59/100] training 44.4% loss=0.22958, acc=0.92188
# [59/100] training 44.6% loss=0.14156, acc=0.92188
# [59/100] training 44.7% loss=0.33337, acc=0.84375
# [59/100] training 45.0% loss=0.07885, acc=0.95312
# [59/100] training 45.1% loss=0.23753, acc=0.87500
# [59/100] training 45.3% loss=0.13975, acc=0.93750
# [59/100] training 45.5% loss=0.07621, acc=0.96875
# [59/100] training 45.6% loss=0.23226, acc=0.87500
# [59/100] training 45.8% loss=0.13969, acc=0.95312
# [59/100] training 45.9% loss=0.23585, acc=0.89062
# [59/100] training 46.2% loss=0.12202, acc=0.98438
# [59/100] training 46.3% loss=0.11385, acc=0.93750
# [59/100] training 46.5% loss=0.35385, acc=0.90625
# [59/100] training 46.6% loss=0.23641, acc=0.93750
# [59/100] training 46.8% loss=0.12487, acc=0.92188
# [59/100] training 47.0% loss=0.12311, acc=0.95312
# [59/100] training 47.2% loss=0.23197, acc=0.93750
# [59/100] training 47.4% loss=0.16968, acc=0.93750
# [59/100] training 47.5% loss=0.19342, acc=0.92188
# [59/100] training 47.7% loss=0.17460, acc=0.90625
# [59/100] training 47.8% loss=0.15899, acc=0.95312
# [59/100] training 48.0% loss=0.11960, acc=0.95312
# [59/100] training 48.3% loss=0.14960, acc=0.96875
# [59/100] training 48.4% loss=0.08851, acc=0.98438
# [59/100] training 48.6% loss=0.12011, acc=0.93750
# [59/100] training 48.7% loss=0.10170, acc=0.95312
# [59/100] training 48.9% loss=0.19337, acc=0.92188
# [59/100] training 49.0% loss=0.10312, acc=0.96875
# [59/100] training 49.2% loss=0.08917, acc=0.98438
# [59/100] training 49.3% loss=0.15148, acc=0.95312
# [59/100] training 49.6% loss=0.24769, acc=0.90625
# [59/100] training 49.8% loss=0.18200, acc=0.92188
# [59/100] training 49.9% loss=0.20014, acc=0.90625
# [59/100] training 50.1% loss=0.15534, acc=0.92188
# [59/100] training 50.2% loss=0.21070, acc=0.92188
# [59/100] training 50.4% loss=0.40071, acc=0.85938
# [59/100] training 50.6% loss=0.17304, acc=0.93750
# [59/100] training 50.8% loss=0.19045, acc=0.93750
# [59/100] training 51.0% loss=0.30550, acc=0.89062
# [59/100] training 51.1% loss=0.17594, acc=0.95312
# [59/100] training 51.3% loss=0.24870, acc=0.89062
# [59/100] training 51.4% loss=0.23066, acc=0.95312
# [59/100] training 51.7% loss=0.16176, acc=0.92188
# [59/100] training 51.8% loss=0.20008, acc=0.90625
# [59/100] training 52.0% loss=0.20818, acc=0.93750
# [59/100] training 52.1% loss=0.10471, acc=0.93750
# [59/100] training 52.3% loss=0.17819, acc=0.92188
# [59/100] training 52.5% loss=0.08090, acc=0.96875
# [59/100] training 52.6% loss=0.11935, acc=0.95312
# [59/100] training 52.9% loss=0.20685, acc=0.95312
# [59/100] training 53.0% loss=0.03363, acc=1.00000
# [59/100] training 53.2% loss=0.22063, acc=0.95312
# [59/100] training 53.3% loss=0.08093, acc=0.96875
# [59/100] training 53.5% loss=0.23620, acc=0.89062
# [59/100] training 53.7% loss=0.04052, acc=0.98438
# [59/100] training 53.8% loss=0.20689, acc=0.89062
# [59/100] training 54.1% loss=0.20431, acc=0.89062
# [59/100] training 54.2% loss=0.13716, acc=0.96875
# [59/100] training 54.4% loss=0.08015, acc=0.96875
# [59/100] training 54.5% loss=0.27720, acc=0.90625
# [59/100] training 54.7% loss=0.30537, acc=0.90625
# [59/100] training 54.8% loss=0.09587, acc=0.95312
# [59/100] training 55.1% loss=0.22228, acc=0.89062
# [59/100] training 55.3% loss=0.18805, acc=0.90625
# [59/100] training 55.4% loss=0.14281, acc=0.93750
# [59/100] training 55.6% loss=0.18593, acc=0.92188
# [59/100] training 55.7% loss=0.11726, acc=0.95312
# [59/100] training 55.9% loss=0.13397, acc=0.93750
# [59/100] training 56.0% loss=0.14110, acc=0.93750
# [59/100] training 56.3% loss=0.27821, acc=0.85938
# [59/100] training 56.5% loss=0.09685, acc=0.93750
# [59/100] training 56.6% loss=0.24033, acc=0.92188
# [59/100] training 56.8% loss=0.11528, acc=0.95312
# [59/100] training 56.9% loss=0.13852, acc=0.92188
# [59/100] training 57.1% loss=0.23463, acc=0.93750
# [59/100] training 57.2% loss=0.13892, acc=0.93750
# [59/100] training 57.5% loss=0.16176, acc=0.93750
# [59/100] training 57.6% loss=0.04576, acc=1.00000
# [59/100] training 57.8% loss=0.15782, acc=0.95312
# [59/100] training 58.0% loss=0.17190, acc=0.92188
# [59/100] training 58.1% loss=0.16146, acc=0.93750
# [59/100] training 58.3% loss=0.13512, acc=0.95312
# [59/100] training 58.4% loss=0.24570, acc=0.87500
# [59/100] training 58.7% loss=0.24227, acc=0.89062
# [59/100] training 58.8% loss=0.26731, acc=0.87500
# [59/100] training 59.0% loss=0.13360, acc=0.93750
# [59/100] training 59.2% loss=0.16732, acc=0.90625
# [59/100] training 59.3% loss=0.15514, acc=0.93750
# [59/100] training 59.5% loss=0.18785, acc=0.93750
# [59/100] training 59.7% loss=0.19638, acc=0.90625
# [59/100] training 59.9% loss=0.12178, acc=0.96875
# [59/100] training 60.0% loss=0.09874, acc=0.96875
# [59/100] training 60.2% loss=0.15241, acc=0.93750
# [59/100] training 60.3% loss=0.14974, acc=0.92188
# [59/100] training 60.5% loss=0.27565, acc=0.89062
# [59/100] training 60.8% loss=0.21127, acc=0.92188
# [59/100] training 60.9% loss=0.20475, acc=0.93750
# [59/100] training 61.1% loss=0.19241, acc=0.90625
# [59/100] training 61.2% loss=0.17851, acc=0.93750
# [59/100] training 61.4% loss=0.21730, acc=0.92188
# [59/100] training 61.5% loss=0.32978, acc=0.84375
# [59/100] training 61.7% loss=0.10615, acc=0.96875
# [59/100] training 62.0% loss=0.37843, acc=0.84375
# [59/100] training 62.1% loss=0.31493, acc=0.90625
# [59/100] training 62.3% loss=0.09496, acc=0.96875
# [59/100] training 62.4% loss=0.15850, acc=0.96875
# [59/100] training 62.6% loss=0.22199, acc=0.90625
# [59/100] training 62.7% loss=0.12097, acc=0.98438
# [59/100] training 62.9% loss=0.23970, acc=0.87500
# [59/100] training 63.1% loss=0.27457, acc=0.89062
# [59/100] training 63.3% loss=0.18203, acc=0.90625
# [59/100] training 63.5% loss=0.19831, acc=0.92188
# [59/100] training 63.6% loss=0.25564, acc=0.92188
# [59/100] training 63.8% loss=0.23940, acc=0.93750
# [59/100] training 63.9% loss=0.14527, acc=0.95312
# [59/100] training 64.2% loss=0.15780, acc=0.95312
# [59/100] training 64.3% loss=0.21135, acc=0.92188
# [59/100] training 64.5% loss=0.19899, acc=0.92188
# [59/100] training 64.7% loss=0.13821, acc=0.95312
# [59/100] training 64.8% loss=0.28604, acc=0.89062
# [59/100] training 65.0% loss=0.14303, acc=0.92188
# [59/100] training 65.1% loss=0.13224, acc=0.96875
# [59/100] training 65.4% loss=0.19666, acc=0.95312
# [59/100] training 65.5% loss=0.06824, acc=0.98438
# [59/100] training 65.7% loss=0.13567, acc=0.96875
# [59/100] training 65.8% loss=0.16081, acc=0.93750
# [59/100] training 66.0% loss=0.15454, acc=0.95312
# [59/100] training 66.2% loss=0.07476, acc=0.98438
# [59/100] training 66.3% loss=0.24584, acc=0.92188
# [59/100] training 66.6% loss=0.11603, acc=0.95312
# [59/100] training 66.7% loss=0.15693, acc=0.92188
# [59/100] training 66.9% loss=0.28151, acc=0.87500
# [59/100] training 67.0% loss=0.21683, acc=0.93750
# [59/100] training 67.2% loss=0.06190, acc=0.96875
# [59/100] training 67.4% loss=0.10216, acc=0.96875
# [59/100] training 67.6% loss=0.10302, acc=0.96875
# [59/100] training 67.8% loss=0.11965, acc=0.96875
# [59/100] training 67.9% loss=0.16849, acc=0.92188
# [59/100] training 68.1% loss=0.17142, acc=0.92188
# [59/100] training 68.2% loss=0.26460, acc=0.92188
# [59/100] training 68.4% loss=0.08454, acc=0.96875
# [59/100] training 68.5% loss=0.30398, acc=0.93750
# [59/100] training 68.8% loss=0.11604, acc=0.95312
# [59/100] training 69.0% loss=0.24547, acc=0.89062
# [59/100] training 69.1% loss=0.09860, acc=0.93750
# [59/100] training 69.3% loss=0.12946, acc=0.93750
# [59/100] training 69.4% loss=0.09570, acc=0.96875
# [59/100] training 69.6% loss=0.15238, acc=0.96875
# [59/100] training 69.7% loss=0.15574, acc=0.93750
# [59/100] training 70.0% loss=0.11527, acc=0.92188
# [59/100] training 70.2% loss=0.22330, acc=0.90625
# [59/100] training 70.3% loss=0.21302, acc=0.89062
# [59/100] training 70.5% loss=0.15146, acc=0.92188
# [59/100] training 70.6% loss=0.22013, acc=0.92188
# [59/100] training 70.8% loss=0.16586, acc=0.92188
# [59/100] training 71.0% loss=0.15067, acc=0.95312
# [59/100] training 71.2% loss=0.08919, acc=0.96875
# [59/100] training 71.3% loss=0.08047, acc=0.96875
# [59/100] training 71.5% loss=0.26710, acc=0.92188
# [59/100] training 71.7% loss=0.16549, acc=0.92188
# [59/100] training 71.8% loss=0.25968, acc=0.90625
# [59/100] training 72.0% loss=0.07479, acc=0.98438
# [59/100] training 72.2% loss=0.18949, acc=0.90625
# [59/100] training 72.4% loss=0.25949, acc=0.89062
# [59/100] training 72.5% loss=0.17460, acc=0.93750
# [59/100] training 72.7% loss=0.33243, acc=0.90625
# [59/100] training 72.9% loss=0.09189, acc=0.96875
# [59/100] training 73.0% loss=0.06151, acc=0.98438
# [59/100] training 73.3% loss=0.29887, acc=0.87500
# [59/100] training 73.4% loss=0.11360, acc=0.95312
# [59/100] training 73.6% loss=0.11949, acc=0.95312
# [59/100] training 73.7% loss=0.17197, acc=0.95312
# [59/100] training 73.9% loss=0.15468, acc=0.93750
# [59/100] training 74.0% loss=0.21228, acc=0.89062
# [59/100] training 74.2% loss=0.10756, acc=0.96875
# [59/100] training 74.5% loss=0.13322, acc=0.93750
# [59/100] training 74.6% loss=0.29417, acc=0.89062
# [59/100] training 74.8% loss=0.38364, acc=0.85938
# [59/100] training 74.9% loss=0.15438, acc=0.96875
# [59/100] training 75.1% loss=0.13105, acc=0.93750
# [59/100] training 75.2% loss=0.12087, acc=0.95312
# [59/100] training 75.4% loss=0.12645, acc=0.93750
# [59/100] training 75.7% loss=0.20312, acc=0.92188
# [59/100] training 75.8% loss=0.18528, acc=0.95312
# [59/100] training 76.0% loss=0.11994, acc=0.95312
# [59/100] training 76.1% loss=0.19655, acc=0.90625
# [59/100] training 76.3% loss=0.14024, acc=0.93750
# [59/100] training 76.4% loss=0.18479, acc=0.90625
# [59/100] training 76.7% loss=0.12301, acc=0.96875
# [59/100] training 76.8% loss=0.09505, acc=0.98438
# [59/100] training 77.0% loss=0.07449, acc=0.95312
# [59/100] training 77.2% loss=0.14080, acc=0.90625
# [59/100] training 77.3% loss=0.08721, acc=0.96875
# [59/100] training 77.5% loss=0.19410, acc=0.92188
# [59/100] training 77.6% loss=0.21029, acc=0.92188
# [59/100] training 77.9% loss=0.17356, acc=0.93750
# [59/100] training 78.0% loss=0.09793, acc=0.95312
# [59/100] training 78.2% loss=0.22904, acc=0.92188
# [59/100] training 78.4% loss=0.12042, acc=0.93750
# [59/100] training 78.5% loss=0.23539, acc=0.92188
# [59/100] training 78.7% loss=0.14409, acc=0.93750
# [59/100] training 78.8% loss=0.08341, acc=0.95312
# [59/100] training 79.1% loss=0.10793, acc=0.93750
# [59/100] training 79.2% loss=0.13423, acc=0.95312
# [59/100] training 79.4% loss=0.11247, acc=0.98438
# [59/100] training 79.5% loss=0.12920, acc=0.96875
# [59/100] training 79.7% loss=0.13912, acc=0.93750
# [59/100] training 79.9% loss=0.17259, acc=0.90625
# [59/100] training 80.1% loss=0.12735, acc=0.93750
# [59/100] training 80.3% loss=0.21872, acc=0.87500
# [59/100] training 80.4% loss=0.12155, acc=0.95312
# [59/100] training 80.6% loss=0.26818, acc=0.89062
# [59/100] training 80.7% loss=0.16076, acc=0.96875
# [59/100] training 80.9% loss=0.16259, acc=0.93750
# [59/100] training 81.2% loss=0.23604, acc=0.92188
# [59/100] training 81.3% loss=0.16520, acc=0.95312
# [59/100] training 81.5% loss=0.12378, acc=0.93750
# [59/100] training 81.6% loss=0.27847, acc=0.89062
# [59/100] training 81.8% loss=0.23515, acc=0.89062
# [59/100] training 81.9% loss=0.29694, acc=0.92188
# [59/100] training 82.1% loss=0.13660, acc=0.96875
# [59/100] training 82.2% loss=0.19422, acc=0.93750
# [59/100] training 82.5% loss=0.13261, acc=0.95312
# [59/100] training 82.7% loss=0.20483, acc=0.93750
# [59/100] training 82.8% loss=0.10025, acc=0.92188
# [59/100] training 83.0% loss=0.11376, acc=0.95312
# [59/100] training 83.1% loss=0.16826, acc=0.92188
# [59/100] training 83.3% loss=0.09587, acc=0.95312
# [59/100] training 83.5% loss=0.10258, acc=0.96875
# [59/100] training 83.7% loss=0.21680, acc=0.92188
# [59/100] training 83.9% loss=0.22349, acc=0.89062
# [59/100] training 84.0% loss=0.11449, acc=0.95312
# [59/100] training 84.2% loss=0.07677, acc=0.96875
# [59/100] training 84.3% loss=0.18503, acc=0.95312
# [59/100] training 84.5% loss=0.13050, acc=0.98438
# [59/100] training 84.7% loss=0.14559, acc=0.92188
# [59/100] training 84.9% loss=0.11542, acc=0.95312
# [59/100] training 85.0% loss=0.16806, acc=0.90625
# [59/100] training 85.2% loss=0.17188, acc=0.93750
# [59/100] training 85.4% loss=0.08518, acc=0.96875
# [59/100] training 85.5% loss=0.08630, acc=0.93750
# [59/100] training 85.8% loss=0.30456, acc=0.92188
# [59/100] training 85.9% loss=0.07583, acc=0.96875
# [59/100] training 86.1% loss=0.12997, acc=0.96875
# [59/100] training 86.2% loss=0.04994, acc=0.98438
# [59/100] training 86.4% loss=0.32144, acc=0.87500
# [59/100] training 86.6% loss=0.12959, acc=0.93750
# [59/100] training 86.7% loss=0.11766, acc=0.93750
# [59/100] training 87.0% loss=0.19815, acc=0.90625
# [59/100] training 87.1% loss=0.13807, acc=0.95312
# [59/100] training 87.3% loss=0.10187, acc=0.96875
# [59/100] training 87.4% loss=0.13949, acc=0.96875
# [59/100] training 87.6% loss=0.21003, acc=0.90625
# [59/100] training 87.7% loss=0.29489, acc=0.89062
# [59/100] training 87.9% loss=0.24345, acc=0.89062
# [59/100] training 88.2% loss=0.14421, acc=0.95312
# [59/100] training 88.3% loss=0.20060, acc=0.92188
# [59/100] training 88.5% loss=0.19493, acc=0.93750
# [59/100] training 88.6% loss=0.09373, acc=0.96875
# [59/100] training 88.8% loss=0.20278, acc=0.90625
# [59/100] training 88.9% loss=0.22684, acc=0.92188
# [59/100] training 89.2% loss=0.11172, acc=0.96875
# [59/100] training 89.4% loss=0.19388, acc=0.90625
# [59/100] training 89.5% loss=0.32977, acc=0.87500
# [59/100] training 89.7% loss=0.17127, acc=0.95312
# [59/100] training 89.8% loss=0.13852, acc=0.96875
# [59/100] training 90.0% loss=0.15076, acc=0.92188
# [59/100] training 90.1% loss=0.20690, acc=0.92188
# [59/100] training 90.4% loss=0.20739, acc=0.90625
# [59/100] training 90.5% loss=0.15100, acc=0.92188
# [59/100] training 90.7% loss=0.18118, acc=0.92188
# [59/100] training 90.9% loss=0.08175, acc=0.95312
# [59/100] training 91.0% loss=0.15132, acc=0.95312
# [59/100] training 91.2% loss=0.14629, acc=0.93750
# [59/100] training 91.3% loss=0.24661, acc=0.92188
# [59/100] training 91.6% loss=0.26208, acc=0.90625
# [59/100] training 91.7% loss=0.14231, acc=0.93750
# [59/100] training 91.9% loss=0.28953, acc=0.87500
# [59/100] training 92.1% loss=0.06381, acc=0.98438
# [59/100] training 92.2% loss=0.06836, acc=0.96875
# [59/100] training 92.4% loss=0.22001, acc=0.92188
# [59/100] training 92.6% loss=0.17018, acc=0.89062
# [59/100] training 92.8% loss=0.20963, acc=0.92188
# [59/100] training 92.9% loss=0.28930, acc=0.90625
# [59/100] training 93.1% loss=0.46535, acc=0.79688
# [59/100] training 93.2% loss=0.25162, acc=0.90625
# [59/100] training 93.4% loss=0.19230, acc=0.90625
# [59/100] training 93.7% loss=0.18151, acc=0.93750
# [59/100] training 93.8% loss=0.19810, acc=0.89062
# [59/100] training 94.0% loss=0.17004, acc=0.96875
# [59/100] training 94.1% loss=0.15286, acc=0.93750
# [59/100] training 94.3% loss=0.08320, acc=0.98438
# [59/100] training 94.4% loss=0.19348, acc=0.92188
# [59/100] training 94.6% loss=0.06451, acc=0.98438
# [59/100] training 94.9% loss=0.06805, acc=0.98438
# [59/100] training 95.0% loss=0.15677, acc=0.93750
# [59/100] training 95.2% loss=0.45703, acc=0.89062
# [59/100] training 95.3% loss=0.23647, acc=0.92188
# [59/100] training 95.5% loss=0.10830, acc=0.95312
# [59/100] training 95.6% loss=0.19704, acc=0.92188
# [59/100] training 95.8% loss=0.14030, acc=0.95312
# [59/100] training 96.0% loss=0.22511, acc=0.93750
# [59/100] training 96.2% loss=0.10913, acc=0.93750
# [59/100] training 96.4% loss=0.11421, acc=0.96875
# [59/100] training 96.5% loss=0.18539, acc=0.93750
# [59/100] training 96.7% loss=0.15648, acc=0.93750
# [59/100] training 96.8% loss=0.28694, acc=0.92188
# [59/100] training 97.1% loss=0.11505, acc=0.93750
# [59/100] training 97.2% loss=0.19285, acc=0.90625
# [59/100] training 97.4% loss=0.21247, acc=0.92188
# [59/100] training 97.6% loss=0.19670, acc=0.89062
# [59/100] training 97.7% loss=0.20125, acc=0.90625
# [59/100] training 97.9% loss=0.15768, acc=0.93750
# [59/100] training 98.0% loss=0.16430, acc=0.93750
# [59/100] training 98.3% loss=0.30408, acc=0.87500
# [59/100] training 98.4% loss=0.18707, acc=0.90625
# [59/100] training 98.6% loss=0.26117, acc=0.93750
# [59/100] training 98.7% loss=0.25647, acc=0.92188
# [59/100] training 98.9% loss=0.21335, acc=0.90625
# [59/100] training 99.1% loss=0.14473, acc=0.93750
# [59/100] training 99.2% loss=0.18177, acc=0.93750
# [59/100] training 99.5% loss=0.20787, acc=0.89062
# [59/100] training 99.6% loss=0.12552, acc=0.95312
# [59/100] training 99.8% loss=0.09071, acc=0.96875
# [59/100] training 99.9% loss=0.04808, acc=0.98438
# [59/100] testing 0.9% loss=0.24034, acc=0.89062
# [59/100] testing 1.8% loss=0.34956, acc=0.85938
# [59/100] testing 2.2% loss=0.31355, acc=0.90625
# [59/100] testing 3.1% loss=0.44136, acc=0.87500
# [59/100] testing 3.5% loss=0.11571, acc=0.96875
# [59/100] testing 4.4% loss=0.15579, acc=0.92188
# [59/100] testing 4.8% loss=0.40219, acc=0.87500
# [59/100] testing 5.7% loss=0.25975, acc=0.89062
# [59/100] testing 6.6% loss=0.16848, acc=0.95312
# [59/100] testing 7.0% loss=0.13522, acc=0.95312
# [59/100] testing 7.9% loss=0.25294, acc=0.90625
# [59/100] testing 8.3% loss=0.32622, acc=0.90625
# [59/100] testing 9.2% loss=0.23194, acc=0.95312
# [59/100] testing 9.7% loss=0.16079, acc=0.95312
# [59/100] testing 10.5% loss=0.18939, acc=0.93750
# [59/100] testing 11.0% loss=0.31268, acc=0.87500
# [59/100] testing 11.8% loss=0.17700, acc=0.93750
# [59/100] testing 12.7% loss=0.35120, acc=0.90625
# [59/100] testing 13.2% loss=0.28460, acc=0.85938
# [59/100] testing 14.0% loss=0.40263, acc=0.90625
# [59/100] testing 14.5% loss=0.25390, acc=0.92188
# [59/100] testing 15.4% loss=0.28564, acc=0.93750
# [59/100] testing 15.8% loss=0.18782, acc=0.93750
# [59/100] testing 16.7% loss=0.18946, acc=0.93750
# [59/100] testing 17.5% loss=0.28849, acc=0.87500
# [59/100] testing 18.0% loss=0.22130, acc=0.90625
# [59/100] testing 18.9% loss=0.08840, acc=0.95312
# [59/100] testing 19.3% loss=0.35740, acc=0.87500
# [59/100] testing 20.2% loss=0.26637, acc=0.92188
# [59/100] testing 20.6% loss=0.22119, acc=0.92188
# [59/100] testing 21.5% loss=0.22184, acc=0.92188
# [59/100] testing 21.9% loss=0.42593, acc=0.84375
# [59/100] testing 22.8% loss=0.33305, acc=0.89062
# [59/100] testing 23.7% loss=0.37605, acc=0.89062
# [59/100] testing 24.1% loss=0.15613, acc=0.95312
# [59/100] testing 25.0% loss=0.23326, acc=0.92188
# [59/100] testing 25.4% loss=0.07203, acc=0.96875
# [59/100] testing 26.3% loss=0.36387, acc=0.90625
# [59/100] testing 26.8% loss=0.28104, acc=0.90625
# [59/100] testing 27.6% loss=0.21614, acc=0.92188
# [59/100] testing 28.5% loss=0.23932, acc=0.90625
# [59/100] testing 29.0% loss=0.26424, acc=0.92188
# [59/100] testing 29.8% loss=0.40005, acc=0.93750
# [59/100] testing 30.3% loss=0.29408, acc=0.95312
# [59/100] testing 31.1% loss=0.38653, acc=0.85938
# [59/100] testing 31.6% loss=0.28740, acc=0.93750
# [59/100] testing 32.5% loss=0.28454, acc=0.93750
# [59/100] testing 32.9% loss=0.57674, acc=0.87500
# [59/100] testing 33.8% loss=0.29664, acc=0.89062
# [59/100] testing 34.7% loss=0.51952, acc=0.84375
# [59/100] testing 35.1% loss=0.20218, acc=0.92188
# [59/100] testing 36.0% loss=0.31619, acc=0.89062
# [59/100] testing 36.4% loss=0.23261, acc=0.93750
# [59/100] testing 37.3% loss=0.25275, acc=0.95312
# [59/100] testing 37.7% loss=0.44453, acc=0.84375
# [59/100] testing 38.6% loss=0.26161, acc=0.96875
# [59/100] testing 39.5% loss=0.29106, acc=0.93750
# [59/100] testing 39.9% loss=0.18571, acc=0.90625
# [59/100] testing 40.8% loss=0.36821, acc=0.92188
# [59/100] testing 41.2% loss=0.32755, acc=0.93750
# [59/100] testing 42.1% loss=0.34940, acc=0.87500
# [59/100] testing 42.5% loss=0.13990, acc=0.93750
# [59/100] testing 43.4% loss=0.22380, acc=0.92188
# [59/100] testing 43.9% loss=0.07536, acc=0.96875
# [59/100] testing 44.7% loss=0.24073, acc=0.92188
# [59/100] testing 45.6% loss=0.23249, acc=0.89062
# [59/100] testing 46.1% loss=0.28946, acc=0.90625
# [59/100] testing 46.9% loss=0.20149, acc=0.92188
# [59/100] testing 47.4% loss=0.21618, acc=0.92188
# [59/100] testing 48.3% loss=0.48144, acc=0.85938
# [59/100] testing 48.7% loss=0.43540, acc=0.85938
# [59/100] testing 49.6% loss=0.53639, acc=0.85938
# [59/100] testing 50.4% loss=0.19970, acc=0.95312
# [59/100] testing 50.9% loss=0.45665, acc=0.89062
# [59/100] testing 51.8% loss=0.27206, acc=0.90625
# [59/100] testing 52.2% loss=0.21985, acc=0.92188
# [59/100] testing 53.1% loss=0.17002, acc=0.92188
# [59/100] testing 53.5% loss=0.19735, acc=0.93750
# [59/100] testing 54.4% loss=0.31539, acc=0.85938
# [59/100] testing 54.8% loss=0.24272, acc=0.89062
# [59/100] testing 55.7% loss=0.13044, acc=0.96875
# [59/100] testing 56.6% loss=0.30949, acc=0.89062
# [59/100] testing 57.0% loss=0.26087, acc=0.92188
# [59/100] testing 57.9% loss=0.30025, acc=0.89062
# [59/100] testing 58.3% loss=0.27035, acc=0.89062
# [59/100] testing 59.2% loss=0.27270, acc=0.90625
# [59/100] testing 59.7% loss=0.10738, acc=0.95312
# [59/100] testing 60.5% loss=0.41398, acc=0.82812
# [59/100] testing 61.4% loss=0.11225, acc=0.95312
# [59/100] testing 61.9% loss=0.23077, acc=0.92188
# [59/100] testing 62.7% loss=0.10740, acc=0.96875
# [59/100] testing 63.2% loss=0.37418, acc=0.87500
# [59/100] testing 64.0% loss=0.54768, acc=0.89062
# [59/100] testing 64.5% loss=0.13136, acc=0.93750
# [59/100] testing 65.4% loss=0.12259, acc=0.93750
# [59/100] testing 65.8% loss=0.28650, acc=0.90625
# [59/100] testing 66.7% loss=0.12877, acc=0.95312
# [59/100] testing 67.6% loss=0.44740, acc=0.90625
# [59/100] testing 68.0% loss=0.04635, acc=0.98438
# [59/100] testing 68.9% loss=0.25713, acc=0.96875
# [59/100] testing 69.3% loss=0.24897, acc=0.89062
# [59/100] testing 70.2% loss=0.41269, acc=0.82812
# [59/100] testing 70.6% loss=0.23859, acc=0.93750
# [59/100] testing 71.5% loss=0.29291, acc=0.92188
# [59/100] testing 72.4% loss=0.13230, acc=0.95312
# [59/100] testing 72.8% loss=0.15180, acc=0.93750
# [59/100] testing 73.7% loss=0.11813, acc=0.93750
# [59/100] testing 74.1% loss=0.49647, acc=0.85938
# [59/100] testing 75.0% loss=0.17761, acc=0.89062
# [59/100] testing 75.4% loss=0.45690, acc=0.87500
# [59/100] testing 76.3% loss=0.06198, acc=0.98438
# [59/100] testing 76.8% loss=0.19284, acc=0.93750
# [59/100] testing 77.6% loss=0.15312, acc=0.90625
# [59/100] testing 78.5% loss=0.31924, acc=0.87500
# [59/100] testing 79.0% loss=0.17269, acc=0.95312
# [59/100] testing 79.8% loss=0.20795, acc=0.90625
# [59/100] testing 80.3% loss=0.19553, acc=0.92188
# [59/100] testing 81.2% loss=0.43417, acc=0.89062
# [59/100] testing 81.6% loss=0.15258, acc=0.95312
# [59/100] testing 82.5% loss=0.17095, acc=0.92188
# [59/100] testing 83.3% loss=0.21959, acc=0.96875
# [59/100] testing 83.8% loss=0.09324, acc=0.95312
# [59/100] testing 84.7% loss=0.32091, acc=0.87500
# [59/100] testing 85.1% loss=0.22238, acc=0.90625
# [59/100] testing 86.0% loss=0.28359, acc=0.92188
# [59/100] testing 86.4% loss=0.35045, acc=0.85938
# [59/100] testing 87.3% loss=0.26132, acc=0.87500
# [59/100] testing 87.7% loss=0.34951, acc=0.85938
# [59/100] testing 88.6% loss=0.21953, acc=0.90625
# [59/100] testing 89.5% loss=0.57361, acc=0.81250
# [59/100] testing 89.9% loss=0.16011, acc=0.92188
# [59/100] testing 90.8% loss=0.38888, acc=0.93750
# [59/100] testing 91.2% loss=0.21812, acc=0.92188
# [59/100] testing 92.1% loss=0.25329, acc=0.90625
# [59/100] testing 92.6% loss=0.37497, acc=0.87500
# [59/100] testing 93.4% loss=0.26611, acc=0.90625
# [59/100] testing 94.3% loss=0.08856, acc=0.96875
# [59/100] testing 94.7% loss=0.24662, acc=0.89062
# [59/100] testing 95.6% loss=0.42853, acc=0.84375
# [59/100] testing 96.1% loss=0.12281, acc=0.96875
# [59/100] testing 96.9% loss=0.47313, acc=0.81250
# [59/100] testing 97.4% loss=0.09159, acc=0.98438
# [59/100] testing 98.3% loss=0.20028, acc=0.93750
# [59/100] testing 98.7% loss=0.19078, acc=0.92188
# [59/100] testing 99.6% loss=0.35310, acc=0.89062
# [60/100] training 0.2% loss=0.23407, acc=0.89062
# [60/100] training 0.4% loss=0.30703, acc=0.82812
# [60/100] training 0.5% loss=0.10550, acc=0.95312
# [60/100] training 0.8% loss=0.07734, acc=0.96875
# [60/100] training 0.9% loss=0.19093, acc=0.92188
# [60/100] training 1.1% loss=0.18427, acc=0.95312
# [60/100] training 1.2% loss=0.17446, acc=0.90625
# [60/100] training 1.4% loss=0.14594, acc=0.92188
# [60/100] training 1.6% loss=0.12381, acc=0.93750
# [60/100] training 1.8% loss=0.17363, acc=0.92188
# [60/100] training 2.0% loss=0.20620, acc=0.90625
# [60/100] training 2.1% loss=0.19132, acc=0.89062
# [60/100] training 2.3% loss=0.18106, acc=0.89062
# [60/100] training 2.4% loss=0.18959, acc=0.92188
# [60/100] training 2.6% loss=0.11130, acc=0.96875
# [60/100] training 2.7% loss=0.18036, acc=0.93750
# [60/100] training 3.0% loss=0.14869, acc=0.95312
# [60/100] training 3.2% loss=0.08537, acc=0.95312
# [60/100] training 3.3% loss=0.27732, acc=0.92188
# [60/100] training 3.5% loss=0.14161, acc=0.95312
# [60/100] training 3.6% loss=0.23720, acc=0.85938
# [60/100] training 3.8% loss=0.21989, acc=0.92188
# [60/100] training 3.9% loss=0.17592, acc=0.92188
# [60/100] training 4.2% loss=0.13697, acc=0.95312
# [60/100] training 4.4% loss=0.08946, acc=0.95312
# [60/100] training 4.5% loss=0.07543, acc=1.00000
# [60/100] training 4.7% loss=0.28462, acc=0.89062
# [60/100] training 4.8% loss=0.18027, acc=0.93750
# [60/100] training 5.0% loss=0.12603, acc=0.95312
# [60/100] training 5.2% loss=0.20268, acc=0.87500
# [60/100] training 5.4% loss=0.11737, acc=0.96875
# [60/100] training 5.5% loss=0.16694, acc=0.92188
# [60/100] training 5.7% loss=0.16636, acc=0.92188
# [60/100] training 5.9% loss=0.17493, acc=0.96875
# [60/100] training 6.0% loss=0.26779, acc=0.87500
# [60/100] training 6.3% loss=0.18037, acc=0.90625
# [60/100] training 6.4% loss=0.21981, acc=0.92188
# [60/100] training 6.6% loss=0.18052, acc=0.89062
# [60/100] training 6.7% loss=0.23230, acc=0.85938
# [60/100] training 6.9% loss=0.17986, acc=0.90625
# [60/100] training 7.1% loss=0.12472, acc=0.95312
# [60/100] training 7.2% loss=0.21587, acc=0.90625
# [60/100] training 7.5% loss=0.15742, acc=0.92188
# [60/100] training 7.6% loss=0.28032, acc=0.90625
# [60/100] training 7.8% loss=0.12252, acc=0.93750
# [60/100] training 7.9% loss=0.16311, acc=0.90625
# [60/100] training 8.1% loss=0.09545, acc=0.98438
# [60/100] training 8.2% loss=0.19381, acc=0.92188
# [60/100] training 8.4% loss=0.22433, acc=0.90625
# [60/100] training 8.7% loss=0.18310, acc=0.92188
# [60/100] training 8.8% loss=0.16135, acc=0.96875
# [60/100] training 9.0% loss=0.16455, acc=0.90625
# [60/100] training 9.1% loss=0.16742, acc=0.96875
# [60/100] training 9.3% loss=0.19869, acc=0.93750
# [60/100] training 9.4% loss=0.07454, acc=0.96875
# [60/100] training 9.7% loss=0.11909, acc=0.95312
# [60/100] training 9.9% loss=0.33122, acc=0.85938
# [60/100] training 10.0% loss=0.20963, acc=0.93750
# [60/100] training 10.2% loss=0.16247, acc=0.95312
# [60/100] training 10.3% loss=0.15793, acc=0.90625
# [60/100] training 10.5% loss=0.24150, acc=0.92188
# [60/100] training 10.6% loss=0.16150, acc=0.92188
# [60/100] training 10.9% loss=0.10897, acc=0.95312
# [60/100] training 11.0% loss=0.25840, acc=0.92188
# [60/100] training 11.2% loss=0.08711, acc=0.98438
# [60/100] training 11.4% loss=0.15380, acc=0.92188
# [60/100] training 11.5% loss=0.25844, acc=0.90625
# [60/100] training 11.7% loss=0.06556, acc=1.00000
# [60/100] training 11.8% loss=0.19656, acc=0.89062
# [60/100] training 12.1% loss=0.18863, acc=0.93750
# [60/100] training 12.2% loss=0.09194, acc=0.95312
# [60/100] training 12.4% loss=0.19022, acc=0.92188
# [60/100] training 12.6% loss=0.24421, acc=0.93750
# [60/100] training 12.7% loss=0.12224, acc=0.93750
# [60/100] training 12.9% loss=0.28051, acc=0.89062
# [60/100] training 13.0% loss=0.12919, acc=0.95312
# [60/100] training 13.3% loss=0.14872, acc=0.92188
# [60/100] training 13.4% loss=0.19951, acc=0.92188
# [60/100] training 13.6% loss=0.17884, acc=0.90625
# [60/100] training 13.7% loss=0.25981, acc=0.89062
# [60/100] training 13.9% loss=0.22724, acc=0.89062
# [60/100] training 14.1% loss=0.27327, acc=0.92188
# [60/100] training 14.3% loss=0.12979, acc=0.92188
# [60/100] training 14.5% loss=0.19873, acc=0.96875
# [60/100] training 14.6% loss=0.15279, acc=0.93750
# [60/100] training 14.8% loss=0.25474, acc=0.89062
# [60/100] training 14.9% loss=0.09377, acc=0.96875
# [60/100] training 15.1% loss=0.27465, acc=0.89062
# [60/100] training 15.4% loss=0.19167, acc=0.92188
# [60/100] training 15.5% loss=0.05205, acc=1.00000
# [60/100] training 15.7% loss=0.29050, acc=0.90625
# [60/100] training 15.8% loss=0.13322, acc=0.95312
# [60/100] training 16.0% loss=0.15209, acc=0.93750
# [60/100] training 16.1% loss=0.36446, acc=0.89062
# [60/100] training 16.3% loss=0.13881, acc=0.90625
# [60/100] training 16.4% loss=0.19084, acc=0.89062
# [60/100] training 16.7% loss=0.22860, acc=0.89062
# [60/100] training 16.9% loss=0.15939, acc=0.95312
# [60/100] training 17.0% loss=0.22481, acc=0.92188
# [60/100] training 17.2% loss=0.22310, acc=0.90625
# [60/100] training 17.3% loss=0.14693, acc=0.95312
# [60/100] training 17.5% loss=0.18681, acc=0.89062
# [60/100] training 17.7% loss=0.14797, acc=0.95312
# [60/100] training 17.9% loss=0.21312, acc=0.87500
# [60/100] training 18.1% loss=0.21270, acc=0.95312
# [60/100] training 18.2% loss=0.14280, acc=0.92188
# [60/100] training 18.4% loss=0.25119, acc=0.85938
# [60/100] training 18.5% loss=0.17872, acc=0.92188
# [60/100] training 18.8% loss=0.16894, acc=0.92188
# [60/100] training 18.9% loss=0.07394, acc=0.98438
# [60/100] training 19.1% loss=0.17148, acc=0.90625
# [60/100] training 19.2% loss=0.10797, acc=0.95312
# [60/100] training 19.4% loss=0.12584, acc=0.95312
# [60/100] training 19.6% loss=0.16739, acc=0.89062
# [60/100] training 19.7% loss=0.15537, acc=0.95312
# [60/100] training 20.0% loss=0.15454, acc=0.93750
# [60/100] training 20.1% loss=0.22250, acc=0.89062
# [60/100] training 20.3% loss=0.16465, acc=0.90625
# [60/100] training 20.4% loss=0.15532, acc=0.95312
# [60/100] training 20.6% loss=0.30489, acc=0.92188
# [60/100] training 20.8% loss=0.12819, acc=0.93750
# [60/100] training 20.9% loss=0.16027, acc=0.92188
# [60/100] training 21.2% loss=0.19892, acc=0.92188
# [60/100] training 21.3% loss=0.20993, acc=0.93750
# [60/100] training 21.5% loss=0.26298, acc=0.92188
# [60/100] training 21.6% loss=0.07953, acc=0.96875
# [60/100] training 21.8% loss=0.11745, acc=0.93750
# [60/100] training 21.9% loss=0.18422, acc=0.93750
# [60/100] training 22.2% loss=0.09957, acc=0.96875
# [60/100] training 22.4% loss=0.23932, acc=0.89062
# [60/100] training 22.5% loss=0.06362, acc=0.98438
# [60/100] training 22.7% loss=0.14742, acc=0.95312
# [60/100] training 22.8% loss=0.18805, acc=0.95312
# [60/100] training 23.0% loss=0.09811, acc=0.93750
# [60/100] training 23.1% loss=0.27051, acc=0.90625
# [60/100] training 23.4% loss=0.15139, acc=0.95312
# [60/100] training 23.6% loss=0.21523, acc=0.95312
# [60/100] training 23.7% loss=0.13726, acc=0.95312
# [60/100] training 23.9% loss=0.23369, acc=0.87500
# [60/100] training 24.0% loss=0.10886, acc=0.95312
# [60/100] training 24.2% loss=0.14816, acc=0.93750
# [60/100] training 24.3% loss=0.20971, acc=0.92188
# [60/100] training 24.6% loss=0.24182, acc=0.92188
# [60/100] training 24.7% loss=0.24543, acc=0.93750
# [60/100] training 24.9% loss=0.17058, acc=0.93750
# [60/100] training 25.1% loss=0.23319, acc=0.90625
# [60/100] training 25.2% loss=0.15821, acc=0.93750
# [60/100] training 25.4% loss=0.14286, acc=0.96875
# [60/100] training 25.6% loss=0.14737, acc=0.96875
# [60/100] training 25.8% loss=0.20904, acc=0.90625
# [60/100] training 25.9% loss=0.17478, acc=0.90625
# [60/100] training 26.1% loss=0.09840, acc=0.96875
# [60/100] training 26.3% loss=0.09016, acc=0.96875
# [60/100] training 26.4% loss=0.11253, acc=0.96875
# [60/100] training 26.6% loss=0.05958, acc=0.96875
# [60/100] training 26.8% loss=0.07383, acc=0.96875
# [60/100] training 27.0% loss=0.13245, acc=0.93750
# [60/100] training 27.1% loss=0.09824, acc=0.95312
# [60/100] training 27.3% loss=0.10831, acc=0.95312
# [60/100] training 27.4% loss=0.09094, acc=0.98438
# [60/100] training 27.6% loss=0.32299, acc=0.93750
# [60/100] training 27.9% loss=0.16327, acc=0.90625
# [60/100] training 28.0% loss=0.57350, acc=0.87500
# [60/100] training 28.2% loss=0.12663, acc=0.93750
# [60/100] training 28.3% loss=0.09685, acc=0.95312
# [60/100] training 28.5% loss=0.13972, acc=0.95312
# [60/100] training 28.6% loss=0.21181, acc=0.92188
# [60/100] training 28.8% loss=0.15209, acc=0.95312
# [60/100] training 29.1% loss=0.26378, acc=0.89062
# [60/100] training 29.2% loss=0.10362, acc=0.98438
# [60/100] training 29.4% loss=0.23185, acc=0.89062
# [60/100] training 29.5% loss=0.11932, acc=0.93750
# [60/100] training 29.7% loss=0.26873, acc=0.92188
# [60/100] training 29.8% loss=0.22786, acc=0.90625
# [60/100] training 30.0% loss=0.25172, acc=0.89062
# [60/100] training 30.2% loss=0.09796, acc=0.98438
# [60/100] training 30.4% loss=0.06361, acc=0.98438
# [60/100] training 30.6% loss=0.13151, acc=0.92188
# [60/100] training 30.7% loss=0.20565, acc=0.93750
# [60/100] training 30.9% loss=0.36115, acc=0.90625
# [60/100] training 31.0% loss=0.10511, acc=0.96875
# [60/100] training 31.3% loss=0.11988, acc=0.93750
# [60/100] training 31.4% loss=0.24618, acc=0.90625
# [60/100] training 31.6% loss=0.30814, acc=0.92188
# [60/100] training 31.8% loss=0.17992, acc=0.90625
# [60/100] training 31.9% loss=0.16773, acc=0.90625
# [60/100] training 32.1% loss=0.18690, acc=0.92188
# [60/100] training 32.2% loss=0.25105, acc=0.90625
# [60/100] training 32.5% loss=0.13200, acc=0.95312
# [60/100] training 32.6% loss=0.20233, acc=0.89062
# [60/100] training 32.8% loss=0.12672, acc=0.98438
# [60/100] training 32.9% loss=0.21876, acc=0.92188
# [60/100] training 33.1% loss=0.11010, acc=0.96875
# [60/100] training 33.3% loss=0.20630, acc=0.90625
# [60/100] training 33.4% loss=0.13694, acc=0.90625
# [60/100] training 33.7% loss=0.15131, acc=0.92188
# [60/100] training 33.8% loss=0.17383, acc=0.95312
# [60/100] training 34.0% loss=0.17057, acc=0.95312
# [60/100] training 34.1% loss=0.14731, acc=0.92188
# [60/100] training 34.3% loss=0.18673, acc=0.92188
# [60/100] training 34.5% loss=0.24980, acc=0.89062
# [60/100] training 34.7% loss=0.14367, acc=0.95312
# [60/100] training 34.9% loss=0.11799, acc=0.93750
# [60/100] training 35.0% loss=0.10309, acc=0.96875
# [60/100] training 35.2% loss=0.23656, acc=0.90625
# [60/100] training 35.3% loss=0.14335, acc=0.95312
# [60/100] training 35.5% loss=0.25630, acc=0.87500
# [60/100] training 35.6% loss=0.23146, acc=0.92188
# [60/100] training 35.9% loss=0.16124, acc=0.92188
# [60/100] training 36.1% loss=0.27078, acc=0.92188
# [60/100] training 36.2% loss=0.23377, acc=0.90625
# [60/100] training 36.4% loss=0.30943, acc=0.87500
# [60/100] training 36.5% loss=0.22590, acc=0.90625
# [60/100] training 36.7% loss=0.16486, acc=0.93750
# [60/100] training 36.8% loss=0.07394, acc=0.96875
# [60/100] training 37.1% loss=0.24584, acc=0.92188
# [60/100] training 37.3% loss=0.18578, acc=0.92188
# [60/100] training 37.4% loss=0.09146, acc=0.95312
# [60/100] training 37.6% loss=0.13519, acc=0.93750
# [60/100] training 37.7% loss=0.23571, acc=0.90625
# [60/100] training 37.9% loss=0.06449, acc=1.00000
# [60/100] training 38.1% loss=0.31539, acc=0.93750
# [60/100] training 38.3% loss=0.13947, acc=0.93750
# [60/100] training 38.4% loss=0.04431, acc=0.98438
# [60/100] training 38.6% loss=0.10345, acc=0.93750
# [60/100] training 38.8% loss=0.21387, acc=0.92188
# [60/100] training 38.9% loss=0.10974, acc=0.96875
# [60/100] training 39.1% loss=0.18520, acc=0.93750
# [60/100] training 39.3% loss=0.16917, acc=0.93750
# [60/100] training 39.5% loss=0.13071, acc=0.95312
# [60/100] training 39.6% loss=0.15503, acc=0.92188
# [60/100] training 39.8% loss=0.15540, acc=0.92188
# [60/100] training 40.0% loss=0.18599, acc=0.92188
# [60/100] training 40.1% loss=0.23902, acc=0.85938
# [60/100] training 40.4% loss=0.16129, acc=0.92188
# [60/100] training 40.5% loss=0.25231, acc=0.89062
# [60/100] training 40.7% loss=0.16421, acc=0.96875
# [60/100] training 40.8% loss=0.12810, acc=0.95312
# [60/100] training 41.0% loss=0.09400, acc=0.95312
# [60/100] training 41.1% loss=0.16594, acc=0.93750
# [60/100] training 41.3% loss=0.18608, acc=0.92188
# [60/100] training 41.6% loss=0.19675, acc=0.92188
# [60/100] training 41.7% loss=0.14583, acc=0.93750
# [60/100] training 41.9% loss=0.11363, acc=0.96875
# [60/100] training 42.0% loss=0.24874, acc=0.92188
# [60/100] training 42.2% loss=0.17190, acc=0.93750
# [60/100] training 42.3% loss=0.18589, acc=0.90625
# [60/100] training 42.5% loss=0.18512, acc=0.90625
# [60/100] training 42.8% loss=0.07801, acc=0.96875
# [60/100] training 42.9% loss=0.15879, acc=0.92188
# [60/100] training 43.1% loss=0.19747, acc=0.95312
# [60/100] training 43.2% loss=0.15836, acc=0.93750
# [60/100] training 43.4% loss=0.20584, acc=0.92188
# [60/100] training 43.5% loss=0.14838, acc=0.93750
# [60/100] training 43.8% loss=0.19600, acc=0.93750
# [60/100] training 43.9% loss=0.15315, acc=0.92188
# [60/100] training 44.1% loss=0.26066, acc=0.95312
# [60/100] training 44.3% loss=0.13597, acc=0.98438
# [60/100] training 44.4% loss=0.24214, acc=0.90625
# [60/100] training 44.6% loss=0.29309, acc=0.89062
# [60/100] training 44.7% loss=0.23083, acc=0.90625
# [60/100] training 45.0% loss=0.10501, acc=0.95312
# [60/100] training 45.1% loss=0.26426, acc=0.92188
# [60/100] training 45.3% loss=0.10539, acc=0.96875
# [60/100] training 45.5% loss=0.06003, acc=1.00000
# [60/100] training 45.6% loss=0.18328, acc=0.95312
# [60/100] training 45.8% loss=0.07114, acc=0.98438
# [60/100] training 45.9% loss=0.16468, acc=0.93750
# [60/100] training 46.2% loss=0.13591, acc=0.95312
# [60/100] training 46.3% loss=0.14702, acc=0.93750
# [60/100] training 46.5% loss=0.33606, acc=0.85938
# [60/100] training 46.6% loss=0.21893, acc=0.95312
# [60/100] training 46.8% loss=0.11780, acc=0.96875
# [60/100] training 47.0% loss=0.10588, acc=0.96875
# [60/100] training 47.2% loss=0.21942, acc=0.92188
# [60/100] training 47.4% loss=0.20820, acc=0.92188
# [60/100] training 47.5% loss=0.23312, acc=0.90625
# [60/100] training 47.7% loss=0.15274, acc=0.95312
# [60/100] training 47.8% loss=0.34808, acc=0.85938
# [60/100] training 48.0% loss=0.13282, acc=0.96875
# [60/100] training 48.3% loss=0.12629, acc=0.96875
# [60/100] training 48.4% loss=0.07753, acc=0.96875
# [60/100] training 48.6% loss=0.12993, acc=0.92188
# [60/100] training 48.7% loss=0.21695, acc=0.92188
# [60/100] training 48.9% loss=0.15250, acc=0.96875
# [60/100] training 49.0% loss=0.11106, acc=0.96875
# [60/100] training 49.2% loss=0.10387, acc=0.96875
# [60/100] training 49.3% loss=0.14464, acc=0.92188
# [60/100] training 49.6% loss=0.14830, acc=0.95312
# [60/100] training 49.8% loss=0.20094, acc=0.92188
# [60/100] training 49.9% loss=0.18357, acc=0.95312
# [60/100] training 50.1% loss=0.13241, acc=0.95312
# [60/100] training 50.2% loss=0.16253, acc=0.92188
# [60/100] training 50.4% loss=0.30803, acc=0.85938
# [60/100] training 50.6% loss=0.22182, acc=0.92188
# [60/100] training 50.8% loss=0.18305, acc=0.92188
# [60/100] training 51.0% loss=0.30683, acc=0.87500
# [60/100] training 51.1% loss=0.18849, acc=0.92188
# [60/100] training 51.3% loss=0.26565, acc=0.89062
# [60/100] training 51.4% loss=0.17197, acc=0.93750
# [60/100] training 51.7% loss=0.14719, acc=0.93750
# [60/100] training 51.8% loss=0.21453, acc=0.93750
# [60/100] training 52.0% loss=0.13875, acc=0.95312
# [60/100] training 52.1% loss=0.05467, acc=1.00000
# [60/100] training 52.3% loss=0.18161, acc=0.95312
# [60/100] training 52.5% loss=0.07735, acc=0.96875
# [60/100] training 52.6% loss=0.14416, acc=0.95312
# [60/100] training 52.9% loss=0.27742, acc=0.89062
# [60/100] training 53.0% loss=0.16401, acc=0.95312
# [60/100] training 53.2% loss=0.25469, acc=0.93750
# [60/100] training 53.3% loss=0.11766, acc=0.93750
# [60/100] training 53.5% loss=0.22796, acc=0.90625
# [60/100] training 53.7% loss=0.11173, acc=0.96875
# [60/100] training 53.8% loss=0.23448, acc=0.90625
# [60/100] training 54.1% loss=0.20973, acc=0.89062
# [60/100] training 54.2% loss=0.06063, acc=0.98438
# [60/100] training 54.4% loss=0.16520, acc=0.90625
# [60/100] training 54.5% loss=0.20568, acc=0.92188
# [60/100] training 54.7% loss=0.26440, acc=0.89062
# [60/100] training 54.8% loss=0.12542, acc=0.96875
# [60/100] training 55.1% loss=0.11344, acc=0.95312
# [60/100] training 55.3% loss=0.15093, acc=0.92188
# [60/100] training 55.4% loss=0.09109, acc=0.96875
# [60/100] training 55.6% loss=0.21595, acc=0.93750
# [60/100] training 55.7% loss=0.09529, acc=0.96875
# [60/100] training 55.9% loss=0.12156, acc=0.95312
# [60/100] training 56.0% loss=0.17486, acc=0.92188
# [60/100] training 56.3% loss=0.24603, acc=0.89062
# [60/100] training 56.5% loss=0.17698, acc=0.95312
# [60/100] training 56.6% loss=0.10171, acc=0.93750
# [60/100] training 56.8% loss=0.17715, acc=0.93750
# [60/100] training 56.9% loss=0.06880, acc=1.00000
# [60/100] training 57.1% loss=0.08723, acc=0.96875
# [60/100] training 57.2% loss=0.05910, acc=0.96875
# [60/100] training 57.5% loss=0.09230, acc=0.96875
# [60/100] training 57.6% loss=0.15434, acc=0.90625
# [60/100] training 57.8% loss=0.08321, acc=0.98438
# [60/100] training 58.0% loss=0.13496, acc=0.95312
# [60/100] training 58.1% loss=0.10018, acc=0.95312
# [60/100] training 58.3% loss=0.18226, acc=0.93750
# [60/100] training 58.4% loss=0.21547, acc=0.92188
# [60/100] training 58.7% loss=0.21173, acc=0.90625
# [60/100] training 58.8% loss=0.27831, acc=0.89062
# [60/100] training 59.0% loss=0.10836, acc=0.95312
# [60/100] training 59.2% loss=0.25720, acc=0.90625
# [60/100] training 59.3% loss=0.23802, acc=0.90625
# [60/100] training 59.5% loss=0.17917, acc=0.95312
# [60/100] training 59.7% loss=0.09517, acc=0.95312
# [60/100] training 59.9% loss=0.15654, acc=0.98438
# [60/100] training 60.0% loss=0.17162, acc=0.90625
# [60/100] training 60.2% loss=0.13956, acc=0.93750
# [60/100] training 60.3% loss=0.11223, acc=0.93750
# [60/100] training 60.5% loss=0.25598, acc=0.87500
# [60/100] training 60.8% loss=0.24500, acc=0.89062
# [60/100] training 60.9% loss=0.20584, acc=0.93750
# [60/100] training 61.1% loss=0.23105, acc=0.92188
# [60/100] training 61.2% loss=0.22584, acc=0.92188
# [60/100] training 61.4% loss=0.23528, acc=0.93750
# [60/100] training 61.5% loss=0.27983, acc=0.90625
# [60/100] training 61.7% loss=0.16643, acc=0.92188
# [60/100] training 62.0% loss=0.18498, acc=0.93750
# [60/100] training 62.1% loss=0.20386, acc=0.90625
# [60/100] training 62.3% loss=0.11823, acc=0.95312
# [60/100] training 62.4% loss=0.24119, acc=0.87500
# [60/100] training 62.6% loss=0.17235, acc=0.90625
# [60/100] training 62.7% loss=0.10734, acc=0.96875
# [60/100] training 62.9% loss=0.13950, acc=0.98438
# [60/100] training 63.1% loss=0.23579, acc=0.89062
# [60/100] training 63.3% loss=0.12600, acc=0.95312
# [60/100] training 63.5% loss=0.16135, acc=0.95312
# [60/100] training 63.6% loss=0.21655, acc=0.90625
# [60/100] training 63.8% loss=0.26048, acc=0.89062
# [60/100] training 63.9% loss=0.21156, acc=0.92188
# [60/100] training 64.2% loss=0.15987, acc=0.92188
# [60/100] training 64.3% loss=0.13056, acc=0.95312
# [60/100] training 64.5% loss=0.19617, acc=0.93750
# [60/100] training 64.7% loss=0.26424, acc=0.87500
# [60/100] training 64.8% loss=0.20785, acc=0.93750
# [60/100] training 65.0% loss=0.21701, acc=0.90625
# [60/100] training 65.1% loss=0.19252, acc=0.93750
# [60/100] training 65.4% loss=0.14637, acc=0.93750
# [60/100] training 65.5% loss=0.16887, acc=0.92188
# [60/100] training 65.7% loss=0.17046, acc=0.95312
# [60/100] training 65.8% loss=0.13448, acc=0.95312
# [60/100] training 66.0% loss=0.12021, acc=0.96875
# [60/100] training 66.2% loss=0.03888, acc=1.00000
# [60/100] training 66.3% loss=0.22886, acc=0.89062
# [60/100] training 66.6% loss=0.23302, acc=0.89062
# [60/100] training 66.7% loss=0.11211, acc=0.92188
# [60/100] training 66.9% loss=0.13591, acc=0.95312
# [60/100] training 67.0% loss=0.12595, acc=0.92188
# [60/100] training 67.2% loss=0.10706, acc=0.92188
# [60/100] training 67.4% loss=0.09290, acc=0.96875
# [60/100] training 67.6% loss=0.07922, acc=0.95312
# [60/100] training 67.8% loss=0.12117, acc=0.96875
# [60/100] training 67.9% loss=0.09147, acc=0.95312
# [60/100] training 68.1% loss=0.10969, acc=0.96875
# [60/100] training 68.2% loss=0.11821, acc=0.95312
# [60/100] training 68.4% loss=0.08786, acc=0.95312
# [60/100] training 68.5% loss=0.11158, acc=0.95312
# [60/100] training 68.8% loss=0.12533, acc=0.92188
# [60/100] training 69.0% loss=0.23564, acc=0.92188
# [60/100] training 69.1% loss=0.07159, acc=0.96875
# [60/100] training 69.3% loss=0.16149, acc=0.93750
# [60/100] training 69.4% loss=0.11666, acc=0.96875
# [60/100] training 69.6% loss=0.11525, acc=0.95312
# [60/100] training 69.7% loss=0.23344, acc=0.92188
# [60/100] training 70.0% loss=0.11386, acc=0.96875
# [60/100] training 70.2% loss=0.19637, acc=0.89062
# [60/100] training 70.3% loss=0.12507, acc=0.93750
# [60/100] training 70.5% loss=0.15393, acc=0.98438
# [60/100] training 70.6% loss=0.13787, acc=0.93750
# [60/100] training 70.8% loss=0.20724, acc=0.90625
# [60/100] training 71.0% loss=0.28419, acc=0.93750
# [60/100] training 71.2% loss=0.11599, acc=0.95312
# [60/100] training 71.3% loss=0.09514, acc=0.93750
# [60/100] training 71.5% loss=0.31369, acc=0.92188
# [60/100] training 71.7% loss=0.19827, acc=0.90625
# [60/100] training 71.8% loss=0.26983, acc=0.93750
# [60/100] training 72.0% loss=0.08626, acc=0.96875
# [60/100] training 72.2% loss=0.39751, acc=0.87500
# [60/100] training 72.4% loss=0.19058, acc=0.92188
# [60/100] training 72.5% loss=0.13600, acc=0.93750
# [60/100] training 72.7% loss=0.30416, acc=0.92188
# [60/100] training 72.9% loss=0.09857, acc=0.98438
# [60/100] training 73.0% loss=0.10015, acc=0.96875
# [60/100] training 73.3% loss=0.27296, acc=0.89062
# [60/100] training 73.4% loss=0.09133, acc=0.95312
# [60/100] training 73.6% loss=0.11400, acc=0.96875
# [60/100] training 73.7% loss=0.20271, acc=0.95312
# [60/100] training 73.9% loss=0.09157, acc=0.95312
# [60/100] training 74.0% loss=0.20399, acc=0.92188
# [60/100] training 74.2% loss=0.10098, acc=0.96875
# [60/100] training 74.5% loss=0.06488, acc=1.00000
# [60/100] training 74.6% loss=0.31747, acc=0.87500
# [60/100] training 74.8% loss=0.20775, acc=0.89062
# [60/100] training 74.9% loss=0.28754, acc=0.90625
# [60/100] training 75.1% loss=0.13167, acc=0.95312
# [60/100] training 75.2% loss=0.11864, acc=0.95312
# [60/100] training 75.4% loss=0.13729, acc=0.93750
# [60/100] training 75.7% loss=0.18513, acc=0.90625
# [60/100] training 75.8% loss=0.27338, acc=0.87500
# [60/100] training 76.0% loss=0.14530, acc=0.92188
# [60/100] training 76.1% loss=0.23412, acc=0.87500
# [60/100] training 76.3% loss=0.15846, acc=0.93750
# [60/100] training 76.4% loss=0.29561, acc=0.85938
# [60/100] training 76.7% loss=0.15823, acc=0.93750
# [60/100] training 76.8% loss=0.17834, acc=0.93750
# [60/100] training 77.0% loss=0.11392, acc=0.93750
# [60/100] training 77.2% loss=0.10260, acc=0.95312
# [60/100] training 77.3% loss=0.06778, acc=0.98438
# [60/100] training 77.5% loss=0.16198, acc=0.92188
# [60/100] training 77.6% loss=0.16215, acc=0.95312
# [60/100] training 77.9% loss=0.12940, acc=0.95312
# [60/100] training 78.0% loss=0.18826, acc=0.90625
# [60/100] training 78.2% loss=0.16836, acc=0.95312
# [60/100] training 78.4% loss=0.04099, acc=0.98438
# [60/100] training 78.5% loss=0.17842, acc=0.93750
# [60/100] training 78.7% loss=0.14167, acc=0.95312
# [60/100] training 78.8% loss=0.10661, acc=0.96875
# [60/100] training 79.1% loss=0.12146, acc=0.96875
# [60/100] training 79.2% loss=0.13169, acc=0.95312
# [60/100] training 79.4% loss=0.13861, acc=0.95312
# [60/100] training 79.5% loss=0.13090, acc=0.93750
# [60/100] training 79.7% loss=0.08617, acc=0.95312
# [60/100] training 79.9% loss=0.09352, acc=0.96875
# [60/100] training 80.1% loss=0.07479, acc=0.98438
# [60/100] training 80.3% loss=0.16760, acc=0.92188
# [60/100] training 80.4% loss=0.18616, acc=0.95312
# [60/100] training 80.6% loss=0.23746, acc=0.92188
# [60/100] training 80.7% loss=0.08654, acc=0.95312
# [60/100] training 80.9% loss=0.17285, acc=0.92188
# [60/100] training 81.2% loss=0.23852, acc=0.92188
# [60/100] training 81.3% loss=0.23743, acc=0.95312
# [60/100] training 81.5% loss=0.19963, acc=0.90625
# [60/100] training 81.6% loss=0.32582, acc=0.89062
# [60/100] training 81.8% loss=0.16355, acc=0.92188
# [60/100] training 81.9% loss=0.29441, acc=0.92188
# [60/100] training 82.1% loss=0.10954, acc=0.96875
# [60/100] training 82.2% loss=0.15517, acc=0.95312
# [60/100] training 82.5% loss=0.14546, acc=0.95312
# [60/100] training 82.7% loss=0.16311, acc=0.92188
# [60/100] training 82.8% loss=0.13263, acc=0.95312
# [60/100] training 83.0% loss=0.12927, acc=0.95312
# [60/100] training 83.1% loss=0.15576, acc=0.93750
# [60/100] training 83.3% loss=0.09134, acc=0.95312
# [60/100] training 83.5% loss=0.08702, acc=0.96875
# [60/100] training 83.7% loss=0.31659, acc=0.89062
# [60/100] training 83.9% loss=0.22251, acc=0.89062
# [60/100] training 84.0% loss=0.05450, acc=0.98438
# [60/100] training 84.2% loss=0.09963, acc=0.98438
# [60/100] training 84.3% loss=0.09852, acc=0.95312
# [60/100] training 84.5% loss=0.24935, acc=0.95312
# [60/100] training 84.7% loss=0.13764, acc=0.93750
# [60/100] training 84.9% loss=0.12662, acc=0.95312
# [60/100] training 85.0% loss=0.07734, acc=0.96875
# [60/100] training 85.2% loss=0.17137, acc=0.92188
# [60/100] training 85.4% loss=0.07129, acc=0.96875
# [60/100] training 85.5% loss=0.22086, acc=0.90625
# [60/100] training 85.8% loss=0.13268, acc=0.93750
# [60/100] training 85.9% loss=0.25762, acc=0.92188
# [60/100] training 86.1% loss=0.13624, acc=0.93750
# [60/100] training 86.2% loss=0.11950, acc=0.95312
# [60/100] training 86.4% loss=0.20429, acc=0.90625
# [60/100] training 86.6% loss=0.20509, acc=0.92188
# [60/100] training 86.7% loss=0.15391, acc=0.93750
# [60/100] training 87.0% loss=0.21033, acc=0.85938
# [60/100] training 87.1% loss=0.14862, acc=0.93750
# [60/100] training 87.3% loss=0.12166, acc=0.95312
# [60/100] training 87.4% loss=0.18062, acc=0.93750
# [60/100] training 87.6% loss=0.18917, acc=0.93750
# [60/100] training 87.7% loss=0.24801, acc=0.90625
# [60/100] training 87.9% loss=0.24276, acc=0.92188
# [60/100] training 88.2% loss=0.11905, acc=0.93750
# [60/100] training 88.3% loss=0.28357, acc=0.92188
# [60/100] training 88.5% loss=0.18364, acc=0.92188
# [60/100] training 88.6% loss=0.16894, acc=0.95312
# [60/100] training 88.8% loss=0.10339, acc=0.96875
# [60/100] training 88.9% loss=0.23689, acc=0.90625
# [60/100] training 89.2% loss=0.13103, acc=0.96875
# [60/100] training 89.4% loss=0.10894, acc=0.93750
# [60/100] training 89.5% loss=0.24790, acc=0.90625
# [60/100] training 89.7% loss=0.26929, acc=0.90625
# [60/100] training 89.8% loss=0.19361, acc=0.92188
# [60/100] training 90.0% loss=0.10953, acc=0.98438
# [60/100] training 90.1% loss=0.20927, acc=0.90625
# [60/100] training 90.4% loss=0.26334, acc=0.90625
# [60/100] training 90.5% loss=0.16136, acc=0.93750
# [60/100] training 90.7% loss=0.22117, acc=0.93750
# [60/100] training 90.9% loss=0.14483, acc=0.95312
# [60/100] training 91.0% loss=0.18264, acc=0.93750
# [60/100] training 91.2% loss=0.25213, acc=0.87500
# [60/100] training 91.3% loss=0.24653, acc=0.90625
# [60/100] training 91.6% loss=0.32128, acc=0.90625
# [60/100] training 91.7% loss=0.11954, acc=0.96875
# [60/100] training 91.9% loss=0.30165, acc=0.87500
# [60/100] training 92.1% loss=0.16116, acc=0.92188
# [60/100] training 92.2% loss=0.11126, acc=0.95312
# [60/100] training 92.4% loss=0.13360, acc=0.93750
# [60/100] training 92.6% loss=0.24784, acc=0.87500
# [60/100] training 92.8% loss=0.15706, acc=0.93750
# [60/100] training 92.9% loss=0.11562, acc=0.92188
# [60/100] training 93.1% loss=0.40178, acc=0.89062
# [60/100] training 93.2% loss=0.21212, acc=0.90625
# [60/100] training 93.4% loss=0.25069, acc=0.85938
# [60/100] training 93.7% loss=0.09691, acc=0.96875
# [60/100] training 93.8% loss=0.20353, acc=0.92188
# [60/100] training 94.0% loss=0.13628, acc=0.95312
# [60/100] training 94.1% loss=0.15831, acc=0.92188
# [60/100] training 94.3% loss=0.07964, acc=0.98438
# [60/100] training 94.4% loss=0.20846, acc=0.93750
# [60/100] training 94.6% loss=0.10154, acc=0.96875
# [60/100] training 94.9% loss=0.14016, acc=0.95312
# [60/100] training 95.0% loss=0.23923, acc=0.89062
# [60/100] training 95.2% loss=0.43737, acc=0.85938
# [60/100] training 95.3% loss=0.21337, acc=0.93750
# [60/100] training 95.5% loss=0.13502, acc=0.95312
# [60/100] training 95.6% loss=0.20457, acc=0.92188
# [60/100] training 95.8% loss=0.16194, acc=0.92188
# [60/100] training 96.0% loss=0.27025, acc=0.93750
# [60/100] training 96.2% loss=0.12247, acc=0.95312
# [60/100] training 96.4% loss=0.13189, acc=0.98438
# [60/100] training 96.5% loss=0.17484, acc=0.93750
# [60/100] training 96.7% loss=0.13423, acc=0.96875
# [60/100] training 96.8% loss=0.17444, acc=0.95312
# [60/100] training 97.1% loss=0.08681, acc=0.98438
# [60/100] training 97.2% loss=0.18034, acc=0.93750
# [60/100] training 97.4% loss=0.16066, acc=0.92188
# [60/100] training 97.6% loss=0.19191, acc=0.89062
# [60/100] training 97.7% loss=0.10234, acc=0.95312
# [60/100] training 97.9% loss=0.14705, acc=0.92188
# [60/100] training 98.0% loss=0.16169, acc=0.93750
# [60/100] training 98.3% loss=0.27113, acc=0.89062
# [60/100] training 98.4% loss=0.16156, acc=0.92188
# [60/100] training 98.6% loss=0.35385, acc=0.82812
# [60/100] training 98.7% loss=0.19157, acc=0.95312
# [60/100] training 98.9% loss=0.26803, acc=0.85938
# [60/100] training 99.1% loss=0.12348, acc=0.93750
# [60/100] training 99.2% loss=0.09922, acc=0.96875
# [60/100] training 99.5% loss=0.19490, acc=0.90625
# [60/100] training 99.6% loss=0.15148, acc=0.95312
# [60/100] training 99.8% loss=0.06571, acc=0.96875
# [60/100] training 99.9% loss=0.02979, acc=1.00000
# [60/100] testing 0.9% loss=0.10068, acc=0.96875
# [60/100] testing 1.8% loss=0.35931, acc=0.85938
# [60/100] testing 2.2% loss=0.22029, acc=0.92188
# [60/100] testing 3.1% loss=0.35675, acc=0.84375
# [60/100] testing 3.5% loss=0.13006, acc=0.96875
# [60/100] testing 4.4% loss=0.15612, acc=0.93750
# [60/100] testing 4.8% loss=0.39278, acc=0.82812
# [60/100] testing 5.7% loss=0.36069, acc=0.87500
# [60/100] testing 6.6% loss=0.16670, acc=0.93750
# [60/100] testing 7.0% loss=0.16704, acc=0.93750
# [60/100] testing 7.9% loss=0.44946, acc=0.89062
# [60/100] testing 8.3% loss=0.25828, acc=0.92188
# [60/100] testing 9.2% loss=0.29001, acc=0.93750
# [60/100] testing 9.7% loss=0.07473, acc=0.96875
# [60/100] testing 10.5% loss=0.23394, acc=0.92188
# [60/100] testing 11.0% loss=0.20132, acc=0.92188
# [60/100] testing 11.8% loss=0.16148, acc=0.93750
# [60/100] testing 12.7% loss=0.45176, acc=0.92188
# [60/100] testing 13.2% loss=0.22914, acc=0.90625
# [60/100] testing 14.0% loss=0.41326, acc=0.90625
# [60/100] testing 14.5% loss=0.21035, acc=0.90625
# [60/100] testing 15.4% loss=0.32819, acc=0.90625
# [60/100] testing 15.8% loss=0.17897, acc=0.92188
# [60/100] testing 16.7% loss=0.23276, acc=0.92188
# [60/100] testing 17.5% loss=0.27023, acc=0.90625
# [60/100] testing 18.0% loss=0.18376, acc=0.92188
# [60/100] testing 18.9% loss=0.07512, acc=0.96875
# [60/100] testing 19.3% loss=0.39079, acc=0.90625
# [60/100] testing 20.2% loss=0.39846, acc=0.90625
# [60/100] testing 20.6% loss=0.24760, acc=0.89062
# [60/100] testing 21.5% loss=0.26219, acc=0.93750
# [60/100] testing 21.9% loss=0.61340, acc=0.81250
# [60/100] testing 22.8% loss=0.36896, acc=0.89062
# [60/100] testing 23.7% loss=0.40185, acc=0.84375
# [60/100] testing 24.1% loss=0.15638, acc=0.95312
# [60/100] testing 25.0% loss=0.43053, acc=0.90625
# [60/100] testing 25.4% loss=0.10764, acc=0.96875
# [60/100] testing 26.3% loss=0.25234, acc=0.90625
# [60/100] testing 26.8% loss=0.36410, acc=0.85938
# [60/100] testing 27.6% loss=0.29115, acc=0.93750
# [60/100] testing 28.5% loss=0.14004, acc=0.96875
# [60/100] testing 29.0% loss=0.30426, acc=0.89062
# [60/100] testing 29.8% loss=0.46392, acc=0.90625
# [60/100] testing 30.3% loss=0.26342, acc=0.93750
# [60/100] testing 31.1% loss=0.43783, acc=0.85938
# [60/100] testing 31.6% loss=0.31256, acc=0.87500
# [60/100] testing 32.5% loss=0.28430, acc=0.89062
# [60/100] testing 32.9% loss=0.46592, acc=0.89062
# [60/100] testing 33.8% loss=0.29747, acc=0.93750
# [60/100] testing 34.7% loss=0.45664, acc=0.87500
# [60/100] testing 35.1% loss=0.23577, acc=0.92188
# [60/100] testing 36.0% loss=0.33351, acc=0.90625
# [60/100] testing 36.4% loss=0.29098, acc=0.95312
# [60/100] testing 37.3% loss=0.30833, acc=0.92188
# [60/100] testing 37.7% loss=0.45813, acc=0.82812
# [60/100] testing 38.6% loss=0.19394, acc=0.93750
# [60/100] testing 39.5% loss=0.35309, acc=0.92188
# [60/100] testing 39.9% loss=0.30923, acc=0.93750
# [60/100] testing 40.8% loss=0.37983, acc=0.90625
# [60/100] testing 41.2% loss=0.23628, acc=0.96875
# [60/100] testing 42.1% loss=0.34933, acc=0.92188
# [60/100] testing 42.5% loss=0.11149, acc=0.96875
# [60/100] testing 43.4% loss=0.32622, acc=0.89062
# [60/100] testing 43.9% loss=0.05057, acc=1.00000
# [60/100] testing 44.7% loss=0.37327, acc=0.90625
# [60/100] testing 45.6% loss=0.27500, acc=0.92188
# [60/100] testing 46.1% loss=0.26719, acc=0.90625
# [60/100] testing 46.9% loss=0.28934, acc=0.89062
# [60/100] testing 47.4% loss=0.12037, acc=0.95312
# [60/100] testing 48.3% loss=0.42547, acc=0.82812
# [60/100] testing 48.7% loss=0.35729, acc=0.85938
# [60/100] testing 49.6% loss=0.46108, acc=0.82812
# [60/100] testing 50.4% loss=0.18378, acc=0.92188
# [60/100] testing 50.9% loss=0.31055, acc=0.90625
# [60/100] testing 51.8% loss=0.38547, acc=0.89062
# [60/100] testing 52.2% loss=0.23007, acc=0.89062
# [60/100] testing 53.1% loss=0.22114, acc=0.89062
# [60/100] testing 53.5% loss=0.20924, acc=0.93750
# [60/100] testing 54.4% loss=0.30529, acc=0.89062
# [60/100] testing 54.8% loss=0.44503, acc=0.89062
# [60/100] testing 55.7% loss=0.09597, acc=0.95312
# [60/100] testing 56.6% loss=0.37736, acc=0.87500
# [60/100] testing 57.0% loss=0.43407, acc=0.89062
# [60/100] testing 57.9% loss=0.22226, acc=0.92188
# [60/100] testing 58.3% loss=0.53450, acc=0.85938
# [60/100] testing 59.2% loss=0.22436, acc=0.90625
# [60/100] testing 59.7% loss=0.13863, acc=0.92188
# [60/100] testing 60.5% loss=0.35145, acc=0.85938
# [60/100] testing 61.4% loss=0.13769, acc=0.93750
# [60/100] testing 61.9% loss=0.19856, acc=0.90625
# [60/100] testing 62.7% loss=0.18596, acc=0.93750
# [60/100] testing 63.2% loss=0.34227, acc=0.89062
# [60/100] testing 64.0% loss=0.43861, acc=0.90625
# [60/100] testing 64.5% loss=0.14693, acc=0.89062
# [60/100] testing 65.4% loss=0.19220, acc=0.92188
# [60/100] testing 65.8% loss=0.31856, acc=0.87500
# [60/100] testing 66.7% loss=0.15232, acc=0.93750
# [60/100] testing 67.6% loss=0.51559, acc=0.87500
# [60/100] testing 68.0% loss=0.06538, acc=0.96875
# [60/100] testing 68.9% loss=0.36840, acc=0.90625
# [60/100] testing 69.3% loss=0.32797, acc=0.92188
# [60/100] testing 70.2% loss=0.49214, acc=0.82812
# [60/100] testing 70.6% loss=0.33035, acc=0.87500
# [60/100] testing 71.5% loss=0.41415, acc=0.92188
# [60/100] testing 72.4% loss=0.19508, acc=0.92188
# [60/100] testing 72.8% loss=0.17745, acc=0.90625
# [60/100] testing 73.7% loss=0.14272, acc=0.92188
# [60/100] testing 74.1% loss=0.40977, acc=0.85938
# [60/100] testing 75.0% loss=0.16801, acc=0.90625
# [60/100] testing 75.4% loss=0.34650, acc=0.85938
# [60/100] testing 76.3% loss=0.11995, acc=0.95312
# [60/100] testing 76.8% loss=0.36305, acc=0.84375
# [60/100] testing 77.6% loss=0.22783, acc=0.92188
# [60/100] testing 78.5% loss=0.31653, acc=0.90625
# [60/100] testing 79.0% loss=0.14557, acc=0.92188
# [60/100] testing 79.8% loss=0.15382, acc=0.90625
# [60/100] testing 80.3% loss=0.28202, acc=0.90625
# [60/100] testing 81.2% loss=0.45412, acc=0.87500
# [60/100] testing 81.6% loss=0.17251, acc=0.95312
# [60/100] testing 82.5% loss=0.17666, acc=0.92188
# [60/100] testing 83.3% loss=0.24107, acc=0.95312
# [60/100] testing 83.8% loss=0.22906, acc=0.93750
# [60/100] testing 84.7% loss=0.24183, acc=0.92188
# [60/100] testing 85.1% loss=0.26844, acc=0.92188
# [60/100] testing 86.0% loss=0.27608, acc=0.93750
# [60/100] testing 86.4% loss=0.43461, acc=0.85938
# [60/100] testing 87.3% loss=0.32657, acc=0.85938
# [60/100] testing 87.7% loss=0.42316, acc=0.89062
# [60/100] testing 88.6% loss=0.25462, acc=0.90625
# [60/100] testing 89.5% loss=0.57895, acc=0.82812
# [60/100] testing 89.9% loss=0.16227, acc=0.92188
# [60/100] testing 90.8% loss=0.38748, acc=0.92188
# [60/100] testing 91.2% loss=0.24204, acc=0.92188
# [60/100] testing 92.1% loss=0.35362, acc=0.90625
# [60/100] testing 92.6% loss=0.32991, acc=0.89062
# [60/100] testing 93.4% loss=0.22742, acc=0.90625
# [60/100] testing 94.3% loss=0.13683, acc=0.95312
# [60/100] testing 94.7% loss=0.30233, acc=0.90625
# [60/100] testing 95.6% loss=0.57780, acc=0.84375
# [60/100] testing 96.1% loss=0.18709, acc=0.92188
# [60/100] testing 96.9% loss=0.29131, acc=0.90625
# [60/100] testing 97.4% loss=0.14778, acc=0.98438
# [60/100] testing 98.3% loss=0.13324, acc=0.92188
# [60/100] testing 98.7% loss=0.25297, acc=0.92188
# [60/100] testing 99.6% loss=0.33103, acc=0.90625
# [61/100] training 0.2% loss=0.34626, acc=0.89062
# [61/100] training 0.4% loss=0.34369, acc=0.87500
# [61/100] training 0.5% loss=0.09345, acc=0.98438
# [61/100] training 0.8% loss=0.10639, acc=0.93750
# [61/100] training 0.9% loss=0.11740, acc=0.95312
# [61/100] training 1.1% loss=0.32143, acc=0.93750
# [61/100] training 1.2% loss=0.17928, acc=0.95312
# [61/100] training 1.4% loss=0.08256, acc=0.96875
# [61/100] training 1.6% loss=0.09650, acc=0.95312
# [61/100] training 1.8% loss=0.10807, acc=0.95312
# [61/100] training 2.0% loss=0.18062, acc=0.93750
# [61/100] training 2.1% loss=0.16896, acc=0.92188
# [61/100] training 2.3% loss=0.18012, acc=0.95312
# [61/100] training 2.4% loss=0.19656, acc=0.87500
# [61/100] training 2.6% loss=0.07822, acc=0.98438
# [61/100] training 2.7% loss=0.14371, acc=0.93750
# [61/100] training 3.0% loss=0.12200, acc=0.93750
# [61/100] training 3.2% loss=0.09544, acc=0.95312
# [61/100] training 3.3% loss=0.28876, acc=0.87500
# [61/100] training 3.5% loss=0.12082, acc=0.93750
# [61/100] training 3.6% loss=0.21968, acc=0.89062
# [61/100] training 3.8% loss=0.22054, acc=0.90625
# [61/100] training 3.9% loss=0.16501, acc=0.90625
# [61/100] training 4.2% loss=0.06187, acc=0.98438
# [61/100] training 4.4% loss=0.14135, acc=0.93750
# [61/100] training 4.5% loss=0.17742, acc=0.92188
# [61/100] training 4.7% loss=0.31923, acc=0.90625
# [61/100] training 4.8% loss=0.11518, acc=0.93750
# [61/100] training 5.0% loss=0.05775, acc=1.00000
# [61/100] training 5.2% loss=0.16008, acc=0.92188
# [61/100] training 5.4% loss=0.08155, acc=0.98438
# [61/100] training 5.5% loss=0.19707, acc=0.90625
# [61/100] training 5.7% loss=0.12011, acc=0.93750
# [61/100] training 5.9% loss=0.14932, acc=0.93750
# [61/100] training 6.0% loss=0.21841, acc=0.90625
# [61/100] training 6.3% loss=0.17891, acc=0.87500
# [61/100] training 6.4% loss=0.14886, acc=0.90625
# [61/100] training 6.6% loss=0.13097, acc=0.93750
# [61/100] training 6.7% loss=0.22504, acc=0.93750
# [61/100] training 6.9% loss=0.16062, acc=0.95312
# [61/100] training 7.1% loss=0.06510, acc=0.98438
# [61/100] training 7.2% loss=0.23092, acc=0.92188
# [61/100] training 7.5% loss=0.17276, acc=0.95312
# [61/100] training 7.6% loss=0.29838, acc=0.93750
# [61/100] training 7.8% loss=0.11230, acc=0.96875
# [61/100] training 7.9% loss=0.16550, acc=0.93750
# [61/100] training 8.1% loss=0.10162, acc=0.96875
# [61/100] training 8.2% loss=0.18494, acc=0.90625
# [61/100] training 8.4% loss=0.24937, acc=0.92188
# [61/100] training 8.7% loss=0.16659, acc=0.92188
# [61/100] training 8.8% loss=0.13342, acc=0.95312
# [61/100] training 9.0% loss=0.25665, acc=0.89062
# [61/100] training 9.1% loss=0.17435, acc=0.93750
# [61/100] training 9.3% loss=0.23498, acc=0.92188
# [61/100] training 9.4% loss=0.14502, acc=0.95312
# [61/100] training 9.7% loss=0.16714, acc=0.95312
# [61/100] training 9.9% loss=0.20801, acc=0.89062
# [61/100] training 10.0% loss=0.29298, acc=0.87500
# [61/100] training 10.2% loss=0.12115, acc=0.93750
# [61/100] training 10.3% loss=0.12379, acc=0.96875
# [61/100] training 10.5% loss=0.24750, acc=0.89062
# [61/100] training 10.6% loss=0.13962, acc=0.92188
# [61/100] training 10.9% loss=0.15638, acc=0.95312
# [61/100] training 11.0% loss=0.26751, acc=0.92188
# [61/100] training 11.2% loss=0.15117, acc=0.92188
# [61/100] training 11.4% loss=0.18347, acc=0.89062
# [61/100] training 11.5% loss=0.31333, acc=0.87500
# [61/100] training 11.7% loss=0.10464, acc=0.98438
# [61/100] training 11.8% loss=0.11131, acc=0.95312
# [61/100] training 12.1% loss=0.14448, acc=0.93750
# [61/100] training 12.2% loss=0.04113, acc=1.00000
# [61/100] training 12.4% loss=0.08960, acc=0.95312
# [61/100] training 12.6% loss=0.40351, acc=0.90625
# [61/100] training 12.7% loss=0.14799, acc=0.95312
# [61/100] training 12.9% loss=0.22777, acc=0.92188
# [61/100] training 13.0% loss=0.19135, acc=0.96875
# [61/100] training 13.3% loss=0.20130, acc=0.92188
# [61/100] training 13.4% loss=0.23123, acc=0.89062
# [61/100] training 13.6% loss=0.19795, acc=0.92188
# [61/100] training 13.7% loss=0.20711, acc=0.92188
# [61/100] training 13.9% loss=0.20504, acc=0.92188
# [61/100] training 14.1% loss=0.30266, acc=0.93750
# [61/100] training 14.3% loss=0.08183, acc=0.96875
# [61/100] training 14.5% loss=0.16784, acc=0.93750
# [61/100] training 14.6% loss=0.28561, acc=0.92188
# [61/100] training 14.8% loss=0.19745, acc=0.90625
# [61/100] training 14.9% loss=0.10294, acc=0.95312
# [61/100] training 15.1% loss=0.23739, acc=0.89062
# [61/100] training 15.4% loss=0.19746, acc=0.92188
# [61/100] training 15.5% loss=0.13374, acc=0.95312
# [61/100] training 15.7% loss=0.22657, acc=0.93750
# [61/100] training 15.8% loss=0.13107, acc=0.96875
# [61/100] training 16.0% loss=0.21898, acc=0.93750
# [61/100] training 16.1% loss=0.27398, acc=0.90625
# [61/100] training 16.3% loss=0.13832, acc=0.93750
# [61/100] training 16.4% loss=0.11657, acc=0.95312
# [61/100] training 16.7% loss=0.16694, acc=0.92188
# [61/100] training 16.9% loss=0.12835, acc=0.92188
# [61/100] training 17.0% loss=0.26848, acc=0.89062
# [61/100] training 17.2% loss=0.18912, acc=0.93750
# [61/100] training 17.3% loss=0.15274, acc=0.93750
# [61/100] training 17.5% loss=0.22364, acc=0.89062
# [61/100] training 17.7% loss=0.12520, acc=0.95312
# [61/100] training 17.9% loss=0.25523, acc=0.92188
# [61/100] training 18.1% loss=0.19051, acc=0.90625
# [61/100] training 18.2% loss=0.15866, acc=0.95312
# [61/100] training 18.4% loss=0.15095, acc=0.96875
# [61/100] training 18.5% loss=0.17176, acc=0.93750
# [61/100] training 18.8% loss=0.16025, acc=0.95312
# [61/100] training 18.9% loss=0.06180, acc=0.98438
# [61/100] training 19.1% loss=0.18505, acc=0.92188
# [61/100] training 19.2% loss=0.07114, acc=0.95312
# [61/100] training 19.4% loss=0.18469, acc=0.92188
# [61/100] training 19.6% loss=0.16977, acc=0.90625
# [61/100] training 19.7% loss=0.13784, acc=0.92188
# [61/100] training 20.0% loss=0.12553, acc=0.93750
# [61/100] training 20.1% loss=0.15883, acc=0.92188
# [61/100] training 20.3% loss=0.18609, acc=0.93750
# [61/100] training 20.4% loss=0.11788, acc=0.95312
# [61/100] training 20.6% loss=0.33686, acc=0.93750
# [61/100] training 20.8% loss=0.12607, acc=0.93750
# [61/100] training 20.9% loss=0.10304, acc=0.96875
# [61/100] training 21.2% loss=0.23110, acc=0.93750
# [61/100] training 21.3% loss=0.22481, acc=0.90625
# [61/100] training 21.5% loss=0.27137, acc=0.92188
# [61/100] training 21.6% loss=0.04629, acc=0.98438
# [61/100] training 21.8% loss=0.09473, acc=0.95312
# [61/100] training 21.9% loss=0.20371, acc=0.95312
# [61/100] training 22.2% loss=0.15918, acc=0.92188
# [61/100] training 22.4% loss=0.18569, acc=0.90625
# [61/100] training 22.5% loss=0.11692, acc=0.95312
# [61/100] training 22.7% loss=0.17760, acc=0.92188
# [61/100] training 22.8% loss=0.16159, acc=0.95312
# [61/100] training 23.0% loss=0.09275, acc=0.96875
# [61/100] training 23.1% loss=0.33300, acc=0.87500
# [61/100] training 23.4% loss=0.15439, acc=0.95312
# [61/100] training 23.6% loss=0.19175, acc=0.92188
# [61/100] training 23.7% loss=0.18509, acc=0.92188
# [61/100] training 23.9% loss=0.15219, acc=0.93750
# [61/100] training 24.0% loss=0.10560, acc=0.96875
# [61/100] training 24.2% loss=0.16293, acc=0.92188
# [61/100] training 24.3% loss=0.23323, acc=0.93750
# [61/100] training 24.6% loss=0.24714, acc=0.90625
# [61/100] training 24.7% loss=0.25662, acc=0.92188
# [61/100] training 24.9% loss=0.19683, acc=0.92188
# [61/100] training 25.1% loss=0.29493, acc=0.89062
# [61/100] training 25.2% loss=0.08394, acc=0.98438
# [61/100] training 25.4% loss=0.12062, acc=0.93750
# [61/100] training 25.6% loss=0.12323, acc=0.95312
# [61/100] training 25.8% loss=0.12947, acc=0.95312
# [61/100] training 25.9% loss=0.16433, acc=0.92188
# [61/100] training 26.1% loss=0.12942, acc=0.93750
# [61/100] training 26.3% loss=0.13578, acc=0.95312
# [61/100] training 26.4% loss=0.15249, acc=0.92188
# [61/100] training 26.6% loss=0.03982, acc=1.00000
# [61/100] training 26.8% loss=0.13265, acc=0.96875
# [61/100] training 27.0% loss=0.20562, acc=0.89062
# [61/100] training 27.1% loss=0.12548, acc=0.92188
# [61/100] training 27.3% loss=0.12493, acc=0.93750
# [61/100] training 27.4% loss=0.10084, acc=0.96875
# [61/100] training 27.6% loss=0.29006, acc=0.93750
# [61/100] training 27.9% loss=0.14920, acc=0.92188
# [61/100] training 28.0% loss=0.43250, acc=0.82812
# [61/100] training 28.2% loss=0.20571, acc=0.92188
# [61/100] training 28.3% loss=0.07243, acc=0.98438
# [61/100] training 28.5% loss=0.15851, acc=0.90625
# [61/100] training 28.6% loss=0.16042, acc=0.95312
# [61/100] training 28.8% loss=0.09100, acc=1.00000
# [61/100] training 29.1% loss=0.11936, acc=0.96875
# [61/100] training 29.2% loss=0.11748, acc=0.95312
# [61/100] training 29.4% loss=0.15982, acc=0.93750
# [61/100] training 29.5% loss=0.07533, acc=0.96875
# [61/100] training 29.7% loss=0.21678, acc=0.93750
# [61/100] training 29.8% loss=0.16711, acc=0.96875
# [61/100] training 30.0% loss=0.17940, acc=0.92188
# [61/100] training 30.2% loss=0.16135, acc=0.95312
# [61/100] training 30.4% loss=0.08481, acc=0.96875
# [61/100] training 30.6% loss=0.26227, acc=0.92188
# [61/100] training 30.7% loss=0.08316, acc=0.98438
# [61/100] training 30.9% loss=0.32551, acc=0.93750
# [61/100] training 31.0% loss=0.13041, acc=0.95312
# [61/100] training 31.3% loss=0.14206, acc=0.95312
# [61/100] training 31.4% loss=0.27856, acc=0.87500
# [61/100] training 31.6% loss=0.25981, acc=0.90625
# [61/100] training 31.8% loss=0.16338, acc=0.92188
# [61/100] training 31.9% loss=0.18790, acc=0.92188
# [61/100] training 32.1% loss=0.21368, acc=0.95312
# [61/100] training 32.2% loss=0.24398, acc=0.92188
# [61/100] training 32.5% loss=0.09284, acc=0.96875
# [61/100] training 32.6% loss=0.18082, acc=0.92188
# [61/100] training 32.8% loss=0.14433, acc=0.95312
# [61/100] training 32.9% loss=0.19720, acc=0.90625
# [61/100] training 33.1% loss=0.13959, acc=0.93750
# [61/100] training 33.3% loss=0.19940, acc=0.95312
# [61/100] training 33.4% loss=0.09784, acc=0.93750
# [61/100] training 33.7% loss=0.16303, acc=0.93750
# [61/100] training 33.8% loss=0.18481, acc=0.93750
# [61/100] training 34.0% loss=0.19418, acc=0.95312
# [61/100] training 34.1% loss=0.11316, acc=0.95312
# [61/100] training 34.3% loss=0.14216, acc=0.92188
# [61/100] training 34.5% loss=0.18724, acc=0.95312
# [61/100] training 34.7% loss=0.10108, acc=0.96875
# [61/100] training 34.9% loss=0.11525, acc=0.90625
# [61/100] training 35.0% loss=0.11774, acc=0.93750
# [61/100] training 35.2% loss=0.15182, acc=0.95312
# [61/100] training 35.3% loss=0.29226, acc=0.85938
# [61/100] training 35.5% loss=0.14579, acc=0.95312
# [61/100] training 35.6% loss=0.27484, acc=0.90625
# [61/100] training 35.9% loss=0.18075, acc=0.90625
# [61/100] training 36.1% loss=0.20523, acc=0.93750
# [61/100] training 36.2% loss=0.17221, acc=0.92188
# [61/100] training 36.4% loss=0.24227, acc=0.92188
# [61/100] training 36.5% loss=0.18013, acc=0.92188
# [61/100] training 36.7% loss=0.18004, acc=0.90625
# [61/100] training 36.8% loss=0.07220, acc=0.96875
# [61/100] training 37.1% loss=0.26017, acc=0.85938
# [61/100] training 37.3% loss=0.18882, acc=0.92188
# [61/100] training 37.4% loss=0.13545, acc=0.93750
# [61/100] training 37.6% loss=0.20047, acc=0.93750
# [61/100] training 37.7% loss=0.14441, acc=0.96875
# [61/100] training 37.9% loss=0.13630, acc=0.93750
# [61/100] training 38.1% loss=0.18566, acc=0.92188
# [61/100] training 38.3% loss=0.14534, acc=0.95312
# [61/100] training 38.4% loss=0.09400, acc=0.96875
# [61/100] training 38.6% loss=0.12308, acc=0.95312
# [61/100] training 38.8% loss=0.31548, acc=0.87500
# [61/100] training 38.9% loss=0.16873, acc=0.92188
# [61/100] training 39.1% loss=0.17675, acc=0.90625
# [61/100] training 39.3% loss=0.28104, acc=0.84375
# [61/100] training 39.5% loss=0.20311, acc=0.93750
# [61/100] training 39.6% loss=0.15301, acc=0.95312
# [61/100] training 39.8% loss=0.08318, acc=0.98438
# [61/100] training 40.0% loss=0.22825, acc=0.90625
# [61/100] training 40.1% loss=0.20318, acc=0.87500
# [61/100] training 40.4% loss=0.13883, acc=0.90625
# [61/100] training 40.5% loss=0.26221, acc=0.87500
# [61/100] training 40.7% loss=0.19392, acc=0.92188
# [61/100] training 40.8% loss=0.14776, acc=0.95312
# [61/100] training 41.0% loss=0.14981, acc=0.92188
# [61/100] training 41.1% loss=0.22839, acc=0.89062
# [61/100] training 41.3% loss=0.21081, acc=0.90625
# [61/100] training 41.6% loss=0.23670, acc=0.90625
# [61/100] training 41.7% loss=0.17013, acc=0.93750
# [61/100] training 41.9% loss=0.12405, acc=0.95312
# [61/100] training 42.0% loss=0.16341, acc=0.96875
# [61/100] training 42.2% loss=0.23256, acc=0.90625
# [61/100] training 42.3% loss=0.10549, acc=0.95312
# [61/100] training 42.5% loss=0.11249, acc=0.96875
# [61/100] training 42.8% loss=0.16702, acc=0.95312
# [61/100] training 42.9% loss=0.20349, acc=0.89062
# [61/100] training 43.1% loss=0.11692, acc=0.98438
# [61/100] training 43.2% loss=0.15309, acc=0.92188
# [61/100] training 43.4% loss=0.17500, acc=0.90625
# [61/100] training 43.5% loss=0.12322, acc=0.93750
# [61/100] training 43.8% loss=0.17306, acc=0.92188
# [61/100] training 43.9% loss=0.11422, acc=0.98438
# [61/100] training 44.1% loss=0.27096, acc=0.93750
# [61/100] training 44.3% loss=0.18060, acc=0.93750
# [61/100] training 44.4% loss=0.14935, acc=0.93750
# [61/100] training 44.6% loss=0.11681, acc=0.92188
# [61/100] training 44.7% loss=0.20904, acc=0.89062
# [61/100] training 45.0% loss=0.07865, acc=0.96875
# [61/100] training 45.1% loss=0.16544, acc=0.92188
# [61/100] training 45.3% loss=0.26698, acc=0.85938
# [61/100] training 45.5% loss=0.03467, acc=1.00000
# [61/100] training 45.6% loss=0.16721, acc=0.90625
# [61/100] training 45.8% loss=0.05904, acc=0.98438
# [61/100] training 45.9% loss=0.12795, acc=0.92188
# [61/100] training 46.2% loss=0.18474, acc=0.95312
# [61/100] training 46.3% loss=0.14183, acc=0.93750
# [61/100] training 46.5% loss=0.22314, acc=0.92188
# [61/100] training 46.6% loss=0.15075, acc=0.92188
# [61/100] training 46.8% loss=0.14756, acc=0.90625
# [61/100] training 47.0% loss=0.19064, acc=0.85938
# [61/100] training 47.2% loss=0.20117, acc=0.93750
# [61/100] training 47.4% loss=0.31840, acc=0.85938
# [61/100] training 47.5% loss=0.22034, acc=0.92188
# [61/100] training 47.7% loss=0.18065, acc=0.92188
# [61/100] training 47.8% loss=0.15791, acc=0.90625
# [61/100] training 48.0% loss=0.21987, acc=0.93750
# [61/100] training 48.3% loss=0.13705, acc=0.93750
# [61/100] training 48.4% loss=0.11879, acc=0.96875
# [61/100] training 48.6% loss=0.15460, acc=0.93750
# [61/100] training 48.7% loss=0.16737, acc=0.92188
# [61/100] training 48.9% loss=0.19372, acc=0.90625
# [61/100] training 49.0% loss=0.11401, acc=0.95312
# [61/100] training 49.2% loss=0.12168, acc=0.95312
# [61/100] training 49.3% loss=0.08597, acc=0.96875
# [61/100] training 49.6% loss=0.25077, acc=0.92188
# [61/100] training 49.8% loss=0.21166, acc=0.89062
# [61/100] training 49.9% loss=0.14131, acc=0.93750
# [61/100] training 50.1% loss=0.07843, acc=0.96875
# [61/100] training 50.2% loss=0.12349, acc=0.92188
# [61/100] training 50.4% loss=0.22737, acc=0.90625
# [61/100] training 50.6% loss=0.08848, acc=0.96875
# [61/100] training 50.8% loss=0.19156, acc=0.92188
# [61/100] training 51.0% loss=0.22800, acc=0.92188
# [61/100] training 51.1% loss=0.25698, acc=0.90625
# [61/100] training 51.3% loss=0.20600, acc=0.95312
# [61/100] training 51.4% loss=0.22487, acc=0.90625
# [61/100] training 51.7% loss=0.21883, acc=0.87500
# [61/100] training 51.8% loss=0.13656, acc=0.95312
# [61/100] training 52.0% loss=0.20116, acc=0.92188
# [61/100] training 52.1% loss=0.18856, acc=0.92188
# [61/100] training 52.3% loss=0.28971, acc=0.92188
# [61/100] training 52.5% loss=0.06290, acc=0.98438
# [61/100] training 52.6% loss=0.19108, acc=0.90625
# [61/100] training 52.9% loss=0.13774, acc=0.93750
# [61/100] training 53.0% loss=0.06450, acc=0.98438
# [61/100] training 53.2% loss=0.20379, acc=0.95312
# [61/100] training 53.3% loss=0.10306, acc=0.95312
# [61/100] training 53.5% loss=0.21137, acc=0.92188
# [61/100] training 53.7% loss=0.03004, acc=1.00000
# [61/100] training 53.8% loss=0.20011, acc=0.93750
# [61/100] training 54.1% loss=0.13330, acc=0.93750
# [61/100] training 54.2% loss=0.07758, acc=0.96875
# [61/100] training 54.4% loss=0.13171, acc=0.93750
# [61/100] training 54.5% loss=0.20682, acc=0.90625
# [61/100] training 54.7% loss=0.24168, acc=0.90625
# [61/100] training 54.8% loss=0.10708, acc=0.95312
# [61/100] training 55.1% loss=0.23014, acc=0.90625
# [61/100] training 55.3% loss=0.19247, acc=0.95312
# [61/100] training 55.4% loss=0.07842, acc=0.96875
# [61/100] training 55.6% loss=0.15943, acc=0.93750
# [61/100] training 55.7% loss=0.09738, acc=0.96875
# [61/100] training 55.9% loss=0.11624, acc=0.93750
# [61/100] training 56.0% loss=0.07508, acc=0.96875
# [61/100] training 56.3% loss=0.35373, acc=0.87500
# [61/100] training 56.5% loss=0.12919, acc=0.95312
# [61/100] training 56.6% loss=0.13088, acc=0.93750
# [61/100] training 56.8% loss=0.16291, acc=0.92188
# [61/100] training 56.9% loss=0.12835, acc=0.95312
# [61/100] training 57.1% loss=0.23881, acc=0.92188
# [61/100] training 57.2% loss=0.12088, acc=0.95312
# [61/100] training 57.5% loss=0.11878, acc=0.93750
# [61/100] training 57.6% loss=0.28653, acc=0.90625
# [61/100] training 57.8% loss=0.12963, acc=0.93750
# [61/100] training 58.0% loss=0.16446, acc=0.95312
# [61/100] training 58.1% loss=0.18096, acc=0.92188
# [61/100] training 58.3% loss=0.10418, acc=0.95312
# [61/100] training 58.4% loss=0.30398, acc=0.90625
# [61/100] training 58.7% loss=0.20620, acc=0.92188
# [61/100] training 58.8% loss=0.22593, acc=0.85938
# [61/100] training 59.0% loss=0.10041, acc=0.98438
# [61/100] training 59.2% loss=0.16227, acc=0.93750
# [61/100] training 59.3% loss=0.20632, acc=0.90625
# [61/100] training 59.5% loss=0.19847, acc=0.92188
# [61/100] training 59.7% loss=0.20141, acc=0.93750
# [61/100] training 59.9% loss=0.19597, acc=0.95312
# [61/100] training 60.0% loss=0.24307, acc=0.85938
# [61/100] training 60.2% loss=0.21275, acc=0.90625
# [61/100] training 60.3% loss=0.12511, acc=0.95312
# [61/100] training 60.5% loss=0.19724, acc=0.92188
# [61/100] training 60.8% loss=0.20461, acc=0.90625
# [61/100] training 60.9% loss=0.27762, acc=0.87500
# [61/100] training 61.1% loss=0.27362, acc=0.89062
# [61/100] training 61.2% loss=0.19788, acc=0.90625
# [61/100] training 61.4% loss=0.19569, acc=0.93750
# [61/100] training 61.5% loss=0.36448, acc=0.85938
# [61/100] training 61.7% loss=0.20092, acc=0.90625
# [61/100] training 62.0% loss=0.28401, acc=0.85938
# [61/100] training 62.1% loss=0.21781, acc=0.90625
# [61/100] training 62.3% loss=0.12231, acc=0.92188
# [61/100] training 62.4% loss=0.10847, acc=0.95312
# [61/100] training 62.6% loss=0.11335, acc=0.98438
# [61/100] training 62.7% loss=0.09161, acc=0.95312
# [61/100] training 62.9% loss=0.21915, acc=0.92188
# [61/100] training 63.1% loss=0.26587, acc=0.90625
# [61/100] training 63.3% loss=0.06889, acc=0.98438
# [61/100] training 63.5% loss=0.16361, acc=0.95312
# [61/100] training 63.6% loss=0.17293, acc=0.95312
# [61/100] training 63.8% loss=0.24104, acc=0.89062
# [61/100] training 63.9% loss=0.12865, acc=0.96875
# [61/100] training 64.2% loss=0.14901, acc=0.90625
# [61/100] training 64.3% loss=0.17588, acc=0.92188
# [61/100] training 64.5% loss=0.10221, acc=0.95312
# [61/100] training 64.7% loss=0.15439, acc=0.90625
# [61/100] training 64.8% loss=0.26306, acc=0.92188
# [61/100] training 65.0% loss=0.09304, acc=0.95312
# [61/100] training 65.1% loss=0.08399, acc=0.96875
# [61/100] training 65.4% loss=0.11646, acc=0.96875
# [61/100] training 65.5% loss=0.09979, acc=0.95312
# [61/100] training 65.7% loss=0.08025, acc=0.96875
# [61/100] training 65.8% loss=0.20302, acc=0.92188
# [61/100] training 66.0% loss=0.08055, acc=0.98438
# [61/100] training 66.2% loss=0.07583, acc=0.96875
# [61/100] training 66.3% loss=0.16768, acc=0.93750
# [61/100] training 66.6% loss=0.12253, acc=0.95312
# [61/100] training 66.7% loss=0.17767, acc=0.93750
# [61/100] training 66.9% loss=0.17015, acc=0.90625
# [61/100] training 67.0% loss=0.15370, acc=0.93750
# [61/100] training 67.2% loss=0.05421, acc=0.96875
# [61/100] training 67.4% loss=0.18783, acc=0.93750
# [61/100] training 67.6% loss=0.15554, acc=0.95312
# [61/100] training 67.8% loss=0.13078, acc=0.93750
# [61/100] training 67.9% loss=0.10177, acc=0.96875
# [61/100] training 68.1% loss=0.24556, acc=0.89062
# [61/100] training 68.2% loss=0.08103, acc=0.96875
# [61/100] training 68.4% loss=0.16740, acc=0.90625
# [61/100] training 68.5% loss=0.19587, acc=0.93750
# [61/100] training 68.8% loss=0.16443, acc=0.89062
# [61/100] training 69.0% loss=0.14171, acc=0.93750
# [61/100] training 69.1% loss=0.19808, acc=0.90625
# [61/100] training 69.3% loss=0.24631, acc=0.90625
# [61/100] training 69.4% loss=0.17746, acc=0.92188
# [61/100] training 69.6% loss=0.18195, acc=0.90625
# [61/100] training 69.7% loss=0.22558, acc=0.90625
# [61/100] training 70.0% loss=0.20350, acc=0.89062
# [61/100] training 70.2% loss=0.18061, acc=0.95312
# [61/100] training 70.3% loss=0.12173, acc=0.95312
# [61/100] training 70.5% loss=0.21396, acc=0.89062
# [61/100] training 70.6% loss=0.16336, acc=0.93750
# [61/100] training 70.8% loss=0.16903, acc=0.93750
# [61/100] training 71.0% loss=0.19444, acc=0.90625
# [61/100] training 71.2% loss=0.12769, acc=0.93750
# [61/100] training 71.3% loss=0.16161, acc=0.96875
# [61/100] training 71.5% loss=0.19244, acc=0.92188
# [61/100] training 71.7% loss=0.21629, acc=0.90625
# [61/100] training 71.8% loss=0.21028, acc=0.92188
# [61/100] training 72.0% loss=0.09636, acc=0.95312
# [61/100] training 72.2% loss=0.24305, acc=0.89062
# [61/100] training 72.4% loss=0.23722, acc=0.93750
# [61/100] training 72.5% loss=0.21729, acc=0.90625
# [61/100] training 72.7% loss=0.25244, acc=0.93750
# [61/100] training 72.9% loss=0.11651, acc=0.93750
# [61/100] training 73.0% loss=0.10351, acc=0.96875
# [61/100] training 73.3% loss=0.36429, acc=0.89062
# [61/100] training 73.4% loss=0.14692, acc=0.90625
# [61/100] training 73.6% loss=0.12132, acc=0.93750
# [61/100] training 73.7% loss=0.13722, acc=0.95312
# [61/100] training 73.9% loss=0.14513, acc=0.90625
# [61/100] training 74.0% loss=0.11803, acc=0.96875
# [61/100] training 74.2% loss=0.07967, acc=0.96875
# [61/100] training 74.5% loss=0.08419, acc=0.98438
# [61/100] training 74.6% loss=0.30366, acc=0.89062
# [61/100] training 74.8% loss=0.23729, acc=0.90625
# [61/100] training 74.9% loss=0.24705, acc=0.89062
# [61/100] training 75.1% loss=0.11777, acc=0.93750
# [61/100] training 75.2% loss=0.08173, acc=1.00000
# [61/100] training 75.4% loss=0.12831, acc=0.93750
# [61/100] training 75.7% loss=0.15726, acc=0.95312
# [61/100] training 75.8% loss=0.25558, acc=0.89062
# [61/100] training 76.0% loss=0.16296, acc=0.92188
# [61/100] training 76.1% loss=0.35113, acc=0.87500
# [61/100] training 76.3% loss=0.09714, acc=0.96875
# [61/100] training 76.4% loss=0.22357, acc=0.90625
# [61/100] training 76.7% loss=0.14238, acc=0.96875
# [61/100] training 76.8% loss=0.14463, acc=0.93750
# [61/100] training 77.0% loss=0.11844, acc=0.93750
# [61/100] training 77.2% loss=0.19229, acc=0.95312
# [61/100] training 77.3% loss=0.10969, acc=0.95312
# [61/100] training 77.5% loss=0.18546, acc=0.90625
# [61/100] training 77.6% loss=0.17551, acc=0.90625
# [61/100] training 77.9% loss=0.15015, acc=0.92188
# [61/100] training 78.0% loss=0.14683, acc=0.95312
# [61/100] training 78.2% loss=0.17656, acc=0.90625
# [61/100] training 78.4% loss=0.05337, acc=0.98438
# [61/100] training 78.5% loss=0.34003, acc=0.87500
# [61/100] training 78.7% loss=0.18011, acc=0.90625
# [61/100] training 78.8% loss=0.05954, acc=0.98438
# [61/100] training 79.1% loss=0.11988, acc=0.93750
# [61/100] training 79.2% loss=0.15583, acc=0.96875
# [61/100] training 79.4% loss=0.16548, acc=0.93750
# [61/100] training 79.5% loss=0.17817, acc=0.89062
# [61/100] training 79.7% loss=0.08253, acc=0.98438
# [61/100] training 79.9% loss=0.09041, acc=0.98438
# [61/100] training 80.1% loss=0.08251, acc=0.98438
# [61/100] training 80.3% loss=0.22244, acc=0.93750
# [61/100] training 80.4% loss=0.18974, acc=0.95312
# [61/100] training 80.6% loss=0.26690, acc=0.89062
# [61/100] training 80.7% loss=0.09836, acc=1.00000
# [61/100] training 80.9% loss=0.17481, acc=0.95312
# [61/100] training 81.2% loss=0.19214, acc=0.90625
# [61/100] training 81.3% loss=0.15110, acc=0.96875
# [61/100] training 81.5% loss=0.14650, acc=0.93750
# [61/100] training 81.6% loss=0.18630, acc=0.93750
# [61/100] training 81.8% loss=0.16377, acc=0.93750
# [61/100] training 81.9% loss=0.28393, acc=0.89062
# [61/100] training 82.1% loss=0.06695, acc=0.98438
# [61/100] training 82.2% loss=0.13048, acc=0.95312
# [61/100] training 82.5% loss=0.13357, acc=0.95312
# [61/100] training 82.7% loss=0.14528, acc=0.93750
# [61/100] training 82.8% loss=0.11239, acc=0.93750
# [61/100] training 83.0% loss=0.12398, acc=0.96875
# [61/100] training 83.1% loss=0.12534, acc=0.95312
# [61/100] training 83.3% loss=0.10871, acc=0.98438
# [61/100] training 83.5% loss=0.07013, acc=0.98438
# [61/100] training 83.7% loss=0.27419, acc=0.89062
# [61/100] training 83.9% loss=0.17820, acc=0.93750
# [61/100] training 84.0% loss=0.09305, acc=0.98438
# [61/100] training 84.2% loss=0.10363, acc=0.95312
# [61/100] training 84.3% loss=0.07808, acc=0.98438
# [61/100] training 84.5% loss=0.03855, acc=1.00000
# [61/100] training 84.7% loss=0.18275, acc=0.92188
# [61/100] training 84.9% loss=0.16107, acc=0.93750
# [61/100] training 85.0% loss=0.16285, acc=0.92188
# [61/100] training 85.2% loss=0.17263, acc=0.93750
# [61/100] training 85.4% loss=0.06786, acc=0.96875
# [61/100] training 85.5% loss=0.12212, acc=0.98438
# [61/100] training 85.8% loss=0.21122, acc=0.92188
# [61/100] training 85.9% loss=0.09600, acc=0.96875
# [61/100] training 86.1% loss=0.04853, acc=0.96875
# [61/100] training 86.2% loss=0.08930, acc=0.98438
# [61/100] training 86.4% loss=0.24558, acc=0.89062
# [61/100] training 86.6% loss=0.17052, acc=0.92188
# [61/100] training 86.7% loss=0.10750, acc=0.95312
# [61/100] training 87.0% loss=0.18401, acc=0.93750
# [61/100] training 87.1% loss=0.12075, acc=0.95312
# [61/100] training 87.3% loss=0.21598, acc=0.90625
# [61/100] training 87.4% loss=0.18254, acc=0.96875
# [61/100] training 87.6% loss=0.19712, acc=0.85938
# [61/100] training 87.7% loss=0.19112, acc=0.93750
# [61/100] training 87.9% loss=0.23281, acc=0.90625
# [61/100] training 88.2% loss=0.06233, acc=0.96875
# [61/100] training 88.3% loss=0.23093, acc=0.92188
# [61/100] training 88.5% loss=0.24758, acc=0.90625
# [61/100] training 88.6% loss=0.07591, acc=0.98438
# [61/100] training 88.8% loss=0.16104, acc=0.93750
# [61/100] training 88.9% loss=0.27024, acc=0.89062
# [61/100] training 89.2% loss=0.17837, acc=0.92188
# [61/100] training 89.4% loss=0.15713, acc=0.92188
# [61/100] training 89.5% loss=0.25837, acc=0.92188
# [61/100] training 89.7% loss=0.30155, acc=0.87500
# [61/100] training 89.8% loss=0.11865, acc=0.93750
# [61/100] training 90.0% loss=0.13834, acc=0.95312
# [61/100] training 90.1% loss=0.20231, acc=0.92188
# [61/100] training 90.4% loss=0.29029, acc=0.87500
# [61/100] training 90.5% loss=0.07734, acc=0.98438
# [61/100] training 90.7% loss=0.11402, acc=0.95312
# [61/100] training 90.9% loss=0.07542, acc=0.96875
# [61/100] training 91.0% loss=0.13771, acc=0.95312
# [61/100] training 91.2% loss=0.06599, acc=0.98438
# [61/100] training 91.3% loss=0.29188, acc=0.89062
# [61/100] training 91.6% loss=0.32989, acc=0.90625
# [61/100] training 91.7% loss=0.12388, acc=0.95312
# [61/100] training 91.9% loss=0.33213, acc=0.89062
# [61/100] training 92.1% loss=0.07325, acc=0.95312
# [61/100] training 92.2% loss=0.12695, acc=0.96875
# [61/100] training 92.4% loss=0.11558, acc=0.95312
# [61/100] training 92.6% loss=0.21125, acc=0.92188
# [61/100] training 92.8% loss=0.12607, acc=0.93750
# [61/100] training 92.9% loss=0.20828, acc=0.89062
# [61/100] training 93.1% loss=0.41376, acc=0.85938
# [61/100] training 93.2% loss=0.23111, acc=0.90625
# [61/100] training 93.4% loss=0.20130, acc=0.90625
# [61/100] training 93.7% loss=0.14076, acc=0.93750
# [61/100] training 93.8% loss=0.17076, acc=0.92188
# [61/100] training 94.0% loss=0.21759, acc=0.93750
# [61/100] training 94.1% loss=0.16819, acc=0.93750
# [61/100] training 94.3% loss=0.11804, acc=0.95312
# [61/100] training 94.4% loss=0.18923, acc=0.92188
# [61/100] training 94.6% loss=0.09174, acc=0.95312
# [61/100] training 94.9% loss=0.11147, acc=0.93750
# [61/100] training 95.0% loss=0.17749, acc=0.92188
# [61/100] training 95.2% loss=0.33513, acc=0.90625
# [61/100] training 95.3% loss=0.20445, acc=0.92188
# [61/100] training 95.5% loss=0.08818, acc=0.96875
# [61/100] training 95.6% loss=0.21551, acc=0.92188
# [61/100] training 95.8% loss=0.13999, acc=0.90625
# [61/100] training 96.0% loss=0.22892, acc=0.89062
# [61/100] training 96.2% loss=0.09057, acc=0.98438
# [61/100] training 96.4% loss=0.09819, acc=0.98438
# [61/100] training 96.5% loss=0.17075, acc=0.95312
# [61/100] training 96.7% loss=0.10646, acc=0.96875
# [61/100] training 96.8% loss=0.22982, acc=0.89062
# [61/100] training 97.1% loss=0.13011, acc=0.93750
# [61/100] training 97.2% loss=0.25622, acc=0.92188
# [61/100] training 97.4% loss=0.17060, acc=0.92188
# [61/100] training 97.6% loss=0.17465, acc=0.87500
# [61/100] training 97.7% loss=0.19675, acc=0.92188
# [61/100] training 97.9% loss=0.15679, acc=0.90625
# [61/100] training 98.0% loss=0.14710, acc=0.95312
# [61/100] training 98.3% loss=0.23306, acc=0.92188
# [61/100] training 98.4% loss=0.25214, acc=0.87500
# [61/100] training 98.6% loss=0.20155, acc=0.93750
# [61/100] training 98.7% loss=0.25356, acc=0.93750
# [61/100] training 98.9% loss=0.13362, acc=0.95312
# [61/100] training 99.1% loss=0.18985, acc=0.93750
# [61/100] training 99.2% loss=0.11606, acc=0.95312
# [61/100] training 99.5% loss=0.28403, acc=0.89062
# [61/100] training 99.6% loss=0.13860, acc=0.95312
# [61/100] training 99.8% loss=0.10996, acc=0.95312
# [61/100] training 99.9% loss=0.02368, acc=1.00000
# [61/100] testing 0.9% loss=0.16157, acc=0.90625
# [61/100] testing 1.8% loss=0.28073, acc=0.89062
# [61/100] testing 2.2% loss=0.24894, acc=0.90625
# [61/100] testing 3.1% loss=0.44428, acc=0.85938
# [61/100] testing 3.5% loss=0.09482, acc=0.98438
# [61/100] testing 4.4% loss=0.15037, acc=0.92188
# [61/100] testing 4.8% loss=0.37612, acc=0.87500
# [61/100] testing 5.7% loss=0.20689, acc=0.93750
# [61/100] testing 6.6% loss=0.12014, acc=0.96875
# [61/100] testing 7.0% loss=0.08978, acc=0.96875
# [61/100] testing 7.9% loss=0.31304, acc=0.90625
# [61/100] testing 8.3% loss=0.37456, acc=0.87500
# [61/100] testing 9.2% loss=0.30455, acc=0.95312
# [61/100] testing 9.7% loss=0.14111, acc=0.95312
# [61/100] testing 10.5% loss=0.19436, acc=0.89062
# [61/100] testing 11.0% loss=0.38392, acc=0.85938
# [61/100] testing 11.8% loss=0.14996, acc=0.93750
# [61/100] testing 12.7% loss=0.33089, acc=0.90625
# [61/100] testing 13.2% loss=0.15702, acc=0.92188
# [61/100] testing 14.0% loss=0.35440, acc=0.90625
# [61/100] testing 14.5% loss=0.15927, acc=0.92188
# [61/100] testing 15.4% loss=0.33324, acc=0.90625
# [61/100] testing 15.8% loss=0.14489, acc=0.93750
# [61/100] testing 16.7% loss=0.37442, acc=0.87500
# [61/100] testing 17.5% loss=0.22281, acc=0.92188
# [61/100] testing 18.0% loss=0.26123, acc=0.89062
# [61/100] testing 18.9% loss=0.08546, acc=0.98438
# [61/100] testing 19.3% loss=0.36409, acc=0.89062
# [61/100] testing 20.2% loss=0.18459, acc=0.93750
# [61/100] testing 20.6% loss=0.22180, acc=0.92188
# [61/100] testing 21.5% loss=0.21469, acc=0.92188
# [61/100] testing 21.9% loss=0.45871, acc=0.85938
# [61/100] testing 22.8% loss=0.22456, acc=0.92188
# [61/100] testing 23.7% loss=0.36444, acc=0.90625
# [61/100] testing 24.1% loss=0.18412, acc=0.93750
# [61/100] testing 25.0% loss=0.27247, acc=0.92188
# [61/100] testing 25.4% loss=0.12386, acc=0.95312
# [61/100] testing 26.3% loss=0.30577, acc=0.87500
# [61/100] testing 26.8% loss=0.30373, acc=0.90625
# [61/100] testing 27.6% loss=0.14567, acc=0.95312
# [61/100] testing 28.5% loss=0.20856, acc=0.95312
# [61/100] testing 29.0% loss=0.21959, acc=0.93750
# [61/100] testing 29.8% loss=0.42555, acc=0.90625
# [61/100] testing 30.3% loss=0.28707, acc=0.90625
# [61/100] testing 31.1% loss=0.42309, acc=0.84375
# [61/100] testing 31.6% loss=0.27008, acc=0.90625
# [61/100] testing 32.5% loss=0.22336, acc=0.93750
# [61/100] testing 32.9% loss=0.45212, acc=0.89062
# [61/100] testing 33.8% loss=0.40622, acc=0.89062
# [61/100] testing 34.7% loss=0.36655, acc=0.87500
# [61/100] testing 35.1% loss=0.20574, acc=0.95312
# [61/100] testing 36.0% loss=0.29215, acc=0.92188
# [61/100] testing 36.4% loss=0.29615, acc=0.87500
# [61/100] testing 37.3% loss=0.23538, acc=0.92188
# [61/100] testing 37.7% loss=0.54695, acc=0.84375
# [61/100] testing 38.6% loss=0.15824, acc=0.95312
# [61/100] testing 39.5% loss=0.19381, acc=0.96875
# [61/100] testing 39.9% loss=0.23190, acc=0.92188
# [61/100] testing 40.8% loss=0.32573, acc=0.90625
# [61/100] testing 41.2% loss=0.28474, acc=0.95312
# [61/100] testing 42.1% loss=0.27903, acc=0.90625
# [61/100] testing 42.5% loss=0.14168, acc=0.95312
# [61/100] testing 43.4% loss=0.23427, acc=0.93750
# [61/100] testing 43.9% loss=0.13445, acc=0.95312
# [61/100] testing 44.7% loss=0.30429, acc=0.90625
# [61/100] testing 45.6% loss=0.27749, acc=0.90625
# [61/100] testing 46.1% loss=0.18953, acc=0.92188
# [61/100] testing 46.9% loss=0.18355, acc=0.93750
# [61/100] testing 47.4% loss=0.20963, acc=0.92188
# [61/100] testing 48.3% loss=0.43701, acc=0.85938
# [61/100] testing 48.7% loss=0.33914, acc=0.85938
# [61/100] testing 49.6% loss=0.38946, acc=0.82812
# [61/100] testing 50.4% loss=0.25446, acc=0.93750
# [61/100] testing 50.9% loss=0.34669, acc=0.87500
# [61/100] testing 51.8% loss=0.25292, acc=0.92188
# [61/100] testing 52.2% loss=0.24929, acc=0.92188
# [61/100] testing 53.1% loss=0.25827, acc=0.92188
# [61/100] testing 53.5% loss=0.20688, acc=0.92188
# [61/100] testing 54.4% loss=0.37943, acc=0.87500
# [61/100] testing 54.8% loss=0.34461, acc=0.84375
# [61/100] testing 55.7% loss=0.12509, acc=0.93750
# [61/100] testing 56.6% loss=0.27887, acc=0.84375
# [61/100] testing 57.0% loss=0.27214, acc=0.90625
# [61/100] testing 57.9% loss=0.19110, acc=0.93750
# [61/100] testing 58.3% loss=0.31849, acc=0.87500
# [61/100] testing 59.2% loss=0.18175, acc=0.92188
# [61/100] testing 59.7% loss=0.29531, acc=0.87500
# [61/100] testing 60.5% loss=0.38919, acc=0.82812
# [61/100] testing 61.4% loss=0.12881, acc=0.95312
# [61/100] testing 61.9% loss=0.13591, acc=0.92188
# [61/100] testing 62.7% loss=0.14820, acc=0.95312
# [61/100] testing 63.2% loss=0.34736, acc=0.87500
# [61/100] testing 64.0% loss=0.46448, acc=0.90625
# [61/100] testing 64.5% loss=0.19205, acc=0.93750
# [61/100] testing 65.4% loss=0.30399, acc=0.87500
# [61/100] testing 65.8% loss=0.25537, acc=0.92188
# [61/100] testing 66.7% loss=0.20356, acc=0.93750
# [61/100] testing 67.6% loss=0.45447, acc=0.89062
# [61/100] testing 68.0% loss=0.06550, acc=0.98438
# [61/100] testing 68.9% loss=0.21404, acc=0.92188
# [61/100] testing 69.3% loss=0.12939, acc=0.95312
# [61/100] testing 70.2% loss=0.36266, acc=0.85938
# [61/100] testing 70.6% loss=0.33944, acc=0.90625
# [61/100] testing 71.5% loss=0.33372, acc=0.92188
# [61/100] testing 72.4% loss=0.13028, acc=0.93750
# [61/100] testing 72.8% loss=0.19289, acc=0.90625
# [61/100] testing 73.7% loss=0.11270, acc=0.93750
# [61/100] testing 74.1% loss=0.43073, acc=0.85938
# [61/100] testing 75.0% loss=0.22143, acc=0.89062
# [61/100] testing 75.4% loss=0.41903, acc=0.85938
# [61/100] testing 76.3% loss=0.15221, acc=0.93750
# [61/100] testing 76.8% loss=0.39795, acc=0.87500
# [61/100] testing 77.6% loss=0.18328, acc=0.92188
# [61/100] testing 78.5% loss=0.35800, acc=0.87500
# [61/100] testing 79.0% loss=0.18160, acc=0.93750
# [61/100] testing 79.8% loss=0.23780, acc=0.90625
# [61/100] testing 80.3% loss=0.34827, acc=0.90625
# [61/100] testing 81.2% loss=0.39282, acc=0.89062
# [61/100] testing 81.6% loss=0.17165, acc=0.95312
# [61/100] testing 82.5% loss=0.11037, acc=0.95312
# [61/100] testing 83.3% loss=0.18962, acc=0.93750
# [61/100] testing 83.8% loss=0.14261, acc=0.92188
# [61/100] testing 84.7% loss=0.33178, acc=0.87500
# [61/100] testing 85.1% loss=0.23320, acc=0.90625
# [61/100] testing 86.0% loss=0.25169, acc=0.92188
# [61/100] testing 86.4% loss=0.38346, acc=0.89062
# [61/100] testing 87.3% loss=0.35546, acc=0.85938
# [61/100] testing 87.7% loss=0.41616, acc=0.87500
# [61/100] testing 88.6% loss=0.23782, acc=0.87500
# [61/100] testing 89.5% loss=0.57738, acc=0.81250
# [61/100] testing 89.9% loss=0.26245, acc=0.87500
# [61/100] testing 90.8% loss=0.40121, acc=0.90625
# [61/100] testing 91.2% loss=0.26453, acc=0.93750
# [61/100] testing 92.1% loss=0.39769, acc=0.87500
# [61/100] testing 92.6% loss=0.36216, acc=0.85938
# [61/100] testing 93.4% loss=0.30992, acc=0.90625
# [61/100] testing 94.3% loss=0.20030, acc=0.93750
# [61/100] testing 94.7% loss=0.26160, acc=0.90625
# [61/100] testing 95.6% loss=0.33457, acc=0.90625
# [61/100] testing 96.1% loss=0.12808, acc=0.95312
# [61/100] testing 96.9% loss=0.21027, acc=0.92188
# [61/100] testing 97.4% loss=0.09511, acc=0.98438
# [61/100] testing 98.3% loss=0.15710, acc=0.93750
# [61/100] testing 98.7% loss=0.31529, acc=0.90625
# [61/100] testing 99.6% loss=0.33789, acc=0.89062
# [62/100] training 0.2% loss=0.22930, acc=0.89062
# [62/100] training 0.4% loss=0.25398, acc=0.87500
# [62/100] training 0.5% loss=0.11839, acc=0.98438
# [62/100] training 0.8% loss=0.06335, acc=0.98438
# [62/100] training 0.9% loss=0.12629, acc=0.96875
# [62/100] training 1.1% loss=0.25390, acc=0.95312
# [62/100] training 1.2% loss=0.23627, acc=0.89062
# [62/100] training 1.4% loss=0.08342, acc=0.96875
# [62/100] training 1.6% loss=0.07123, acc=1.00000
# [62/100] training 1.8% loss=0.12076, acc=0.93750
# [62/100] training 2.0% loss=0.24143, acc=0.87500
# [62/100] training 2.1% loss=0.16339, acc=0.89062
# [62/100] training 2.3% loss=0.11641, acc=0.95312
# [62/100] training 2.4% loss=0.27599, acc=0.89062
# [62/100] training 2.6% loss=0.13803, acc=0.93750
# [62/100] training 2.7% loss=0.17869, acc=0.92188
# [62/100] training 3.0% loss=0.21828, acc=0.90625
# [62/100] training 3.2% loss=0.08982, acc=0.96875
# [62/100] training 3.3% loss=0.25424, acc=0.90625
# [62/100] training 3.5% loss=0.26212, acc=0.92188
# [62/100] training 3.6% loss=0.22422, acc=0.90625
# [62/100] training 3.8% loss=0.17340, acc=0.92188
# [62/100] training 3.9% loss=0.19478, acc=0.92188
# [62/100] training 4.2% loss=0.10641, acc=0.95312
# [62/100] training 4.4% loss=0.20013, acc=0.93750
# [62/100] training 4.5% loss=0.16911, acc=0.93750
# [62/100] training 4.7% loss=0.23552, acc=0.90625
# [62/100] training 4.8% loss=0.17301, acc=0.93750
# [62/100] training 5.0% loss=0.12053, acc=0.93750
# [62/100] training 5.2% loss=0.16676, acc=0.92188
# [62/100] training 5.4% loss=0.12513, acc=0.96875
# [62/100] training 5.5% loss=0.26902, acc=0.89062
# [62/100] training 5.7% loss=0.21082, acc=0.93750
# [62/100] training 5.9% loss=0.13282, acc=0.95312
# [62/100] training 6.0% loss=0.29542, acc=0.87500
# [62/100] training 6.3% loss=0.17514, acc=0.90625
# [62/100] training 6.4% loss=0.11288, acc=0.95312
# [62/100] training 6.6% loss=0.15383, acc=0.93750
# [62/100] training 6.7% loss=0.32200, acc=0.84375
# [62/100] training 6.9% loss=0.10912, acc=0.95312
# [62/100] training 7.1% loss=0.17017, acc=0.92188
# [62/100] training 7.2% loss=0.10211, acc=0.93750
# [62/100] training 7.5% loss=0.10218, acc=0.96875
# [62/100] training 7.6% loss=0.26466, acc=0.92188
# [62/100] training 7.8% loss=0.10231, acc=0.96875
# [62/100] training 7.9% loss=0.23115, acc=0.90625
# [62/100] training 8.1% loss=0.06957, acc=0.98438
# [62/100] training 8.2% loss=0.27525, acc=0.90625
# [62/100] training 8.4% loss=0.23986, acc=0.92188
# [62/100] training 8.7% loss=0.15681, acc=0.95312
# [62/100] training 8.8% loss=0.22533, acc=0.90625
# [62/100] training 9.0% loss=0.19113, acc=0.90625
# [62/100] training 9.1% loss=0.14325, acc=0.96875
# [62/100] training 9.3% loss=0.36946, acc=0.89062
# [62/100] training 9.4% loss=0.09878, acc=1.00000
# [62/100] training 9.7% loss=0.14430, acc=0.93750
# [62/100] training 9.9% loss=0.20687, acc=0.89062
# [62/100] training 10.0% loss=0.17560, acc=0.90625
# [62/100] training 10.2% loss=0.10298, acc=0.93750
# [62/100] training 10.3% loss=0.12015, acc=0.95312
# [62/100] training 10.5% loss=0.19476, acc=0.93750
# [62/100] training 10.6% loss=0.23164, acc=0.92188
# [62/100] training 10.9% loss=0.16672, acc=0.90625
# [62/100] training 11.0% loss=0.20271, acc=0.93750
# [62/100] training 11.2% loss=0.06077, acc=0.98438
# [62/100] training 11.4% loss=0.20098, acc=0.89062
# [62/100] training 11.5% loss=0.26459, acc=0.90625
# [62/100] training 11.7% loss=0.06091, acc=1.00000
# [62/100] training 11.8% loss=0.16790, acc=0.92188
# [62/100] training 12.1% loss=0.16632, acc=0.92188
# [62/100] training 12.2% loss=0.07689, acc=0.98438
# [62/100] training 12.4% loss=0.14696, acc=0.93750
# [62/100] training 12.6% loss=0.23944, acc=0.92188
# [62/100] training 12.7% loss=0.23840, acc=0.90625
# [62/100] training 12.9% loss=0.18957, acc=0.90625
# [62/100] training 13.0% loss=0.14401, acc=0.93750
# [62/100] training 13.3% loss=0.18291, acc=0.93750
# [62/100] training 13.4% loss=0.13832, acc=0.96875
# [62/100] training 13.6% loss=0.13414, acc=0.95312
# [62/100] training 13.7% loss=0.27106, acc=0.90625
# [62/100] training 13.9% loss=0.19566, acc=0.89062
# [62/100] training 14.1% loss=0.24989, acc=0.93750
# [62/100] training 14.3% loss=0.14883, acc=0.96875
# [62/100] training 14.5% loss=0.25357, acc=0.92188
# [62/100] training 14.6% loss=0.30693, acc=0.90625
# [62/100] training 14.8% loss=0.17206, acc=0.92188
# [62/100] training 14.9% loss=0.12991, acc=0.95312
# [62/100] training 15.1% loss=0.27675, acc=0.85938
# [62/100] training 15.4% loss=0.22132, acc=0.90625
# [62/100] training 15.5% loss=0.13206, acc=0.93750
# [62/100] training 15.7% loss=0.20391, acc=0.93750
# [62/100] training 15.8% loss=0.08737, acc=0.95312
# [62/100] training 16.0% loss=0.29586, acc=0.95312
# [62/100] training 16.1% loss=0.20752, acc=0.90625
# [62/100] training 16.3% loss=0.16096, acc=0.95312
# [62/100] training 16.4% loss=0.16505, acc=0.93750
# [62/100] training 16.7% loss=0.26520, acc=0.89062
# [62/100] training 16.9% loss=0.17979, acc=0.93750
# [62/100] training 17.0% loss=0.16115, acc=0.93750
# [62/100] training 17.2% loss=0.15237, acc=0.95312
# [62/100] training 17.3% loss=0.12158, acc=0.95312
# [62/100] training 17.5% loss=0.16436, acc=0.90625
# [62/100] training 17.7% loss=0.18280, acc=0.92188
# [62/100] training 17.9% loss=0.18803, acc=0.93750
# [62/100] training 18.1% loss=0.18867, acc=0.89062
# [62/100] training 18.2% loss=0.16369, acc=0.95312
# [62/100] training 18.4% loss=0.18899, acc=0.93750
# [62/100] training 18.5% loss=0.17428, acc=0.95312
# [62/100] training 18.8% loss=0.13043, acc=0.93750
# [62/100] training 18.9% loss=0.05512, acc=0.98438
# [62/100] training 19.1% loss=0.22495, acc=0.89062
# [62/100] training 19.2% loss=0.10546, acc=0.95312
# [62/100] training 19.4% loss=0.10064, acc=0.95312
# [62/100] training 19.6% loss=0.23980, acc=0.90625
# [62/100] training 19.7% loss=0.23432, acc=0.92188
# [62/100] training 20.0% loss=0.11108, acc=0.92188
# [62/100] training 20.1% loss=0.12189, acc=0.95312
# [62/100] training 20.3% loss=0.19889, acc=0.90625
# [62/100] training 20.4% loss=0.12218, acc=0.93750
# [62/100] training 20.6% loss=0.17744, acc=0.92188
# [62/100] training 20.8% loss=0.11417, acc=0.96875
# [62/100] training 20.9% loss=0.24354, acc=0.90625
# [62/100] training 21.2% loss=0.17626, acc=0.93750
# [62/100] training 21.3% loss=0.16523, acc=0.95312
# [62/100] training 21.5% loss=0.27162, acc=0.92188
# [62/100] training 21.6% loss=0.05748, acc=0.98438
# [62/100] training 21.8% loss=0.11480, acc=0.95312
# [62/100] training 21.9% loss=0.18747, acc=0.92188
# [62/100] training 22.2% loss=0.15071, acc=0.98438
# [62/100] training 22.4% loss=0.28161, acc=0.87500
# [62/100] training 22.5% loss=0.09957, acc=0.95312
# [62/100] training 22.7% loss=0.17183, acc=0.92188
# [62/100] training 22.8% loss=0.14496, acc=0.95312
# [62/100] training 23.0% loss=0.10685, acc=0.95312
# [62/100] training 23.1% loss=0.33682, acc=0.89062
# [62/100] training 23.4% loss=0.15285, acc=0.92188
# [62/100] training 23.6% loss=0.16119, acc=0.93750
# [62/100] training 23.7% loss=0.15683, acc=0.95312
# [62/100] training 23.9% loss=0.17873, acc=0.92188
# [62/100] training 24.0% loss=0.09100, acc=0.95312
# [62/100] training 24.2% loss=0.09780, acc=0.96875
# [62/100] training 24.3% loss=0.26828, acc=0.92188
# [62/100] training 24.6% loss=0.12746, acc=0.93750
# [62/100] training 24.7% loss=0.20211, acc=0.93750
# [62/100] training 24.9% loss=0.16026, acc=0.93750
# [62/100] training 25.1% loss=0.17607, acc=0.93750
# [62/100] training 25.2% loss=0.14736, acc=0.90625
# [62/100] training 25.4% loss=0.16221, acc=0.90625
# [62/100] training 25.6% loss=0.10849, acc=0.93750
# [62/100] training 25.8% loss=0.24775, acc=0.95312
# [62/100] training 25.9% loss=0.20685, acc=0.90625
# [62/100] training 26.1% loss=0.14757, acc=0.92188
# [62/100] training 26.3% loss=0.13692, acc=0.93750
# [62/100] training 26.4% loss=0.06150, acc=1.00000
# [62/100] training 26.6% loss=0.05487, acc=0.98438
# [62/100] training 26.8% loss=0.09367, acc=0.93750
# [62/100] training 27.0% loss=0.08674, acc=0.96875
# [62/100] training 27.1% loss=0.12337, acc=0.96875
# [62/100] training 27.3% loss=0.13913, acc=0.95312
# [62/100] training 27.4% loss=0.07681, acc=0.96875
# [62/100] training 27.6% loss=0.18214, acc=0.95312
# [62/100] training 27.9% loss=0.13965, acc=0.92188
# [62/100] training 28.0% loss=0.26327, acc=0.93750
# [62/100] training 28.2% loss=0.12466, acc=0.96875
# [62/100] training 28.3% loss=0.08513, acc=0.98438
# [62/100] training 28.5% loss=0.15359, acc=0.89062
# [62/100] training 28.6% loss=0.19915, acc=0.92188
# [62/100] training 28.8% loss=0.08569, acc=0.95312
# [62/100] training 29.1% loss=0.04916, acc=0.98438
# [62/100] training 29.2% loss=0.11421, acc=0.93750
# [62/100] training 29.4% loss=0.18678, acc=0.92188
# [62/100] training 29.5% loss=0.09159, acc=0.96875
# [62/100] training 29.7% loss=0.20066, acc=0.90625
# [62/100] training 29.8% loss=0.20274, acc=0.96875
# [62/100] training 30.0% loss=0.14707, acc=0.93750
# [62/100] training 30.2% loss=0.06794, acc=0.98438
# [62/100] training 30.4% loss=0.15167, acc=0.95312
# [62/100] training 30.6% loss=0.11332, acc=0.92188
# [62/100] training 30.7% loss=0.17282, acc=0.93750
# [62/100] training 30.9% loss=0.31369, acc=0.93750
# [62/100] training 31.0% loss=0.07774, acc=0.95312
# [62/100] training 31.3% loss=0.16248, acc=0.93750
# [62/100] training 31.4% loss=0.22868, acc=0.90625
# [62/100] training 31.6% loss=0.20374, acc=0.90625
# [62/100] training 31.8% loss=0.11875, acc=0.96875
# [62/100] training 31.9% loss=0.18722, acc=0.92188
# [62/100] training 32.1% loss=0.16178, acc=0.93750
# [62/100] training 32.2% loss=0.31068, acc=0.89062
# [62/100] training 32.5% loss=0.08702, acc=0.96875
# [62/100] training 32.6% loss=0.11753, acc=0.93750
# [62/100] training 32.8% loss=0.12299, acc=0.95312
# [62/100] training 32.9% loss=0.23097, acc=0.92188
# [62/100] training 33.1% loss=0.11162, acc=0.95312
# [62/100] training 33.3% loss=0.20877, acc=0.92188
# [62/100] training 33.4% loss=0.08820, acc=0.98438
# [62/100] training 33.7% loss=0.16810, acc=0.92188
# [62/100] training 33.8% loss=0.13755, acc=0.96875
# [62/100] training 34.0% loss=0.14894, acc=0.92188
# [62/100] training 34.1% loss=0.14400, acc=0.92188
# [62/100] training 34.3% loss=0.20616, acc=0.90625
# [62/100] training 34.5% loss=0.15304, acc=0.90625
# [62/100] training 34.7% loss=0.14670, acc=0.95312
# [62/100] training 34.9% loss=0.12165, acc=0.95312
# [62/100] training 35.0% loss=0.14093, acc=0.93750
# [62/100] training 35.2% loss=0.16742, acc=0.93750
# [62/100] training 35.3% loss=0.14700, acc=0.93750
# [62/100] training 35.5% loss=0.09902, acc=0.98438
# [62/100] training 35.6% loss=0.27749, acc=0.89062
# [62/100] training 35.9% loss=0.27761, acc=0.85938
# [62/100] training 36.1% loss=0.24073, acc=0.87500
# [62/100] training 36.2% loss=0.16318, acc=0.93750
# [62/100] training 36.4% loss=0.22310, acc=0.93750
# [62/100] training 36.5% loss=0.19857, acc=0.93750
# [62/100] training 36.7% loss=0.13665, acc=0.90625
# [62/100] training 36.8% loss=0.08124, acc=0.98438
# [62/100] training 37.1% loss=0.13578, acc=0.92188
# [62/100] training 37.3% loss=0.17927, acc=0.92188
# [62/100] training 37.4% loss=0.14616, acc=0.95312
# [62/100] training 37.6% loss=0.06573, acc=1.00000
# [62/100] training 37.7% loss=0.29790, acc=0.92188
# [62/100] training 37.9% loss=0.07588, acc=0.98438
# [62/100] training 38.1% loss=0.23865, acc=0.95312
# [62/100] training 38.3% loss=0.15003, acc=0.93750
# [62/100] training 38.4% loss=0.02487, acc=1.00000
# [62/100] training 38.6% loss=0.07585, acc=0.95312
# [62/100] training 38.8% loss=0.22142, acc=0.92188
# [62/100] training 38.9% loss=0.10781, acc=0.93750
# [62/100] training 39.1% loss=0.24646, acc=0.90625
# [62/100] training 39.3% loss=0.16104, acc=0.93750
# [62/100] training 39.5% loss=0.25613, acc=0.89062
# [62/100] training 39.6% loss=0.15990, acc=0.95312
# [62/100] training 39.8% loss=0.10275, acc=0.93750
# [62/100] training 40.0% loss=0.21555, acc=0.90625
# [62/100] training 40.1% loss=0.21870, acc=0.90625
# [62/100] training 40.4% loss=0.12435, acc=0.95312
# [62/100] training 40.5% loss=0.20056, acc=0.93750
# [62/100] training 40.7% loss=0.24151, acc=0.89062
# [62/100] training 40.8% loss=0.15048, acc=0.95312
# [62/100] training 41.0% loss=0.14363, acc=0.92188
# [62/100] training 41.1% loss=0.32380, acc=0.87500
# [62/100] training 41.3% loss=0.19105, acc=0.95312
# [62/100] training 41.6% loss=0.25394, acc=0.85938
# [62/100] training 41.7% loss=0.17214, acc=0.93750
# [62/100] training 41.9% loss=0.11898, acc=0.96875
# [62/100] training 42.0% loss=0.19823, acc=0.93750
# [62/100] training 42.2% loss=0.20035, acc=0.92188
# [62/100] training 42.3% loss=0.08710, acc=0.95312
# [62/100] training 42.5% loss=0.14719, acc=0.93750
# [62/100] training 42.8% loss=0.19656, acc=0.92188
# [62/100] training 42.9% loss=0.17280, acc=0.95312
# [62/100] training 43.1% loss=0.18037, acc=0.93750
# [62/100] training 43.2% loss=0.16946, acc=0.93750
# [62/100] training 43.4% loss=0.20808, acc=0.90625
# [62/100] training 43.5% loss=0.13506, acc=0.92188
# [62/100] training 43.8% loss=0.12713, acc=0.95312
# [62/100] training 43.9% loss=0.13866, acc=0.95312
# [62/100] training 44.1% loss=0.30503, acc=0.90625
# [62/100] training 44.3% loss=0.13279, acc=0.95312
# [62/100] training 44.4% loss=0.15977, acc=0.93750
# [62/100] training 44.6% loss=0.17220, acc=0.93750
# [62/100] training 44.7% loss=0.17773, acc=0.90625
# [62/100] training 45.0% loss=0.09557, acc=0.93750
# [62/100] training 45.1% loss=0.19741, acc=0.93750
# [62/100] training 45.3% loss=0.12401, acc=0.95312
# [62/100] training 45.5% loss=0.08533, acc=0.95312
# [62/100] training 45.6% loss=0.17704, acc=0.93750
# [62/100] training 45.8% loss=0.17754, acc=0.90625
# [62/100] training 45.9% loss=0.12472, acc=0.95312
# [62/100] training 46.2% loss=0.05601, acc=0.98438
# [62/100] training 46.3% loss=0.07266, acc=0.96875
# [62/100] training 46.5% loss=0.31103, acc=0.92188
# [62/100] training 46.6% loss=0.17096, acc=0.92188
# [62/100] training 46.8% loss=0.14009, acc=0.93750
# [62/100] training 47.0% loss=0.18590, acc=0.92188
# [62/100] training 47.2% loss=0.18744, acc=0.93750
# [62/100] training 47.4% loss=0.22068, acc=0.95312
# [62/100] training 47.5% loss=0.22022, acc=0.93750
# [62/100] training 47.7% loss=0.12467, acc=0.96875
# [62/100] training 47.8% loss=0.32460, acc=0.85938
# [62/100] training 48.0% loss=0.13091, acc=0.93750
# [62/100] training 48.3% loss=0.10504, acc=0.95312
# [62/100] training 48.4% loss=0.10363, acc=0.95312
# [62/100] training 48.6% loss=0.13832, acc=0.92188
# [62/100] training 48.7% loss=0.16658, acc=0.93750
# [62/100] training 48.9% loss=0.12145, acc=0.95312
# [62/100] training 49.0% loss=0.09927, acc=0.98438
# [62/100] training 49.2% loss=0.15608, acc=0.92188
# [62/100] training 49.3% loss=0.08624, acc=0.95312
# [62/100] training 49.6% loss=0.20451, acc=0.93750
# [62/100] training 49.8% loss=0.21390, acc=0.87500
# [62/100] training 49.9% loss=0.17784, acc=0.90625
# [62/100] training 50.1% loss=0.09354, acc=0.95312
# [62/100] training 50.2% loss=0.13625, acc=0.95312
# [62/100] training 50.4% loss=0.32636, acc=0.87500
# [62/100] training 50.6% loss=0.19495, acc=0.92188
# [62/100] training 50.8% loss=0.26227, acc=0.92188
# [62/100] training 51.0% loss=0.32003, acc=0.87500
# [62/100] training 51.1% loss=0.23154, acc=0.93750
# [62/100] training 51.3% loss=0.15698, acc=0.93750
# [62/100] training 51.4% loss=0.14853, acc=0.93750
# [62/100] training 51.7% loss=0.14961, acc=0.93750
# [62/100] training 51.8% loss=0.20992, acc=0.89062
# [62/100] training 52.0% loss=0.21486, acc=0.92188
# [62/100] training 52.1% loss=0.19572, acc=0.90625
# [62/100] training 52.3% loss=0.24936, acc=0.92188
# [62/100] training 52.5% loss=0.04989, acc=1.00000
# [62/100] training 52.6% loss=0.15451, acc=0.93750
# [62/100] training 52.9% loss=0.20965, acc=0.93750
# [62/100] training 53.0% loss=0.09975, acc=0.95312
# [62/100] training 53.2% loss=0.18270, acc=0.96875
# [62/100] training 53.3% loss=0.15147, acc=0.93750
# [62/100] training 53.5% loss=0.25573, acc=0.89062
# [62/100] training 53.7% loss=0.10778, acc=0.96875
# [62/100] training 53.8% loss=0.19268, acc=0.92188
# [62/100] training 54.1% loss=0.23175, acc=0.89062
# [62/100] training 54.2% loss=0.13769, acc=0.92188
# [62/100] training 54.4% loss=0.08269, acc=0.98438
# [62/100] training 54.5% loss=0.20354, acc=0.93750
# [62/100] training 54.7% loss=0.24130, acc=0.89062
# [62/100] training 54.8% loss=0.15072, acc=0.95312
# [62/100] training 55.1% loss=0.20937, acc=0.90625
# [62/100] training 55.3% loss=0.20674, acc=0.92188
# [62/100] training 55.4% loss=0.12467, acc=0.93750
# [62/100] training 55.6% loss=0.12801, acc=0.93750
# [62/100] training 55.7% loss=0.10046, acc=0.96875
# [62/100] training 55.9% loss=0.13181, acc=0.95312
# [62/100] training 56.0% loss=0.10080, acc=0.95312
# [62/100] training 56.3% loss=0.26181, acc=0.90625
# [62/100] training 56.5% loss=0.10965, acc=0.95312
# [62/100] training 56.6% loss=0.14381, acc=0.90625
# [62/100] training 56.8% loss=0.09439, acc=0.95312
# [62/100] training 56.9% loss=0.10198, acc=0.98438
# [62/100] training 57.1% loss=0.19619, acc=0.92188
# [62/100] training 57.2% loss=0.12960, acc=0.92188
# [62/100] training 57.5% loss=0.12540, acc=0.95312
# [62/100] training 57.6% loss=0.16933, acc=0.90625
# [62/100] training 57.8% loss=0.09029, acc=0.98438
# [62/100] training 58.0% loss=0.12923, acc=0.96875
# [62/100] training 58.1% loss=0.11182, acc=0.96875
# [62/100] training 58.3% loss=0.13370, acc=0.93750
# [62/100] training 58.4% loss=0.26956, acc=0.92188
# [62/100] training 58.7% loss=0.28030, acc=0.93750
# [62/100] training 58.8% loss=0.26559, acc=0.89062
# [62/100] training 59.0% loss=0.07503, acc=0.96875
# [62/100] training 59.2% loss=0.22298, acc=0.90625
# [62/100] training 59.3% loss=0.11869, acc=0.96875
# [62/100] training 59.5% loss=0.12497, acc=0.93750
# [62/100] training 59.7% loss=0.17076, acc=0.92188
# [62/100] training 59.9% loss=0.15794, acc=0.93750
# [62/100] training 60.0% loss=0.18938, acc=0.92188
# [62/100] training 60.2% loss=0.15386, acc=0.90625
# [62/100] training 60.3% loss=0.11232, acc=0.93750
# [62/100] training 60.5% loss=0.15830, acc=0.92188
# [62/100] training 60.8% loss=0.24168, acc=0.92188
# [62/100] training 60.9% loss=0.23669, acc=0.92188
# [62/100] training 61.1% loss=0.25054, acc=0.92188
# [62/100] training 61.2% loss=0.18385, acc=0.90625
# [62/100] training 61.4% loss=0.27189, acc=0.87500
# [62/100] training 61.5% loss=0.30576, acc=0.87500
# [62/100] training 61.7% loss=0.13760, acc=0.90625
# [62/100] training 62.0% loss=0.22388, acc=0.89062
# [62/100] training 62.1% loss=0.25168, acc=0.90625
# [62/100] training 62.3% loss=0.16538, acc=0.93750
# [62/100] training 62.4% loss=0.14945, acc=0.96875
# [62/100] training 62.6% loss=0.18297, acc=0.89062
# [62/100] training 62.7% loss=0.06989, acc=0.98438
# [62/100] training 62.9% loss=0.13487, acc=0.96875
# [62/100] training 63.1% loss=0.12757, acc=0.98438
# [62/100] training 63.3% loss=0.08351, acc=0.98438
# [62/100] training 63.5% loss=0.06615, acc=0.96875
# [62/100] training 63.6% loss=0.17110, acc=0.92188
# [62/100] training 63.8% loss=0.24334, acc=0.90625
# [62/100] training 63.9% loss=0.17475, acc=0.95312
# [62/100] training 64.2% loss=0.07366, acc=0.96875
# [62/100] training 64.3% loss=0.28710, acc=0.89062
# [62/100] training 64.5% loss=0.20876, acc=0.89062
# [62/100] training 64.7% loss=0.18509, acc=0.96875
# [62/100] training 64.8% loss=0.21287, acc=0.90625
# [62/100] training 65.0% loss=0.20380, acc=0.90625
# [62/100] training 65.1% loss=0.19855, acc=0.90625
# [62/100] training 65.4% loss=0.23755, acc=0.93750
# [62/100] training 65.5% loss=0.07259, acc=0.96875
# [62/100] training 65.7% loss=0.12067, acc=0.96875
# [62/100] training 65.8% loss=0.24084, acc=0.90625
# [62/100] training 66.0% loss=0.20619, acc=0.92188
# [62/100] training 66.2% loss=0.08439, acc=0.96875
# [62/100] training 66.3% loss=0.31859, acc=0.89062
# [62/100] training 66.6% loss=0.08324, acc=0.96875
# [62/100] training 66.7% loss=0.14630, acc=0.93750
# [62/100] training 66.9% loss=0.16058, acc=0.90625
# [62/100] training 67.0% loss=0.24550, acc=0.93750
# [62/100] training 67.2% loss=0.11721, acc=0.95312
# [62/100] training 67.4% loss=0.10999, acc=0.96875
# [62/100] training 67.6% loss=0.09945, acc=0.96875
# [62/100] training 67.8% loss=0.21379, acc=0.90625
# [62/100] training 67.9% loss=0.14600, acc=0.95312
# [62/100] training 68.1% loss=0.15234, acc=0.93750
# [62/100] training 68.2% loss=0.10118, acc=0.95312
# [62/100] training 68.4% loss=0.24291, acc=0.90625
# [62/100] training 68.5% loss=0.20893, acc=0.85938
# [62/100] training 68.8% loss=0.12052, acc=0.95312
# [62/100] training 69.0% loss=0.22061, acc=0.89062
# [62/100] training 69.1% loss=0.13217, acc=0.95312
# [62/100] training 69.3% loss=0.17393, acc=0.89062
# [62/100] training 69.4% loss=0.12982, acc=0.93750
# [62/100] training 69.6% loss=0.05994, acc=0.98438
# [62/100] training 69.7% loss=0.28418, acc=0.90625
# [62/100] training 70.0% loss=0.28027, acc=0.92188
# [62/100] training 70.2% loss=0.17759, acc=0.95312
# [62/100] training 70.3% loss=0.18852, acc=0.93750
# [62/100] training 70.5% loss=0.16801, acc=0.92188
# [62/100] training 70.6% loss=0.11814, acc=0.95312
# [62/100] training 70.8% loss=0.33629, acc=0.85938
# [62/100] training 71.0% loss=0.26341, acc=0.87500
# [62/100] training 71.2% loss=0.11476, acc=0.92188
# [62/100] training 71.3% loss=0.10392, acc=0.96875
# [62/100] training 71.5% loss=0.09244, acc=0.95312
# [62/100] training 71.7% loss=0.16829, acc=0.92188
# [62/100] training 71.8% loss=0.18075, acc=0.95312
# [62/100] training 72.0% loss=0.12694, acc=0.95312
# [62/100] training 72.2% loss=0.09740, acc=0.95312
# [62/100] training 72.4% loss=0.13116, acc=0.93750
# [62/100] training 72.5% loss=0.19784, acc=0.90625
# [62/100] training 72.7% loss=0.28731, acc=0.92188
# [62/100] training 72.9% loss=0.11218, acc=0.93750
# [62/100] training 73.0% loss=0.07655, acc=0.95312
# [62/100] training 73.3% loss=0.22278, acc=0.90625
# [62/100] training 73.4% loss=0.11617, acc=0.93750
# [62/100] training 73.6% loss=0.16646, acc=0.93750
# [62/100] training 73.7% loss=0.16898, acc=0.92188
# [62/100] training 73.9% loss=0.11620, acc=0.95312
# [62/100] training 74.0% loss=0.20712, acc=0.90625
# [62/100] training 74.2% loss=0.10455, acc=0.95312
# [62/100] training 74.5% loss=0.10023, acc=0.96875
# [62/100] training 74.6% loss=0.25049, acc=0.90625
# [62/100] training 74.8% loss=0.21512, acc=0.90625
# [62/100] training 74.9% loss=0.18841, acc=0.92188
# [62/100] training 75.1% loss=0.12236, acc=0.95312
# [62/100] training 75.2% loss=0.19592, acc=0.95312
# [62/100] training 75.4% loss=0.10384, acc=0.93750
# [62/100] training 75.7% loss=0.19211, acc=0.90625
# [62/100] training 75.8% loss=0.20425, acc=0.90625
# [62/100] training 76.0% loss=0.17602, acc=0.92188
# [62/100] training 76.1% loss=0.15190, acc=0.93750
# [62/100] training 76.3% loss=0.10897, acc=0.96875
# [62/100] training 76.4% loss=0.24758, acc=0.89062
# [62/100] training 76.7% loss=0.11956, acc=0.95312
# [62/100] training 76.8% loss=0.09416, acc=0.96875
# [62/100] training 77.0% loss=0.07342, acc=0.96875
# [62/100] training 77.2% loss=0.10386, acc=0.93750
# [62/100] training 77.3% loss=0.08111, acc=0.95312
# [62/100] training 77.5% loss=0.17968, acc=0.92188
# [62/100] training 77.6% loss=0.16907, acc=0.92188
# [62/100] training 77.9% loss=0.23964, acc=0.95312
# [62/100] training 78.0% loss=0.11824, acc=0.95312
# [62/100] training 78.2% loss=0.21984, acc=0.90625
# [62/100] training 78.4% loss=0.09001, acc=0.98438
# [62/100] training 78.5% loss=0.13105, acc=0.96875
# [62/100] training 78.7% loss=0.19669, acc=0.92188
# [62/100] training 78.8% loss=0.10199, acc=0.95312
# [62/100] training 79.1% loss=0.09844, acc=0.96875
# [62/100] training 79.2% loss=0.12693, acc=0.96875
# [62/100] training 79.4% loss=0.16049, acc=0.93750
# [62/100] training 79.5% loss=0.19411, acc=0.93750
# [62/100] training 79.7% loss=0.03146, acc=1.00000
# [62/100] training 79.9% loss=0.17427, acc=0.92188
# [62/100] training 80.1% loss=0.11215, acc=0.93750
# [62/100] training 80.3% loss=0.17683, acc=0.92188
# [62/100] training 80.4% loss=0.19249, acc=0.96875
# [62/100] training 80.6% loss=0.25003, acc=0.87500
# [62/100] training 80.7% loss=0.14882, acc=0.93750
# [62/100] training 80.9% loss=0.15630, acc=0.95312
# [62/100] training 81.2% loss=0.20809, acc=0.90625
# [62/100] training 81.3% loss=0.16609, acc=0.89062
# [62/100] training 81.5% loss=0.19587, acc=0.90625
# [62/100] training 81.6% loss=0.24959, acc=0.89062
# [62/100] training 81.8% loss=0.14989, acc=0.92188
# [62/100] training 81.9% loss=0.27798, acc=0.92188
# [62/100] training 82.1% loss=0.10336, acc=0.96875
# [62/100] training 82.2% loss=0.18764, acc=0.92188
# [62/100] training 82.5% loss=0.19963, acc=0.90625
# [62/100] training 82.7% loss=0.24844, acc=0.93750
# [62/100] training 82.8% loss=0.13996, acc=0.93750
# [62/100] training 83.0% loss=0.14472, acc=0.93750
# [62/100] training 83.1% loss=0.13044, acc=0.95312
# [62/100] training 83.3% loss=0.05714, acc=0.98438
# [62/100] training 83.5% loss=0.09551, acc=0.96875
# [62/100] training 83.7% loss=0.34806, acc=0.89062
# [62/100] training 83.9% loss=0.26265, acc=0.89062
# [62/100] training 84.0% loss=0.14410, acc=0.96875
# [62/100] training 84.2% loss=0.05361, acc=1.00000
# [62/100] training 84.3% loss=0.14044, acc=0.92188
# [62/100] training 84.5% loss=0.09368, acc=0.96875
# [62/100] training 84.7% loss=0.15223, acc=0.93750
# [62/100] training 84.9% loss=0.16272, acc=0.92188
# [62/100] training 85.0% loss=0.21082, acc=0.89062
# [62/100] training 85.2% loss=0.14190, acc=0.93750
# [62/100] training 85.4% loss=0.05552, acc=0.98438
# [62/100] training 85.5% loss=0.20699, acc=0.90625
# [62/100] training 85.8% loss=0.18961, acc=0.90625
# [62/100] training 85.9% loss=0.06427, acc=0.95312
# [62/100] training 86.1% loss=0.14787, acc=0.95312
# [62/100] training 86.2% loss=0.09325, acc=0.96875
# [62/100] training 86.4% loss=0.41203, acc=0.85938
# [62/100] training 86.6% loss=0.10944, acc=0.96875
# [62/100] training 86.7% loss=0.10174, acc=0.98438
# [62/100] training 87.0% loss=0.27926, acc=0.85938
# [62/100] training 87.1% loss=0.16398, acc=0.93750
# [62/100] training 87.3% loss=0.13052, acc=0.93750
# [62/100] training 87.4% loss=0.19366, acc=0.90625
# [62/100] training 87.6% loss=0.11128, acc=0.95312
# [62/100] training 87.7% loss=0.21208, acc=0.93750
# [62/100] training 87.9% loss=0.14499, acc=0.93750
# [62/100] training 88.2% loss=0.15261, acc=0.95312
# [62/100] training 88.3% loss=0.26313, acc=0.90625
# [62/100] training 88.5% loss=0.20429, acc=0.90625
# [62/100] training 88.6% loss=0.13102, acc=0.95312
# [62/100] training 88.8% loss=0.08795, acc=0.96875
# [62/100] training 88.9% loss=0.24998, acc=0.89062
# [62/100] training 89.2% loss=0.17967, acc=0.92188
# [62/100] training 89.4% loss=0.11322, acc=0.96875
# [62/100] training 89.5% loss=0.22600, acc=0.92188
# [62/100] training 89.7% loss=0.16719, acc=0.96875
# [62/100] training 89.8% loss=0.09449, acc=0.96875
# [62/100] training 90.0% loss=0.08689, acc=0.98438
# [62/100] training 90.1% loss=0.18258, acc=0.93750
# [62/100] training 90.4% loss=0.21324, acc=0.90625
# [62/100] training 90.5% loss=0.10700, acc=0.96875
# [62/100] training 90.7% loss=0.13127, acc=0.95312
# [62/100] training 90.9% loss=0.09959, acc=0.96875
# [62/100] training 91.0% loss=0.11271, acc=0.96875
# [62/100] training 91.2% loss=0.15168, acc=0.93750
# [62/100] training 91.3% loss=0.30531, acc=0.90625
# [62/100] training 91.6% loss=0.25105, acc=0.92188
# [62/100] training 91.7% loss=0.12555, acc=0.95312
# [62/100] training 91.9% loss=0.27716, acc=0.87500
# [62/100] training 92.1% loss=0.11428, acc=0.96875
# [62/100] training 92.2% loss=0.08031, acc=0.95312
# [62/100] training 92.4% loss=0.19064, acc=0.90625
# [62/100] training 92.6% loss=0.17413, acc=0.90625
# [62/100] training 92.8% loss=0.19150, acc=0.92188
# [62/100] training 92.9% loss=0.29143, acc=0.90625
# [62/100] training 93.1% loss=0.45639, acc=0.84375
# [62/100] training 93.2% loss=0.31763, acc=0.89062
# [62/100] training 93.4% loss=0.24522, acc=0.89062
# [62/100] training 93.7% loss=0.16215, acc=0.92188
# [62/100] training 93.8% loss=0.12890, acc=0.93750
# [62/100] training 94.0% loss=0.16816, acc=0.90625
# [62/100] training 94.1% loss=0.18765, acc=0.89062
# [62/100] training 94.3% loss=0.09218, acc=0.96875
# [62/100] training 94.4% loss=0.27719, acc=0.92188
# [62/100] training 94.6% loss=0.03399, acc=1.00000
# [62/100] training 94.9% loss=0.12946, acc=0.92188
# [62/100] training 95.0% loss=0.22934, acc=0.90625
# [62/100] training 95.2% loss=0.36411, acc=0.87500
# [62/100] training 95.3% loss=0.27153, acc=0.95312
# [62/100] training 95.5% loss=0.13128, acc=0.98438
# [62/100] training 95.6% loss=0.28802, acc=0.93750
# [62/100] training 95.8% loss=0.19400, acc=0.93750
# [62/100] training 96.0% loss=0.25811, acc=0.89062
# [62/100] training 96.2% loss=0.08837, acc=0.98438
# [62/100] training 96.4% loss=0.11031, acc=0.98438
# [62/100] training 96.5% loss=0.13826, acc=0.96875
# [62/100] training 96.7% loss=0.12770, acc=0.93750
# [62/100] training 96.8% loss=0.23261, acc=0.89062
# [62/100] training 97.1% loss=0.15022, acc=0.95312
# [62/100] training 97.2% loss=0.16862, acc=0.93750
# [62/100] training 97.4% loss=0.18183, acc=0.90625
# [62/100] training 97.6% loss=0.17891, acc=0.92188
# [62/100] training 97.7% loss=0.17927, acc=0.92188
# [62/100] training 97.9% loss=0.12544, acc=0.96875
# [62/100] training 98.0% loss=0.14611, acc=0.93750
# [62/100] training 98.3% loss=0.36932, acc=0.84375
# [62/100] training 98.4% loss=0.17978, acc=0.92188
# [62/100] training 98.6% loss=0.25773, acc=0.89062
# [62/100] training 98.7% loss=0.27847, acc=0.89062
# [62/100] training 98.9% loss=0.17429, acc=0.93750
# [62/100] training 99.1% loss=0.20552, acc=0.89062
# [62/100] training 99.2% loss=0.15229, acc=0.93750
# [62/100] training 99.5% loss=0.16337, acc=0.93750
# [62/100] training 99.6% loss=0.27225, acc=0.92188
# [62/100] training 99.8% loss=0.11998, acc=0.95312
# [62/100] training 99.9% loss=0.05740, acc=0.96875
# [62/100] testing 0.9% loss=0.11280, acc=0.96875
# [62/100] testing 1.8% loss=0.29307, acc=0.89062
# [62/100] testing 2.2% loss=0.31199, acc=0.87500
# [62/100] testing 3.1% loss=0.36062, acc=0.87500
# [62/100] testing 3.5% loss=0.11400, acc=0.93750
# [62/100] testing 4.4% loss=0.15950, acc=0.92188
# [62/100] testing 4.8% loss=0.31115, acc=0.85938
# [62/100] testing 5.7% loss=0.21784, acc=0.85938
# [62/100] testing 6.6% loss=0.16447, acc=0.93750
# [62/100] testing 7.0% loss=0.14371, acc=0.92188
# [62/100] testing 7.9% loss=0.25941, acc=0.90625
# [62/100] testing 8.3% loss=0.34238, acc=0.85938
# [62/100] testing 9.2% loss=0.25742, acc=0.87500
# [62/100] testing 9.7% loss=0.11321, acc=0.95312
# [62/100] testing 10.5% loss=0.16459, acc=0.95312
# [62/100] testing 11.0% loss=0.33571, acc=0.85938
# [62/100] testing 11.8% loss=0.14224, acc=0.93750
# [62/100] testing 12.7% loss=0.32490, acc=0.87500
# [62/100] testing 13.2% loss=0.20101, acc=0.93750
# [62/100] testing 14.0% loss=0.31977, acc=0.93750
# [62/100] testing 14.5% loss=0.17373, acc=0.90625
# [62/100] testing 15.4% loss=0.24017, acc=0.90625
# [62/100] testing 15.8% loss=0.13576, acc=0.95312
# [62/100] testing 16.7% loss=0.30801, acc=0.87500
# [62/100] testing 17.5% loss=0.14015, acc=0.92188
# [62/100] testing 18.0% loss=0.20565, acc=0.93750
# [62/100] testing 18.9% loss=0.08487, acc=0.96875
# [62/100] testing 19.3% loss=0.28927, acc=0.90625
# [62/100] testing 20.2% loss=0.28331, acc=0.90625
# [62/100] testing 20.6% loss=0.23504, acc=0.87500
# [62/100] testing 21.5% loss=0.15807, acc=0.92188
# [62/100] testing 21.9% loss=0.42526, acc=0.84375
# [62/100] testing 22.8% loss=0.33116, acc=0.90625
# [62/100] testing 23.7% loss=0.28795, acc=0.89062
# [62/100] testing 24.1% loss=0.17403, acc=0.90625
# [62/100] testing 25.0% loss=0.28597, acc=0.89062
# [62/100] testing 25.4% loss=0.14199, acc=0.93750
# [62/100] testing 26.3% loss=0.26696, acc=0.90625
# [62/100] testing 26.8% loss=0.26989, acc=0.87500
# [62/100] testing 27.6% loss=0.18756, acc=0.93750
# [62/100] testing 28.5% loss=0.21500, acc=0.93750
# [62/100] testing 29.0% loss=0.18194, acc=0.92188
# [62/100] testing 29.8% loss=0.34288, acc=0.90625
# [62/100] testing 30.3% loss=0.21589, acc=0.93750
# [62/100] testing 31.1% loss=0.33382, acc=0.89062
# [62/100] testing 31.6% loss=0.22146, acc=0.93750
# [62/100] testing 32.5% loss=0.25600, acc=0.89062
# [62/100] testing 32.9% loss=0.41366, acc=0.87500
# [62/100] testing 33.8% loss=0.21259, acc=0.93750
# [62/100] testing 34.7% loss=0.33691, acc=0.84375
# [62/100] testing 35.1% loss=0.10550, acc=0.92188
# [62/100] testing 36.0% loss=0.23303, acc=0.93750
# [62/100] testing 36.4% loss=0.33014, acc=0.90625
# [62/100] testing 37.3% loss=0.26010, acc=0.93750
# [62/100] testing 37.7% loss=0.46572, acc=0.85938
# [62/100] testing 38.6% loss=0.22872, acc=0.93750
# [62/100] testing 39.5% loss=0.19529, acc=0.96875
# [62/100] testing 39.9% loss=0.18374, acc=0.92188
# [62/100] testing 40.8% loss=0.25704, acc=0.89062
# [62/100] testing 41.2% loss=0.20096, acc=0.93750
# [62/100] testing 42.1% loss=0.24990, acc=0.89062
# [62/100] testing 42.5% loss=0.09649, acc=0.98438
# [62/100] testing 43.4% loss=0.31905, acc=0.90625
# [62/100] testing 43.9% loss=0.13682, acc=0.93750
# [62/100] testing 44.7% loss=0.20734, acc=0.93750
# [62/100] testing 45.6% loss=0.20619, acc=0.89062
# [62/100] testing 46.1% loss=0.21547, acc=0.87500
# [62/100] testing 46.9% loss=0.20095, acc=0.89062
# [62/100] testing 47.4% loss=0.13384, acc=0.93750
# [62/100] testing 48.3% loss=0.35233, acc=0.89062
# [62/100] testing 48.7% loss=0.24678, acc=0.90625
# [62/100] testing 49.6% loss=0.31102, acc=0.85938
# [62/100] testing 50.4% loss=0.19809, acc=0.92188
# [62/100] testing 50.9% loss=0.28438, acc=0.90625
# [62/100] testing 51.8% loss=0.20492, acc=0.92188
# [62/100] testing 52.2% loss=0.25853, acc=0.87500
# [62/100] testing 53.1% loss=0.16200, acc=0.95312
# [62/100] testing 53.5% loss=0.13067, acc=0.93750
# [62/100] testing 54.4% loss=0.28394, acc=0.85938
# [62/100] testing 54.8% loss=0.26260, acc=0.92188
# [62/100] testing 55.7% loss=0.10770, acc=0.98438
# [62/100] testing 56.6% loss=0.19657, acc=0.90625
# [62/100] testing 57.0% loss=0.29353, acc=0.90625
# [62/100] testing 57.9% loss=0.22955, acc=0.92188
# [62/100] testing 58.3% loss=0.27212, acc=0.90625
# [62/100] testing 59.2% loss=0.24084, acc=0.90625
# [62/100] testing 59.7% loss=0.24121, acc=0.87500
# [62/100] testing 60.5% loss=0.38236, acc=0.84375
# [62/100] testing 61.4% loss=0.09967, acc=0.95312
# [62/100] testing 61.9% loss=0.16202, acc=0.93750
# [62/100] testing 62.7% loss=0.15598, acc=0.95312
# [62/100] testing 63.2% loss=0.32960, acc=0.84375
# [62/100] testing 64.0% loss=0.35314, acc=0.89062
# [62/100] testing 64.5% loss=0.18650, acc=0.89062
# [62/100] testing 65.4% loss=0.19397, acc=0.92188
# [62/100] testing 65.8% loss=0.22607, acc=0.90625
# [62/100] testing 66.7% loss=0.23830, acc=0.89062
# [62/100] testing 67.6% loss=0.27787, acc=0.90625
# [62/100] testing 68.0% loss=0.07795, acc=0.98438
# [62/100] testing 68.9% loss=0.20082, acc=0.92188
# [62/100] testing 69.3% loss=0.19091, acc=0.89062
# [62/100] testing 70.2% loss=0.35890, acc=0.82812
# [62/100] testing 70.6% loss=0.25055, acc=0.92188
# [62/100] testing 71.5% loss=0.25903, acc=0.92188
# [62/100] testing 72.4% loss=0.11215, acc=0.96875
# [62/100] testing 72.8% loss=0.15232, acc=0.93750
# [62/100] testing 73.7% loss=0.14620, acc=0.95312
# [62/100] testing 74.1% loss=0.37249, acc=0.89062
# [62/100] testing 75.0% loss=0.21683, acc=0.87500
# [62/100] testing 75.4% loss=0.41039, acc=0.87500
# [62/100] testing 76.3% loss=0.10454, acc=0.95312
# [62/100] testing 76.8% loss=0.26664, acc=0.90625
# [62/100] testing 77.6% loss=0.16605, acc=0.90625
# [62/100] testing 78.5% loss=0.30228, acc=0.90625
# [62/100] testing 79.0% loss=0.17388, acc=0.93750
# [62/100] testing 79.8% loss=0.22424, acc=0.87500
# [62/100] testing 80.3% loss=0.35184, acc=0.89062
# [62/100] testing 81.2% loss=0.36981, acc=0.89062
# [62/100] testing 81.6% loss=0.15138, acc=0.92188
# [62/100] testing 82.5% loss=0.20750, acc=0.90625
# [62/100] testing 83.3% loss=0.18594, acc=0.93750
# [62/100] testing 83.8% loss=0.20346, acc=0.92188
# [62/100] testing 84.7% loss=0.32382, acc=0.85938
# [62/100] testing 85.1% loss=0.19412, acc=0.87500
# [62/100] testing 86.0% loss=0.25479, acc=0.93750
# [62/100] testing 86.4% loss=0.26631, acc=0.90625
# [62/100] testing 87.3% loss=0.26639, acc=0.84375
# [62/100] testing 87.7% loss=0.23901, acc=0.89062
# [62/100] testing 88.6% loss=0.23122, acc=0.87500
# [62/100] testing 89.5% loss=0.51912, acc=0.85938
# [62/100] testing 89.9% loss=0.15790, acc=0.92188
# [62/100] testing 90.8% loss=0.25223, acc=0.93750
# [62/100] testing 91.2% loss=0.16392, acc=0.92188
# [62/100] testing 92.1% loss=0.35968, acc=0.89062
# [62/100] testing 92.6% loss=0.38841, acc=0.87500
# [62/100] testing 93.4% loss=0.27317, acc=0.89062
# [62/100] testing 94.3% loss=0.15728, acc=0.95312
# [62/100] testing 94.7% loss=0.24936, acc=0.90625
# [62/100] testing 95.6% loss=0.37077, acc=0.82812
# [62/100] testing 96.1% loss=0.17559, acc=0.95312
# [62/100] testing 96.9% loss=0.22806, acc=0.90625
# [62/100] testing 97.4% loss=0.14428, acc=0.95312
# [62/100] testing 98.3% loss=0.14454, acc=0.92188
# [62/100] testing 98.7% loss=0.31900, acc=0.89062
# [62/100] testing 99.6% loss=0.28415, acc=0.89062
# [63/100] training 0.2% loss=0.26469, acc=0.84375
# [63/100] training 0.4% loss=0.24081, acc=0.90625
# [63/100] training 0.5% loss=0.11605, acc=0.96875
# [63/100] training 0.8% loss=0.12741, acc=0.95312
# [63/100] training 0.9% loss=0.10494, acc=0.96875
# [63/100] training 1.1% loss=0.20872, acc=0.95312
# [63/100] training 1.2% loss=0.16173, acc=0.92188
# [63/100] training 1.4% loss=0.07377, acc=0.98438
# [63/100] training 1.6% loss=0.09052, acc=0.98438
# [63/100] training 1.8% loss=0.10068, acc=0.96875
# [63/100] training 2.0% loss=0.21178, acc=0.90625
# [63/100] training 2.1% loss=0.18015, acc=0.89062
# [63/100] training 2.3% loss=0.08949, acc=0.96875
# [63/100] training 2.4% loss=0.09863, acc=0.95312
# [63/100] training 2.6% loss=0.10358, acc=0.93750
# [63/100] training 2.7% loss=0.28390, acc=0.85938
# [63/100] training 3.0% loss=0.29082, acc=0.89062
# [63/100] training 3.2% loss=0.07978, acc=0.98438
# [63/100] training 3.3% loss=0.13051, acc=0.96875
# [63/100] training 3.5% loss=0.11332, acc=0.95312
# [63/100] training 3.6% loss=0.28481, acc=0.87500
# [63/100] training 3.8% loss=0.18645, acc=0.93750
# [63/100] training 3.9% loss=0.10733, acc=0.96875
# [63/100] training 4.2% loss=0.12271, acc=0.96875
# [63/100] training 4.4% loss=0.07669, acc=0.96875
# [63/100] training 4.5% loss=0.17294, acc=0.90625
# [63/100] training 4.7% loss=0.30623, acc=0.92188
# [63/100] training 4.8% loss=0.09907, acc=0.96875
# [63/100] training 5.0% loss=0.04773, acc=0.98438
# [63/100] training 5.2% loss=0.20207, acc=0.92188
# [63/100] training 5.4% loss=0.10115, acc=0.96875
# [63/100] training 5.5% loss=0.23166, acc=0.89062
# [63/100] training 5.7% loss=0.13230, acc=0.95312
# [63/100] training 5.9% loss=0.10263, acc=0.98438
# [63/100] training 6.0% loss=0.32955, acc=0.89062
# [63/100] training 6.3% loss=0.13960, acc=0.93750
# [63/100] training 6.4% loss=0.17210, acc=0.93750
# [63/100] training 6.6% loss=0.10126, acc=0.98438
# [63/100] training 6.7% loss=0.22273, acc=0.90625
# [63/100] training 6.9% loss=0.18243, acc=0.90625
# [63/100] training 7.1% loss=0.12315, acc=0.96875
# [63/100] training 7.2% loss=0.20380, acc=0.90625
# [63/100] training 7.5% loss=0.11599, acc=0.93750
# [63/100] training 7.6% loss=0.27785, acc=0.92188
# [63/100] training 7.8% loss=0.07652, acc=0.96875
# [63/100] training 7.9% loss=0.14912, acc=0.93750
# [63/100] training 8.1% loss=0.06944, acc=1.00000
# [63/100] training 8.2% loss=0.22690, acc=0.93750
# [63/100] training 8.4% loss=0.28365, acc=0.89062
# [63/100] training 8.7% loss=0.16309, acc=0.95312
# [63/100] training 8.8% loss=0.14244, acc=0.93750
# [63/100] training 9.0% loss=0.17561, acc=0.90625
# [63/100] training 9.1% loss=0.19794, acc=0.93750
# [63/100] training 9.3% loss=0.28246, acc=0.89062
# [63/100] training 9.4% loss=0.09730, acc=0.96875
# [63/100] training 9.7% loss=0.14236, acc=0.95312
# [63/100] training 9.9% loss=0.15834, acc=0.93750
# [63/100] training 10.0% loss=0.27878, acc=0.89062
# [63/100] training 10.2% loss=0.12883, acc=0.93750
# [63/100] training 10.3% loss=0.18597, acc=0.90625
# [63/100] training 10.5% loss=0.24561, acc=0.90625
# [63/100] training 10.6% loss=0.14297, acc=0.92188
# [63/100] training 10.9% loss=0.19700, acc=0.90625
# [63/100] training 11.0% loss=0.19957, acc=0.93750
# [63/100] training 11.2% loss=0.10286, acc=0.95312
# [63/100] training 11.4% loss=0.15511, acc=0.90625
# [63/100] training 11.5% loss=0.21605, acc=0.90625
# [63/100] training 11.7% loss=0.09836, acc=0.96875
# [63/100] training 11.8% loss=0.23574, acc=0.90625
# [63/100] training 12.1% loss=0.15054, acc=0.93750
# [63/100] training 12.2% loss=0.03146, acc=1.00000
# [63/100] training 12.4% loss=0.14716, acc=0.95312
# [63/100] training 12.6% loss=0.25247, acc=0.93750
# [63/100] training 12.7% loss=0.12926, acc=0.92188
# [63/100] training 12.9% loss=0.26762, acc=0.87500
# [63/100] training 13.0% loss=0.17488, acc=0.93750
# [63/100] training 13.3% loss=0.15675, acc=0.90625
# [63/100] training 13.4% loss=0.17880, acc=0.93750
# [63/100] training 13.6% loss=0.10311, acc=0.95312
# [63/100] training 13.7% loss=0.25026, acc=0.89062
# [63/100] training 13.9% loss=0.25894, acc=0.92188
# [63/100] training 14.1% loss=0.21588, acc=0.93750
# [63/100] training 14.3% loss=0.26656, acc=0.89062
# [63/100] training 14.5% loss=0.19982, acc=0.90625
# [63/100] training 14.6% loss=0.25715, acc=0.92188
# [63/100] training 14.8% loss=0.24474, acc=0.92188
# [63/100] training 14.9% loss=0.13334, acc=0.95312
# [63/100] training 15.1% loss=0.27717, acc=0.87500
# [63/100] training 15.4% loss=0.20023, acc=0.90625
# [63/100] training 15.5% loss=0.14372, acc=0.96875
# [63/100] training 15.7% loss=0.29284, acc=0.90625
# [63/100] training 15.8% loss=0.09349, acc=0.95312
# [63/100] training 16.0% loss=0.22233, acc=0.85938
# [63/100] training 16.1% loss=0.19494, acc=0.92188
# [63/100] training 16.3% loss=0.19377, acc=0.93750
# [63/100] training 16.4% loss=0.09885, acc=0.96875
# [63/100] training 16.7% loss=0.18013, acc=0.90625
# [63/100] training 16.9% loss=0.17727, acc=0.92188
# [63/100] training 17.0% loss=0.13279, acc=0.95312
# [63/100] training 17.2% loss=0.14208, acc=0.93750
# [63/100] training 17.3% loss=0.18557, acc=0.92188
# [63/100] training 17.5% loss=0.14028, acc=0.93750
# [63/100] training 17.7% loss=0.12580, acc=0.93750
# [63/100] training 17.9% loss=0.22431, acc=0.90625
# [63/100] training 18.1% loss=0.20801, acc=0.89062
# [63/100] training 18.2% loss=0.15665, acc=0.95312
# [63/100] training 18.4% loss=0.22172, acc=0.87500
# [63/100] training 18.5% loss=0.26792, acc=0.84375
# [63/100] training 18.8% loss=0.16935, acc=0.92188
# [63/100] training 18.9% loss=0.06984, acc=1.00000
# [63/100] training 19.1% loss=0.21966, acc=0.89062
# [63/100] training 19.2% loss=0.05631, acc=1.00000
# [63/100] training 19.4% loss=0.12885, acc=0.96875
# [63/100] training 19.6% loss=0.21701, acc=0.92188
# [63/100] training 19.7% loss=0.21686, acc=0.90625
# [63/100] training 20.0% loss=0.11722, acc=0.93750
# [63/100] training 20.1% loss=0.22704, acc=0.89062
# [63/100] training 20.3% loss=0.21501, acc=0.89062
# [63/100] training 20.4% loss=0.15969, acc=0.93750
# [63/100] training 20.6% loss=0.33554, acc=0.89062
# [63/100] training 20.8% loss=0.14007, acc=0.93750
# [63/100] training 20.9% loss=0.25501, acc=0.92188
# [63/100] training 21.2% loss=0.18729, acc=0.95312
# [63/100] training 21.3% loss=0.22426, acc=0.93750
# [63/100] training 21.5% loss=0.18860, acc=0.95312
# [63/100] training 21.6% loss=0.09501, acc=0.96875
# [63/100] training 21.8% loss=0.23136, acc=0.87500
# [63/100] training 21.9% loss=0.18490, acc=0.93750
# [63/100] training 22.2% loss=0.21369, acc=0.90625
# [63/100] training 22.4% loss=0.17295, acc=0.93750
# [63/100] training 22.5% loss=0.06636, acc=0.98438
# [63/100] training 22.7% loss=0.18045, acc=0.90625
# [63/100] training 22.8% loss=0.21049, acc=0.95312
# [63/100] training 23.0% loss=0.08994, acc=0.96875
# [63/100] training 23.1% loss=0.28236, acc=0.89062
# [63/100] training 23.4% loss=0.12762, acc=0.93750
# [63/100] training 23.6% loss=0.25685, acc=0.90625
# [63/100] training 23.7% loss=0.17124, acc=0.90625
# [63/100] training 23.9% loss=0.14047, acc=0.95312
# [63/100] training 24.0% loss=0.11341, acc=0.95312
# [63/100] training 24.2% loss=0.12580, acc=0.95312
# [63/100] training 24.3% loss=0.16203, acc=0.95312
# [63/100] training 24.6% loss=0.28185, acc=0.89062
# [63/100] training 24.7% loss=0.28018, acc=0.90625
# [63/100] training 24.9% loss=0.15324, acc=0.92188
# [63/100] training 25.1% loss=0.16482, acc=0.93750
# [63/100] training 25.2% loss=0.16708, acc=0.95312
# [63/100] training 25.4% loss=0.14110, acc=0.90625
# [63/100] training 25.6% loss=0.19612, acc=0.96875
# [63/100] training 25.8% loss=0.22023, acc=0.90625
# [63/100] training 25.9% loss=0.13408, acc=0.93750
# [63/100] training 26.1% loss=0.16041, acc=0.93750
# [63/100] training 26.3% loss=0.12299, acc=0.93750
# [63/100] training 26.4% loss=0.18623, acc=0.90625
# [63/100] training 26.6% loss=0.15428, acc=0.95312
# [63/100] training 26.8% loss=0.04883, acc=0.98438
# [63/100] training 27.0% loss=0.11063, acc=0.98438
# [63/100] training 27.1% loss=0.13984, acc=0.93750
# [63/100] training 27.3% loss=0.14528, acc=0.96875
# [63/100] training 27.4% loss=0.13606, acc=0.95312
# [63/100] training 27.6% loss=0.20931, acc=0.92188
# [63/100] training 27.9% loss=0.09555, acc=0.96875
# [63/100] training 28.0% loss=0.22652, acc=0.92188
# [63/100] training 28.2% loss=0.11718, acc=0.96875
# [63/100] training 28.3% loss=0.07988, acc=0.96875
# [63/100] training 28.5% loss=0.20982, acc=0.93750
# [63/100] training 28.6% loss=0.14959, acc=0.95312
# [63/100] training 28.8% loss=0.09776, acc=0.95312
# [63/100] training 29.1% loss=0.08871, acc=1.00000
# [63/100] training 29.2% loss=0.09203, acc=0.98438
# [63/100] training 29.4% loss=0.21933, acc=0.89062
# [63/100] training 29.5% loss=0.04499, acc=0.98438
# [63/100] training 29.7% loss=0.16009, acc=0.92188
# [63/100] training 29.8% loss=0.24291, acc=0.93750
# [63/100] training 30.0% loss=0.23713, acc=0.89062
# [63/100] training 30.2% loss=0.09352, acc=0.96875
# [63/100] training 30.4% loss=0.08516, acc=0.98438
# [63/100] training 30.6% loss=0.13948, acc=0.93750
# [63/100] training 30.7% loss=0.13397, acc=0.95312
# [63/100] training 30.9% loss=0.28163, acc=0.93750
# [63/100] training 31.0% loss=0.13180, acc=0.95312
# [63/100] training 31.3% loss=0.11146, acc=0.95312
# [63/100] training 31.4% loss=0.25568, acc=0.89062
# [63/100] training 31.6% loss=0.29539, acc=0.92188
# [63/100] training 31.8% loss=0.18568, acc=0.92188
# [63/100] training 31.9% loss=0.16496, acc=0.93750
# [63/100] training 32.1% loss=0.16072, acc=0.90625
# [63/100] training 32.2% loss=0.21411, acc=0.92188
# [63/100] training 32.5% loss=0.10903, acc=0.96875
# [63/100] training 32.6% loss=0.10638, acc=0.96875
# [63/100] training 32.8% loss=0.16838, acc=0.93750
# [63/100] training 32.9% loss=0.17543, acc=0.93750
# [63/100] training 33.1% loss=0.12660, acc=0.93750
# [63/100] training 33.3% loss=0.19133, acc=0.92188
# [63/100] training 33.4% loss=0.10465, acc=0.96875
# [63/100] training 33.7% loss=0.13672, acc=0.93750
# [63/100] training 33.8% loss=0.12697, acc=0.96875
# [63/100] training 34.0% loss=0.17042, acc=0.93750
# [63/100] training 34.1% loss=0.16364, acc=0.93750
# [63/100] training 34.3% loss=0.11655, acc=0.96875
# [63/100] training 34.5% loss=0.16874, acc=0.92188
# [63/100] training 34.7% loss=0.11292, acc=0.95312
# [63/100] training 34.9% loss=0.13303, acc=0.90625
# [63/100] training 35.0% loss=0.07858, acc=0.98438
# [63/100] training 35.2% loss=0.20147, acc=0.93750
# [63/100] training 35.3% loss=0.26073, acc=0.90625
# [63/100] training 35.5% loss=0.16089, acc=0.92188
# [63/100] training 35.6% loss=0.29115, acc=0.92188
# [63/100] training 35.9% loss=0.12625, acc=0.92188
# [63/100] training 36.1% loss=0.31058, acc=0.84375
# [63/100] training 36.2% loss=0.16558, acc=0.90625
# [63/100] training 36.4% loss=0.17316, acc=0.95312
# [63/100] training 36.5% loss=0.11792, acc=0.98438
# [63/100] training 36.7% loss=0.17485, acc=0.96875
# [63/100] training 36.8% loss=0.10320, acc=0.96875
# [63/100] training 37.1% loss=0.30183, acc=0.87500
# [63/100] training 37.3% loss=0.15151, acc=0.93750
# [63/100] training 37.4% loss=0.10494, acc=0.96875
# [63/100] training 37.6% loss=0.07527, acc=0.96875
# [63/100] training 37.7% loss=0.34625, acc=0.90625
# [63/100] training 37.9% loss=0.04418, acc=0.98438
# [63/100] training 38.1% loss=0.22564, acc=0.92188
# [63/100] training 38.3% loss=0.13835, acc=0.95312
# [63/100] training 38.4% loss=0.09947, acc=0.95312
# [63/100] training 38.6% loss=0.14181, acc=0.92188
# [63/100] training 38.8% loss=0.19308, acc=0.90625
# [63/100] training 38.9% loss=0.16252, acc=0.92188
# [63/100] training 39.1% loss=0.21341, acc=0.89062
# [63/100] training 39.3% loss=0.21288, acc=0.89062
# [63/100] training 39.5% loss=0.16139, acc=0.95312
# [63/100] training 39.6% loss=0.12008, acc=0.95312
# [63/100] training 39.8% loss=0.05232, acc=0.98438
# [63/100] training 40.0% loss=0.16936, acc=0.95312
# [63/100] training 40.1% loss=0.28303, acc=0.87500
# [63/100] training 40.4% loss=0.14311, acc=0.90625
# [63/100] training 40.5% loss=0.24830, acc=0.93750
# [63/100] training 40.7% loss=0.22143, acc=0.90625
# [63/100] training 40.8% loss=0.14481, acc=0.93750
# [63/100] training 41.0% loss=0.18498, acc=0.90625
# [63/100] training 41.1% loss=0.22100, acc=0.87500
# [63/100] training 41.3% loss=0.22972, acc=0.92188
# [63/100] training 41.6% loss=0.28118, acc=0.90625
# [63/100] training 41.7% loss=0.18685, acc=0.92188
# [63/100] training 41.9% loss=0.13209, acc=0.95312
# [63/100] training 42.0% loss=0.19480, acc=0.93750
# [63/100] training 42.2% loss=0.24513, acc=0.90625
# [63/100] training 42.3% loss=0.11968, acc=0.96875
# [63/100] training 42.5% loss=0.15708, acc=0.93750
# [63/100] training 42.8% loss=0.11694, acc=0.93750
# [63/100] training 42.9% loss=0.15117, acc=0.93750
# [63/100] training 43.1% loss=0.18164, acc=0.96875
# [63/100] training 43.2% loss=0.06083, acc=0.96875
# [63/100] training 43.4% loss=0.17282, acc=0.92188
# [63/100] training 43.5% loss=0.21251, acc=0.90625
# [63/100] training 43.8% loss=0.20824, acc=0.89062
# [63/100] training 43.9% loss=0.12092, acc=0.93750
# [63/100] training 44.1% loss=0.32352, acc=0.93750
# [63/100] training 44.3% loss=0.19630, acc=0.93750
# [63/100] training 44.4% loss=0.19118, acc=0.92188
# [63/100] training 44.6% loss=0.19466, acc=0.93750
# [63/100] training 44.7% loss=0.19363, acc=0.90625
# [63/100] training 45.0% loss=0.07642, acc=0.95312
# [63/100] training 45.1% loss=0.22251, acc=0.93750
# [63/100] training 45.3% loss=0.15187, acc=0.95312
# [63/100] training 45.5% loss=0.10384, acc=0.98438
# [63/100] training 45.6% loss=0.24211, acc=0.90625
# [63/100] training 45.8% loss=0.08566, acc=0.98438
# [63/100] training 45.9% loss=0.16702, acc=0.93750
# [63/100] training 46.2% loss=0.05591, acc=1.00000
# [63/100] training 46.3% loss=0.06412, acc=0.98438
# [63/100] training 46.5% loss=0.25017, acc=0.93750
# [63/100] training 46.6% loss=0.13370, acc=0.92188
# [63/100] training 46.8% loss=0.11961, acc=0.95312
# [63/100] training 47.0% loss=0.09645, acc=0.95312
# [63/100] training 47.2% loss=0.13054, acc=0.95312
# [63/100] training 47.4% loss=0.32336, acc=0.87500
# [63/100] training 47.5% loss=0.21421, acc=0.93750
# [63/100] training 47.7% loss=0.08353, acc=0.96875
# [63/100] training 47.8% loss=0.32237, acc=0.93750
# [63/100] training 48.0% loss=0.08581, acc=0.96875
# [63/100] training 48.3% loss=0.10986, acc=0.96875
# [63/100] training 48.4% loss=0.11978, acc=0.96875
# [63/100] training 48.6% loss=0.16908, acc=0.92188
# [63/100] training 48.7% loss=0.14805, acc=0.93750
# [63/100] training 48.9% loss=0.16901, acc=0.95312
# [63/100] training 49.0% loss=0.13615, acc=0.96875
# [63/100] training 49.2% loss=0.14426, acc=0.93750
# [63/100] training 49.3% loss=0.18252, acc=0.93750
# [63/100] training 49.6% loss=0.16775, acc=0.93750
# [63/100] training 49.8% loss=0.21844, acc=0.90625
# [63/100] training 49.9% loss=0.26408, acc=0.87500
# [63/100] training 50.1% loss=0.09146, acc=0.95312
# [63/100] training 50.2% loss=0.15184, acc=0.90625
# [63/100] training 50.4% loss=0.45821, acc=0.85938
# [63/100] training 50.6% loss=0.19371, acc=0.93750
# [63/100] training 50.8% loss=0.28921, acc=0.89062
# [63/100] training 51.0% loss=0.32324, acc=0.92188
# [63/100] training 51.1% loss=0.18497, acc=0.92188
# [63/100] training 51.3% loss=0.30243, acc=0.85938
# [63/100] training 51.4% loss=0.17862, acc=0.93750
# [63/100] training 51.7% loss=0.13949, acc=0.93750
# [63/100] training 51.8% loss=0.25733, acc=0.82812
# [63/100] training 52.0% loss=0.20150, acc=0.93750
# [63/100] training 52.1% loss=0.11028, acc=0.98438
# [63/100] training 52.3% loss=0.12901, acc=0.93750
# [63/100] training 52.5% loss=0.08371, acc=0.96875
# [63/100] training 52.6% loss=0.11978, acc=0.96875
# [63/100] training 52.9% loss=0.31537, acc=0.89062
# [63/100] training 53.0% loss=0.03566, acc=0.98438
# [63/100] training 53.2% loss=0.20037, acc=0.90625
# [63/100] training 53.3% loss=0.08892, acc=0.98438
# [63/100] training 53.5% loss=0.16102, acc=0.92188
# [63/100] training 53.7% loss=0.06807, acc=0.96875
# [63/100] training 53.8% loss=0.27479, acc=0.89062
# [63/100] training 54.1% loss=0.17774, acc=0.92188
# [63/100] training 54.2% loss=0.03302, acc=1.00000
# [63/100] training 54.4% loss=0.26731, acc=0.90625
# [63/100] training 54.5% loss=0.20190, acc=0.90625
# [63/100] training 54.7% loss=0.29483, acc=0.87500
# [63/100] training 54.8% loss=0.11214, acc=0.92188
# [63/100] training 55.1% loss=0.11543, acc=0.93750
# [63/100] training 55.3% loss=0.11362, acc=0.95312
# [63/100] training 55.4% loss=0.06439, acc=0.98438
# [63/100] training 55.6% loss=0.20737, acc=0.93750
# [63/100] training 55.7% loss=0.14346, acc=0.93750
# [63/100] training 55.9% loss=0.14528, acc=0.92188
# [63/100] training 56.0% loss=0.13566, acc=0.93750
# [63/100] training 56.3% loss=0.36909, acc=0.82812
# [63/100] training 56.5% loss=0.13715, acc=0.93750
# [63/100] training 56.6% loss=0.10172, acc=0.95312
# [63/100] training 56.8% loss=0.22341, acc=0.89062
# [63/100] training 56.9% loss=0.21290, acc=0.92188
# [63/100] training 57.1% loss=0.19016, acc=0.92188
# [63/100] training 57.2% loss=0.10570, acc=0.93750
# [63/100] training 57.5% loss=0.14351, acc=0.93750
# [63/100] training 57.6% loss=0.08023, acc=0.96875
# [63/100] training 57.8% loss=0.10169, acc=0.96875
# [63/100] training 58.0% loss=0.14164, acc=0.95312
# [63/100] training 58.1% loss=0.06075, acc=0.96875
# [63/100] training 58.3% loss=0.08680, acc=0.96875
# [63/100] training 58.4% loss=0.17567, acc=0.93750
# [63/100] training 58.7% loss=0.23689, acc=0.89062
# [63/100] training 58.8% loss=0.26756, acc=0.90625
# [63/100] training 59.0% loss=0.08374, acc=0.98438
# [63/100] training 59.2% loss=0.17542, acc=0.93750
# [63/100] training 59.3% loss=0.21021, acc=0.92188
# [63/100] training 59.5% loss=0.25404, acc=0.87500
# [63/100] training 59.7% loss=0.20772, acc=0.90625
# [63/100] training 59.9% loss=0.11266, acc=0.96875
# [63/100] training 60.0% loss=0.21522, acc=0.92188
# [63/100] training 60.2% loss=0.19765, acc=0.90625
# [63/100] training 60.3% loss=0.12248, acc=0.93750
# [63/100] training 60.5% loss=0.36063, acc=0.85938
# [63/100] training 60.8% loss=0.20760, acc=0.90625
# [63/100] training 60.9% loss=0.33440, acc=0.89062
# [63/100] training 61.1% loss=0.25036, acc=0.89062
# [63/100] training 61.2% loss=0.20552, acc=0.93750
# [63/100] training 61.4% loss=0.30409, acc=0.87500
# [63/100] training 61.5% loss=0.38507, acc=0.84375
# [63/100] training 61.7% loss=0.16562, acc=0.93750
# [63/100] training 62.0% loss=0.21981, acc=0.89062
# [63/100] training 62.1% loss=0.45304, acc=0.79688
# [63/100] training 62.3% loss=0.11840, acc=0.93750
# [63/100] training 62.4% loss=0.14317, acc=0.95312
# [63/100] training 62.6% loss=0.21234, acc=0.89062
# [63/100] training 62.7% loss=0.16266, acc=0.93750
# [63/100] training 62.9% loss=0.11794, acc=0.93750
# [63/100] training 63.1% loss=0.25910, acc=0.85938
# [63/100] training 63.3% loss=0.21227, acc=0.89062
# [63/100] training 63.5% loss=0.10923, acc=0.96875
# [63/100] training 63.6% loss=0.16877, acc=0.90625
# [63/100] training 63.8% loss=0.26404, acc=0.92188
# [63/100] training 63.9% loss=0.08921, acc=0.95312
# [63/100] training 64.2% loss=0.08973, acc=0.96875
# [63/100] training 64.3% loss=0.17551, acc=0.95312
# [63/100] training 64.5% loss=0.18991, acc=0.92188
# [63/100] training 64.7% loss=0.20454, acc=0.93750
# [63/100] training 64.8% loss=0.37350, acc=0.84375
# [63/100] training 65.0% loss=0.17099, acc=0.90625
# [63/100] training 65.1% loss=0.13945, acc=0.93750
# [63/100] training 65.4% loss=0.12504, acc=0.93750
# [63/100] training 65.5% loss=0.06829, acc=1.00000
# [63/100] training 65.7% loss=0.19875, acc=0.93750
# [63/100] training 65.8% loss=0.16565, acc=0.89062
# [63/100] training 66.0% loss=0.16453, acc=0.95312
# [63/100] training 66.2% loss=0.11366, acc=0.98438
# [63/100] training 66.3% loss=0.11715, acc=0.95312
# [63/100] training 66.6% loss=0.08621, acc=0.95312
# [63/100] training 66.7% loss=0.08787, acc=0.98438
# [63/100] training 66.9% loss=0.14888, acc=0.90625
# [63/100] training 67.0% loss=0.13877, acc=0.96875
# [63/100] training 67.2% loss=0.11305, acc=0.95312
# [63/100] training 67.4% loss=0.11011, acc=0.98438
# [63/100] training 67.6% loss=0.05883, acc=0.96875
# [63/100] training 67.8% loss=0.14046, acc=0.92188
# [63/100] training 67.9% loss=0.13363, acc=0.95312
# [63/100] training 68.1% loss=0.11203, acc=0.93750
# [63/100] training 68.2% loss=0.07768, acc=0.98438
# [63/100] training 68.4% loss=0.17457, acc=0.92188
# [63/100] training 68.5% loss=0.19656, acc=0.92188
# [63/100] training 68.8% loss=0.15418, acc=0.92188
# [63/100] training 69.0% loss=0.22390, acc=0.95312
# [63/100] training 69.1% loss=0.15893, acc=0.95312
# [63/100] training 69.3% loss=0.16317, acc=0.92188
# [63/100] training 69.4% loss=0.20867, acc=0.92188
# [63/100] training 69.6% loss=0.15262, acc=0.95312
# [63/100] training 69.7% loss=0.19540, acc=0.89062
# [63/100] training 70.0% loss=0.16623, acc=0.93750
# [63/100] training 70.2% loss=0.20404, acc=0.95312
# [63/100] training 70.3% loss=0.21056, acc=0.93750
# [63/100] training 70.5% loss=0.14128, acc=0.92188
# [63/100] training 70.6% loss=0.10363, acc=0.93750
# [63/100] training 70.8% loss=0.22736, acc=0.90625
# [63/100] training 71.0% loss=0.20354, acc=0.92188
# [63/100] training 71.2% loss=0.19116, acc=0.93750
# [63/100] training 71.3% loss=0.07408, acc=0.96875
# [63/100] training 71.5% loss=0.11193, acc=0.95312
# [63/100] training 71.7% loss=0.16656, acc=0.90625
# [63/100] training 71.8% loss=0.17282, acc=0.93750
# [63/100] training 72.0% loss=0.13794, acc=0.96875
# [63/100] training 72.2% loss=0.21769, acc=0.92188
# [63/100] training 72.4% loss=0.24009, acc=0.93750
# [63/100] training 72.5% loss=0.15672, acc=0.92188
# [63/100] training 72.7% loss=0.23442, acc=0.92188
# [63/100] training 72.9% loss=0.06914, acc=0.96875
# [63/100] training 73.0% loss=0.06226, acc=0.96875
# [63/100] training 73.3% loss=0.23044, acc=0.92188
# [63/100] training 73.4% loss=0.19626, acc=0.90625
# [63/100] training 73.6% loss=0.11924, acc=0.93750
# [63/100] training 73.7% loss=0.17691, acc=0.92188
# [63/100] training 73.9% loss=0.10062, acc=0.95312
# [63/100] training 74.0% loss=0.16888, acc=0.95312
# [63/100] training 74.2% loss=0.15261, acc=0.93750
# [63/100] training 74.5% loss=0.11268, acc=0.95312
# [63/100] training 74.6% loss=0.38542, acc=0.85938
# [63/100] training 74.8% loss=0.31898, acc=0.87500
# [63/100] training 74.9% loss=0.21058, acc=0.90625
# [63/100] training 75.1% loss=0.14166, acc=0.95312
# [63/100] training 75.2% loss=0.11922, acc=0.95312
# [63/100] training 75.4% loss=0.16690, acc=0.93750
# [63/100] training 75.7% loss=0.26448, acc=0.87500
# [63/100] training 75.8% loss=0.17691, acc=0.93750
# [63/100] training 76.0% loss=0.18465, acc=0.92188
# [63/100] training 76.1% loss=0.26859, acc=0.89062
# [63/100] training 76.3% loss=0.12234, acc=0.93750
# [63/100] training 76.4% loss=0.30647, acc=0.85938
# [63/100] training 76.7% loss=0.20813, acc=0.92188
# [63/100] training 76.8% loss=0.21124, acc=0.93750
# [63/100] training 77.0% loss=0.07831, acc=0.98438
# [63/100] training 77.2% loss=0.12016, acc=0.93750
# [63/100] training 77.3% loss=0.11824, acc=0.93750
# [63/100] training 77.5% loss=0.16877, acc=0.90625
# [63/100] training 77.6% loss=0.16709, acc=0.95312
# [63/100] training 77.9% loss=0.12937, acc=0.93750
# [63/100] training 78.0% loss=0.15402, acc=0.92188
# [63/100] training 78.2% loss=0.27123, acc=0.89062
# [63/100] training 78.4% loss=0.06010, acc=0.96875
# [63/100] training 78.5% loss=0.16610, acc=0.92188
# [63/100] training 78.7% loss=0.21436, acc=0.92188
# [63/100] training 78.8% loss=0.07828, acc=0.98438
# [63/100] training 79.1% loss=0.12084, acc=0.98438
# [63/100] training 79.2% loss=0.14549, acc=0.96875
# [63/100] training 79.4% loss=0.21076, acc=0.93750
# [63/100] training 79.5% loss=0.11778, acc=0.93750
# [63/100] training 79.7% loss=0.06236, acc=1.00000
# [63/100] training 79.9% loss=0.10251, acc=0.96875
# [63/100] training 80.1% loss=0.07756, acc=0.95312
# [63/100] training 80.3% loss=0.16011, acc=0.90625
# [63/100] training 80.4% loss=0.31423, acc=0.92188
# [63/100] training 80.6% loss=0.20087, acc=0.92188
# [63/100] training 80.7% loss=0.19053, acc=0.92188
# [63/100] training 80.9% loss=0.19622, acc=0.92188
# [63/100] training 81.2% loss=0.21433, acc=0.90625
# [63/100] training 81.3% loss=0.27481, acc=0.87500
# [63/100] training 81.5% loss=0.13188, acc=0.93750
# [63/100] training 81.6% loss=0.22999, acc=0.89062
# [63/100] training 81.8% loss=0.17148, acc=0.90625
# [63/100] training 81.9% loss=0.26055, acc=0.90625
# [63/100] training 82.1% loss=0.11998, acc=0.95312
# [63/100] training 82.2% loss=0.22539, acc=0.90625
# [63/100] training 82.5% loss=0.14851, acc=0.92188
# [63/100] training 82.7% loss=0.24344, acc=0.92188
# [63/100] training 82.8% loss=0.13111, acc=0.93750
# [63/100] training 83.0% loss=0.07762, acc=0.98438
# [63/100] training 83.1% loss=0.15931, acc=0.95312
# [63/100] training 83.3% loss=0.08434, acc=0.96875
# [63/100] training 83.5% loss=0.15225, acc=0.92188
# [63/100] training 83.7% loss=0.48277, acc=0.84375
# [63/100] training 83.9% loss=0.13131, acc=0.95312
# [63/100] training 84.0% loss=0.09397, acc=0.96875
# [63/100] training 84.2% loss=0.10253, acc=0.96875
# [63/100] training 84.3% loss=0.15363, acc=0.90625
# [63/100] training 84.5% loss=0.10170, acc=0.93750
# [63/100] training 84.7% loss=0.09883, acc=0.95312
# [63/100] training 84.9% loss=0.21241, acc=0.89062
# [63/100] training 85.0% loss=0.17147, acc=0.90625
# [63/100] training 85.2% loss=0.12009, acc=0.96875
# [63/100] training 85.4% loss=0.13876, acc=0.95312
# [63/100] training 85.5% loss=0.20809, acc=0.90625
# [63/100] training 85.8% loss=0.11438, acc=0.95312
# [63/100] training 85.9% loss=0.19710, acc=0.90625
# [63/100] training 86.1% loss=0.12310, acc=0.93750
# [63/100] training 86.2% loss=0.11179, acc=0.95312
# [63/100] training 86.4% loss=0.24200, acc=0.89062
# [63/100] training 86.6% loss=0.11009, acc=0.95312
# [63/100] training 86.7% loss=0.09247, acc=0.96875
# [63/100] training 87.0% loss=0.24686, acc=0.90625
# [63/100] training 87.1% loss=0.12510, acc=0.95312
# [63/100] training 87.3% loss=0.12387, acc=0.93750
# [63/100] training 87.4% loss=0.18750, acc=0.92188
# [63/100] training 87.6% loss=0.10926, acc=0.95312
# [63/100] training 87.7% loss=0.13680, acc=0.95312
# [63/100] training 87.9% loss=0.19977, acc=0.95312
# [63/100] training 88.2% loss=0.11257, acc=0.95312
# [63/100] training 88.3% loss=0.22302, acc=0.95312
# [63/100] training 88.5% loss=0.17348, acc=0.92188
# [63/100] training 88.6% loss=0.10913, acc=0.95312
# [63/100] training 88.8% loss=0.16132, acc=0.95312
# [63/100] training 88.9% loss=0.24740, acc=0.90625
# [63/100] training 89.2% loss=0.15380, acc=0.95312
# [63/100] training 89.4% loss=0.06866, acc=0.96875
# [63/100] training 89.5% loss=0.20484, acc=0.90625
# [63/100] training 89.7% loss=0.15058, acc=0.93750
# [63/100] training 89.8% loss=0.09839, acc=0.96875
# [63/100] training 90.0% loss=0.10168, acc=0.96875
# [63/100] training 90.1% loss=0.12321, acc=0.95312
# [63/100] training 90.4% loss=0.20355, acc=0.89062
# [63/100] training 90.5% loss=0.14950, acc=0.92188
# [63/100] training 90.7% loss=0.15017, acc=0.92188
# [63/100] training 90.9% loss=0.07804, acc=0.96875
# [63/100] training 91.0% loss=0.12233, acc=0.92188
# [63/100] training 91.2% loss=0.15395, acc=0.95312
# [63/100] training 91.3% loss=0.34845, acc=0.90625
# [63/100] training 91.6% loss=0.24108, acc=0.90625
# [63/100] training 91.7% loss=0.20218, acc=0.95312
# [63/100] training 91.9% loss=0.20467, acc=0.90625
# [63/100] training 92.1% loss=0.14893, acc=0.93750
# [63/100] training 92.2% loss=0.16370, acc=0.95312
# [63/100] training 92.4% loss=0.11490, acc=0.95312
# [63/100] training 92.6% loss=0.20112, acc=0.90625
# [63/100] training 92.8% loss=0.09859, acc=0.98438
# [63/100] training 92.9% loss=0.23455, acc=0.90625
# [63/100] training 93.1% loss=0.39202, acc=0.87500
# [63/100] training 93.2% loss=0.18335, acc=0.93750
# [63/100] training 93.4% loss=0.30822, acc=0.90625
# [63/100] training 93.7% loss=0.10529, acc=0.95312
# [63/100] training 93.8% loss=0.15700, acc=0.92188
# [63/100] training 94.0% loss=0.10844, acc=0.96875
# [63/100] training 94.1% loss=0.17667, acc=0.92188
# [63/100] training 94.3% loss=0.14190, acc=0.95312
# [63/100] training 94.4% loss=0.23093, acc=0.93750
# [63/100] training 94.6% loss=0.11499, acc=0.93750
# [63/100] training 94.9% loss=0.14089, acc=0.93750
# [63/100] training 95.0% loss=0.13013, acc=0.92188
# [63/100] training 95.2% loss=0.39535, acc=0.87500
# [63/100] training 95.3% loss=0.16378, acc=0.93750
# [63/100] training 95.5% loss=0.14715, acc=0.92188
# [63/100] training 95.6% loss=0.20926, acc=0.92188
# [63/100] training 95.8% loss=0.12610, acc=0.96875
# [63/100] training 96.0% loss=0.18648, acc=0.92188
# [63/100] training 96.2% loss=0.11983, acc=0.96875
# [63/100] training 96.4% loss=0.13668, acc=0.95312
# [63/100] training 96.5% loss=0.13033, acc=0.95312
# [63/100] training 96.7% loss=0.09728, acc=0.96875
# [63/100] training 96.8% loss=0.26176, acc=0.90625
# [63/100] training 97.1% loss=0.09459, acc=0.93750
# [63/100] training 97.2% loss=0.21830, acc=0.90625
# [63/100] training 97.4% loss=0.18681, acc=0.93750
# [63/100] training 97.6% loss=0.21260, acc=0.89062
# [63/100] training 97.7% loss=0.13683, acc=0.92188
# [63/100] training 97.9% loss=0.21396, acc=0.92188
# [63/100] training 98.0% loss=0.16173, acc=0.93750
# [63/100] training 98.3% loss=0.21860, acc=0.87500
# [63/100] training 98.4% loss=0.14853, acc=0.93750
# [63/100] training 98.6% loss=0.28404, acc=0.89062
# [63/100] training 98.7% loss=0.34228, acc=0.87500
# [63/100] training 98.9% loss=0.20444, acc=0.89062
# [63/100] training 99.1% loss=0.16526, acc=0.95312
# [63/100] training 99.2% loss=0.13052, acc=0.96875
# [63/100] training 99.5% loss=0.27099, acc=0.89062
# [63/100] training 99.6% loss=0.20295, acc=0.90625
# [63/100] training 99.8% loss=0.11503, acc=0.96875
# [63/100] training 99.9% loss=0.03042, acc=0.98438
# [63/100] testing 0.9% loss=0.15766, acc=0.92188
# [63/100] testing 1.8% loss=0.29162, acc=0.85938
# [63/100] testing 2.2% loss=0.23104, acc=0.89062
# [63/100] testing 3.1% loss=0.46188, acc=0.84375
# [63/100] testing 3.5% loss=0.15946, acc=0.93750
# [63/100] testing 4.4% loss=0.18720, acc=0.92188
# [63/100] testing 4.8% loss=0.33173, acc=0.84375
# [63/100] testing 5.7% loss=0.14480, acc=0.95312
# [63/100] testing 6.6% loss=0.12800, acc=0.93750
# [63/100] testing 7.0% loss=0.13658, acc=0.90625
# [63/100] testing 7.9% loss=0.24440, acc=0.92188
# [63/100] testing 8.3% loss=0.29980, acc=0.84375
# [63/100] testing 9.2% loss=0.20267, acc=0.93750
# [63/100] testing 9.7% loss=0.14066, acc=0.96875
# [63/100] testing 10.5% loss=0.17109, acc=0.93750
# [63/100] testing 11.0% loss=0.26102, acc=0.89062
# [63/100] testing 11.8% loss=0.15070, acc=0.95312
# [63/100] testing 12.7% loss=0.33379, acc=0.92188
# [63/100] testing 13.2% loss=0.24016, acc=0.93750
# [63/100] testing 14.0% loss=0.39564, acc=0.89062
# [63/100] testing 14.5% loss=0.15696, acc=0.89062
# [63/100] testing 15.4% loss=0.25034, acc=0.92188
# [63/100] testing 15.8% loss=0.14744, acc=0.93750
# [63/100] testing 16.7% loss=0.18205, acc=0.89062
# [63/100] testing 17.5% loss=0.15183, acc=0.93750
# [63/100] testing 18.0% loss=0.28321, acc=0.90625
# [63/100] testing 18.9% loss=0.10646, acc=0.95312
# [63/100] testing 19.3% loss=0.35555, acc=0.87500
# [63/100] testing 20.2% loss=0.19279, acc=0.90625
# [63/100] testing 20.6% loss=0.24354, acc=0.90625
# [63/100] testing 21.5% loss=0.12619, acc=0.93750
# [63/100] testing 21.9% loss=0.43664, acc=0.84375
# [63/100] testing 22.8% loss=0.30657, acc=0.90625
# [63/100] testing 23.7% loss=0.39454, acc=0.84375
# [63/100] testing 24.1% loss=0.19524, acc=0.95312
# [63/100] testing 25.0% loss=0.27769, acc=0.93750
# [63/100] testing 25.4% loss=0.10348, acc=0.95312
# [63/100] testing 26.3% loss=0.38501, acc=0.84375
# [63/100] testing 26.8% loss=0.27065, acc=0.89062
# [63/100] testing 27.6% loss=0.13540, acc=0.95312
# [63/100] testing 28.5% loss=0.22581, acc=0.92188
# [63/100] testing 29.0% loss=0.31048, acc=0.90625
# [63/100] testing 29.8% loss=0.39923, acc=0.93750
# [63/100] testing 30.3% loss=0.30231, acc=0.89062
# [63/100] testing 31.1% loss=0.37882, acc=0.85938
# [63/100] testing 31.6% loss=0.25207, acc=0.93750
# [63/100] testing 32.5% loss=0.23637, acc=0.92188
# [63/100] testing 32.9% loss=0.38492, acc=0.90625
# [63/100] testing 33.8% loss=0.33609, acc=0.89062
# [63/100] testing 34.7% loss=0.38994, acc=0.85938
# [63/100] testing 35.1% loss=0.12857, acc=0.92188
# [63/100] testing 36.0% loss=0.31093, acc=0.90625
# [63/100] testing 36.4% loss=0.14602, acc=0.93750
# [63/100] testing 37.3% loss=0.29176, acc=0.93750
# [63/100] testing 37.7% loss=0.43300, acc=0.81250
# [63/100] testing 38.6% loss=0.20875, acc=0.95312
# [63/100] testing 39.5% loss=0.20737, acc=0.96875
# [63/100] testing 39.9% loss=0.19140, acc=0.93750
# [63/100] testing 40.8% loss=0.30115, acc=0.90625
# [63/100] testing 41.2% loss=0.21975, acc=0.93750
# [63/100] testing 42.1% loss=0.31341, acc=0.85938
# [63/100] testing 42.5% loss=0.19130, acc=0.95312
# [63/100] testing 43.4% loss=0.42847, acc=0.87500
# [63/100] testing 43.9% loss=0.09279, acc=0.96875
# [63/100] testing 44.7% loss=0.26212, acc=0.89062
# [63/100] testing 45.6% loss=0.21301, acc=0.92188
# [63/100] testing 46.1% loss=0.27078, acc=0.85938
# [63/100] testing 46.9% loss=0.24273, acc=0.90625
# [63/100] testing 47.4% loss=0.13625, acc=0.95312
# [63/100] testing 48.3% loss=0.35697, acc=0.89062
# [63/100] testing 48.7% loss=0.29283, acc=0.87500
# [63/100] testing 49.6% loss=0.38059, acc=0.87500
# [63/100] testing 50.4% loss=0.18357, acc=0.95312
# [63/100] testing 50.9% loss=0.37203, acc=0.89062
# [63/100] testing 51.8% loss=0.31006, acc=0.92188
# [63/100] testing 52.2% loss=0.23439, acc=0.89062
# [63/100] testing 53.1% loss=0.14587, acc=0.93750
# [63/100] testing 53.5% loss=0.18632, acc=0.93750
# [63/100] testing 54.4% loss=0.29645, acc=0.87500
# [63/100] testing 54.8% loss=0.32936, acc=0.89062
# [63/100] testing 55.7% loss=0.12429, acc=0.95312
# [63/100] testing 56.6% loss=0.21499, acc=0.90625
# [63/100] testing 57.0% loss=0.34219, acc=0.92188
# [63/100] testing 57.9% loss=0.22292, acc=0.90625
# [63/100] testing 58.3% loss=0.25218, acc=0.90625
# [63/100] testing 59.2% loss=0.25507, acc=0.90625
# [63/100] testing 59.7% loss=0.19052, acc=0.90625
# [63/100] testing 60.5% loss=0.40133, acc=0.84375
# [63/100] testing 61.4% loss=0.10497, acc=0.98438
# [63/100] testing 61.9% loss=0.20154, acc=0.89062
# [63/100] testing 62.7% loss=0.17519, acc=0.93750
# [63/100] testing 63.2% loss=0.26900, acc=0.87500
# [63/100] testing 64.0% loss=0.42329, acc=0.90625
# [63/100] testing 64.5% loss=0.12045, acc=0.92188
# [63/100] testing 65.4% loss=0.14173, acc=0.93750
# [63/100] testing 65.8% loss=0.19696, acc=0.92188
# [63/100] testing 66.7% loss=0.19152, acc=0.92188
# [63/100] testing 67.6% loss=0.39256, acc=0.90625
# [63/100] testing 68.0% loss=0.11485, acc=0.96875
# [63/100] testing 68.9% loss=0.24495, acc=0.93750
# [63/100] testing 69.3% loss=0.23831, acc=0.92188
# [63/100] testing 70.2% loss=0.30723, acc=0.89062
# [63/100] testing 70.6% loss=0.28991, acc=0.89062
# [63/100] testing 71.5% loss=0.24514, acc=0.92188
# [63/100] testing 72.4% loss=0.12289, acc=0.93750
# [63/100] testing 72.8% loss=0.15221, acc=0.93750
# [63/100] testing 73.7% loss=0.18946, acc=0.95312
# [63/100] testing 74.1% loss=0.37270, acc=0.92188
# [63/100] testing 75.0% loss=0.25299, acc=0.89062
# [63/100] testing 75.4% loss=0.43942, acc=0.87500
# [63/100] testing 76.3% loss=0.11208, acc=0.95312
# [63/100] testing 76.8% loss=0.29231, acc=0.89062
# [63/100] testing 77.6% loss=0.20302, acc=0.92188
# [63/100] testing 78.5% loss=0.23157, acc=0.90625
# [63/100] testing 79.0% loss=0.25956, acc=0.92188
# [63/100] testing 79.8% loss=0.26664, acc=0.89062
# [63/100] testing 80.3% loss=0.17620, acc=0.92188
# [63/100] testing 81.2% loss=0.37324, acc=0.89062
# [63/100] testing 81.6% loss=0.16854, acc=0.92188
# [63/100] testing 82.5% loss=0.19191, acc=0.92188
# [63/100] testing 83.3% loss=0.30011, acc=0.92188
# [63/100] testing 83.8% loss=0.26821, acc=0.90625
# [63/100] testing 84.7% loss=0.27598, acc=0.90625
# [63/100] testing 85.1% loss=0.18516, acc=0.92188
# [63/100] testing 86.0% loss=0.31222, acc=0.92188
# [63/100] testing 86.4% loss=0.35461, acc=0.89062
# [63/100] testing 87.3% loss=0.29135, acc=0.89062
# [63/100] testing 87.7% loss=0.24846, acc=0.90625
# [63/100] testing 88.6% loss=0.25901, acc=0.85938
# [63/100] testing 89.5% loss=0.55098, acc=0.82812
# [63/100] testing 89.9% loss=0.21102, acc=0.89062
# [63/100] testing 90.8% loss=0.31419, acc=0.95312
# [63/100] testing 91.2% loss=0.22146, acc=0.93750
# [63/100] testing 92.1% loss=0.27662, acc=0.92188
# [63/100] testing 92.6% loss=0.36700, acc=0.90625
# [63/100] testing 93.4% loss=0.23166, acc=0.90625
# [63/100] testing 94.3% loss=0.10360, acc=0.96875
# [63/100] testing 94.7% loss=0.28527, acc=0.90625
# [63/100] testing 95.6% loss=0.35477, acc=0.85938
# [63/100] testing 96.1% loss=0.15128, acc=0.93750
# [63/100] testing 96.9% loss=0.20605, acc=0.90625
# [63/100] testing 97.4% loss=0.06543, acc=0.98438
# [63/100] testing 98.3% loss=0.20630, acc=0.90625
# [63/100] testing 98.7% loss=0.18976, acc=0.90625
# [63/100] testing 99.6% loss=0.27934, acc=0.89062
# [64/100] training 0.2% loss=0.24148, acc=0.90625
# [64/100] training 0.4% loss=0.35644, acc=0.85938
# [64/100] training 0.5% loss=0.11244, acc=0.98438
# [64/100] training 0.8% loss=0.07170, acc=0.98438
# [64/100] training 0.9% loss=0.17476, acc=0.95312
# [64/100] training 1.1% loss=0.25274, acc=0.95312
# [64/100] training 1.2% loss=0.14962, acc=0.93750
# [64/100] training 1.4% loss=0.10002, acc=0.95312
# [64/100] training 1.6% loss=0.05500, acc=1.00000
# [64/100] training 1.8% loss=0.10848, acc=0.96875
# [64/100] training 2.0% loss=0.12365, acc=0.93750
# [64/100] training 2.1% loss=0.12618, acc=0.92188
# [64/100] training 2.3% loss=0.12859, acc=0.93750
# [64/100] training 2.4% loss=0.19593, acc=0.90625
# [64/100] training 2.6% loss=0.03808, acc=1.00000
# [64/100] training 2.7% loss=0.10510, acc=0.98438
# [64/100] training 3.0% loss=0.13466, acc=0.93750
# [64/100] training 3.2% loss=0.03747, acc=0.98438
# [64/100] training 3.3% loss=0.43427, acc=0.93750
# [64/100] training 3.5% loss=0.17521, acc=0.93750
# [64/100] training 3.6% loss=0.19850, acc=0.92188
# [64/100] training 3.8% loss=0.18366, acc=0.93750
# [64/100] training 3.9% loss=0.17787, acc=0.93750
# [64/100] training 4.2% loss=0.07256, acc=0.98438
# [64/100] training 4.4% loss=0.17383, acc=0.95312
# [64/100] training 4.5% loss=0.11311, acc=0.95312
# [64/100] training 4.7% loss=0.20237, acc=0.93750
# [64/100] training 4.8% loss=0.11523, acc=0.93750
# [64/100] training 5.0% loss=0.06928, acc=0.98438
# [64/100] training 5.2% loss=0.25848, acc=0.89062
# [64/100] training 5.4% loss=0.08740, acc=0.96875
# [64/100] training 5.5% loss=0.21691, acc=0.85938
# [64/100] training 5.7% loss=0.19961, acc=0.92188
# [64/100] training 5.9% loss=0.18692, acc=0.95312
# [64/100] training 6.0% loss=0.20584, acc=0.90625
# [64/100] training 6.3% loss=0.14104, acc=0.96875
# [64/100] training 6.4% loss=0.21235, acc=0.89062
# [64/100] training 6.6% loss=0.16828, acc=0.92188
# [64/100] training 6.7% loss=0.23807, acc=0.89062
# [64/100] training 6.9% loss=0.05572, acc=0.98438
# [64/100] training 7.1% loss=0.09211, acc=0.96875
# [64/100] training 7.2% loss=0.18602, acc=0.93750
# [64/100] training 7.5% loss=0.16321, acc=0.93750
# [64/100] training 7.6% loss=0.21304, acc=0.89062
# [64/100] training 7.8% loss=0.14996, acc=0.92188
# [64/100] training 7.9% loss=0.12418, acc=0.96875
# [64/100] training 8.1% loss=0.07851, acc=0.96875
# [64/100] training 8.2% loss=0.17767, acc=0.90625
# [64/100] training 8.4% loss=0.20099, acc=0.90625
# [64/100] training 8.7% loss=0.12742, acc=0.96875
# [64/100] training 8.8% loss=0.18635, acc=0.92188
# [64/100] training 9.0% loss=0.19411, acc=0.90625
# [64/100] training 9.1% loss=0.21223, acc=0.95312
# [64/100] training 9.3% loss=0.26236, acc=0.92188
# [64/100] training 9.4% loss=0.11019, acc=0.95312
# [64/100] training 9.7% loss=0.11355, acc=0.96875
# [64/100] training 9.9% loss=0.17049, acc=0.93750
# [64/100] training 10.0% loss=0.20332, acc=0.90625
# [64/100] training 10.2% loss=0.09395, acc=0.96875
# [64/100] training 10.3% loss=0.10780, acc=0.95312
# [64/100] training 10.5% loss=0.15007, acc=0.93750
# [64/100] training 10.6% loss=0.25869, acc=0.92188
# [64/100] training 10.9% loss=0.11150, acc=0.93750
# [64/100] training 11.0% loss=0.27893, acc=0.90625
# [64/100] training 11.2% loss=0.08835, acc=0.93750
# [64/100] training 11.4% loss=0.22285, acc=0.90625
# [64/100] training 11.5% loss=0.29087, acc=0.92188
# [64/100] training 11.7% loss=0.07423, acc=0.98438
# [64/100] training 11.8% loss=0.10164, acc=0.98438
# [64/100] training 12.1% loss=0.17863, acc=0.92188
# [64/100] training 12.2% loss=0.08022, acc=0.95312
# [64/100] training 12.4% loss=0.20971, acc=0.89062
# [64/100] training 12.6% loss=0.20053, acc=0.93750
# [64/100] training 12.7% loss=0.08986, acc=0.96875
# [64/100] training 12.9% loss=0.21354, acc=0.90625
# [64/100] training 13.0% loss=0.13198, acc=0.93750
# [64/100] training 13.3% loss=0.13214, acc=0.96875
# [64/100] training 13.4% loss=0.21362, acc=0.87500
# [64/100] training 13.6% loss=0.18324, acc=0.92188
# [64/100] training 13.7% loss=0.15499, acc=0.95312
# [64/100] training 13.9% loss=0.26136, acc=0.90625
# [64/100] training 14.1% loss=0.17093, acc=0.92188
# [64/100] training 14.3% loss=0.19897, acc=0.90625
# [64/100] training 14.5% loss=0.21607, acc=0.95312
# [64/100] training 14.6% loss=0.18777, acc=0.93750
# [64/100] training 14.8% loss=0.29046, acc=0.85938
# [64/100] training 14.9% loss=0.07665, acc=0.96875
# [64/100] training 15.1% loss=0.18496, acc=0.90625
# [64/100] training 15.4% loss=0.24708, acc=0.92188
# [64/100] training 15.5% loss=0.17240, acc=0.96875
# [64/100] training 15.7% loss=0.25298, acc=0.90625
# [64/100] training 15.8% loss=0.18756, acc=0.92188
# [64/100] training 16.0% loss=0.23670, acc=0.85938
# [64/100] training 16.1% loss=0.18906, acc=0.92188
# [64/100] training 16.3% loss=0.15248, acc=0.93750
# [64/100] training 16.4% loss=0.16523, acc=0.89062
# [64/100] training 16.7% loss=0.19937, acc=0.90625
# [64/100] training 16.9% loss=0.18917, acc=0.92188
# [64/100] training 17.0% loss=0.12074, acc=0.96875
# [64/100] training 17.2% loss=0.12968, acc=0.95312
# [64/100] training 17.3% loss=0.10097, acc=0.96875
# [64/100] training 17.5% loss=0.23431, acc=0.87500
# [64/100] training 17.7% loss=0.24172, acc=0.96875
# [64/100] training 17.9% loss=0.25753, acc=0.90625
# [64/100] training 18.1% loss=0.21002, acc=0.92188
# [64/100] training 18.2% loss=0.13060, acc=0.96875
# [64/100] training 18.4% loss=0.23952, acc=0.89062
# [64/100] training 18.5% loss=0.21622, acc=0.92188
# [64/100] training 18.8% loss=0.14663, acc=0.93750
# [64/100] training 18.9% loss=0.13816, acc=0.96875
# [64/100] training 19.1% loss=0.18142, acc=0.90625
# [64/100] training 19.2% loss=0.15251, acc=0.96875
# [64/100] training 19.4% loss=0.17619, acc=0.93750
# [64/100] training 19.6% loss=0.19065, acc=0.90625
# [64/100] training 19.7% loss=0.22578, acc=0.90625
# [64/100] training 20.0% loss=0.14583, acc=0.93750
# [64/100] training 20.1% loss=0.27099, acc=0.92188
# [64/100] training 20.3% loss=0.18840, acc=0.93750
# [64/100] training 20.4% loss=0.21225, acc=0.92188
# [64/100] training 20.6% loss=0.27278, acc=0.92188
# [64/100] training 20.8% loss=0.15472, acc=0.95312
# [64/100] training 20.9% loss=0.10764, acc=0.98438
# [64/100] training 21.2% loss=0.18918, acc=0.96875
# [64/100] training 21.3% loss=0.27225, acc=0.89062
# [64/100] training 21.5% loss=0.19266, acc=0.93750
# [64/100] training 21.6% loss=0.09097, acc=0.96875
# [64/100] training 21.8% loss=0.13127, acc=0.90625
# [64/100] training 21.9% loss=0.27648, acc=0.90625
# [64/100] training 22.2% loss=0.16446, acc=0.92188
# [64/100] training 22.4% loss=0.22658, acc=0.92188
# [64/100] training 22.5% loss=0.17476, acc=0.93750
# [64/100] training 22.7% loss=0.19403, acc=0.95312
# [64/100] training 22.8% loss=0.18599, acc=0.93750
# [64/100] training 23.0% loss=0.09044, acc=0.98438
# [64/100] training 23.1% loss=0.20946, acc=0.92188
# [64/100] training 23.4% loss=0.16632, acc=0.95312
# [64/100] training 23.6% loss=0.24863, acc=0.90625
# [64/100] training 23.7% loss=0.19803, acc=0.93750
# [64/100] training 23.9% loss=0.18187, acc=0.92188
# [64/100] training 24.0% loss=0.13664, acc=0.93750
# [64/100] training 24.2% loss=0.15969, acc=0.92188
# [64/100] training 24.3% loss=0.23782, acc=0.90625
# [64/100] training 24.6% loss=0.15564, acc=0.93750
# [64/100] training 24.7% loss=0.30883, acc=0.85938
# [64/100] training 24.9% loss=0.14317, acc=0.93750
# [64/100] training 25.1% loss=0.18984, acc=0.92188
# [64/100] training 25.2% loss=0.13816, acc=0.92188
# [64/100] training 25.4% loss=0.21307, acc=0.93750
# [64/100] training 25.6% loss=0.16622, acc=0.95312
# [64/100] training 25.8% loss=0.16752, acc=0.92188
# [64/100] training 25.9% loss=0.17760, acc=0.92188
# [64/100] training 26.1% loss=0.11526, acc=0.98438
# [64/100] training 26.3% loss=0.09877, acc=0.95312
# [64/100] training 26.4% loss=0.11211, acc=0.92188
# [64/100] training 26.6% loss=0.04454, acc=0.98438
# [64/100] training 26.8% loss=0.09734, acc=0.95312
# [64/100] training 27.0% loss=0.12802, acc=0.95312
# [64/100] training 27.1% loss=0.17002, acc=0.90625
# [64/100] training 27.3% loss=0.14907, acc=0.93750
# [64/100] training 27.4% loss=0.05775, acc=0.98438
# [64/100] training 27.6% loss=0.29299, acc=0.92188
# [64/100] training 27.9% loss=0.08827, acc=0.95312
# [64/100] training 28.0% loss=0.17374, acc=0.95312
# [64/100] training 28.2% loss=0.21290, acc=0.92188
# [64/100] training 28.3% loss=0.07961, acc=0.98438
# [64/100] training 28.5% loss=0.21129, acc=0.87500
# [64/100] training 28.6% loss=0.16669, acc=0.93750
# [64/100] training 28.8% loss=0.05411, acc=1.00000
# [64/100] training 29.1% loss=0.13452, acc=0.95312
# [64/100] training 29.2% loss=0.11955, acc=0.96875
# [64/100] training 29.4% loss=0.26710, acc=0.89062
# [64/100] training 29.5% loss=0.09811, acc=0.95312
# [64/100] training 29.7% loss=0.16644, acc=0.92188
# [64/100] training 29.8% loss=0.17145, acc=0.93750
# [64/100] training 30.0% loss=0.14042, acc=0.95312
# [64/100] training 30.2% loss=0.10660, acc=0.96875
# [64/100] training 30.4% loss=0.03696, acc=1.00000
# [64/100] training 30.6% loss=0.40213, acc=0.89062
# [64/100] training 30.7% loss=0.14511, acc=0.92188
# [64/100] training 30.9% loss=0.26476, acc=0.96875
# [64/100] training 31.0% loss=0.05305, acc=0.96875
# [64/100] training 31.3% loss=0.10767, acc=0.95312
# [64/100] training 31.4% loss=0.22294, acc=0.89062
# [64/100] training 31.6% loss=0.24207, acc=0.93750
# [64/100] training 31.8% loss=0.11547, acc=0.95312
# [64/100] training 31.9% loss=0.24801, acc=0.92188
# [64/100] training 32.1% loss=0.14556, acc=0.95312
# [64/100] training 32.2% loss=0.24778, acc=0.92188
# [64/100] training 32.5% loss=0.06492, acc=1.00000
# [64/100] training 32.6% loss=0.15128, acc=0.95312
# [64/100] training 32.8% loss=0.10973, acc=0.96875
# [64/100] training 32.9% loss=0.22378, acc=0.93750
# [64/100] training 33.1% loss=0.10456, acc=0.95312
# [64/100] training 33.3% loss=0.18457, acc=0.92188
# [64/100] training 33.4% loss=0.07771, acc=0.96875
# [64/100] training 33.7% loss=0.15967, acc=0.95312
# [64/100] training 33.8% loss=0.14992, acc=0.93750
# [64/100] training 34.0% loss=0.22052, acc=0.93750
# [64/100] training 34.1% loss=0.08961, acc=0.95312
# [64/100] training 34.3% loss=0.12898, acc=0.93750
# [64/100] training 34.5% loss=0.23550, acc=0.90625
# [64/100] training 34.7% loss=0.11393, acc=0.95312
# [64/100] training 34.9% loss=0.10399, acc=0.93750
# [64/100] training 35.0% loss=0.16107, acc=0.95312
# [64/100] training 35.2% loss=0.15274, acc=0.92188
# [64/100] training 35.3% loss=0.19152, acc=0.93750
# [64/100] training 35.5% loss=0.14860, acc=0.95312
# [64/100] training 35.6% loss=0.32779, acc=0.89062
# [64/100] training 35.9% loss=0.15666, acc=0.90625
# [64/100] training 36.1% loss=0.24186, acc=0.89062
# [64/100] training 36.2% loss=0.18588, acc=0.90625
# [64/100] training 36.4% loss=0.28100, acc=0.90625
# [64/100] training 36.5% loss=0.23490, acc=0.93750
# [64/100] training 36.7% loss=0.15776, acc=0.96875
# [64/100] training 36.8% loss=0.08566, acc=0.98438
# [64/100] training 37.1% loss=0.15722, acc=0.95312
# [64/100] training 37.3% loss=0.10713, acc=0.95312
# [64/100] training 37.4% loss=0.14004, acc=0.95312
# [64/100] training 37.6% loss=0.06162, acc=0.98438
# [64/100] training 37.7% loss=0.20518, acc=0.90625
# [64/100] training 37.9% loss=0.11719, acc=0.93750
# [64/100] training 38.1% loss=0.15692, acc=0.92188
# [64/100] training 38.3% loss=0.10950, acc=0.95312
# [64/100] training 38.4% loss=0.02311, acc=1.00000
# [64/100] training 38.6% loss=0.04655, acc=0.98438
# [64/100] training 38.8% loss=0.14113, acc=0.95312
# [64/100] training 38.9% loss=0.09420, acc=0.92188
# [64/100] training 39.1% loss=0.20868, acc=0.93750
# [64/100] training 39.3% loss=0.16046, acc=0.93750
# [64/100] training 39.5% loss=0.25301, acc=0.93750
# [64/100] training 39.6% loss=0.11143, acc=0.95312
# [64/100] training 39.8% loss=0.08315, acc=0.96875
# [64/100] training 40.0% loss=0.25114, acc=0.89062
# [64/100] training 40.1% loss=0.26512, acc=0.87500
# [64/100] training 40.4% loss=0.16642, acc=0.95312
# [64/100] training 40.5% loss=0.23699, acc=0.92188
# [64/100] training 40.7% loss=0.16282, acc=0.95312
# [64/100] training 40.8% loss=0.14655, acc=0.95312
# [64/100] training 41.0% loss=0.16105, acc=0.92188
# [64/100] training 41.1% loss=0.20584, acc=0.93750
# [64/100] training 41.3% loss=0.25122, acc=0.89062
# [64/100] training 41.6% loss=0.17706, acc=0.95312
# [64/100] training 41.7% loss=0.17710, acc=0.90625
# [64/100] training 41.9% loss=0.15615, acc=0.95312
# [64/100] training 42.0% loss=0.26364, acc=0.92188
# [64/100] training 42.2% loss=0.24582, acc=0.93750
# [64/100] training 42.3% loss=0.11156, acc=0.93750
# [64/100] training 42.5% loss=0.08953, acc=0.96875
# [64/100] training 42.8% loss=0.09784, acc=0.96875
# [64/100] training 42.9% loss=0.14993, acc=0.95312
# [64/100] training 43.1% loss=0.23120, acc=0.93750
# [64/100] training 43.2% loss=0.18203, acc=0.92188
# [64/100] training 43.4% loss=0.18075, acc=0.92188
# [64/100] training 43.5% loss=0.22674, acc=0.90625
# [64/100] training 43.8% loss=0.13561, acc=0.96875
# [64/100] training 43.9% loss=0.15544, acc=0.93750
# [64/100] training 44.1% loss=0.21084, acc=0.92188
# [64/100] training 44.3% loss=0.15383, acc=0.95312
# [64/100] training 44.4% loss=0.28979, acc=0.92188
# [64/100] training 44.6% loss=0.15807, acc=0.92188
# [64/100] training 44.7% loss=0.21513, acc=0.90625
# [64/100] training 45.0% loss=0.08434, acc=0.96875
# [64/100] training 45.1% loss=0.21999, acc=0.92188
# [64/100] training 45.3% loss=0.19695, acc=0.90625
# [64/100] training 45.5% loss=0.02414, acc=1.00000
# [64/100] training 45.6% loss=0.22064, acc=0.90625
# [64/100] training 45.8% loss=0.16458, acc=0.92188
# [64/100] training 45.9% loss=0.05055, acc=0.98438
# [64/100] training 46.2% loss=0.03650, acc=1.00000
# [64/100] training 46.3% loss=0.08277, acc=0.96875
# [64/100] training 46.5% loss=0.31375, acc=0.87500
# [64/100] training 46.6% loss=0.13655, acc=0.96875
# [64/100] training 46.8% loss=0.07203, acc=0.95312
# [64/100] training 47.0% loss=0.13935, acc=0.95312
# [64/100] training 47.2% loss=0.17132, acc=0.92188
# [64/100] training 47.4% loss=0.30928, acc=0.92188
# [64/100] training 47.5% loss=0.25380, acc=0.93750
# [64/100] training 47.7% loss=0.07763, acc=0.96875
# [64/100] training 47.8% loss=0.34828, acc=0.93750
# [64/100] training 48.0% loss=0.08268, acc=0.98438
# [64/100] training 48.3% loss=0.13568, acc=0.96875
# [64/100] training 48.4% loss=0.07055, acc=0.96875
# [64/100] training 48.6% loss=0.15733, acc=0.93750
# [64/100] training 48.7% loss=0.22255, acc=0.93750
# [64/100] training 48.9% loss=0.11653, acc=0.93750
# [64/100] training 49.0% loss=0.11752, acc=0.96875
# [64/100] training 49.2% loss=0.11695, acc=0.96875
# [64/100] training 49.3% loss=0.13517, acc=0.93750
# [64/100] training 49.6% loss=0.29085, acc=0.85938
# [64/100] training 49.8% loss=0.21897, acc=0.90625
# [64/100] training 49.9% loss=0.15400, acc=0.92188
# [64/100] training 50.1% loss=0.11549, acc=0.95312
# [64/100] training 50.2% loss=0.13559, acc=0.95312
# [64/100] training 50.4% loss=0.31676, acc=0.92188
# [64/100] training 50.6% loss=0.12498, acc=0.93750
# [64/100] training 50.8% loss=0.27026, acc=0.85938
# [64/100] training 51.0% loss=0.24309, acc=0.90625
# [64/100] training 51.1% loss=0.17782, acc=0.93750
# [64/100] training 51.3% loss=0.31138, acc=0.89062
# [64/100] training 51.4% loss=0.27290, acc=0.93750
# [64/100] training 51.7% loss=0.18530, acc=0.92188
# [64/100] training 51.8% loss=0.19739, acc=0.90625
# [64/100] training 52.0% loss=0.20789, acc=0.95312
# [64/100] training 52.1% loss=0.12457, acc=0.96875
# [64/100] training 52.3% loss=0.23901, acc=0.92188
# [64/100] training 52.5% loss=0.06931, acc=0.98438
# [64/100] training 52.6% loss=0.10184, acc=0.95312
# [64/100] training 52.9% loss=0.23012, acc=0.92188
# [64/100] training 53.0% loss=0.07336, acc=0.98438
# [64/100] training 53.2% loss=0.18469, acc=0.96875
# [64/100] training 53.3% loss=0.15505, acc=0.95312
# [64/100] training 53.5% loss=0.11914, acc=0.96875
# [64/100] training 53.7% loss=0.06894, acc=0.96875
# [64/100] training 53.8% loss=0.20801, acc=0.90625
# [64/100] training 54.1% loss=0.16194, acc=0.90625
# [64/100] training 54.2% loss=0.15990, acc=0.93750
# [64/100] training 54.4% loss=0.17924, acc=0.92188
# [64/100] training 54.5% loss=0.17660, acc=0.92188
# [64/100] training 54.7% loss=0.26742, acc=0.85938
# [64/100] training 54.8% loss=0.09428, acc=0.95312
# [64/100] training 55.1% loss=0.24614, acc=0.90625
# [64/100] training 55.3% loss=0.09567, acc=0.96875
# [64/100] training 55.4% loss=0.18185, acc=0.93750
# [64/100] training 55.6% loss=0.24401, acc=0.92188
# [64/100] training 55.7% loss=0.12256, acc=0.95312
# [64/100] training 55.9% loss=0.17492, acc=0.93750
# [64/100] training 56.0% loss=0.07283, acc=0.98438
# [64/100] training 56.3% loss=0.20391, acc=0.92188
# [64/100] training 56.5% loss=0.10441, acc=0.92188
# [64/100] training 56.6% loss=0.19935, acc=0.90625
# [64/100] training 56.8% loss=0.21571, acc=0.90625
# [64/100] training 56.9% loss=0.17043, acc=0.93750
# [64/100] training 57.1% loss=0.14633, acc=0.93750
# [64/100] training 57.2% loss=0.13834, acc=0.95312
# [64/100] training 57.5% loss=0.10683, acc=0.95312
# [64/100] training 57.6% loss=0.10570, acc=0.96875
# [64/100] training 57.8% loss=0.08622, acc=0.96875
# [64/100] training 58.0% loss=0.10987, acc=0.93750
# [64/100] training 58.1% loss=0.06589, acc=1.00000
# [64/100] training 58.3% loss=0.21093, acc=0.93750
# [64/100] training 58.4% loss=0.12540, acc=0.96875
# [64/100] training 58.7% loss=0.28803, acc=0.90625
# [64/100] training 58.8% loss=0.19947, acc=0.89062
# [64/100] training 59.0% loss=0.09546, acc=0.96875
# [64/100] training 59.2% loss=0.16901, acc=0.93750
# [64/100] training 59.3% loss=0.15106, acc=0.92188
# [64/100] training 59.5% loss=0.17141, acc=0.90625
# [64/100] training 59.7% loss=0.14180, acc=0.92188
# [64/100] training 59.9% loss=0.15909, acc=0.96875
# [64/100] training 60.0% loss=0.15420, acc=0.92188
# [64/100] training 60.2% loss=0.13306, acc=0.93750
# [64/100] training 60.3% loss=0.12611, acc=0.92188
# [64/100] training 60.5% loss=0.24379, acc=0.87500
# [64/100] training 60.8% loss=0.28962, acc=0.90625
# [64/100] training 60.9% loss=0.15217, acc=0.95312
# [64/100] training 61.1% loss=0.24176, acc=0.87500
# [64/100] training 61.2% loss=0.12284, acc=0.93750
# [64/100] training 61.4% loss=0.32860, acc=0.85938
# [64/100] training 61.5% loss=0.25188, acc=0.89062
# [64/100] training 61.7% loss=0.16139, acc=0.93750
# [64/100] training 62.0% loss=0.16300, acc=0.90625
# [64/100] training 62.1% loss=0.27617, acc=0.90625
# [64/100] training 62.3% loss=0.10763, acc=0.95312
# [64/100] training 62.4% loss=0.21339, acc=0.92188
# [64/100] training 62.6% loss=0.23430, acc=0.92188
# [64/100] training 62.7% loss=0.08706, acc=0.96875
# [64/100] training 62.9% loss=0.11537, acc=0.96875
# [64/100] training 63.1% loss=0.40364, acc=0.87500
# [64/100] training 63.3% loss=0.17574, acc=0.90625
# [64/100] training 63.5% loss=0.20016, acc=0.95312
# [64/100] training 63.6% loss=0.27205, acc=0.89062
# [64/100] training 63.8% loss=0.27653, acc=0.89062
# [64/100] training 63.9% loss=0.14634, acc=0.96875
# [64/100] training 64.2% loss=0.15039, acc=0.93750
# [64/100] training 64.3% loss=0.23645, acc=0.92188
# [64/100] training 64.5% loss=0.26092, acc=0.93750
# [64/100] training 64.7% loss=0.22182, acc=0.92188
# [64/100] training 64.8% loss=0.30972, acc=0.92188
# [64/100] training 65.0% loss=0.20607, acc=0.93750
# [64/100] training 65.1% loss=0.31953, acc=0.89062
# [64/100] training 65.4% loss=0.15181, acc=0.93750
# [64/100] training 65.5% loss=0.30030, acc=0.84375
# [64/100] training 65.7% loss=0.12212, acc=0.95312
# [64/100] training 65.8% loss=0.12017, acc=0.96875
# [64/100] training 66.0% loss=0.22459, acc=0.92188
# [64/100] training 66.2% loss=0.04623, acc=0.98438
# [64/100] training 66.3% loss=0.24646, acc=0.92188
# [64/100] training 66.6% loss=0.18322, acc=0.90625
# [64/100] training 66.7% loss=0.11088, acc=0.92188
# [64/100] training 66.9% loss=0.22622, acc=0.89062
# [64/100] training 67.0% loss=0.12558, acc=0.96875
# [64/100] training 67.2% loss=0.15294, acc=0.92188
# [64/100] training 67.4% loss=0.17143, acc=0.95312
# [64/100] training 67.6% loss=0.15448, acc=0.92188
# [64/100] training 67.8% loss=0.12930, acc=0.95312
# [64/100] training 67.9% loss=0.20792, acc=0.90625
# [64/100] training 68.1% loss=0.28255, acc=0.87500
# [64/100] training 68.2% loss=0.11585, acc=0.96875
# [64/100] training 68.4% loss=0.10505, acc=0.95312
# [64/100] training 68.5% loss=0.17896, acc=0.92188
# [64/100] training 68.8% loss=0.13643, acc=0.93750
# [64/100] training 69.0% loss=0.27768, acc=0.90625
# [64/100] training 69.1% loss=0.17844, acc=0.93750
# [64/100] training 69.3% loss=0.18321, acc=0.93750
# [64/100] training 69.4% loss=0.14403, acc=0.92188
# [64/100] training 69.6% loss=0.24872, acc=0.90625
# [64/100] training 69.7% loss=0.20197, acc=0.92188
# [64/100] training 70.0% loss=0.16587, acc=0.93750
# [64/100] training 70.2% loss=0.22994, acc=0.89062
# [64/100] training 70.3% loss=0.11541, acc=0.98438
# [64/100] training 70.5% loss=0.14289, acc=0.93750
# [64/100] training 70.6% loss=0.08781, acc=0.96875
# [64/100] training 70.8% loss=0.20711, acc=0.90625
# [64/100] training 71.0% loss=0.34858, acc=0.87500
# [64/100] training 71.2% loss=0.16478, acc=0.93750
# [64/100] training 71.3% loss=0.13637, acc=0.95312
# [64/100] training 71.5% loss=0.19011, acc=0.93750
# [64/100] training 71.7% loss=0.17124, acc=0.95312
# [64/100] training 71.8% loss=0.20450, acc=0.95312
# [64/100] training 72.0% loss=0.18216, acc=0.96875
# [64/100] training 72.2% loss=0.16365, acc=0.92188
# [64/100] training 72.4% loss=0.28208, acc=0.84375
# [64/100] training 72.5% loss=0.30146, acc=0.90625
# [64/100] training 72.7% loss=0.22454, acc=0.92188
# [64/100] training 72.9% loss=0.09524, acc=0.95312
# [64/100] training 73.0% loss=0.12619, acc=0.95312
# [64/100] training 73.3% loss=0.29697, acc=0.90625
# [64/100] training 73.4% loss=0.13217, acc=0.95312
# [64/100] training 73.6% loss=0.17041, acc=0.93750
# [64/100] training 73.7% loss=0.13784, acc=0.93750
# [64/100] training 73.9% loss=0.13748, acc=0.95312
# [64/100] training 74.0% loss=0.20106, acc=0.90625
# [64/100] training 74.2% loss=0.14897, acc=0.96875
# [64/100] training 74.5% loss=0.15158, acc=0.93750
# [64/100] training 74.6% loss=0.35736, acc=0.85938
# [64/100] training 74.8% loss=0.25800, acc=0.87500
# [64/100] training 74.9% loss=0.19868, acc=0.87500
# [64/100] training 75.1% loss=0.13708, acc=0.95312
# [64/100] training 75.2% loss=0.10928, acc=0.95312
# [64/100] training 75.4% loss=0.21600, acc=0.90625
# [64/100] training 75.7% loss=0.16172, acc=0.90625
# [64/100] training 75.8% loss=0.29337, acc=0.87500
# [64/100] training 76.0% loss=0.16798, acc=0.92188
# [64/100] training 76.1% loss=0.34920, acc=0.85938
# [64/100] training 76.3% loss=0.15702, acc=0.92188
# [64/100] training 76.4% loss=0.21921, acc=0.92188
# [64/100] training 76.7% loss=0.11092, acc=0.96875
# [64/100] training 76.8% loss=0.19081, acc=0.92188
# [64/100] training 77.0% loss=0.15278, acc=0.93750
# [64/100] training 77.2% loss=0.22099, acc=0.85938
# [64/100] training 77.3% loss=0.11363, acc=0.95312
# [64/100] training 77.5% loss=0.14290, acc=0.92188
# [64/100] training 77.6% loss=0.22481, acc=0.90625
# [64/100] training 77.9% loss=0.17100, acc=0.92188
# [64/100] training 78.0% loss=0.15610, acc=0.93750
# [64/100] training 78.2% loss=0.20368, acc=0.92188
# [64/100] training 78.4% loss=0.10255, acc=0.95312
# [64/100] training 78.5% loss=0.19223, acc=0.93750
# [64/100] training 78.7% loss=0.19758, acc=0.90625
# [64/100] training 78.8% loss=0.11897, acc=0.96875
# [64/100] training 79.1% loss=0.11362, acc=0.95312
# [64/100] training 79.2% loss=0.13309, acc=0.95312
# [64/100] training 79.4% loss=0.20695, acc=0.95312
# [64/100] training 79.5% loss=0.20880, acc=0.90625
# [64/100] training 79.7% loss=0.08175, acc=0.95312
# [64/100] training 79.9% loss=0.15484, acc=0.90625
# [64/100] training 80.1% loss=0.09346, acc=0.96875
# [64/100] training 80.3% loss=0.11030, acc=0.96875
# [64/100] training 80.4% loss=0.18561, acc=0.92188
# [64/100] training 80.6% loss=0.26489, acc=0.92188
# [64/100] training 80.7% loss=0.11324, acc=0.96875
# [64/100] training 80.9% loss=0.20212, acc=0.92188
# [64/100] training 81.2% loss=0.21861, acc=0.90625
# [64/100] training 81.3% loss=0.22875, acc=0.89062
# [64/100] training 81.5% loss=0.13989, acc=0.93750
# [64/100] training 81.6% loss=0.33028, acc=0.81250
# [64/100] training 81.8% loss=0.10695, acc=0.95312
# [64/100] training 81.9% loss=0.37448, acc=0.90625
# [64/100] training 82.1% loss=0.07941, acc=0.96875
# [64/100] training 82.2% loss=0.20423, acc=0.89062
# [64/100] training 82.5% loss=0.09819, acc=0.96875
# [64/100] training 82.7% loss=0.25942, acc=0.93750
# [64/100] training 82.8% loss=0.12885, acc=0.96875
# [64/100] training 83.0% loss=0.11536, acc=0.98438
# [64/100] training 83.1% loss=0.22789, acc=0.93750
# [64/100] training 83.3% loss=0.09616, acc=0.98438
# [64/100] training 83.5% loss=0.11991, acc=0.95312
# [64/100] training 83.7% loss=0.29490, acc=0.87500
# [64/100] training 83.9% loss=0.24746, acc=0.90625
# [64/100] training 84.0% loss=0.13800, acc=0.96875
# [64/100] training 84.2% loss=0.07922, acc=0.96875
# [64/100] training 84.3% loss=0.12947, acc=0.95312
# [64/100] training 84.5% loss=0.15437, acc=0.96875
# [64/100] training 84.7% loss=0.17289, acc=0.93750
# [64/100] training 84.9% loss=0.13327, acc=0.96875
# [64/100] training 85.0% loss=0.16484, acc=0.92188
# [64/100] training 85.2% loss=0.13913, acc=0.93750
# [64/100] training 85.4% loss=0.19196, acc=0.90625
# [64/100] training 85.5% loss=0.16475, acc=0.90625
# [64/100] training 85.8% loss=0.23352, acc=0.92188
# [64/100] training 85.9% loss=0.19885, acc=0.90625
# [64/100] training 86.1% loss=0.14108, acc=0.92188
# [64/100] training 86.2% loss=0.09555, acc=0.95312
# [64/100] training 86.4% loss=0.18972, acc=0.92188
# [64/100] training 86.6% loss=0.24779, acc=0.89062
# [64/100] training 86.7% loss=0.18898, acc=0.87500
# [64/100] training 87.0% loss=0.22288, acc=0.89062
# [64/100] training 87.1% loss=0.21048, acc=0.87500
# [64/100] training 87.3% loss=0.08312, acc=0.98438
# [64/100] training 87.4% loss=0.24367, acc=0.90625
# [64/100] training 87.6% loss=0.21560, acc=0.90625
# [64/100] training 87.7% loss=0.36164, acc=0.92188
# [64/100] training 87.9% loss=0.21208, acc=0.90625
# [64/100] training 88.2% loss=0.07831, acc=0.96875
# [64/100] training 88.3% loss=0.33690, acc=0.87500
# [64/100] training 88.5% loss=0.19482, acc=0.89062
# [64/100] training 88.6% loss=0.13493, acc=0.96875
# [64/100] training 88.8% loss=0.19091, acc=0.92188
# [64/100] training 88.9% loss=0.22598, acc=0.92188
# [64/100] training 89.2% loss=0.13613, acc=0.92188
# [64/100] training 89.4% loss=0.23992, acc=0.89062
# [64/100] training 89.5% loss=0.23190, acc=0.90625
# [64/100] training 89.7% loss=0.31752, acc=0.90625
# [64/100] training 89.8% loss=0.14375, acc=0.93750
# [64/100] training 90.0% loss=0.21241, acc=0.87500
# [64/100] training 90.1% loss=0.21344, acc=0.92188
# [64/100] training 90.4% loss=0.22152, acc=0.90625
# [64/100] training 90.5% loss=0.09628, acc=0.98438
# [64/100] training 90.7% loss=0.14625, acc=0.93750
# [64/100] training 90.9% loss=0.19010, acc=0.96875
# [64/100] training 91.0% loss=0.23561, acc=0.93750
# [64/100] training 91.2% loss=0.15124, acc=0.92188
# [64/100] training 91.3% loss=0.21588, acc=0.92188
# [64/100] training 91.6% loss=0.27987, acc=0.89062
# [64/100] training 91.7% loss=0.15772, acc=0.92188
# [64/100] training 91.9% loss=0.36660, acc=0.82812
# [64/100] training 92.1% loss=0.18965, acc=0.90625
# [64/100] training 92.2% loss=0.11795, acc=0.93750
# [64/100] training 92.4% loss=0.21865, acc=0.85938
# [64/100] training 92.6% loss=0.24095, acc=0.90625
# [64/100] training 92.8% loss=0.14266, acc=0.92188
# [64/100] training 92.9% loss=0.18310, acc=0.92188
# [64/100] training 93.1% loss=0.35091, acc=0.85938
# [64/100] training 93.2% loss=0.23289, acc=0.92188
# [64/100] training 93.4% loss=0.17919, acc=0.92188
# [64/100] training 93.7% loss=0.06554, acc=0.98438
# [64/100] training 93.8% loss=0.14279, acc=0.95312
# [64/100] training 94.0% loss=0.11330, acc=0.96875
# [64/100] training 94.1% loss=0.14284, acc=0.95312
# [64/100] training 94.3% loss=0.08740, acc=0.98438
# [64/100] training 94.4% loss=0.17691, acc=0.96875
# [64/100] training 94.6% loss=0.11619, acc=0.93750
# [64/100] training 94.9% loss=0.09283, acc=0.93750
# [64/100] training 95.0% loss=0.24275, acc=0.89062
# [64/100] training 95.2% loss=0.42364, acc=0.85938
# [64/100] training 95.3% loss=0.24134, acc=0.89062
# [64/100] training 95.5% loss=0.08466, acc=0.96875
# [64/100] training 95.6% loss=0.23892, acc=0.87500
# [64/100] training 95.8% loss=0.11772, acc=0.96875
# [64/100] training 96.0% loss=0.19495, acc=0.93750
# [64/100] training 96.2% loss=0.08521, acc=0.98438
# [64/100] training 96.4% loss=0.16911, acc=0.96875
# [64/100] training 96.5% loss=0.13105, acc=0.90625
# [64/100] training 96.7% loss=0.11850, acc=0.95312
# [64/100] training 96.8% loss=0.22551, acc=0.93750
# [64/100] training 97.1% loss=0.22615, acc=0.92188
# [64/100] training 97.2% loss=0.17617, acc=0.93750
# [64/100] training 97.4% loss=0.14492, acc=0.96875
# [64/100] training 97.6% loss=0.27332, acc=0.89062
# [64/100] training 97.7% loss=0.13877, acc=0.96875
# [64/100] training 97.9% loss=0.18388, acc=0.89062
# [64/100] training 98.0% loss=0.15139, acc=0.95312
# [64/100] training 98.3% loss=0.22216, acc=0.89062
# [64/100] training 98.4% loss=0.23185, acc=0.90625
# [64/100] training 98.6% loss=0.33362, acc=0.84375
# [64/100] training 98.7% loss=0.19137, acc=0.95312
# [64/100] training 98.9% loss=0.18365, acc=0.95312
# [64/100] training 99.1% loss=0.13910, acc=0.95312
# [64/100] training 99.2% loss=0.07754, acc=0.96875
# [64/100] training 99.5% loss=0.26475, acc=0.92188
# [64/100] training 99.6% loss=0.19024, acc=0.90625
# [64/100] training 99.8% loss=0.15575, acc=0.90625
# [64/100] training 99.9% loss=0.06098, acc=0.98438
# [64/100] testing 0.9% loss=0.22756, acc=0.93750
# [64/100] testing 1.8% loss=0.45054, acc=0.85938
# [64/100] testing 2.2% loss=0.34411, acc=0.90625
# [64/100] testing 3.1% loss=0.36082, acc=0.85938
# [64/100] testing 3.5% loss=0.12857, acc=0.95312
# [64/100] testing 4.4% loss=0.21093, acc=0.92188
# [64/100] testing 4.8% loss=0.35317, acc=0.85938
# [64/100] testing 5.7% loss=0.30721, acc=0.90625
# [64/100] testing 6.6% loss=0.15379, acc=0.93750
# [64/100] testing 7.0% loss=0.08498, acc=0.95312
# [64/100] testing 7.9% loss=0.32075, acc=0.89062
# [64/100] testing 8.3% loss=0.42320, acc=0.84375
# [64/100] testing 9.2% loss=0.20512, acc=0.92188
# [64/100] testing 9.7% loss=0.06058, acc=0.98438
# [64/100] testing 10.5% loss=0.22918, acc=0.89062
# [64/100] testing 11.0% loss=0.24547, acc=0.90625
# [64/100] testing 11.8% loss=0.14197, acc=0.96875
# [64/100] testing 12.7% loss=0.46628, acc=0.90625
# [64/100] testing 13.2% loss=0.25353, acc=0.89062
# [64/100] testing 14.0% loss=0.54898, acc=0.90625
# [64/100] testing 14.5% loss=0.19016, acc=0.90625
# [64/100] testing 15.4% loss=0.17510, acc=0.92188
# [64/100] testing 15.8% loss=0.17838, acc=0.93750
# [64/100] testing 16.7% loss=0.38968, acc=0.92188
# [64/100] testing 17.5% loss=0.23027, acc=0.90625
# [64/100] testing 18.0% loss=0.31983, acc=0.92188
# [64/100] testing 18.9% loss=0.05881, acc=0.98438
# [64/100] testing 19.3% loss=0.37105, acc=0.89062
# [64/100] testing 20.2% loss=0.30433, acc=0.90625
# [64/100] testing 20.6% loss=0.27122, acc=0.90625
# [64/100] testing 21.5% loss=0.09136, acc=0.96875
# [64/100] testing 21.9% loss=0.49622, acc=0.87500
# [64/100] testing 22.8% loss=0.38850, acc=0.90625
# [64/100] testing 23.7% loss=0.35620, acc=0.89062
# [64/100] testing 24.1% loss=0.18419, acc=0.92188
# [64/100] testing 25.0% loss=0.34785, acc=0.90625
# [64/100] testing 25.4% loss=0.07046, acc=0.98438
# [64/100] testing 26.3% loss=0.32035, acc=0.82812
# [64/100] testing 26.8% loss=0.27590, acc=0.90625
# [64/100] testing 27.6% loss=0.24103, acc=0.89062
# [64/100] testing 28.5% loss=0.14447, acc=0.93750
# [64/100] testing 29.0% loss=0.27656, acc=0.93750
# [64/100] testing 29.8% loss=0.43590, acc=0.92188
# [64/100] testing 30.3% loss=0.32519, acc=0.93750
# [64/100] testing 31.1% loss=0.43270, acc=0.89062
# [64/100] testing 31.6% loss=0.29520, acc=0.95312
# [64/100] testing 32.5% loss=0.27364, acc=0.95312
# [64/100] testing 32.9% loss=0.49542, acc=0.87500
# [64/100] testing 33.8% loss=0.28610, acc=0.84375
# [64/100] testing 34.7% loss=0.27977, acc=0.90625
# [64/100] testing 35.1% loss=0.10985, acc=0.93750
# [64/100] testing 36.0% loss=0.34337, acc=0.92188
# [64/100] testing 36.4% loss=0.21820, acc=0.93750
# [64/100] testing 37.3% loss=0.35865, acc=0.92188
# [64/100] testing 37.7% loss=0.45011, acc=0.87500
# [64/100] testing 38.6% loss=0.30037, acc=0.92188
# [64/100] testing 39.5% loss=0.24036, acc=0.96875
# [64/100] testing 39.9% loss=0.20067, acc=0.93750
# [64/100] testing 40.8% loss=0.24547, acc=0.92188
# [64/100] testing 41.2% loss=0.16701, acc=0.95312
# [64/100] testing 42.1% loss=0.31710, acc=0.89062
# [64/100] testing 42.5% loss=0.17569, acc=0.93750
# [64/100] testing 43.4% loss=0.27466, acc=0.93750
# [64/100] testing 43.9% loss=0.11679, acc=0.95312
# [64/100] testing 44.7% loss=0.24373, acc=0.93750
# [64/100] testing 45.6% loss=0.40532, acc=0.90625
# [64/100] testing 46.1% loss=0.33614, acc=0.87500
# [64/100] testing 46.9% loss=0.23235, acc=0.90625
# [64/100] testing 47.4% loss=0.05112, acc=0.98438
# [64/100] testing 48.3% loss=0.50936, acc=0.85938
# [64/100] testing 48.7% loss=0.32887, acc=0.92188
# [64/100] testing 49.6% loss=0.48867, acc=0.84375
# [64/100] testing 50.4% loss=0.24002, acc=0.93750
# [64/100] testing 50.9% loss=0.35456, acc=0.87500
# [64/100] testing 51.8% loss=0.24855, acc=0.93750
# [64/100] testing 52.2% loss=0.27639, acc=0.90625
# [64/100] testing 53.1% loss=0.20291, acc=0.93750
# [64/100] testing 53.5% loss=0.28321, acc=0.92188
# [64/100] testing 54.4% loss=0.33270, acc=0.85938
# [64/100] testing 54.8% loss=0.37763, acc=0.85938
# [64/100] testing 55.7% loss=0.09102, acc=0.96875
# [64/100] testing 56.6% loss=0.34520, acc=0.85938
# [64/100] testing 57.0% loss=0.33514, acc=0.92188
# [64/100] testing 57.9% loss=0.42172, acc=0.87500
# [64/100] testing 58.3% loss=0.29152, acc=0.89062
# [64/100] testing 59.2% loss=0.19954, acc=0.90625
# [64/100] testing 59.7% loss=0.20167, acc=0.93750
# [64/100] testing 60.5% loss=0.47096, acc=0.89062
# [64/100] testing 61.4% loss=0.14541, acc=0.92188
# [64/100] testing 61.9% loss=0.22867, acc=0.87500
# [64/100] testing 62.7% loss=0.21501, acc=0.93750
# [64/100] testing 63.2% loss=0.36646, acc=0.85938
# [64/100] testing 64.0% loss=0.46604, acc=0.87500
# [64/100] testing 64.5% loss=0.24810, acc=0.90625
# [64/100] testing 65.4% loss=0.08379, acc=0.93750
# [64/100] testing 65.8% loss=0.26564, acc=0.89062
# [64/100] testing 66.7% loss=0.20260, acc=0.92188
# [64/100] testing 67.6% loss=0.43501, acc=0.89062
# [64/100] testing 68.0% loss=0.10345, acc=0.93750
# [64/100] testing 68.9% loss=0.38632, acc=0.90625
# [64/100] testing 69.3% loss=0.23848, acc=0.90625
# [64/100] testing 70.2% loss=0.47704, acc=0.90625
# [64/100] testing 70.6% loss=0.27339, acc=0.92188
# [64/100] testing 71.5% loss=0.39545, acc=0.92188
# [64/100] testing 72.4% loss=0.18930, acc=0.93750
# [64/100] testing 72.8% loss=0.24122, acc=0.92188
# [64/100] testing 73.7% loss=0.09830, acc=0.98438
# [64/100] testing 74.1% loss=0.39749, acc=0.90625
# [64/100] testing 75.0% loss=0.13972, acc=0.90625
# [64/100] testing 75.4% loss=0.69093, acc=0.84375
# [64/100] testing 76.3% loss=0.05677, acc=0.98438
# [64/100] testing 76.8% loss=0.22790, acc=0.93750
# [64/100] testing 77.6% loss=0.24533, acc=0.89062
# [64/100] testing 78.5% loss=0.42893, acc=0.90625
# [64/100] testing 79.0% loss=0.21612, acc=0.93750
# [64/100] testing 79.8% loss=0.22191, acc=0.92188
# [64/100] testing 80.3% loss=0.16690, acc=0.90625
# [64/100] testing 81.2% loss=0.39885, acc=0.85938
# [64/100] testing 81.6% loss=0.27469, acc=0.89062
# [64/100] testing 82.5% loss=0.23177, acc=0.92188
# [64/100] testing 83.3% loss=0.22939, acc=0.95312
# [64/100] testing 83.8% loss=0.18777, acc=0.95312
# [64/100] testing 84.7% loss=0.27816, acc=0.89062
# [64/100] testing 85.1% loss=0.19211, acc=0.92188
# [64/100] testing 86.0% loss=0.30674, acc=0.90625
# [64/100] testing 86.4% loss=0.36470, acc=0.89062
# [64/100] testing 87.3% loss=0.28978, acc=0.89062
# [64/100] testing 87.7% loss=0.28190, acc=0.90625
# [64/100] testing 88.6% loss=0.23912, acc=0.92188
# [64/100] testing 89.5% loss=0.60264, acc=0.84375
# [64/100] testing 89.9% loss=0.16458, acc=0.95312
# [64/100] testing 90.8% loss=0.39357, acc=0.92188
# [64/100] testing 91.2% loss=0.15710, acc=0.90625
# [64/100] testing 92.1% loss=0.49264, acc=0.90625
# [64/100] testing 92.6% loss=0.32831, acc=0.87500
# [64/100] testing 93.4% loss=0.38502, acc=0.87500
# [64/100] testing 94.3% loss=0.12173, acc=0.95312
# [64/100] testing 94.7% loss=0.24061, acc=0.90625
# [64/100] testing 95.6% loss=0.60120, acc=0.84375
# [64/100] testing 96.1% loss=0.13721, acc=0.96875
# [64/100] testing 96.9% loss=0.40155, acc=0.90625
# [64/100] testing 97.4% loss=0.06908, acc=0.96875
# [64/100] testing 98.3% loss=0.17207, acc=0.90625
# [64/100] testing 98.7% loss=0.22485, acc=0.93750
# [64/100] testing 99.6% loss=0.28811, acc=0.92188

